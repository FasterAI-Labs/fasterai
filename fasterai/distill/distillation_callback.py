# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/distill/distillation_callback.ipynb.

# %% ../../nbs/distill/distillation_callback.ipynb #numerous-insulin
from __future__ import annotations
from fastai.vision.all import *

import torch
import torch.nn as nn
import torch.nn.functional as F

from functools import reduce
from typing import Callable, Any
from ..core.schedule import Schedule

# %% auto #0
__all__ = ['KnowledgeDistillationCallback', 'get_model_layers', 'get_module_by_name']

# %% ../../nbs/distill/distillation_callback.ipynb #03eeecbf
class KnowledgeDistillationCallback(Callback):
    def __init__(self, 
                 teacher: nn.Module,                                           # Teacher model
                 loss: Callable,                                               # Distillation loss function
                 activations_student: str | list[str] | None = None,           # Student activation layers to match
                 activations_teacher: str | list[str] | None = None,           # Teacher activation layers to match
                 weight: float = 0.5,                                          # Weight for distillation loss
                 schedule: Schedule | None = None                              # Optional schedule for weight progression
    ):
        "Implement knowledge distillation from a teacher model to the student being trained"
        self.stored_activation_student, self.stored_activation_teacher  = {}, {}
        store_attr()
        if self.activations_student is not None:
            self.activations_student, self.activations_teacher = listify(activations_student), listify(activations_teacher)
        self.current_weight = weight
        
    def before_fit(self) -> None:
        "Setup hooks and prepare teacher before training"
        if self.activations_student and self.activations_teacher: self.register_hooks()
        self.teacher.eval()

    def before_batch(self) -> None:
        "Update distillation weight if scheduled"
        if self.schedule is not None:
            progress = self.schedule.progress(self.pct_train)
            self.current_weight = self.weight * progress

    def after_batch(self) -> None:
        "Clear activations after each batch to prevent memory buildup"
        self.stored_activation_student.clear()
        self.stored_activation_teacher.clear()

    def after_loss(self) -> None:
        "Apply distillation loss using teacher predictions"
        teacher_pred = self.teacher(self.x)
        new_loss = self.loss(pred=self.pred, teacher_pred=teacher_pred, fm_s=self.stored_activation_student, fm_t=self.stored_activation_teacher)
        self.learn.loss_grad = torch.lerp(self.learn.loss_grad, new_loss, self.current_weight)
        self.learn.loss = self.learn.loss_grad.clone()
    
    def register_hooks(self) -> None:
        "Set up forward hooks to capture activations"
        self.handles_st, self.handles_t = {}, {}
        for name_st, name_t in zip(self.activations_student, self.activations_teacher):
            self.handles_st[name_st] = get_module_by_name(self.learn, name_st).register_forward_hook(self.get_activation(self.stored_activation_student, name_st))
            self.handles_t[name_t] = get_module_by_name(self.teacher, name_t).register_forward_hook(self.get_activation(self.stored_activation_teacher, name_t))
        
    def get_activation(self, 
                       activation: dict[str, torch.Tensor],  # Dictionary to store activations
                       name: str                             # Name of the layer
    ) -> Callable:
        "Create a hook function to store activations"
        def hook(model, input, output):
            activation[name] = output
        return hook
    
    def find_hook(self, 
                  m: nn.Module
    ) -> list[tuple[str, int, str]]:
        "Find all hooks registered in a module"
        save = []
        module_name = type(m).__name__
        for k, v in m._forward_hooks.items():
            save.append((module_name, k, v.__name__))
        return save
    
    def remove_hooks(self, 
                     handles: dict[str, Any]
    ) -> None:
        "Remove all registered hooks"
        for handle in handles.values():
            handle.remove()
    
    def after_fit(self) -> None:
        "Clean up hooks after training"
        if self.activations_student and self.activations_teacher:
            self.remove_hooks(self.handles_t)
            self.remove_hooks(self.handles_st)

# %% ../../nbs/distill/distillation_callback.ipynb #3378f3af
def get_model_layers(
    model: nn.Module,             # Model to inspect
    getLayerRepr: bool = False    # Whether to return layer representations
) -> list[str] | dict[str, str]:
    "Get all layer names in a model, optionally with their representations"
    layers = OrderedDict() if getLayerRepr else []
    
    def get_layers(net, prefix=[]):
        if hasattr(net, "_modules"):
            for name, layer in net._modules.items():
                if layer is None:
                    continue
                if getLayerRepr:
                    layers[".".join(prefix+[name])] = layer.__repr__()
                else:
                    layers.append(".".join(prefix + [name]))
                get_layers(layer, prefix=prefix+[name])

    get_layers(model)
    return layers



def get_module_by_name(
    module: torch.Tensor | nn.Module,  # Module to search in
    access_string: str                 # Dot-separated path to the submodule
) -> nn.Module | None:
    "Access a nested submodule by its name path"
    try:
        names = access_string.split(sep='.')
        return reduce(getattr, names, module)
    except AttributeError:
        return None
