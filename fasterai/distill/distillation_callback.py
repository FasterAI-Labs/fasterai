# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/05_knowledge_distillation.ipynb (unless otherwise specified).

__all__ = ['KnowledgeDistillation', 'DistillationLoss']

# Cell
from fastai.vision.all import *

# Cell
class KnowledgeDistillation(Callback):
    """Callback to perform Knowledge Distillation between a teacher and a student model

    Implementation inspired by https://github.com/peterliht/knowledge-distillation-pytorch/

    Attributes:
        T: The temperature used to "soften" the softmax
        α: The weighting parameter between classification and teacher loss
    """
    def __init__(self, teacher, T:float=20., α:float=0.7):
        store_attr()

    def after_loss(self):
        self.teacher.model.eval()
        teacher_output = self.teacher.model(self.x)
        new_loss = DistillationLoss(self.pred, self.y, teacher_output, self.T, self.α)
        self.learn.loss_grad = new_loss

def DistillationLoss(y, labels, teacher_scores, T, alpha):
    return nn.KLDivLoss(reduction='batchmean')(F.log_softmax(y/T, dim=-1), F.softmax(teacher_scores/T, dim=-1)) * (T*T * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)