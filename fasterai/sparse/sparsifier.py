# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/sparse/sparsifier.ipynb.

# %% ../../nbs/sparse/sparsifier.ipynb #686a522f
from __future__ import annotations
import torch
import torch.nn as nn
import pickle
from fastcore.basics import store_attr, true
from typing import Callable, Type
from ..core.criteria import *
from einops import rearrange

# %% auto #0
__all__ = ['Sparsifier']

# %% ../../nbs/sparse/sparsifier.ipynb #c15c6bb1
class Sparsifier():
    "Class providing sparsifying capabilities"
    def __init__(self, 
                 model: nn.Module,                        # The model to sparsify
                 granularity: str,                        # Granularity of sparsification (e.g., 'weight', 'filter')
                 context: str,                            # Context for sparsification ('global' or 'local')
                 criteria: Criteria,                      # Criteria to determine which weights to keep
                 nm: bool = False,                        # Whether to use N:M sparsity pattern (forces 2:4 sparsity)
                 layer_type: Type[nn.Module] = nn.Conv2d  # Type of layers to apply sparsification to
    ):
        if nm: print('Sparsity automatically set to 50% with 2:4 pattern')
        store_attr()
        self._save_weights()
        self._reset_threshold()

    def _iter_layers(self, 
                     filter_type: str = 'layer_type',       # Filter: 'layer_type' or 'has_weight'
                     model: nn.Module | None = None         # Model to iterate (default: self.model)
    ):
        "Iterate over model modules with filtering"
        model = model or self.model
        for m in model.modules():
            if filter_type == 'layer_type' and isinstance(m, self.layer_type):
                yield m
            elif filter_type == 'has_weight' and hasattr(m, 'weight'):
                yield m

    def _iter_named_layers(self):
        "Iterate over matching layers with their names"
        for name, m in self.model.named_modules():
            if isinstance(m, self.layer_type):
                yield name, m

    def _to_sparsity_dict(self, 
                          sparsity: float | dict  # Sparsity value or per-layer dict
    ) -> dict:
        "Convert any sparsity input to a {module: sparsity} dict"
        name_to_module = dict(self.model.named_modules())
        
        # Float: apply same sparsity to all layers
        if isinstance(sparsity, (int, float)):
            if not (0 <= sparsity <= 100):
                raise ValueError(f"sparsity must be in range [0, 100], got {sparsity}")
            return {m: sparsity for m in self._iter_layers()}
        
        # Dict: resolve names to modules
        if isinstance(sparsity, dict):
            resolved = {}
            for key, sp in sparsity.items():
                if not (0 <= sp <= 100):
                    raise ValueError(f"sparsity must be in range [0, 100], got {sp}")
                if isinstance(key, str):
                    if key in name_to_module:
                        resolved[name_to_module[key]] = sp
                    else:
                        print(f"Warning: Layer '{key}' not found in model, skipping")
                elif isinstance(key, nn.Module):
                    resolved[key] = sp
            return resolved
        
        raise TypeError(f"sparsity must be float or dict, got {type(sparsity)}")

    def sparsify_layer(self, 
                       m: nn.Module,              # The layer to sparsify
                       sparsity: float,           # Target sparsity level (percentage)
                       round_to: int | None = None  # Round to a multiple of this value
    ) -> None:
        "Apply sparsification to a single layer"
        if not (0 <= sparsity <= 100):
            raise ValueError(f"sparsity must be in range [0, 100], got {sparsity}")
        scores    = self._compute_scores(m, sparsity)
        threshold = self._compute_threshold(scores, sparsity, round_to)
        mask      = self._compute_mask(scores, threshold)
        m.register_buffer('_mask', mask)
        self._apply(m)
        self.criteria.update_weights(m)

    def sparsify_model(self, 
                       sparsity: float | dict,        # Target sparsity level or per-layer dict
                       round_to: int | None = None    # Round to a multiple of this value
    ) -> None:
        "Apply sparsification to all matching layers in the model"
        self._reset_threshold()
        
        # Validate context for non-uniform sparsity
        if isinstance(sparsity, dict) and self.context == 'global':
            raise ValueError("Dict-based sparsity requires 'local' context")
        
        # Convert to unified dict format
        sparsity_map = self._to_sparsity_dict(sparsity)
        
        # Single iteration loop for all cases
        mods = list(self.model.modules())
        for name, m in self._iter_named_layers():
            if m not in sparsity_map:
                continue
            sp = sparsity_map[m]
            self.sparsify_layer(m, sp, round_to)
            # Handle batch norm if present
            mod_idx = mods.index(m)
            if mod_idx + 1 < len(mods) and isinstance(mods[mod_idx + 1], nn.modules.batchnorm._BatchNorm):
                self.sparsify_batchnorm(m, mods[mod_idx + 1])
                
    def sparsify_batchnorm(self, 
                          m: nn.Module,       # The layer before batch norm
                          bn: nn.Module       # The batch norm layer
    ) -> None:
        "Apply filter pruning to batch norm parameters if appropriate"
        mask = getattr(m, "_mask", None)
        if self.granularity == 'filter' and true(mask):
            bn.weight.data.mul_(mask.squeeze())
            bn.bias.data.mul_(mask.squeeze())
            
    def _apply_masks(self) -> None:
        "Apply all stored masks to model weights"
        for m in self._iter_layers():
            self._apply(m)
        
    def _apply(self, 
              m: nn.Module  # Module to apply mask to
    ) -> None:
        "Apply mask to a module's weights"
        mask = getattr(m, "_mask", None)
        if true(mask): m.weight.data.mul_(mask)
        if self.granularity == 'filter' and true(m.bias):
            if true(mask): m.bias.data.mul_(mask.squeeze())
    
    def _reset_weights(self, 
                      model: nn.Module | None = None  # Model to reset (default: self.model)
    ) -> None:
        "Reset weights to their initial values"
        model = model or self.model
        for m in self._iter_layers('has_weight', model):
            init_weights = getattr(m, "_init_weights", m.weight)
            init_biases = getattr(m, "_init_biases", m.bias)
            with torch.no_grad():
                if true(m.weight): m.weight.copy_(init_weights)
                if true(m.bias): m.bias.copy_(init_biases)
            self._apply(m)
            if isinstance(m, nn.modules.batchnorm._BatchNorm): m.reset_parameters()
                
    def _save_weights(self) -> None:
        "Save initial weights of the model"
        for m in self._iter_layers('has_weight'):
            m.register_buffer("_init_weights", m.weight.clone())
            bias = getattr(m, 'bias', None)
            if true(bias): m.register_buffer("_init_biases", bias.clone())
                    
    def save_model(self, 
                  path: str,                            # Path to save the model
                  model: nn.Module | None = None        # Model to save (default: self.model)
    ) -> None:
        "Save model without sparsification buffers"
        model = model or self.model
        tmp_model = copy.deepcopy(model)
        self._reset_weights(tmp_model)
        self._clean_buffers(tmp_model)
        torch.save(tmp_model, path)

    def _clean_buffers(self, 
                      model: nn.Module | None = None  # Model to clean (default: self.model)
    ) -> None:
        "Remove internal buffers used for sparsification"
        model = model or self.model
        for m in self._iter_layers('has_weight', model):
            if hasattr(m, '_mask'): del m._buffers["_mask"]
            if hasattr(m, '_init_weights'): del m._buffers["_init_weights"]
            if hasattr(m, '_init_biases'): del m._buffers["_init_biases"]
                    
    def _reset_threshold(self) -> None:
        "Reset the threshold used for global pruning"
        self.threshold = None
            
    def _rounded_sparsity(self, 
                         n_to_prune: int,  # Number of elements to prune
                         round_to: int     # Rounding value
    ) -> int:
        "Round the number of elements to keep to a multiple of round_to"
        if round_to == 0:
            raise ValueError("round_to must be non-zero")
        return max(round_to * torch.ceil(n_to_prune / round_to), round_to)
    
    def _compute_scores(self, 
                       m: nn.Module,   # Module to compute scores for
                       sparsity: float # Target sparsity level
    ) -> torch.Tensor:
        "Compute importance scores for weights based on criteria"
        return self.criteria(m, self.granularity)
                
    def _compute_threshold(self, 
                          scores: torch.Tensor,  # Importance scores
                          sparsity: float,       # Target sparsity level
                          round_to: int | None   # Rounding value
    ) -> torch.Tensor:
        "Compute threshold for pruning, with optional rounding"
        if self.context == 'global':
            if self.threshold is None: 
                global_scores = torch.cat([self.criteria(m, self.granularity).view(-1) for m in self._iter_layers()])
                self.threshold = torch.quantile(global_scores.view(-1), sparsity / 100)   
        elif self.context == 'local': 
            self.threshold = torch.quantile(scores.view(-1), sparsity / 100)
        else: 
            raise ValueError(f'Invalid context: {self.context}. Must be "global" or "local"')
            
        if round_to:
            n_to_keep = sum(scores.ge(self.threshold)).squeeze()
            self.threshold = torch.topk(scores.squeeze(), int(self._rounded_sparsity(n_to_keep, round_to)))[0].min()
        return self.threshold
    
    def _compute_mask(self, 
                     scores: torch.Tensor,   # Importance scores
                     threshold: torch.Tensor # Threshold for pruning
    ) -> torch.Tensor:
        "Compute binary mask for weights based on scores and threshold"
        if self.nm: return self._apply_nm_sparsity(scores)
        if threshold > scores.max(): threshold = scores.max()
        return scores.ge(threshold).to(dtype=scores.dtype)

    def _apply_nm_sparsity(self, 
                          scores: torch.Tensor  # Importance scores
    ) -> torch.Tensor:
        "Apply 2:4 structured sparsity pattern (N:M sparsity where N=2, M=4)"
        out_channels, in_channels, kernel_height, kernel_width = scores.shape
    
        if in_channels % 4 != 0 or in_channels * kernel_height * kernel_width % 16 != 0:
            print(f"Skipping 2:4 sparsity, Cin * Kh * Kw is not a multiple of 16")
            return torch.ones_like(scores)
    
        blocked_scores = rearrange(scores, 'o (b c) h w -> h w o b c', c=4)
        threshold = blocked_scores.topk(k=2, dim=-1).values[..., -1:]
        mask = (blocked_scores >= threshold).float()
        return rearrange(mask, 'h w o b c -> o (b c) h w')

    def print_sparsity(self) -> None:
        "Print sparsity report for all layers"
        total_params = 0
        total_zeros = 0
        
        print("\nSparsity Report:")
        print("-" * 80)
        print(f"{'Layer':<30} {'Type':<15} {'Params':<10} {'Zeros':<10} {'Sparsity':<10}")
        print("-" * 80)
        
        for name, m in self._iter_named_layers():
            zeros = torch.sum(m.weight == 0).item()
            total = m.weight.nelement()
            sparsity_pct = 100.0 * zeros / total if total > 0 else 0
            
            print(f"{name:<30} {m.__class__.__name__:<15} "
                  f"{total:<10,d} {zeros:<10,d} {sparsity_pct:>8.2f}%")
            
            total_params += total
            total_zeros += zeros
        
        print("-" * 80)
        overall_sparsity = 100.0 * total_zeros / total_params if total_params > 0 else 0
        print(f"{'Overall':<30} {'all':<15} {total_params:<10,d} "
              f"{total_zeros:<10,d} {overall_sparsity:>8.2f}%")
