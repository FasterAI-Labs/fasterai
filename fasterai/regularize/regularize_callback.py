# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/regularize/regularize_callback.ipynb.

# %% ../../nbs/regularize/regularize_callback.ipynb #7b9d82f5
from __future__ import annotations
import warnings

from fastai.callback.all import *
from fastcore.basics import store_attr, listify
from ..core.criteria import *
from ..core.granularity import *
from ..core.schedule import *

import torch
import torch.nn as nn
from typing import Type

# %% auto #0
__all__ = ['RegularizeCallback']

# %% ../../nbs/regularize/regularize_callback.ipynb #46f6973d
class RegularizeCallback(Callback):
    def __init__(self, 
                 criteria: Criteria | list[Criteria],            # Criteria(s) to use for regularization
                 granularity: str | list[str],                   # Granularity level(s) for grouping
                 weight: float = 0.01,                                 # Regularization weight
                 layer_types: Type | list[Type] = nn.Conv2d,     # Layer types to apply regularization to
                 schedule: Schedule | None = None,                  # Optional schedule for regularization weight
                 verbose: bool = False                                 # Whether to report regularization weight
    ):
        "Callback to apply regularization using criteria during training"
        store_attr()
        self.criteria = listify(criteria)
        self.granularity = listify(granularity)
        self.layer_types = listify(layer_types)
        self.current_weight = weight
        
    def before_batch(self) -> None:
        "Update regularization weight if scheduled"
        if self.schedule is not None:
            progress = self.schedule.progress(self.pct_train)
            self.current_weight = self.weight * progress
        
    def after_loss(self) -> None:
        "Apply regularization after computing the main loss"
        reg = self.get_norm()
        self.learn.loss_grad += reg
        self.learn.loss = self.learn.loss_grad.clone()
    
    def _iter_layers(self):
        "Iterate over matching layers with weights"
        for m in self.learn.model.modules():
            if any(isinstance(m, lt) for lt in self.layer_types) and hasattr(m, 'weight'):
                yield m
            
    def get_norm(self) -> torch.Tensor:
        "Compute regularization using the specified criteria and granularities"
        # Pre-filter modules once
        layers = list(self._iter_layers())
        
        layer_regs = []
        for crit in self.criteria:
            for g in self.granularity:
                for m in layers:
                    try:
                        scores = crit.f(m.weight)[None].abs().sum(Granularities.get_dim(m, g))
                        layer_regs.append(self.current_weight * scores.sum())
                    except (KeyError, ValueError) as e:
                        warnings.warn(f"Skipping regularization for {type(m).__name__}: {e}")
                    except RuntimeError as e:
                        warnings.warn(f"Runtime error in regularization for {type(m).__name__}: {e}")
        
        return torch.stack(layer_regs).sum() if layer_regs else torch.tensor(0.0)
    
    def after_epoch(self) -> None:
        "Report current regularization weight if verbose"
        if self.verbose:
            print(f"Current regularization weight: {self.current_weight:.6f}")
