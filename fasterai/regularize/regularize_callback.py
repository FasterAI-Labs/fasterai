# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/regularize/regularize_callback.ipynb.

# %% ../../nbs/regularize/regularize_callback.ipynb #7b9d82f5
from __future__ import annotations
from fastai.callback.all import *
from fastcore.basics import store_attr, listify
from ..core.criteria import *
from ..core.granularity import *
from ..core.schedule import *

import torch
import torch.nn as nn
from typing import Union, Optional, Type

# %% auto #0
__all__ = ['RegularizeCallback']

# %% ../../nbs/regularize/regularize_callback.ipynb #46f6973d
class RegularizeCallback(Callback):
    def __init__(self, 
                 criteria: Union[Criteria, list[Criteria]],            # Criteria(s) to use for regularization
                 granularity: Union[str, list[str]],                   # Granularity level(s) for grouping
                 weight: float = 0.01,                                 # Regularization weight
                 layer_types: Union[Type, list[Type]] = nn.Conv2d,     # Layer types to apply regularization to
                 schedule: Optional[Schedule] = None,                  # Optional schedule for regularization weight
                 per_layer_weights: Optional[dict[str, float]] = None, # Optional per-layer weights
                 verbose: bool = False                                 # Whether to report regularization weight
    ):
        "Callback to apply regularization using criteria during training"
        store_attr()
        self.criteria = listify(criteria)
        self.granularity = listify(granularity)
        self.layer_types = listify(layer_types)
        self.per_layer_weights = per_layer_weights or {}
        self.current_weight = weight
        
    def before_batch(self) -> None:
        "Update regularization weight if scheduled"
        if self.schedule is not None:
            self.current_weight = self.schedule([self.weight], self.pct_train)[0]
        
    def after_loss(self) -> None:
        "Apply regularization after computing the main loss"
        reg = self.get_norm()
        self.learn.loss_grad += reg
        self.learn.loss = self.learn.loss_grad.clone()
    
    def _iter_layers(self):
        "Iterate over matching layers with weights"
        for m in self.learn.model.modules():
            if any(isinstance(m, lt) for lt in self.layer_types) and hasattr(m, 'weight'):
                yield m
            
    def get_norm(self) -> torch.Tensor:
        "Compute regularization using the specified criteria and granularities"
        # Pre-filter modules once
        layers = list(self._iter_layers())
        
        layer_regs = []
        for crit in self.criteria:
            for g in self.granularity:
                for m in layers:
                    try:
                        scores = crit.f(m.weight)[None].abs().sum(Granularities.get_dim(m, g))
                        layer_regs.append(self.current_weight * scores.sum())
                    except (KeyError, ValueError) as e:
                        import warnings
                        warnings.warn(f"Skipping regularization for {type(m).__name__}: {e}")
                    except RuntimeError as e:
                        import warnings
                        warnings.warn(f"Runtime error in regularization for {type(m).__name__}: {e}")
        
        return torch.stack(layer_regs).sum() if layer_regs else torch.tensor(0.0)
    
    def after_epoch(self) -> None:
        "Report current regularization weight if verbose"
        if self.verbose:
            print(f"Current regularization weight: {self.current_weight:.6f}")
