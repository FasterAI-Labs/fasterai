# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/prune/prune_callback.ipynb.

# %% auto #0
__all__ = ['PruneCallback']

# %% ../../nbs/prune/prune_callback.ipynb #7d58c624
from fastai.vision.all import *
from fastai.callback.all import *
from .pruner import *
from ..core.criteria import *
from ..core.schedule import *

import torch
import torch.nn as nn
import torch.nn.functional as F

# %% ../../nbs/prune/prune_callback.ipynb #50598138-7d55-4774-b711-114c1c42dce8
class PruneCallback(Callback):
    def __init__(self, pruning_ratio, schedule, context, criteria, *args, **kwargs):
        store_attr()
        self.sparsity_levels = []
        self.extra_args = args
        self.extra_kwargs = kwargs

    def _build_pruning_schedule(self, sched_func):
        "Create a schedule function compatible with torch-pruning's Pruner"
        start_val, end_val = self.schedule.start_val, self.schedule.end_val
        def scheduler(pruning_ratio, steps, start=start_val, end=end_val):
            return [
                sched_func(start, end, i / float(steps)) * pruning_ratio
                for i in range(steps + 1)
            ]
        return scheduler

    def before_fit(self) -> None:
        "Setup pruner before training"
        n_batches_per_epoch = len(self.learn.dls.train)
        total_training_steps = n_batches_per_epoch * self.learn.n_epoch
        self.pruning_ratio = self.pruning_ratio/100 if self.pruning_ratio>1 else self.pruning_ratio
        
        # Validate pruning_ratio is in valid range
        if not (0 < self.pruning_ratio <= 1):
            raise ValueError(f"pruning_ratio must be in range (0, 1], got {self.pruning_ratio}")

        self.example_inputs, _ = self.learn.dls.one_batch()
        
        # Build schedule function for torch-pruning compatibility
        pruning_schedule = self._build_pruning_schedule(self.schedule.sched_func)
        self.sparsity_levels = pruning_schedule(self.pruning_ratio, total_training_steps)
        
        self.pruner = Pruner(
            self.learn.model,
            criteria=self.criteria,
            pruning_ratio=self.pruning_ratio, 
            context=self.context,
            iterative_steps=total_training_steps, 
            schedule=pruning_schedule,
            *self.extra_args, 
            **self.extra_kwargs
        )
        
    def before_step(self) -> None:
        "Apply pruning before optimizer step"
        if self.training: 
            self.pruner.prune_model()

    def after_epoch(self) -> None:
        "Log sparsity after each epoch"
        completed_steps = (self.epoch + 1) * len(self.learn.dls.train)
        # Bounds check for sparsity_levels access
        if completed_steps > 0 and completed_steps <= len(self.sparsity_levels):
            current_sparsity = self.sparsity_levels[completed_steps - 1]
            print(f'Sparsity at the end of epoch {self.epoch}: {current_sparsity*100:.2f}%')
