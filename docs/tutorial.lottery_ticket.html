---

title: Lottery Ticket Hypothesis


keywords: fastai
sidebar: home_sidebar

summary: "How to find winning tickets with fastai"
description: "How to find winning tickets with fastai"
nb_path: "nbs/03_tutorial.lottery_ticket.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/03_tutorial.lottery_ticket.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Lottery-Ticket-Hypothesis">The Lottery Ticket Hypothesis<a class="anchor-link" href="#The-Lottery-Ticket-Hypothesis"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <a href="https://arxiv.org/abs/1803.03635">Lottery Ticket Hypothesis</a> is a really intriguing discovery made in 2019 by Frankle &amp; Carbin. It states that:</p>
<blockquote><p>A randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.</p>
</blockquote>
<p>Meaning that, once we find that subnetwork. Every other parameter in the network becomes useless.</p>
<p>The way authors propose to find those subnetwork is as follows:1. Initialize the neural network2. Train it to convergence</p>
<ol>
<li>Prune the smallest magnitude weights by creating a mask $m$</li>
<li>Reinitialize the weights to their original value; i.e at iteration $0$.</li>
<li>Repeat from step 2 until reaching the desired level of sparsity.</li>
</ol>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fasterai.sparse.all</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">PETS</span><span class="p">)</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">get_image_files</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s2">&quot;images&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">label_func</span><span class="p">(</span><span class="n">f</span><span class="p">):</span> <span class="k">return</span> <span class="n">f</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isupper</span><span class="p">()</span>

<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>

<span class="n">dls</span> <span class="o">=</span> <span class="n">ImageDataLoaders</span><span class="o">.</span><span class="n">from_name_func</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">files</span><span class="p">,</span> <span class="n">label_func</span><span class="p">,</span> <span class="n">item_tfms</span><span class="o">=</span><span class="n">Resize</span><span class="p">(</span><span class="mi">64</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy $a_B &gt; a_A$, in a training time $t_B &lt; t_A$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's get the baseline for network A:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's save original weights</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">initial_weights</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.589757</td>
      <td>0.574913</td>
      <td>0.703654</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.560946</td>
      <td>0.546202</td>
      <td>0.702977</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.531937</td>
      <td>0.555280</td>
      <td>0.709066</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.482919</td>
      <td>0.549042</td>
      <td>0.727334</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.443224</td>
      <td>0.511626</td>
      <td>0.756428</td>
      <td>00:07</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now have our accuracy $a_A$ of $74\%$ and our training time $t_A$ of $5$ epochs</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will restart from the same initialization to be sure to not get lucky.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">initial_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can pass the parameters <code>lth=True</code> to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found "lottery ticket" after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the <code>start_epoch</code> parameter to $5$ to begin the pruning process after $5$ epochs.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sp_cb</span> <span class="o">=</span> <span class="n">SparsifyCallback</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;local&#39;</span><span class="p">,</span> <span class="n">large_final</span><span class="p">,</span> <span class="n">iterative</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lth</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As our <a href="/fasterai/schedules.html#iterative"><code>iterative</code></a> schedule makes $3$ pruning steps by default, it means that we have to train our network for <code>start_epoch</code> + $3*t_B$, so $20$ epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="n">sp_cb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Pruning of weight until a sparsity of [50]%
Saving Weights at epoch 0
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.598814</td>
      <td>0.686804</td>
      <td>0.649526</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.560133</td>
      <td>0.529173</td>
      <td>0.732747</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.531159</td>
      <td>0.511585</td>
      <td>0.753045</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.498013</td>
      <td>0.651968</td>
      <td>0.600135</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.463376</td>
      <td>0.491074</td>
      <td>0.751015</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.555985</td>
      <td>0.702323</td>
      <td>0.669147</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.527828</td>
      <td>0.485827</td>
      <td>0.765223</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.468368</td>
      <td>0.487057</td>
      <td>0.782815</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.421725</td>
      <td>0.445869</td>
      <td>0.781461</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.391137</td>
      <td>0.459242</td>
      <td>0.799053</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.471564</td>
      <td>0.622142</td>
      <td>0.631935</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.430352</td>
      <td>0.461739</td>
      <td>0.776049</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.384778</td>
      <td>0.531771</td>
      <td>0.762517</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.352641</td>
      <td>0.488232</td>
      <td>0.794993</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.320290</td>
      <td>0.488196</td>
      <td>0.739513</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.386944</td>
      <td>0.459122</td>
      <td>0.812585</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.349582</td>
      <td>0.569863</td>
      <td>0.721922</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.312838</td>
      <td>0.402464</td>
      <td>0.815291</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.261409</td>
      <td>0.542125</td>
      <td>0.735453</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.245360</td>
      <td>0.414822</td>
      <td>0.809202</td>
      <td>00:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Sparsity at the end of epoch 0: [0.0]%
Sparsity at the end of epoch 1: [0.0]%
Sparsity at the end of epoch 2: [0.0]%
Sparsity at the end of epoch 3: [0.0]%
Sparsity at the end of epoch 4: [0.0]%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 5: [16.67]%
Sparsity at the end of epoch 6: [16.67]%
Sparsity at the end of epoch 7: [16.67]%
Sparsity at the end of epoch 8: [16.67]%
Sparsity at the end of epoch 9: [16.67]%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 10: [33.33]%
Sparsity at the end of epoch 11: [33.33]%
Sparsity at the end of epoch 12: [33.33]%
Sparsity at the end of epoch 13: [33.33]%
Sparsity at the end of epoch 14: [33.33]%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 15: [50.0]%
Sparsity at the end of epoch 16: [50.0]%
Sparsity at the end of epoch 17: [50.0]%
Sparsity at the end of epoch 18: [50.0]%
Sparsity at the end of epoch 19: [50.0]%
Final Sparsity: [50.0]%
Sparsity in Conv2d 1: 50.00%
Sparsity in Conv2d 7: 50.00%
Sparsity in Conv2d 10: 50.00%
Sparsity in Conv2d 13: 50.00%
Sparsity in Conv2d 16: 50.00%
Sparsity in Conv2d 20: 50.00%
Sparsity in Conv2d 23: 50.00%
Sparsity in Conv2d 26: 50.00%
Sparsity in Conv2d 29: 50.00%
Sparsity in Conv2d 32: 50.00%
Sparsity in Conv2d 36: 50.00%
Sparsity in Conv2d 39: 50.00%
Sparsity in Conv2d 42: 50.00%
Sparsity in Conv2d 45: 50.00%
Sparsity in Conv2d 48: 50.00%
Sparsity in Conv2d 52: 50.00%
Sparsity in Conv2d 55: 50.00%
Sparsity in Conv2d 58: 50.00%
Sparsity in Conv2d 61: 50.00%
Sparsity in Conv2d 64: 50.00%
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We indeed have a network B, whose accuracy $a_B &gt; a_A$ in the same training time.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Lottery-Ticket-Hypothesis-with-Rewinding">Lottery Ticket Hypothesis with Rewinding<a class="anchor-link" href="#Lottery-Ticket-Hypothesis-with-Rewinding"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In some case, LTH fails for deeper networks, author then propose a <a href="https://arxiv.org/pdf/1903.01611.pdf">solution</a>, which is to rewind the weights to a more advanced iteration instead of the initialization value.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">initial_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This can be done in fasterai by passing the <code>rewind_epoch</code> parameter, that will save the weights at that epoch, then resetting the weights accordingly.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sp_cb</span> <span class="o">=</span> <span class="n">SparsifyCallback</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;local&#39;</span><span class="p">,</span> <span class="n">large_final</span><span class="p">,</span> <span class="n">iterative</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lth</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rewind_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="n">sp_cb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Pruning of weight until a sparsity of [50]%
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.599985</td>
      <td>0.566728</td>
      <td>0.700271</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.550926</td>
      <td>0.539680</td>
      <td>0.711773</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.524204</td>
      <td>0.647951</td>
      <td>0.638701</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.499537</td>
      <td>0.605123</td>
      <td>0.705683</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.441816</td>
      <td>0.577226</td>
      <td>0.708390</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.514446</td>
      <td>0.545353</td>
      <td>0.740866</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.475099</td>
      <td>0.517007</td>
      <td>0.726658</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.429452</td>
      <td>0.438081</td>
      <td>0.810555</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.377079</td>
      <td>0.444002</td>
      <td>0.780785</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.348466</td>
      <td>0.384129</td>
      <td>0.837618</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.436006</td>
      <td>0.475868</td>
      <td>0.771313</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.390055</td>
      <td>0.398510</td>
      <td>0.822733</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.359748</td>
      <td>0.468656</td>
      <td>0.771313</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.323625</td>
      <td>0.505410</td>
      <td>0.780785</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.299261</td>
      <td>0.387048</td>
      <td>0.828146</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.355692</td>
      <td>0.437216</td>
      <td>0.801759</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.313067</td>
      <td>0.469669</td>
      <td>0.823410</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.271151</td>
      <td>0.433562</td>
      <td>0.832882</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.242624</td>
      <td>0.531901</td>
      <td>0.776049</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.225511</td>
      <td>0.370019</td>
      <td>0.845737</td>
      <td>00:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Sparsity at the end of epoch 0: [0.0]%
Saving Weights at epoch 1
Sparsity at the end of epoch 1: [0.0]%
Sparsity at the end of epoch 2: [0.0]%
Sparsity at the end of epoch 3: [0.0]%
Sparsity at the end of epoch 4: [0.0]%
Resetting Weights to their epoch 1 values
Sparsity at the end of epoch 5: [16.67]%
Sparsity at the end of epoch 6: [16.67]%
Sparsity at the end of epoch 7: [16.67]%
Sparsity at the end of epoch 8: [16.67]%
Sparsity at the end of epoch 9: [16.67]%
Resetting Weights to their epoch 1 values
Sparsity at the end of epoch 10: [33.33]%
Sparsity at the end of epoch 11: [33.33]%
Sparsity at the end of epoch 12: [33.33]%
Sparsity at the end of epoch 13: [33.33]%
Sparsity at the end of epoch 14: [33.33]%
Resetting Weights to their epoch 1 values
Sparsity at the end of epoch 15: [50.0]%
Sparsity at the end of epoch 16: [50.0]%
Sparsity at the end of epoch 17: [50.0]%
Sparsity at the end of epoch 18: [50.0]%
Sparsity at the end of epoch 19: [50.0]%
Final Sparsity: [50.0]%
Sparsity in Conv2d 1: 50.00%
Sparsity in Conv2d 7: 50.00%
Sparsity in Conv2d 10: 50.00%
Sparsity in Conv2d 13: 50.00%
Sparsity in Conv2d 16: 50.00%
Sparsity in Conv2d 20: 50.00%
Sparsity in Conv2d 23: 50.00%
Sparsity in Conv2d 26: 50.00%
Sparsity in Conv2d 29: 50.00%
Sparsity in Conv2d 32: 50.00%
Sparsity in Conv2d 36: 50.00%
Sparsity in Conv2d 39: 50.00%
Sparsity in Conv2d 42: 50.00%
Sparsity in Conv2d 45: 50.00%
Sparsity in Conv2d 48: 50.00%
Sparsity in Conv2d 52: 50.00%
Sparsity in Conv2d 55: 50.00%
Sparsity in Conv2d 58: 50.00%
Sparsity in Conv2d 61: 50.00%
Sparsity in Conv2d 64: 50.00%
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Super-Masks">Super-Masks<a class="anchor-link" href="#Super-Masks"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Researchers from Uber AI <a href="https://arxiv.org/pdf/1905.01067.pdf">investigated</a> the LTH and found the existence of what they call "Super-Masks", i.e. masks that, applied on a untrained neural network, allows to reach better-than-random results.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">initial_weights</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;All keys matched successfully&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To find supermasks, authors perform the LTH method then apply the mask on the original, untrained network. In fasterai, you can pass the parameter <code>reset_end=True</code>, which will reset the weights to their original value at the end of the training, but keeping the pruned weights (i.e. the mask) unchanged.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sp_cb</span> <span class="o">=</span> <span class="n">SparsifyCallback</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="s1">&#39;local&#39;</span><span class="p">,</span> <span class="n">large_final</span><span class="p">,</span> <span class="n">iterative</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lth</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reset_end</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="n">sp_cb</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Pruning of weight until a sparsity of [50]%
Saving Weights at epoch 0
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.588215</td>
      <td>0.603662</td>
      <td>0.649526</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.559910</td>
      <td>0.551536</td>
      <td>0.697564</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.517046</td>
      <td>0.473498</td>
      <td>0.765900</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.460481</td>
      <td>0.544400</td>
      <td>0.706360</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.440268</td>
      <td>0.441409</td>
      <td>0.796346</td>
      <td>00:07</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.559214</td>
      <td>0.614429</td>
      <td>0.648850</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.508002</td>
      <td>0.699705</td>
      <td>0.690798</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.472389</td>
      <td>0.575537</td>
      <td>0.740189</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.427371</td>
      <td>0.578768</td>
      <td>0.696211</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.377523</td>
      <td>0.527881</td>
      <td>0.735453</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.472205</td>
      <td>0.708431</td>
      <td>0.699594</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.441892</td>
      <td>0.465152</td>
      <td>0.776049</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.386189</td>
      <td>0.442156</td>
      <td>0.786874</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.347737</td>
      <td>0.584790</td>
      <td>0.778755</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.321213</td>
      <td>0.891845</td>
      <td>0.690798</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.410824</td>
      <td>0.511506</td>
      <td>0.772666</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.351705</td>
      <td>0.389204</td>
      <td>0.821380</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.308388</td>
      <td>0.363523</td>
      <td>0.845061</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.278367</td>
      <td>0.380290</td>
      <td>0.828146</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.265100</td>
      <td>0.408591</td>
      <td>0.845061</td>
      <td>00:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Sparsity at the end of epoch 0: [0.0]%
Sparsity at the end of epoch 1: [0.0]%
Sparsity at the end of epoch 2: [0.0]%
Sparsity at the end of epoch 3: [0.0]%
Sparsity at the end of epoch 4: [0.0]%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 5: [16.67]%
Sparsity at the end of epoch 6: [16.67]%
Sparsity at the end of epoch 7: [16.67]%
Sparsity at the end of epoch 8: [16.67]%
Sparsity at the end of epoch 9: [16.67]%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 10: [33.33]%
Sparsity at the end of epoch 11: [33.33]%
Sparsity at the end of epoch 12: [33.33]%
Sparsity at the end of epoch 13: [33.33]%
Sparsity at the end of epoch 14: [33.33]%
Resetting Weights to their epoch 0 values
Sparsity at the end of epoch 15: [50.0]%
Sparsity at the end of epoch 16: [50.0]%
Sparsity at the end of epoch 17: [50.0]%
Sparsity at the end of epoch 18: [50.0]%
Sparsity at the end of epoch 19: [50.0]%
Final Sparsity: [50.0]%
Sparsity in Conv2d 1: 50.00%
Sparsity in Conv2d 7: 50.00%
Sparsity in Conv2d 10: 50.00%
Sparsity in Conv2d 13: 50.00%
Sparsity in Conv2d 16: 50.00%
Sparsity in Conv2d 20: 50.00%
Sparsity in Conv2d 23: 50.00%
Sparsity in Conv2d 26: 50.00%
Sparsity in Conv2d 29: 50.00%
Sparsity in Conv2d 32: 50.00%
Sparsity in Conv2d 36: 50.00%
Sparsity in Conv2d 39: 50.00%
Sparsity in Conv2d 42: 50.00%
Sparsity in Conv2d 45: 50.00%
Sparsity in Conv2d 48: 50.00%
Sparsity in Conv2d 52: 50.00%
Sparsity in Conv2d 55: 50.00%
Sparsity in Conv2d 58: 50.00%
Sparsity in Conv2d 61: 50.00%
Sparsity in Conv2d 64: 50.00%
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#2) [2.3516082763671875,0.35520973801612854]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

