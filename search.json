[
  {
    "objectID": "prune/prune_callback.html",
    "href": "prune/prune_callback.html",
    "title": "Prune Callback",
    "section": "",
    "text": "source\n\nPruneCallback\n\n PruneCallback (pruning_ratio, schedule, global_pruning, criteria, *args,\n                **kwargs)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/timm.html",
    "href": "tutorials/timm.html",
    "title": "TIMM Pruning",
    "section": "",
    "text": "from fastai.vision.all import *\nfrom fastai.callback.all import *\nfrom fasterai.core.criteria import *\nimport torch_pruning as tp\nfrom torch_pruning.pruner import function\nimport torch_pruning as tp\nimport timm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef onecycle_scheduler(pruning_ratio_dict, steps, start=0, end=1, α=14, β=6):\n    return [\n        sched_onecycle(start, end, i / float(steps), α, β) * pruning_ratio_dict\n        for i in range(steps + 1)\n    ]\n\ndef sched_onecycle(start, end, pos, α=14, β=6):\n    out = (1 + np.exp(-α + β)) / (1 + np.exp((-α * pos) + β))\n    return start + (end - start) * out\n\n\ndef get_dls(size, bs):\n    path = URLs.IMAGENETTE_160\n    source = untar_data(path)\n    blocks=(ImageBlock, CategoryBlock)\n    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n\n    csv_file = 'noisy_imagenette.csv'\n    inp = pd.read_csv(source/csv_file)\n    dblock = DataBlock(blocks=blocks,\n               splitter=ColSplitter(),\n               get_x=ColReader('path', pref=source),\n               get_y=ColReader(f'noisy_labels_0'),\n               item_tfms=tfms,\n               batch_tfms=batch_tfms)\n\n    return dblock.dataloaders(inp, path=source, bs=bs)\n\n\nmodel = timm.create_model('resnet18', pretrained=False, no_jit=True).eval()\n\n\ndls = get_dls(model.default_cfg['input_size'][2], 16)\n\n\n#learn = vision_learner(dls, 'bat_resnext26ts', metrics = [accuracy])\n#learn.unfreeze()\n\nmodel = timm.create_model('beit_base_patch16_224', pretrained=False, no_jit=True).eval()\n\nignored_layers = []\nnum_heads = {}\npruning_ratio_dict = {}\n#ratios = [0.265625,0.234375,0.265625,0.265625,0.93359375,0.328125,0.2265625,0.58984375,0.54296875,0.701171875,0.919921875,0.04296875,0.796875,0.240966796875,0.07763671875]\n\n\n#k = 0\nfor m in model.modules():\n    #if hasattr(m, 'head'): #isinstance(m, nn.Linear) and m.out_features == model.num_classes:\n    if isinstance(m, nn.Linear) and m.out_features == model.num_classes:\n        ignored_layers.append(m)\n        print(\"Ignore classifier layer: \", m)\n\n    # Attention layers\n    if hasattr(m, 'num_heads'):\n        if hasattr(m, 'qkv'):\n            num_heads[m.qkv] = m.num_heads\n            print(\"Attention layer: \", m.qkv, m.num_heads)\n        elif hasattr(m, 'qkv_proj'):\n            num_heads[m.qkv_proj] = m.num_heads\n    \n    #elif isinstance(m, nn.Conv2d):\n    #    pruning_ratio_dict[m] = ratios[k]\n    #    print(k)\n    #    k+=1\n\nlearn = Learner(dls, model, metrics = [accuracy])\n\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nAttention layer:  Linear(in_features=768, out_features=2304, bias=False) 12\nIgnore classifier layer:  Linear(in_features=768, out_features=1000, bias=True)\n\n\n\nxb, _ = dls.one_batch()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 xb, _ = dls.one_batch()\n\nNameError: name 'dls' is not defined\n\n\n\n\nlearn.fit_one_cycle(3, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.302582\n1.146651\n0.622166\n00:44\n\n\n1\n0.953016\n0.809723\n0.741147\n00:45\n\n\n2\n0.828404\n0.692629\n0.775796\n00:44\n\n\n\n\n\n\npruner = tp.pruner.MetaPruner(\n                        model, \n                        xb.to('cpu'), \n                        global_pruning=False,\n                        importance=tp.importance.GroupNormImportance(), \n                        iterative_steps=10000,\n                        pruning_ratio=0.5,\n                        #pruning_ratio_dict=pruning_ratio_dict,\n                        num_heads=num_heads,\n                        ignored_layers=ignored_layers,\n                    )\n#for g in pruner.step(interactive=True):\n#    g.prune()\npruner.step()\n\n/home/HubensN/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch_pruning/dependency.py:697: UserWarning: Unwrapped parameters detected: ['cls_token', 'blocks.1.attn.v_bias', 'blocks.6.attn.relative_position_bias_table', 'blocks.0.attn.v_bias', 'blocks.3.gamma_2', 'blocks.3.attn.q_bias', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.9.gamma_2', 'blocks.9.attn.q_bias', 'blocks.11.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.4.gamma_2', 'blocks.6.attn.q_bias', 'blocks.7.attn.q_bias', 'blocks.7.attn.relative_position_bias_table', 'blocks.9.attn.v_bias', 'blocks.10.gamma_2', 'blocks.10.attn.relative_position_bias_table', 'blocks.0.attn.q_bias', 'blocks.0.attn.relative_position_bias_table', 'blocks.1.gamma_2', 'blocks.4.attn.q_bias', 'blocks.8.attn.q_bias', 'blocks.8.attn.v_bias', 'blocks.10.attn.q_bias', 'blocks.11.gamma_1', 'blocks.11.gamma_2', 'blocks.0.gamma_2', 'blocks.5.gamma_1', 'blocks.6.gamma_1', 'blocks.6.attn.v_bias', 'blocks.8.gamma_2', 'blocks.8.attn.relative_position_bias_table', 'blocks.3.gamma_1', 'blocks.0.gamma_1', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.gamma_1', 'blocks.3.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.11.attn.v_bias', 'blocks.1.gamma_1', 'blocks.2.attn.v_bias', 'blocks.3.attn.v_bias', 'blocks.4.attn.v_bias', 'blocks.5.gamma_2', 'blocks.5.attn.v_bias', 'blocks.7.attn.v_bias', 'blocks.10.gamma_1', 'blocks.10.attn.v_bias', 'blocks.11.attn.q_bias', 'blocks.1.attn.q_bias', 'blocks.2.gamma_2', 'blocks.2.attn.q_bias', 'blocks.4.gamma_1', 'blocks.5.attn.q_bias', 'blocks.6.gamma_2', 'blocks.7.gamma_1', 'blocks.7.gamma_2', 'blocks.8.gamma_1', 'blocks.9.gamma_1'].\n Torch-Pruning will prune the last non-singleton dimension of these parameters. If you wish to change this behavior, please provide an unwrapped_parameters argument.\n  warnings.warn(warning_str)\n\n\n\nfor m in model.modules():\n    # Attention layers\n    if hasattr(m, 'num_heads'):\n        if hasattr(m, 'qkv'):\n            m.num_heads = num_heads[m.qkv]\n            m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n        elif hasattr(m, 'qkv_proj'):\n            m.num_heads = num_heads[m.qqkv_projkv]\n            m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)\n\n\nmodel_old = timm.create_model('convnext_xxlarge', pretrained=False, no_jit=True).eval()\nbase_macs, base_params = tp.utils.count_ops_and_params(model_old, xb.to('cpu'))\n\n\npruned_macs, pruned_params = tp.utils.count_ops_and_params(model, xb.to('cpu'))\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[137], line 1\n----&gt; 1 pruned_macs, pruned_params = tp.utils.count_ops_and_params(model, xb.to('cpu'))\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/utils/_contextlib.py:116, in context_decorator.&lt;locals&gt;.decorate_context(*args, **kwargs)\n    113 @functools.wraps(func)\n    114 def decorate_context(*args, **kwargs):\n    115     with ctx_factory():\n--&gt; 116         return func(*args, **kwargs)\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch_pruning/utils/op_counter.py:35, in count_ops_and_params(model, example_inputs, layer_wise)\n     33     _ = flops_model(**example_inputs)\n     34 else:\n---&gt; 35     _ = flops_model(example_inputs)\n     36 flops_count, params_count, _layer_flops, _layer_params = flops_model.compute_average_flops_cost()\n     37 layer_flops = {}\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1552 else:\n-&gt; 1553     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1603, in Module._call_impl(self, *args, **kwargs)\n   1600     bw_hook = hooks.BackwardHook(self, full_backward_hooks, backward_pre_hooks)\n   1601     args = bw_hook.setup_input_hook(args)\n-&gt; 1603 result = forward_call(*args, **kwargs)\n   1604 if _global_forward_hooks or self._forward_hooks:\n   1605     for hook_id, hook in (\n   1606         *_global_forward_hooks.items(),\n   1607         *self._forward_hooks.items(),\n   1608     ):\n   1609         # mark that always called hook is run\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:521, in Beit.forward(self, x)\n    520 def forward(self, x):\n--&gt; 521     x = self.forward_features(x)\n    522     x = self.forward_head(x)\n    523     return x\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:509, in Beit.forward_features(self, x)\n    507         x = checkpoint(blk, x, shared_rel_pos_bias=rel_pos_bias)\n    508     else:\n--&gt; 509         x = blk(x, shared_rel_pos_bias=rel_pos_bias)\n    510 x = self.norm(x)\n    511 return x\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1552 else:\n-&gt; 1553     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n   1558 # this function, and just call forward.\n   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1560         or _global_backward_pre_hooks or _global_backward_hooks\n   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1562     return forward_call(*args, **kwargs)\n   1564 try:\n   1565     result = None\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:248, in Block.forward(self, x, shared_rel_pos_bias)\n    246     x = x + self.drop_path2(self.mlp(self.norm2(x)))\n    247 else:\n--&gt; 248     x = x + self.drop_path1(self.gamma_1 * self.attn(self.norm1(x), shared_rel_pos_bias=shared_rel_pos_bias))\n    249     x = x + self.drop_path2(self.gamma_2 * self.mlp(self.norm2(x)))\n    250 return x\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1552 else:\n-&gt; 1553     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n   1558 # this function, and just call forward.\n   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1560         or _global_backward_pre_hooks or _global_backward_hooks\n   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1562     return forward_call(*args, **kwargs)\n   1564 try:\n   1565     result = None\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:149, in Attention.forward(self, x, shared_rel_pos_bias)\n    147         qkv += qkv_bias\n    148     else:\n--&gt; 149         qkv = F.linear(x, weight=self.qkv.weight, bias=qkv_bias)\n    150 qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n    151 q, k, v = qkv.unbind(0)  # B, num_heads, N, head_dim\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/fastai/torch_core.py:382, in TensorBase.__torch_function__(cls, func, types, args, kwargs)\n    380 if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n    381 if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n--&gt; 382 res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n    383 dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n    384 if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n\nFile ~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/_tensor.py:1437, in Tensor.__torch_function__(cls, func, types, args, kwargs)\n   1434     return NotImplemented\n   1436 with _C.DisableTorchFunctionSubclass():\n-&gt; 1437     ret = func(*args, **kwargs)\n   1438     if func in get_default_nowrap_functions():\n   1439         return ret\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3152x767 and 768x2304)\n\n\n\n\nprint(\"MACs: %.4f G =&gt; %.4f G\"%(base_macs/1e9, pruned_macs/1e9))\nprint(\"Params: %.4f M =&gt; %.4f M\"%(base_params/1e6, pruned_params/1e6))\n\nMACs: 151.5881 G =&gt; 7.0919 G\nParams: 846.4710 M =&gt; 58.2010 M\n\n\n\nclass PruneCallback(Callback):\n    def __init__(self, pruning_ratio, schedule, criteria, ignored_layers, *args, **kwargs):\n        store_attr()\n        self.sparsity_levels = []\n        self.extra_args = args\n        self.extra_kwargs = kwargs\n\n    def before_fit(self):\n        n_batches_per_epoch = len(self.learn.dls.train)\n        total_training_steps = n_batches_per_epoch * self.learn.n_epoch\n\n        self.total_training_steps = total_training_steps \n        print(self.total_training_steps)\n        self.example_inputs, _ = self.learn.dls.one_batch()\n        self.sparsity_levels = self.schedule(self.pruning_ratio, total_training_steps)\n\n        self.pruner = tp.pruner.MetaPruner(\n        self.learn.model,\n        example_inputs= torch.randn(self.example_inputs.shape).to('cuda:0'),\n        importance=self.criteria,\n        pruning_ratio=self.pruning_ratio, \n        ignored_layers=self.ignored_layers,\n        iterative_steps= self.total_training_steps, \n        #iterative_steps= 1, \n        #iterative_pruning_ratio_scheduler=self.schedule,\n        #global_pruning=self.context, \n        *self.extra_args, \n        **self.extra_kwargs\n        )\n        \n    def before_step(self):\n        if self.training: \n           #self.pruner.step()\n            for g in self.pruner.step(interactive=True):\n                g.prune()\n            \n        #for m in self.pruner.model.modules():\n        #    # Attention layers\n        #    if hasattr(m, 'num_heads'):\n        #        if hasattr(m, 'qkv'):\n        #            m.num_heads = num_heads[m.qkv]\n        #            m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n        #        elif hasattr(m, 'qkv_proj'):\n        #            m.num_heads = num_heads[m.qqkv_projkv]\n        #            m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)\n\n    def after_epoch(self):\n        completed_steps = (self.epoch + 1) * len(self.learn.dls.train)\n        current_sparsity = self.sparsity_levels[completed_steps - 1]\n        print(f'Sparsity at the end of epoch {self.epoch}: {current_sparsity*100:.2f}%')\n\n\ntimm.list_models()\n\n\nmodel = timm.create_model('resnet18', pretrained=True, no_jit=True).eval()\n\nignored_layers = []\nnum_heads = {}\n\n#k = 0\nfor m in model.modules():\n    if isinstance(m, nn.Linear) and m.out_features == model.num_classes:\n        ignored_layers.append(m)\n        print(\"Ignore classifier layer: \", m)\n\n    # Attention layers\n    if hasattr(m, 'num_heads'):\n        if hasattr(m, 'qkv'):\n            num_heads[m.qkv] = m.num_heads\n            print(\"Attention layer: \", m.qkv, m.num_heads)\n        elif hasattr(m, 'qkv_proj'):\n            num_heads[m.qkv_proj] = m.num_heads\n\nlearn = Learner(dls, model, metrics = [accuracy])\n\nIgnore classifier layer:  Linear(in_features=512, out_features=1000, bias=True)\n\n\n\nlearn.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.410828\n0.291440\n0.907771\n00:12\n\n\n1\n0.378970\n0.237411\n0.920764\n00:12\n\n\n2\n0.254743\n0.161710\n0.946752\n00:12\n\n\n3\n0.195507\n0.142778\n0.954395\n00:12\n\n\n4\n0.124310\n0.112485\n0.965350\n00:12\n\n\n\n\n\n\npr_cb = PruneCallback(pruning_ratio=0.25, schedule=onecycle_scheduler, global_pruning=True, criteria=tp.importance.GroupNormImportance(normalizer=None, target_types=[nn.modules.conv._ConvNd, nn.Linear]), num_heads=num_heads, ignored_layers=ignored_layers)\nlearn.fit_one_cycle(10, 1e-4, cbs=pr_cb)\n\n5910\n\n\n\n\n\n\n\n      \n      40.00% [4/10 00:58&lt;01:27]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.190778\n0.223286\n0.929172\n00:16\n\n\n1\n0.906825\n0.996437\n0.685605\n00:14\n\n\n2\n2.154549\n2.038115\n0.295796\n00:14\n\n\n3\n2.019212\n1.988954\n0.316688\n00:13\n\n\n\n\n\n    \n      \n      27.92% [165/591 00:03&lt;00:09 2.0123]\n    \n\n\nSparsity at the end of epoch 0: 0.25%\nSparsity at the end of epoch 1: 0.98%\nSparsity at the end of epoch 2: 3.54%\nSparsity at the end of epoch 3: 10.02%\n\n\n\nKeyboardInterrupt\n\n\n\n\npr_cb = PruneCallback(pruning_ratio=0.25, schedule=onecycle_scheduler, global_pruning=True, criteria=GroupNormImportance(normalizer=None, target_types=[nn.modules.conv._ConvNd, nn.Linear]), num_heads=num_heads, ignored_layers=ignored_layers)\nlearn.fit_one_cycle(15, 1e-4, cbs=pr_cb)\n\n8865\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.118503\n0.118367\n0.963057\n00:16\n\n\n1\n0.128137\n0.115662\n0.962038\n00:16\n\n\n2\n0.122357\n0.126387\n0.960255\n00:16\n\n\n3\n0.142834\n0.112623\n0.966115\n00:16\n\n\n4\n0.124087\n0.115061\n0.964076\n00:16\n\n\n5\n0.134429\n0.125728\n0.960510\n00:16\n\n\n6\n0.128918\n0.143516\n0.954650\n00:16\n\n\n7\n0.131373\n0.176553\n0.958217\n00:16\n\n\n8\n0.135654\n0.177501\n0.954395\n00:16\n\n\n9\n0.180335\n0.197476\n0.946497\n00:16\n\n\n10\n0.207450\n0.194991\n0.935796\n00:16\n\n\n11\n0.233158\n0.203113\n0.937325\n00:16\n\n\n12\n0.231163\n0.219680\n0.932484\n00:16\n\n\n13\n0.314865\n0.245912\n0.926369\n00:15\n\n\n14\n0.311438\n0.276028\n0.922038\n00:15\n\n\n\n\n\nSparsity at the end of epoch 0: 0.16%\nSparsity at the end of epoch 1: 0.39%\nSparsity at the end of epoch 2: 0.98%\nSparsity at the end of epoch 3: 2.35%\nSparsity at the end of epoch 4: 5.21%\nSparsity at the end of epoch 5: 10.03%\nSparsity at the end of epoch 6: 15.75%\nSparsity at the end of epoch 7: 20.31%\nSparsity at the end of epoch 8: 22.93%\nSparsity at the end of epoch 9: 24.15%\nSparsity at the end of epoch 10: 24.66%\nSparsity at the end of epoch 11: 24.87%\nSparsity at the end of epoch 12: 24.95%\nSparsity at the end of epoch 13: 24.99%\nSparsity at the end of epoch 14: 25.00%\n\n\n\nmodel = timm.create_model('tf_efficientnet_b3', pretrained=False, no_jit=True).eval()\nbase_macs, base_params = tp.utils.count_ops_and_params(model, xb.to('cpu'))\n\n\npruned_macs, pruned_params = tp.utils.count_ops_and_params(learn.model, xb.to('cuda:0'))\n\n\nprint(\"MACs: %.4f G =&gt; %.4f G\"%(base_macs/1e9, pruned_macs/1e9))\nprint(\"Params: %.4f M =&gt; %.4f M\"%(base_params/1e6, pruned_params/1e6))\n\nMACs: 0.9399 G =&gt; 0.5407 G\nParams: 12.2332 M =&gt; 7.2501 M\n\n\n\nimport abc\nimport torch\nimport torch.nn as nn\n\nimport typing\n\nfrom torch_pruning import function\nfrom torch_pruning.dependency import Group\n\nclass Importance(abc.ABC):\n    \"\"\" Estimate the importance of a tp.Dependency.Group, and return an 1-D per-channel importance score.\n\n        It should accept a group as inputs, and return a 1-D tensor with the same length as the number of channels.\n        All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups.\n\n        Example:\n            ```python\n            DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224)) \n            group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )    \n            scorer = MagnitudeImportance()    \n            imp_score = scorer(group)    \n            #imp_score is a 1-D tensor with length 3 for channels [2, 6, 9]  \n            min_score = imp_score.min() \n            ``` \n    \"\"\"\n    @abc.abstractclassmethod\n    def __call__(self, group: Group) -&gt; torch.Tensor: \n        raise NotImplementedError\n\n\nclass GroupNormImportance(Importance):\n\n    def __init__(self, \n                 p: int=2, \n                 group_reduction: str=\"mean\", \n                 normalizer: str='mean', \n                 bias=False,\n                 target_types:list=[nn.modules.conv._ConvNd, nn.Linear, nn.modules.batchnorm._BatchNorm, nn.LayerNorm]):\n        self.p = p\n        self.group_reduction = group_reduction\n        self.normalizer = normalizer\n        self.target_types = target_types\n        self.bias = bias\n\n    def _lamp(self, scores): # Layer-adaptive Sparsity for the Magnitude-based Pruning\n        \"\"\"\n        Normalizing scheme for LAMP.\n        \"\"\"\n        # sort scores in an ascending order\n        sorted_scores,sorted_idx = scores.view(-1).sort(descending=False)\n        # compute cumulative sum\n        scores_cumsum_temp = sorted_scores.cumsum(dim=0)\n        scores_cumsum = torch.zeros(scores_cumsum_temp.shape,device=scores.device)\n        scores_cumsum[1:] = scores_cumsum_temp[:len(scores_cumsum_temp)-1]\n        # normalize by cumulative sum\n        sorted_scores /= (scores.sum() - scores_cumsum)\n        # tidy up and output\n        new_scores = torch.zeros(scores_cumsum.shape,device=scores.device)\n        new_scores[sorted_idx] = sorted_scores\n        \n        return new_scores.view(scores.shape)\n    \n    def _normalize(self, group_importance, normalizer):\n        if normalizer is None:\n            return group_importance\n        elif isinstance(normalizer, typing.Callable):\n            return normalizer(group_importance)\n        elif normalizer == \"sum\":\n            return group_importance / group_importance.sum()\n        elif normalizer == \"standarization\":\n            return (group_importance - group_importance.min()) / (group_importance.max() - group_importance.min()+1e-8)\n        elif normalizer == \"mean\":\n            return group_importance / group_importance.mean()\n        elif normalizer == \"max\":\n            return group_importance / group_importance.max()\n        elif normalizer == 'gaussian':\n            return (group_importance - group_importance.mean()) / (group_importance.std()+1e-8)\n        elif normalizer.startswith('sentinel'): # normalize the score with the k-th smallest element. e.g. sentinel_0.5 means median normalization\n            sentinel = float(normalizer.split('_')[1]) * len(group_importance)\n            sentinel = torch.argsort(group_importance, dim=0, descending=False)[int(sentinel)]\n            return group_importance / (group_importance[sentinel]+1e-8)\n        elif normalizer=='lamp':\n            return self._lamp(group_importance)\n        else:\n            raise NotImplementedError\n\n    def _reduce(self, group_imp: typing.List[torch.Tensor], group_idxs: typing.List[typing.List[int]]):\n        if len(group_imp) == 0: return group_imp\n        if self.group_reduction == 'prod':\n            reduced_imp = torch.ones_like(group_imp[0])\n        elif self.group_reduction == 'max':\n            reduced_imp = torch.ones_like(group_imp[0]) * -99999\n        else:\n            reduced_imp = torch.zeros_like(group_imp[0])\n\n        for i, (imp, root_idxs) in enumerate(zip(group_imp, group_idxs)):\n            imp = imp.to(reduced_imp.device)\n            if self.group_reduction == \"sum\" or self.group_reduction == \"mean\":\n                reduced_imp.scatter_add_(0, torch.tensor(root_idxs, device=imp.device), imp) # accumulated importance\n            elif self.group_reduction == \"max\": # keep the max importance\n                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n                selected_imp = torch.maximum(input=selected_imp, other=imp)\n                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n            elif self.group_reduction == \"prod\": # product of importance\n                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n                torch.mul(selected_imp, imp, out=selected_imp)\n                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n            elif self.group_reduction == 'first':\n                if i == 0:\n                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n            elif self.group_reduction == 'gate':\n                if i == len(group_imp)-1:\n                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n            elif self.group_reduction is None:\n                reduced_imp = torch.stack(group_imp, dim=0) # no reduction\n            else:\n                raise NotImplementedError\n        \n        if self.group_reduction == \"mean\":\n            reduced_imp /= len(group_imp)\n        return reduced_imp\n    \n    @torch.no_grad()\n    def __call__(self, group: Group):\n        group_imp = []\n        group_idxs = []\n        # Iterate over all groups and estimate group importance\n        for i, (dep, idxs) in enumerate(group):\n            layer = dep.layer\n            prune_fn = dep.pruning_fn\n            root_idxs = group[i].root_idxs\n            if not isinstance(layer, tuple(self.target_types)):\n                continue\n            ####################\n            # Conv/Linear Output\n            ####################\n            if prune_fn in [\n                function.prune_conv_out_channels,\n                function.prune_linear_out_channels,\n            ]:\n                if hasattr(layer, \"transposed\") and layer.transposed:\n                    w = layer.weight.data.transpose(1, 0)[idxs].flatten(1)\n                else:\n                    w = layer.weight.data[idxs].flatten(1)\n                #local_imp = w.abs().pow(self.p).sum(1)\n                local_imp = w.abs().pow(self.p).mean(1)\n                group_imp.append(local_imp)\n                group_idxs.append(root_idxs)\n\n                if self.bias and layer.bias is not None:\n                    local_imp = layer.bias.data[idxs].abs().pow(self.p)\n                    group_imp.append(local_imp)\n                    group_idxs.append(root_idxs)\n\n            ####################\n            # Conv/Linear Input\n            ####################\n            elif prune_fn in [\n                function.prune_conv_in_channels,\n                function.prune_linear_in_channels,\n            ]:\n                if hasattr(layer, \"transposed\") and layer.transposed:\n                    w = (layer.weight.data).flatten(1)\n                else:\n                    w = (layer.weight.data).transpose(0, 1).flatten(1)\n                #local_imp = w.abs().pow(self.p).sum(1)\n                local_imp = w.abs().pow(self.p).mean(1)\n\n                # repeat importance for group convolutions\n                if prune_fn == function.prune_conv_in_channels and layer.groups != layer.in_channels and layer.groups != 1:\n                    local_imp = local_imp.repeat(layer.groups)\n                \n                local_imp = local_imp[idxs]\n                group_imp.append(local_imp)\n                group_idxs.append(root_idxs)\n\n            ####################\n            # BatchNorm\n            ####################\n            elif prune_fn == function.prune_batchnorm_out_channels:\n                # regularize BN\n                if layer.affine:\n                    w = layer.weight.data[idxs]\n                    local_imp = w.abs().pow(self.p)\n                    group_imp.append(local_imp)\n                    group_idxs.append(root_idxs)\n\n                    if self.bias and layer.bias is not None:\n                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n                        group_imp.append(local_imp)\n                        group_idxs.append(root_idxs)\n            ####################\n            # LayerNorm\n            ####################\n            elif prune_fn == function.prune_layernorm_out_channels:\n\n                if layer.elementwise_affine:\n                    w = layer.weight.data[idxs]\n                    local_imp = w.abs().pow(self.p)\n                    group_imp.append(local_imp)\n                    group_idxs.append(root_idxs)\n\n                    if self.bias and layer.bias is not None:\n                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n                        group_imp.append(local_imp)\n                        group_idxs.append(root_idxs)\n\n        if len(group_imp) == 0: # skip groups without parameterized layers\n            return None\n\n        group_imp = self._reduce(group_imp, group_idxs)\n        group_imp = self._normalize(group_imp, self.normalizer)\n        return group_imp\n\n\ntimm_models = timm.list_models(module='resnet')\n\n\ntimm.list_models(exclude_filters='*vit*')\n\n['bat_resnext26ts',\n 'beit_base_patch16_224',\n 'beit_base_patch16_384',\n 'beit_large_patch16_224',\n 'beit_large_patch16_384',\n 'beit_large_patch16_512',\n 'beitv2_base_patch16_224',\n 'beitv2_large_patch16_224',\n 'botnet26t_256',\n 'botnet50ts_256',\n 'caformer_b36',\n 'caformer_m36',\n 'caformer_s18',\n 'caformer_s36',\n 'cait_m36_384',\n 'cait_m48_448',\n 'cait_s24_224',\n 'cait_s24_384',\n 'cait_s36_384',\n 'cait_xs24_384',\n 'cait_xxs24_224',\n 'cait_xxs24_384',\n 'cait_xxs36_224',\n 'cait_xxs36_384',\n 'coat_lite_medium',\n 'coat_lite_medium_384',\n 'coat_lite_mini',\n 'coat_lite_small',\n 'coat_lite_tiny',\n 'coat_mini',\n 'coat_small',\n 'coat_tiny',\n 'coatnet_0_224',\n 'coatnet_0_rw_224',\n 'coatnet_1_224',\n 'coatnet_1_rw_224',\n 'coatnet_2_224',\n 'coatnet_2_rw_224',\n 'coatnet_3_224',\n 'coatnet_3_rw_224',\n 'coatnet_4_224',\n 'coatnet_5_224',\n 'coatnet_bn_0_rw_224',\n 'coatnet_nano_cc_224',\n 'coatnet_nano_rw_224',\n 'coatnet_pico_rw_224',\n 'coatnet_rmlp_0_rw_224',\n 'coatnet_rmlp_1_rw2_224',\n 'coatnet_rmlp_1_rw_224',\n 'coatnet_rmlp_2_rw_224',\n 'coatnet_rmlp_2_rw_384',\n 'coatnet_rmlp_3_rw_224',\n 'coatnet_rmlp_nano_rw_224',\n 'coatnext_nano_rw_224',\n 'convformer_b36',\n 'convformer_m36',\n 'convformer_s18',\n 'convformer_s36',\n 'convmixer_768_32',\n 'convmixer_1024_20_ks9_p14',\n 'convmixer_1536_20',\n 'convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny',\n 'cs3darknet_focus_l',\n 'cs3darknet_focus_m',\n 'cs3darknet_focus_s',\n 'cs3darknet_focus_x',\n 'cs3darknet_l',\n 'cs3darknet_m',\n 'cs3darknet_s',\n 'cs3darknet_x',\n 'cs3edgenet_x',\n 'cs3se_edgenet_x',\n 'cs3sedarknet_l',\n 'cs3sedarknet_x',\n 'cs3sedarknet_xdw',\n 'cspdarknet53',\n 'cspresnet50',\n 'cspresnet50d',\n 'cspresnet50w',\n 'cspresnext50',\n 'darknet17',\n 'darknet21',\n 'darknet53',\n 'darknetaa53',\n 'deit3_base_patch16_224',\n 'deit3_base_patch16_384',\n 'deit3_huge_patch14_224',\n 'deit3_large_patch16_224',\n 'deit3_large_patch16_384',\n 'deit3_medium_patch16_224',\n 'deit3_small_patch16_224',\n 'deit3_small_patch16_384',\n 'deit_base_distilled_patch16_224',\n 'deit_base_distilled_patch16_384',\n 'deit_base_patch16_224',\n 'deit_base_patch16_384',\n 'deit_small_distilled_patch16_224',\n 'deit_small_patch16_224',\n 'deit_tiny_distilled_patch16_224',\n 'deit_tiny_patch16_224',\n 'densenet121',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'densenet264d',\n 'densenetblur121d',\n 'dla34',\n 'dla46_c',\n 'dla46x_c',\n 'dla60',\n 'dla60_res2net',\n 'dla60_res2next',\n 'dla60x',\n 'dla60x_c',\n 'dla102',\n 'dla102x',\n 'dla102x2',\n 'dla169',\n 'dm_nfnet_f0',\n 'dm_nfnet_f1',\n 'dm_nfnet_f2',\n 'dm_nfnet_f3',\n 'dm_nfnet_f4',\n 'dm_nfnet_f5',\n 'dm_nfnet_f6',\n 'dpn48b',\n 'dpn68',\n 'dpn68b',\n 'dpn92',\n 'dpn98',\n 'dpn107',\n 'dpn131',\n 'eca_botnext26ts_256',\n 'eca_halonext26ts',\n 'eca_nfnet_l0',\n 'eca_nfnet_l1',\n 'eca_nfnet_l2',\n 'eca_nfnet_l3',\n 'eca_resnet33ts',\n 'eca_resnext26ts',\n 'eca_vovnet39b',\n 'ecaresnet26t',\n 'ecaresnet50d',\n 'ecaresnet50d_pruned',\n 'ecaresnet50t',\n 'ecaresnet101d',\n 'ecaresnet101d_pruned',\n 'ecaresnet200d',\n 'ecaresnet269d',\n 'ecaresnetlight',\n 'ecaresnext26t_32x4d',\n 'ecaresnext50t_32x4d',\n 'edgenext_base',\n 'edgenext_small',\n 'edgenext_small_rw',\n 'edgenext_x_small',\n 'edgenext_xx_small',\n 'efficientformer_l1',\n 'efficientformer_l3',\n 'efficientformer_l7',\n 'efficientformerv2_l',\n 'efficientformerv2_s0',\n 'efficientformerv2_s1',\n 'efficientformerv2_s2',\n 'efficientnet_b0',\n 'efficientnet_b0_g8_gn',\n 'efficientnet_b0_g16_evos',\n 'efficientnet_b0_gn',\n 'efficientnet_b1',\n 'efficientnet_b1_pruned',\n 'efficientnet_b2',\n 'efficientnet_b2_pruned',\n 'efficientnet_b3',\n 'efficientnet_b3_g8_gn',\n 'efficientnet_b3_gn',\n 'efficientnet_b3_pruned',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_b8',\n 'efficientnet_cc_b0_4e',\n 'efficientnet_cc_b0_8e',\n 'efficientnet_cc_b1_8e',\n 'efficientnet_el',\n 'efficientnet_el_pruned',\n 'efficientnet_em',\n 'efficientnet_es',\n 'efficientnet_es_pruned',\n 'efficientnet_l2',\n 'efficientnet_lite0',\n 'efficientnet_lite1',\n 'efficientnet_lite2',\n 'efficientnet_lite3',\n 'efficientnet_lite4',\n 'efficientnetv2_l',\n 'efficientnetv2_m',\n 'efficientnetv2_rw_m',\n 'efficientnetv2_rw_s',\n 'efficientnetv2_rw_t',\n 'efficientnetv2_s',\n 'efficientnetv2_xl',\n 'ese_vovnet19b_dw',\n 'ese_vovnet19b_slim',\n 'ese_vovnet19b_slim_dw',\n 'ese_vovnet39b',\n 'ese_vovnet39b_evos',\n 'ese_vovnet57b',\n 'ese_vovnet99b',\n 'eva02_base_patch14_224',\n 'eva02_base_patch14_448',\n 'eva02_base_patch16_clip_224',\n 'eva02_enormous_patch14_clip_224',\n 'eva02_large_patch14_224',\n 'eva02_large_patch14_448',\n 'eva02_large_patch14_clip_224',\n 'eva02_large_patch14_clip_336',\n 'eva02_small_patch14_224',\n 'eva02_small_patch14_336',\n 'eva02_tiny_patch14_224',\n 'eva02_tiny_patch14_336',\n 'eva_giant_patch14_224',\n 'eva_giant_patch14_336',\n 'eva_giant_patch14_560',\n 'eva_giant_patch14_clip_224',\n 'eva_large_patch14_196',\n 'eva_large_patch14_336',\n 'fbnetc_100',\n 'fbnetv3_b',\n 'fbnetv3_d',\n 'fbnetv3_g',\n 'focalnet_base_lrf',\n 'focalnet_base_srf',\n 'focalnet_huge_fl3',\n 'focalnet_huge_fl4',\n 'focalnet_large_fl3',\n 'focalnet_large_fl4',\n 'focalnet_small_lrf',\n 'focalnet_small_srf',\n 'focalnet_tiny_lrf',\n 'focalnet_tiny_srf',\n 'focalnet_xlarge_fl3',\n 'focalnet_xlarge_fl4',\n 'gc_efficientnetv2_rw_t',\n 'gcresnet33ts',\n 'gcresnet50t',\n 'gcresnext26ts',\n 'gcresnext50ts',\n 'gernet_l',\n 'gernet_m',\n 'gernet_s',\n 'ghostnet_050',\n 'ghostnet_100',\n 'ghostnet_130',\n 'ghostnetv2_100',\n 'ghostnetv2_130',\n 'ghostnetv2_160',\n 'gmixer_12_224',\n 'gmixer_24_224',\n 'gmlp_b16_224',\n 'gmlp_s16_224',\n 'gmlp_ti16_224',\n 'halo2botnet50ts_256',\n 'halonet26t',\n 'halonet50ts',\n 'halonet_h1',\n 'haloregnetz_b',\n 'hardcorenas_a',\n 'hardcorenas_b',\n 'hardcorenas_c',\n 'hardcorenas_d',\n 'hardcorenas_e',\n 'hardcorenas_f',\n 'hgnet_base',\n 'hgnet_small',\n 'hgnet_tiny',\n 'hgnetv2_b0',\n 'hgnetv2_b1',\n 'hgnetv2_b2',\n 'hgnetv2_b3',\n 'hgnetv2_b4',\n 'hgnetv2_b5',\n 'hgnetv2_b6',\n 'hrnet_w18',\n 'hrnet_w18_small',\n 'hrnet_w18_small_v2',\n 'hrnet_w18_ssld',\n 'hrnet_w30',\n 'hrnet_w32',\n 'hrnet_w40',\n 'hrnet_w44',\n 'hrnet_w48',\n 'hrnet_w48_ssld',\n 'hrnet_w64',\n 'inception_next_base',\n 'inception_next_small',\n 'inception_next_tiny',\n 'inception_resnet_v2',\n 'inception_v3',\n 'inception_v4',\n 'lambda_resnet26rpt_256',\n 'lambda_resnet26t',\n 'lambda_resnet50ts',\n 'lamhalobotnet50ts_256',\n 'lcnet_035',\n 'lcnet_050',\n 'lcnet_075',\n 'lcnet_100',\n 'lcnet_150',\n 'legacy_senet154',\n 'legacy_seresnet18',\n 'legacy_seresnet34',\n 'legacy_seresnet50',\n 'legacy_seresnet101',\n 'legacy_seresnet152',\n 'legacy_seresnext26_32x4d',\n 'legacy_seresnext50_32x4d',\n 'legacy_seresnext101_32x4d',\n 'legacy_xception',\n 'mixer_b16_224',\n 'mixer_b32_224',\n 'mixer_l16_224',\n 'mixer_l32_224',\n 'mixer_s16_224',\n 'mixer_s32_224',\n 'mixnet_l',\n 'mixnet_m',\n 'mixnet_s',\n 'mixnet_xl',\n 'mixnet_xxl',\n 'mnasnet_050',\n 'mnasnet_075',\n 'mnasnet_100',\n 'mnasnet_140',\n 'mnasnet_small',\n 'mobilenetv2_035',\n 'mobilenetv2_050',\n 'mobilenetv2_075',\n 'mobilenetv2_100',\n 'mobilenetv2_110d',\n 'mobilenetv2_120d',\n 'mobilenetv2_140',\n 'mobilenetv3_large_075',\n 'mobilenetv3_large_100',\n 'mobilenetv3_rw',\n 'mobilenetv3_small_050',\n 'mobilenetv3_small_075',\n 'mobilenetv3_small_100',\n 'mobileone_s0',\n 'mobileone_s1',\n 'mobileone_s2',\n 'mobileone_s3',\n 'mobileone_s4',\n 'nasnetalarge',\n 'nest_base',\n 'nest_base_jx',\n 'nest_small',\n 'nest_small_jx',\n 'nest_tiny',\n 'nest_tiny_jx',\n 'nf_ecaresnet26',\n 'nf_ecaresnet50',\n 'nf_ecaresnet101',\n 'nf_regnet_b0',\n 'nf_regnet_b1',\n 'nf_regnet_b2',\n 'nf_regnet_b3',\n 'nf_regnet_b4',\n 'nf_regnet_b5',\n 'nf_resnet26',\n 'nf_resnet50',\n 'nf_resnet101',\n 'nf_seresnet26',\n 'nf_seresnet50',\n 'nf_seresnet101',\n 'nfnet_f0',\n 'nfnet_f1',\n 'nfnet_f2',\n 'nfnet_f3',\n 'nfnet_f4',\n 'nfnet_f5',\n 'nfnet_f6',\n 'nfnet_f7',\n 'nfnet_l0',\n 'pit_b_224',\n 'pit_b_distilled_224',\n 'pit_s_224',\n 'pit_s_distilled_224',\n 'pit_ti_224',\n 'pit_ti_distilled_224',\n 'pit_xs_224',\n 'pit_xs_distilled_224',\n 'pnasnet5large',\n 'poolformer_m36',\n 'poolformer_m48',\n 'poolformer_s12',\n 'poolformer_s24',\n 'poolformer_s36',\n 'poolformerv2_m36',\n 'poolformerv2_m48',\n 'poolformerv2_s12',\n 'poolformerv2_s24',\n 'poolformerv2_s36',\n 'pvt_v2_b0',\n 'pvt_v2_b1',\n 'pvt_v2_b2',\n 'pvt_v2_b2_li',\n 'pvt_v2_b3',\n 'pvt_v2_b4',\n 'pvt_v2_b5',\n 'regnetv_040',\n 'regnetv_064',\n 'regnetx_002',\n 'regnetx_004',\n 'regnetx_004_tv',\n 'regnetx_006',\n 'regnetx_008',\n 'regnetx_016',\n 'regnetx_032',\n 'regnetx_040',\n 'regnetx_064',\n 'regnetx_080',\n 'regnetx_120',\n 'regnetx_160',\n 'regnetx_320',\n 'regnety_002',\n 'regnety_004',\n 'regnety_006',\n 'regnety_008',\n 'regnety_008_tv',\n 'regnety_016',\n 'regnety_032',\n 'regnety_040',\n 'regnety_040_sgn',\n 'regnety_064',\n 'regnety_080',\n 'regnety_080_tv',\n 'regnety_120',\n 'regnety_160',\n 'regnety_320',\n 'regnety_640',\n 'regnety_1280',\n 'regnety_2560',\n 'regnetz_005',\n 'regnetz_040',\n 'regnetz_040_h',\n 'regnetz_b16',\n 'regnetz_b16_evos',\n 'regnetz_c16',\n 'regnetz_c16_evos',\n 'regnetz_d8',\n 'regnetz_d8_evos',\n 'regnetz_d32',\n 'regnetz_e8',\n 'repghostnet_050',\n 'repghostnet_058',\n 'repghostnet_080',\n 'repghostnet_100',\n 'repghostnet_111',\n 'repghostnet_130',\n 'repghostnet_150',\n 'repghostnet_200',\n 'repvgg_a0',\n 'repvgg_a1',\n 'repvgg_a2',\n 'repvgg_b0',\n 'repvgg_b1',\n 'repvgg_b1g4',\n 'repvgg_b2',\n 'repvgg_b2g4',\n 'repvgg_b3',\n 'repvgg_b3g4',\n 'repvgg_d2se',\n 'res2net50_14w_8s',\n 'res2net50_26w_4s',\n 'res2net50_26w_6s',\n 'res2net50_26w_8s',\n 'res2net50_48w_2s',\n 'res2net50d',\n 'res2net101_26w_4s',\n 'res2net101d',\n 'res2next50',\n 'resmlp_12_224',\n 'resmlp_24_224',\n 'resmlp_36_224',\n 'resmlp_big_24_224',\n 'resnest14d',\n 'resnest26d',\n 'resnest50d',\n 'resnest50d_1s4x24d',\n 'resnest50d_4s2x40d',\n 'resnest101e',\n 'resnest200e',\n 'resnest269e',\n 'resnet10t',\n 'resnet14t',\n 'resnet18',\n 'resnet18d',\n 'resnet26',\n 'resnet26d',\n 'resnet26t',\n 'resnet32ts',\n 'resnet33ts',\n 'resnet34',\n 'resnet34d',\n 'resnet50',\n 'resnet50_gn',\n 'resnet50c',\n 'resnet50d',\n 'resnet50s',\n 'resnet50t',\n 'resnet51q',\n 'resnet61q',\n 'resnet101',\n 'resnet101c',\n 'resnet101d',\n 'resnet101s',\n 'resnet152',\n 'resnet152c',\n 'resnet152d',\n 'resnet152s',\n 'resnet200',\n 'resnet200d',\n 'resnetaa34d',\n 'resnetaa50',\n 'resnetaa50d',\n 'resnetaa101d',\n 'resnetblur18',\n 'resnetblur50',\n 'resnetblur50d',\n 'resnetblur101d',\n 'resnetrs50',\n 'resnetrs101',\n 'resnetrs152',\n 'resnetrs200',\n 'resnetrs270',\n 'resnetrs350',\n 'resnetrs420',\n 'resnetv2_50',\n 'resnetv2_50d',\n 'resnetv2_50d_evos',\n 'resnetv2_50d_frn',\n 'resnetv2_50d_gn',\n 'resnetv2_50t',\n 'resnetv2_50x1_bit',\n 'resnetv2_50x3_bit',\n 'resnetv2_101',\n 'resnetv2_101d',\n 'resnetv2_101x1_bit',\n 'resnetv2_101x3_bit',\n 'resnetv2_152',\n 'resnetv2_152d',\n 'resnetv2_152x2_bit',\n 'resnetv2_152x4_bit',\n 'resnext26ts',\n 'resnext50_32x4d',\n 'resnext50d_32x4d',\n 'resnext101_32x4d',\n 'resnext101_32x8d',\n 'resnext101_32x16d',\n 'resnext101_32x32d',\n 'resnext101_64x4d',\n 'rexnet_100',\n 'rexnet_130',\n 'rexnet_150',\n 'rexnet_200',\n 'rexnet_300',\n 'rexnetr_100',\n 'rexnetr_130',\n 'rexnetr_150',\n 'rexnetr_200',\n 'rexnetr_300',\n 'sebotnet33ts_256',\n 'sedarknet21',\n 'sehalonet33ts',\n 'selecsls42',\n 'selecsls42b',\n 'selecsls60',\n 'selecsls60b',\n 'selecsls84',\n 'semnasnet_050',\n 'semnasnet_075',\n 'semnasnet_100',\n 'semnasnet_140',\n 'senet154',\n 'sequencer2d_l',\n 'sequencer2d_m',\n 'sequencer2d_s',\n 'seresnet18',\n 'seresnet33ts',\n 'seresnet34',\n 'seresnet50',\n 'seresnet50t',\n 'seresnet101',\n 'seresnet152',\n 'seresnet152d',\n 'seresnet200d',\n 'seresnet269d',\n 'seresnetaa50d',\n 'seresnext26d_32x4d',\n 'seresnext26t_32x4d',\n 'seresnext26ts',\n 'seresnext50_32x4d',\n 'seresnext101_32x4d',\n 'seresnext101_32x8d',\n 'seresnext101_64x4d',\n 'seresnext101d_32x8d',\n 'seresnextaa101d_32x8d',\n 'seresnextaa201d_32x8d',\n 'skresnet18',\n 'skresnet34',\n 'skresnet50',\n 'skresnet50d',\n 'skresnext50_32x4d',\n 'spnasnet_100',\n 'swin_base_patch4_window7_224',\n 'swin_base_patch4_window12_384',\n 'swin_large_patch4_window7_224',\n 'swin_large_patch4_window12_384',\n 'swin_s3_base_224',\n 'swin_s3_small_224',\n 'swin_s3_tiny_224',\n 'swin_small_patch4_window7_224',\n 'swin_tiny_patch4_window7_224',\n 'swinv2_base_window8_256',\n 'swinv2_base_window12_192',\n 'swinv2_base_window12to16_192to256',\n 'swinv2_base_window12to24_192to384',\n 'swinv2_base_window16_256',\n 'swinv2_cr_base_224',\n 'swinv2_cr_base_384',\n 'swinv2_cr_base_ns_224',\n 'swinv2_cr_giant_224',\n 'swinv2_cr_giant_384',\n 'swinv2_cr_huge_224',\n 'swinv2_cr_huge_384',\n 'swinv2_cr_large_224',\n 'swinv2_cr_large_384',\n 'swinv2_cr_small_224',\n 'swinv2_cr_small_384',\n 'swinv2_cr_small_ns_224',\n 'swinv2_cr_small_ns_256',\n 'swinv2_cr_tiny_224',\n 'swinv2_cr_tiny_384',\n 'swinv2_cr_tiny_ns_224',\n 'swinv2_large_window12_192',\n 'swinv2_large_window12to16_192to256',\n 'swinv2_large_window12to24_192to384',\n 'swinv2_small_window8_256',\n 'swinv2_small_window16_256',\n 'swinv2_tiny_window8_256',\n 'swinv2_tiny_window16_256',\n 'tf_efficientnet_b0',\n 'tf_efficientnet_b1',\n 'tf_efficientnet_b2',\n 'tf_efficientnet_b3',\n 'tf_efficientnet_b4',\n 'tf_efficientnet_b5',\n 'tf_efficientnet_b6',\n 'tf_efficientnet_b7',\n 'tf_efficientnet_b8',\n 'tf_efficientnet_cc_b0_4e',\n 'tf_efficientnet_cc_b0_8e',\n 'tf_efficientnet_cc_b1_8e',\n 'tf_efficientnet_el',\n 'tf_efficientnet_em',\n 'tf_efficientnet_es',\n 'tf_efficientnet_l2',\n 'tf_efficientnet_lite0',\n 'tf_efficientnet_lite1',\n 'tf_efficientnet_lite2',\n 'tf_efficientnet_lite3',\n 'tf_efficientnet_lite4',\n 'tf_efficientnetv2_b0',\n 'tf_efficientnetv2_b1',\n 'tf_efficientnetv2_b2',\n 'tf_efficientnetv2_b3',\n 'tf_efficientnetv2_l',\n 'tf_efficientnetv2_m',\n 'tf_efficientnetv2_s',\n 'tf_efficientnetv2_xl',\n 'tf_mixnet_l',\n 'tf_mixnet_m',\n 'tf_mixnet_s',\n 'tf_mobilenetv3_large_075',\n 'tf_mobilenetv3_large_100',\n 'tf_mobilenetv3_large_minimal_100',\n 'tf_mobilenetv3_small_075',\n 'tf_mobilenetv3_small_100',\n 'tf_mobilenetv3_small_minimal_100',\n 'tinynet_a',\n 'tinynet_b',\n 'tinynet_c',\n 'tinynet_d',\n 'tinynet_e',\n 'tnt_b_patch16_224',\n 'tnt_s_patch16_224',\n 'tresnet_l',\n 'tresnet_m',\n 'tresnet_v2_l',\n 'tresnet_xl',\n 'twins_pcpvt_base',\n 'twins_pcpvt_large',\n 'twins_pcpvt_small',\n 'twins_svt_base',\n 'twins_svt_large',\n 'twins_svt_small',\n 'vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn',\n 'visformer_small',\n 'visformer_tiny',\n 'volo_d1_224',\n 'volo_d1_384',\n 'volo_d2_224',\n 'volo_d2_384',\n 'volo_d3_224',\n 'volo_d3_448',\n 'volo_d4_224',\n 'volo_d4_448',\n 'volo_d5_224',\n 'volo_d5_448',\n 'volo_d5_512',\n 'vovnet39a',\n 'vovnet57a',\n 'wide_resnet50_2',\n 'wide_resnet101_2',\n 'xception41',\n 'xception41p',\n 'xception65',\n 'xception65p',\n 'xception71',\n 'xcit_large_24_p8_224',\n 'xcit_large_24_p8_384',\n 'xcit_large_24_p16_224',\n 'xcit_large_24_p16_384',\n 'xcit_medium_24_p8_224',\n 'xcit_medium_24_p8_384',\n 'xcit_medium_24_p16_224',\n 'xcit_medium_24_p16_384',\n 'xcit_nano_12_p8_224',\n 'xcit_nano_12_p8_384',\n 'xcit_nano_12_p16_224',\n 'xcit_nano_12_p16_384',\n 'xcit_small_12_p8_224',\n 'xcit_small_12_p8_384',\n 'xcit_small_12_p16_224',\n 'xcit_small_12_p16_384',\n 'xcit_small_24_p8_224',\n 'xcit_small_24_p8_384',\n 'xcit_small_24_p16_224',\n 'xcit_small_24_p16_384',\n 'xcit_tiny_12_p8_224',\n 'xcit_tiny_12_p8_384',\n 'xcit_tiny_12_p16_224',\n 'xcit_tiny_12_p16_384',\n 'xcit_tiny_24_p8_224',\n 'xcit_tiny_24_p8_384',\n 'xcit_tiny_24_p16_224',\n 'xcit_tiny_24_p16_384']\n\n\n\nm = timm.create_model('seresnet18')\n\n\nm\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act1): ReLU(inplace=True)\n      (aa): Identity()\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (se): SEModule(\n        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n        (bn): Identity()\n        (act): ReLU(inplace=True)\n        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n        (gate): Sigmoid()\n      )\n      (act2): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\nfrom fasterai.prune.all import *\n\n\ndummy_input = torch.randn(16, 3, 224, 224)\n\n\nm = timm.create_model('seresnet18')\n\n\nbenchmark(m, dummy_input)\n\nInference time CPU (ms/image):6.694 ms +/- 0.092 ms\nFPS CPU: 149.3904126039974\nInference time GPU (ms/image): 0.267 ms +/- 0.003 ms\nFPS GPU: 3743.6464639069623\nNombre de paramètres: 11.780 M\nTaille du modèle: 44.936 MiB\nNombre de MACs: 1817.400 M\n\n\n(107.10191986960126, 1.479711068861181, 4.27390784740448, 0.05194234267945458)\n\n\n\npr = Pruner(m, 'local', large_final, layer_type=[nn.Conv2d])\npr.prune_model(30, round_to=8)\n\n\nbenchmark(m, dummy_input)\n\nInference time CPU (ms/image):3.316 ms +/- 0.040 ms\nFPS CPU: 301.55608187204695\nInference time GPU (ms/image): 0.183 ms +/- 0.003 ms\nFPS GPU: 5469.600455484674\nNombre de paramètres: 5.657 M\nTaille du modèle: 21.580 MiB\nNombre de MACs: 836.963 M\n\n\n(53.058124050003244,\n 0.6349589479868257,\n 2.9252593731880188,\n 0.04034922064140752)\n\n\n\nimport abc\nimport torch\nimport torch.nn as nn\n\nimport typing\n\nfrom torch_pruning import function\nfrom torch_pruning.dependency import Group\n\nclass Importance(abc.ABC):\n    \"\"\" Estimate the importance of a tp.Dependency.Group, and return an 1-D per-channel importance score.\n\n        It should accept a group as inputs, and return a 1-D tensor with the same length as the number of channels.\n        All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups.\n\n        Example:\n            ```python\n            DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224)) \n            group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )    \n            scorer = MagnitudeImportance()    \n            imp_score = scorer(group)    \n            #imp_score is a 1-D tensor with length 3 for channels [2, 6, 9]  \n            min_score = imp_score.min() \n            ``` \n    \"\"\"\n    @abc.abstractclassmethod\n    def __call__(self, group: Group) -&gt; torch.Tensor: \n        raise NotImplementedError\n\n\nclass GroupNormImportance(Importance):\n\n    def __init__(self, \n                 p: int=2, \n                 group_reduction: str=\"mean\", \n                 normalizer: str='mean', \n                 bias=False,\n                 target_types:list=[nn.modules.conv._ConvNd, nn.Linear, nn.modules.batchnorm._BatchNorm, nn.LayerNorm]):\n        self.p = p\n        self.group_reduction = group_reduction\n        self.normalizer = normalizer\n        self.target_types = target_types\n        self.bias = bias\n\n    def _lamp(self, scores): # Layer-adaptive Sparsity for the Magnitude-based Pruning\n        \"\"\"\n        Normalizing scheme for LAMP.\n        \"\"\"\n        # sort scores in an ascending order\n        sorted_scores,sorted_idx = scores.view(-1).sort(descending=False)\n        # compute cumulative sum\n        scores_cumsum_temp = sorted_scores.cumsum(dim=0)\n        scores_cumsum = torch.zeros(scores_cumsum_temp.shape,device=scores.device)\n        scores_cumsum[1:] = scores_cumsum_temp[:len(scores_cumsum_temp)-1]\n        # normalize by cumulative sum\n        sorted_scores /= (scores.sum() - scores_cumsum)\n        # tidy up and output\n        new_scores = torch.zeros(scores_cumsum.shape,device=scores.device)\n        new_scores[sorted_idx] = sorted_scores\n        \n        return new_scores.view(scores.shape)\n    \n    def _normalize(self, group_importance, normalizer):\n        if normalizer is None:\n            return group_importance\n        elif isinstance(normalizer, typing.Callable):\n            return normalizer(group_importance)\n        elif normalizer == \"sum\":\n            return group_importance / group_importance.sum()\n        elif normalizer == \"standarization\":\n            return (group_importance - group_importance.min()) / (group_importance.max() - group_importance.min()+1e-8)\n        elif normalizer == \"mean\":\n            return group_importance / group_importance.mean()\n        elif normalizer == \"max\":\n            return group_importance / group_importance.max()\n        elif normalizer == 'gaussian':\n            return (group_importance - group_importance.mean()) / (group_importance.std()+1e-8)\n        elif normalizer.startswith('sentinel'): # normalize the score with the k-th smallest element. e.g. sentinel_0.5 means median normalization\n            sentinel = float(normalizer.split('_')[1]) * len(group_importance)\n            sentinel = torch.argsort(group_importance, dim=0, descending=False)[int(sentinel)]\n            return group_importance / (group_importance[sentinel]+1e-8)\n        elif normalizer=='lamp':\n            return self._lamp(group_importance)\n        else:\n            raise NotImplementedError\n\n    def _reduce(self, group_imp: typing.List[torch.Tensor], group_idxs: typing.List[typing.List[int]]):\n        if len(group_imp) == 0: return group_imp\n        if self.group_reduction == 'prod':\n            reduced_imp = torch.ones_like(group_imp[0])\n        elif self.group_reduction == 'max':\n            reduced_imp = torch.ones_like(group_imp[0]) * -99999\n        else:\n            reduced_imp = torch.zeros_like(group_imp[0])\n\n        for i, (imp, root_idxs) in enumerate(zip(group_imp, group_idxs)):\n            imp = imp.to(reduced_imp.device)\n            if self.group_reduction == \"sum\" or self.group_reduction == \"mean\":\n                reduced_imp.scatter_add_(0, torch.tensor(root_idxs, device=imp.device), imp) # accumulated importance\n            elif self.group_reduction == \"max\": # keep the max importance\n                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n                selected_imp = torch.maximum(input=selected_imp, other=imp)\n                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n            elif self.group_reduction == \"prod\": # product of importance\n                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n                torch.mul(selected_imp, imp, out=selected_imp)\n                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n            elif self.group_reduction == 'first':\n                if i == 0:\n                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n            elif self.group_reduction == 'gate':\n                if i == len(group_imp)-1:\n                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n            elif self.group_reduction is None:\n                reduced_imp = torch.stack(group_imp, dim=0) # no reduction\n            else:\n                raise NotImplementedError\n        \n        if self.group_reduction == \"mean\":\n            reduced_imp /= len(group_imp)\n        return reduced_imp\n    \n    @torch.no_grad()\n    def __call__(self, group: Group):\n        group_imp = []\n        group_idxs = []\n        # Iterate over all groups and estimate group importance\n        for i, (dep, idxs) in enumerate(group):\n            layer = dep.layer\n            prune_fn = dep.pruning_fn\n            root_idxs = group[i].root_idxs\n            if not isinstance(layer, tuple(self.target_types)):\n                continue\n            ####################\n            # Conv/Linear Output\n            ####################\n            if prune_fn in [\n                function.prune_conv_out_channels,\n                function.prune_linear_out_channels,\n            ]:\n                if hasattr(layer, \"transposed\") and layer.transposed:\n                    w = layer.weight.data.transpose(1, 0)[idxs].flatten(1)\n                else:\n                    w = layer.weight.data[idxs].flatten(1)\n                #local_imp = w.abs().pow(self.p).sum(1)\n                local_imp = w.abs().pow(self.p).mean(1)\n                group_imp.append(local_imp)\n                group_idxs.append(root_idxs)\n\n                if self.bias and layer.bias is not None:\n                    local_imp = layer.bias.data[idxs].abs().pow(self.p)\n                    group_imp.append(local_imp)\n                    group_idxs.append(root_idxs)\n\n            ####################\n            # Conv/Linear Input\n            ####################\n            elif prune_fn in [\n                function.prune_conv_in_channels,\n                function.prune_linear_in_channels,\n            ]:\n                if hasattr(layer, \"transposed\") and layer.transposed:\n                    w = (layer.weight.data).flatten(1)\n                else:\n                    w = (layer.weight.data).transpose(0, 1).flatten(1)\n                #local_imp = w.abs().pow(self.p).sum(1)\n                local_imp = w.abs().pow(self.p).mean(1)\n\n                # repeat importance for group convolutions\n                if prune_fn == function.prune_conv_in_channels and layer.groups != layer.in_channels and layer.groups != 1:\n                    local_imp = local_imp.repeat(layer.groups)\n                \n                local_imp = local_imp[idxs]\n                group_imp.append(local_imp)\n                group_idxs.append(root_idxs)\n\n            ####################\n            # BatchNorm\n            ####################\n            elif prune_fn == function.prune_batchnorm_out_channels:\n                # regularize BN\n                if layer.affine:\n                    w = layer.weight.data[idxs]\n                    local_imp = w.abs().pow(self.p)\n                    group_imp.append(local_imp)\n                    group_idxs.append(root_idxs)\n\n                    if self.bias and layer.bias is not None:\n                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n                        group_imp.append(local_imp)\n                        group_idxs.append(root_idxs)\n            ####################\n            # LayerNorm\n            ####################\n            elif prune_fn == function.prune_layernorm_out_channels:\n\n                if layer.elementwise_affine:\n                    w = layer.weight.data[idxs]\n                    local_imp = w.abs().pow(self.p)\n                    group_imp.append(local_imp)\n                    group_idxs.append(root_idxs)\n\n                    if self.bias and layer.bias is not None:\n                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n                        group_imp.append(local_imp)\n                        group_idxs.append(root_idxs)\n\n        if len(group_imp) == 0: # skip groups without parameterized layers\n            return None\n\n        group_imp = self._reduce(group_imp, group_idxs)\n        group_imp = self._normalize(group_imp, self.normalizer)\n        return group_imp"
  },
  {
    "objectID": "tutorials/prune/prune_callback.html",
    "href": "tutorials/prune/prune_callback.html",
    "title": "Prune Callback",
    "section": "",
    "text": "Let’s try our PruneCallback on the Pets dataset\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nWe’ll train a vanilla ResNet18 for 5 epochs to have an idea of the expected performance\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\nlearn.fit_one_cycle(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.612992\n0.329872\n0.860622\n00:02\n\n\n\n\n\n\nbase_macs, base_params = tp.utils.count_ops_and_params(learn.model, torch.randn(1,3,224,224).to(default_device()))\n\nLet’s now try adding to remove some filters in our model\nWe’ll set the sparsity to 50 (i.e. remove 50% of filters), the context to global (i.e. we remove filters from anywhere in the network), the criteria to large_final (i.e. keep the highest value filters and the schedule to one_cycle (i.e. follow the One-Cycle schedule to remove filters along training).\n\npruner = Pruner(\nlearn.model,\ncriteria=large_final,\npruning_ratio=40, \ncontext='global',\niterative_steps=, \nschedule=one_cycle._scheduler,\n)\n\n\npr_cb = PruneCallback(pruning_ratio=40, context='global', criteria=large_final, schedule=one_cycle)\nlearn.fit_one_cycle(10, cbs=pr_cb)\n\n920\nIgnoring output layer: Linear(in_features=512, out_features=2, bias=False)\nTotal ignored layers: 1\n\n\n\n\n\n\n\n      \n      0.00% [0/10 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/24 00:00&lt;?]\n    \n\n\n\nKeyboardInterrupt\n\n\nKeyboardInterrupt\n\n\n\n\npruned_macs, pruned_params = tp.utils.count_ops_and_params(learn.model, torch.randn(1,3,224,224).to(default_device()))\n\nWe observe that our network has lost less than 1% of accuracy. But how much parameters have we removed and how much compute does that save ?\n\nprint(f'The pruned model has {pruned_macs/base_macs:.2f} the compute of original model')\n\nThe pruned model has 0.63 the compute of original model\n\n\n\nprint(f'The pruned model has {pruned_params/base_params:.2f} the parameters of original model')\n\nThe pruned model has 0.18 the parameters of original model\n\n\nSo at the price of a slight decrease in accuracy, we now have a model that is 5x smaller and requires 1.5x fewer compute.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html",
    "href": "tutorials/misc/bn_folding.html",
    "title": "BatchNorm Folding",
    "section": "",
    "text": "This is how to do it with fasterai !\n\nGet the data\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\nTrain the model\n\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(5)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.615641\n0.596630\n0.688092\n00:10\n\n\n1\n0.582679\n0.558671\n0.689445\n00:10\n\n\n2\n0.529308\n0.517995\n0.744926\n00:10\n\n\n3\n0.481804\n0.449941\n0.784168\n00:10\n\n\n4\n0.400030\n0.414093\n0.800406\n00:10\n\n\n\n\n\n\nFold !\n\n\nbn = BN_Folder()\nnew_model = bn.fold(learn.model)\n\nThe batch norm layers have been replaced by an Identity layer, and the weights of the convolutions have been modified accordingly.\n\nnew_model\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n  (bn1): Identity()\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\nWe can see that the new model possess fewer parameters\n\ncount_parameters(learn.model)\n\n11177538\n\n\n\ncount_parameters(new_model)\n\n11172738\n\n\nBut is also faster to run !\n\nx,y = dls.one_batch()\n\n\nlearn.model(x[0][None].cuda())\n\n5.59 ms ± 547 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nnew_model(x[0][None].cuda())\n\n4.14 ms ± 446 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nBut most importantly, has the exact same perfomance as before:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\n\n\nnew_learn.validate()\n\n\n\n\n(#2) [0.4140927791595459,0.8004059791564941]",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html",
    "href": "tutorials/sparse/schedules.html",
    "title": "Schedules",
    "section": "",
    "text": "Neural Network Pruning usually follows one of the next 3 schedules:\nIn fasterai, all those 3 schedules can be applied from the same callback. We’ll cover each below\nIn the SparsifyCallback, there are several parameters to ‘shape’ our pruning schedule: * start_sparsity: the initial sparsity of our model, generally kept at 0 as after initialization, our weights are generally non-zero. * end_sparsity: the target sparsity at the end of the training * start_epoch: we can decide to start pruning right from the beginning or let it train a bit before removing weights. * sched_func: this is where the general shape of the schedule is specified as it specifies how the sparsity evolves along the training. You can either use a schedule available in fastai our even coming with your own !\npath = untar_data(URLs.PETS)\n\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\nWe will first train a network without any pruning, which will serve as a baseline.\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.726983\n0.501899\n0.843031\n00:03\n\n\n1\n0.447103\n0.355498\n0.862652\n00:03\n\n\n2\n0.288626\n0.217356\n0.910690\n00:03\n\n\n3\n0.213532\n0.294699\n0.891746\n00:03\n\n\n4\n0.178062\n0.243345\n0.909337\n00:03\n\n\n5\n0.132118\n0.236394\n0.917456\n00:03\n\n\n6\n0.092771\n0.194155\n0.933018\n00:03\n\n\n7\n0.055006\n0.248453\n0.922192\n00:03\n\n\n8\n0.029332\n0.199173\n0.937754\n00:03\n\n\n9\n0.018201\n0.205575\n0.941813\n00:03",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#one-shot-pruning",
    "href": "tutorials/sparse/schedules.html#one-shot-pruning",
    "title": "Schedules",
    "section": "One-Shot Pruning",
    "text": "One-Shot Pruning\nThe simplest way to perform pruning is called One-Shot Pruning. It consists of the following three steps:\n\nYou first need to train a network\nYou then need to remove some weights (depending on your criteria, needs,…)\nYou fine-tune the remaining weights to recover from the loss of parameters.\n\nWith fasterai, this is really easy to do. Let’s illustrate it by an example:\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning. This training can be done independently from the pruning callback, or simulated by the start_epoch that will delay the pruning process.\nYou thus only need to create the Callback with the one_shot schedule and set the start_epoch argument, i.e. how many epochs you want to train your network before pruning it.\n\nsp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=one_shot)\n\nLet’s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of [90]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.528262\n0.357351\n0.837618\n00:04\n\n\n1\n0.363505\n0.513674\n0.774019\n00:03\n\n\n2\n0.250641\n0.268071\n0.893775\n00:03\n\n\n3\n0.211224\n0.268274\n0.896482\n00:04\n\n\n4\n0.203224\n0.264878\n0.885656\n00:03\n\n\n5\n0.236911\n0.248686\n0.893099\n00:04\n\n\n6\n0.178409\n0.239735\n0.903924\n00:04\n\n\n7\n0.128331\n0.258517\n0.896482\n00:04\n\n\n8\n0.110764\n0.218122\n0.912720\n00:04\n\n\n9\n0.077449\n0.279049\n0.907307\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [90.0]%\nSparsity at the end of epoch 5: [90.0]%\nSparsity at the end of epoch 6: [90.0]%\nSparsity at the end of epoch 7: [90.0]%\nSparsity at the end of epoch 8: [90.0]%\nSparsity at the end of epoch 9: [90.0]%\nFinal Sparsity: [90]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 2              Conv2d          9,408      8,467         90.00%\nLayer 8              Conv2d          36,864     33,177        90.00%\nLayer 11             Conv2d          36,864     33,177        90.00%\nLayer 14             Conv2d          36,864     33,177        90.00%\nLayer 17             Conv2d          36,864     33,177        90.00%\nLayer 21             Conv2d          73,728     66,355        90.00%\nLayer 24             Conv2d          147,456    132,710       90.00%\nLayer 27             Conv2d          8,192      7,372         89.99%\nLayer 30             Conv2d          147,456    132,710       90.00%\nLayer 33             Conv2d          147,456    132,710       90.00%\nLayer 37             Conv2d          294,912    265,420       90.00%\nLayer 40             Conv2d          589,824    530,841       90.00%\nLayer 43             Conv2d          32,768     29,491        90.00%\nLayer 46             Conv2d          589,824    530,841       90.00%\nLayer 49             Conv2d          589,824    530,841       90.00%\nLayer 53             Conv2d          1,179,648  1,061,683     90.00%\nLayer 56             Conv2d          2,359,296  2,123,366     90.00%\nLayer 59             Conv2d          131,072    117,964       90.00%\nLayer 62             Conv2d          2,359,296  2,123,366     90.00%\nLayer 65             Conv2d          2,359,296  2,123,366     90.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 10,050,211    90.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#iterative-pruning",
    "href": "tutorials/sparse/schedules.html#iterative-pruning",
    "title": "Schedules",
    "section": "Iterative Pruning",
    "text": "Iterative Pruning\nResearchers have come up with a better way to do pruning than pruning all the weigths in once (as in One-Shot Pruning). The idea is to perform several iterations of pruning and fine-tuning and is thus called Iterative Pruning.\n\nYou first need to train a network\nYou then need to remove a part of the weights weights (depending on your criteria, needs,…)\nYou fine-tune the remaining weights to recover from the loss of parameters.\nBack to step 2.\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning.\nYou only need to create the Callback with the iterative schedule and set the start_epoch argument, i.e. how many epochs you want to train your network before pruning it.\nThe iterative schedules has a n_stepsparameter, i.e. how many iterations of pruning/fine-tuning you want to perform. To modify its value, we can use the partial function like this:\niterative = partial(iterative, n_steps=5)\n\nsp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=iterative)\n\nLet’s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of [90]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.610821\n0.347474\n0.842355\n00:03\n\n\n1\n0.398858\n0.563407\n0.859269\n00:03\n\n\n2\n0.302287\n0.310804\n0.871448\n00:04\n\n\n3\n0.222971\n0.373315\n0.855886\n00:04\n\n\n4\n0.175162\n0.256725\n0.901894\n00:04\n\n\n5\n0.140398\n0.234549\n0.916779\n00:04\n\n\n6\n0.100763\n0.231317\n0.912720\n00:04\n\n\n7\n0.252507\n0.318962\n0.876861\n00:04\n\n\n8\n0.186143\n0.240475\n0.903924\n00:04\n\n\n9\n0.151650\n0.222071\n0.910014\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [30.0]%\nSparsity at the end of epoch 3: [30.0]%\nSparsity at the end of epoch 4: [60.0]%\nSparsity at the end of epoch 5: [60.0]%\nSparsity at the end of epoch 6: [60.0]%\nSparsity at the end of epoch 7: [90.0]%\nSparsity at the end of epoch 8: [90.0]%\nSparsity at the end of epoch 9: [90.0]%\nFinal Sparsity: [90.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 2              Conv2d          9,408      8,467         90.00%\nLayer 8              Conv2d          36,864     33,177        90.00%\nLayer 11             Conv2d          36,864     33,177        90.00%\nLayer 14             Conv2d          36,864     33,177        90.00%\nLayer 17             Conv2d          36,864     33,177        90.00%\nLayer 21             Conv2d          73,728     66,355        90.00%\nLayer 24             Conv2d          147,456    132,710       90.00%\nLayer 27             Conv2d          8,192      7,372         89.99%\nLayer 30             Conv2d          147,456    132,710       90.00%\nLayer 33             Conv2d          147,456    132,710       90.00%\nLayer 37             Conv2d          294,912    265,420       90.00%\nLayer 40             Conv2d          589,824    530,841       90.00%\nLayer 43             Conv2d          32,768     29,491        90.00%\nLayer 46             Conv2d          589,824    530,841       90.00%\nLayer 49             Conv2d          589,824    530,841       90.00%\nLayer 53             Conv2d          1,179,648  1,061,683     90.00%\nLayer 56             Conv2d          2,359,296  2,123,366     90.00%\nLayer 59             Conv2d          131,072    117,964       90.00%\nLayer 62             Conv2d          2,359,296  2,123,366     90.00%\nLayer 65             Conv2d          2,359,296  2,123,366     90.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 10,050,211    90.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#gradual-pruning",
    "href": "tutorials/sparse/schedules.html#gradual-pruning",
    "title": "Schedules",
    "section": "Gradual Pruning",
    "text": "Gradual Pruning\nHere is for example how to implement the Automated Gradual Pruning schedule.\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=agp)\n\nLet’s start pruning after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of [90]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.606718\n0.444957\n0.817321\n00:03\n\n\n1\n0.404780\n0.325938\n0.863329\n00:03\n\n\n2\n0.308993\n0.298605\n0.873478\n00:04\n\n\n3\n0.232805\n0.356629\n0.865359\n00:04\n\n\n4\n0.184159\n0.285140\n0.891069\n00:04\n\n\n5\n0.176350\n0.300211\n0.882273\n00:04\n\n\n6\n0.163846\n0.267521\n0.891069\n00:04\n\n\n7\n0.158822\n0.218856\n0.909337\n00:04\n\n\n8\n0.115987\n0.269380\n0.909337\n00:04\n\n\n9\n0.079022\n0.240360\n0.915426\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [29.71]%\nSparsity at the end of epoch 3: [52.03]%\nSparsity at the end of epoch 4: [68.03]%\nSparsity at the end of epoch 5: [78.75]%\nSparsity at the end of epoch 6: [85.25]%\nSparsity at the end of epoch 7: [88.59]%\nSparsity at the end of epoch 8: [89.82]%\nSparsity at the end of epoch 9: [90.0]%\nFinal Sparsity: [90.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 2              Conv2d          9,408      8,467         90.00%\nLayer 8              Conv2d          36,864     33,177        90.00%\nLayer 11             Conv2d          36,864     33,177        90.00%\nLayer 14             Conv2d          36,864     33,177        90.00%\nLayer 17             Conv2d          36,864     33,177        90.00%\nLayer 21             Conv2d          73,728     66,355        90.00%\nLayer 24             Conv2d          147,456    132,710       90.00%\nLayer 27             Conv2d          8,192      7,372         89.99%\nLayer 30             Conv2d          147,456    132,710       90.00%\nLayer 33             Conv2d          147,456    132,710       90.00%\nLayer 37             Conv2d          294,912    265,420       90.00%\nLayer 40             Conv2d          589,824    530,841       90.00%\nLayer 43             Conv2d          32,768     29,491        90.00%\nLayer 46             Conv2d          589,824    530,841       90.00%\nLayer 49             Conv2d          589,824    530,841       90.00%\nLayer 53             Conv2d          1,179,648  1,061,683     90.00%\nLayer 56             Conv2d          2,359,296  2,123,366     90.00%\nLayer 59             Conv2d          131,072    117,964       90.00%\nLayer 62             Conv2d          2,359,296  2,123,366     90.00%\nLayer 65             Conv2d          2,359,296  2,123,366     90.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 10,050,211    90.00%\n\n\nEven though they are often considered as different pruning methods, those 3 schedules can be captured by the same Callback. Here is how the sparsity in the network evolves for those methods;\nLet’s take an example here. Let’s say that we want to train our network for 3 epochs without pruning and then 7 epochs with pruning.\nThen this is what our different pruning schedules will look like:\n\n\n\n\n\n\n\n\n\nYou can also come up with your own pruning schedule !",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html",
    "href": "tutorials/sparse/lottery_ticket.html",
    "title": "Lottery Ticket Hypothesis",
    "section": "",
    "text": "The Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B &gt; a_A\\), in a training time \\(t_B &lt; t_A\\).\nLet’s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLet’s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.581151\n1.603907\n0.666441\n00:03\n\n\n1\n0.547510\n0.717316\n0.691475\n00:03\n\n\n2\n0.517336\n0.621597\n0.628552\n00:03\n\n\n3\n0.477595\n1.084812\n0.438430\n00:03\n\n\n4\n0.446734\n0.736970\n0.602842\n00:03\n\n\n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found “lottery ticket” after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.587076\n0.864903\n0.665765\n00:03\n\n\n1\n0.545250\n0.605309\n0.709743\n00:03\n\n\n2\n0.507854\n0.609370\n0.651556\n00:03\n\n\n3\n0.472215\n0.516507\n0.736807\n00:03\n\n\n4\n0.438759\n0.501989\n0.757104\n00:03\n\n\n5\n0.543971\n0.591650\n0.699594\n00:03\n\n\n6\n0.505276\n0.576168\n0.680650\n00:03\n\n\n7\n0.459986\n0.495335\n0.753045\n00:03\n\n\n8\n0.424159\n0.707351\n0.736807\n00:03\n\n\n9\n0.398294\n0.436202\n0.817997\n00:03\n\n\n10\n0.509464\n0.726151\n0.547361\n00:03\n\n\n11\n0.437742\n0.879882\n0.706360\n00:03\n\n\n12\n0.388987\n0.436665\n0.794993\n00:03\n\n\n13\n0.343624\n0.385158\n0.836942\n00:03\n\n\n14\n0.324372\n0.526213\n0.796346\n00:03\n\n\n15\n0.392040\n0.495435\n0.766576\n00:03\n\n\n16\n0.358236\n0.450453\n0.775372\n00:03\n\n\n17\n0.320370\n0.423908\n0.795670\n00:03\n\n\n18\n0.277983\n0.469020\n0.774019\n00:03\n\n\n19\n0.262043\n0.533755\n0.774019\n00:03\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      4,704         50.00%\nLayer 7              Conv2d          36,864     18,432        50.00%\nLayer 10             Conv2d          36,864     18,432        50.00%\nLayer 13             Conv2d          36,864     18,432        50.00%\nLayer 16             Conv2d          36,864     18,432        50.00%\nLayer 20             Conv2d          73,728     36,864        50.00%\nLayer 23             Conv2d          147,456    73,728        50.00%\nLayer 26             Conv2d          8,192      4,096         50.00%\nLayer 29             Conv2d          147,456    73,728        50.00%\nLayer 32             Conv2d          147,456    73,728        50.00%\nLayer 36             Conv2d          294,912    147,456       50.00%\nLayer 39             Conv2d          589,824    294,912       50.00%\nLayer 42             Conv2d          32,768     16,384        50.00%\nLayer 45             Conv2d          589,824    294,912       50.00%\nLayer 48             Conv2d          589,824    294,912       50.00%\nLayer 52             Conv2d          1,179,648  589,824       50.00%\nLayer 55             Conv2d          2,359,296  1,179,648     50.00%\nLayer 58             Conv2d          131,072    65,536        50.00%\nLayer 61             Conv2d          2,359,296  1,179,648     50.00%\nLayer 64             Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 5,583,456     50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B &gt; a_A\\) in the same training time.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html#the-lottery-ticket-hypothesis",
    "href": "tutorials/sparse/lottery_ticket.html#the-lottery-ticket-hypothesis",
    "title": "Lottery Ticket Hypothesis",
    "section": "",
    "text": "The Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B &gt; a_A\\), in a training time \\(t_B &lt; t_A\\).\nLet’s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLet’s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.581151\n1.603907\n0.666441\n00:03\n\n\n1\n0.547510\n0.717316\n0.691475\n00:03\n\n\n2\n0.517336\n0.621597\n0.628552\n00:03\n\n\n3\n0.477595\n1.084812\n0.438430\n00:03\n\n\n4\n0.446734\n0.736970\n0.602842\n00:03\n\n\n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found “lottery ticket” after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.587076\n0.864903\n0.665765\n00:03\n\n\n1\n0.545250\n0.605309\n0.709743\n00:03\n\n\n2\n0.507854\n0.609370\n0.651556\n00:03\n\n\n3\n0.472215\n0.516507\n0.736807\n00:03\n\n\n4\n0.438759\n0.501989\n0.757104\n00:03\n\n\n5\n0.543971\n0.591650\n0.699594\n00:03\n\n\n6\n0.505276\n0.576168\n0.680650\n00:03\n\n\n7\n0.459986\n0.495335\n0.753045\n00:03\n\n\n8\n0.424159\n0.707351\n0.736807\n00:03\n\n\n9\n0.398294\n0.436202\n0.817997\n00:03\n\n\n10\n0.509464\n0.726151\n0.547361\n00:03\n\n\n11\n0.437742\n0.879882\n0.706360\n00:03\n\n\n12\n0.388987\n0.436665\n0.794993\n00:03\n\n\n13\n0.343624\n0.385158\n0.836942\n00:03\n\n\n14\n0.324372\n0.526213\n0.796346\n00:03\n\n\n15\n0.392040\n0.495435\n0.766576\n00:03\n\n\n16\n0.358236\n0.450453\n0.775372\n00:03\n\n\n17\n0.320370\n0.423908\n0.795670\n00:03\n\n\n18\n0.277983\n0.469020\n0.774019\n00:03\n\n\n19\n0.262043\n0.533755\n0.774019\n00:03\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      4,704         50.00%\nLayer 7              Conv2d          36,864     18,432        50.00%\nLayer 10             Conv2d          36,864     18,432        50.00%\nLayer 13             Conv2d          36,864     18,432        50.00%\nLayer 16             Conv2d          36,864     18,432        50.00%\nLayer 20             Conv2d          73,728     36,864        50.00%\nLayer 23             Conv2d          147,456    73,728        50.00%\nLayer 26             Conv2d          8,192      4,096         50.00%\nLayer 29             Conv2d          147,456    73,728        50.00%\nLayer 32             Conv2d          147,456    73,728        50.00%\nLayer 36             Conv2d          294,912    147,456       50.00%\nLayer 39             Conv2d          589,824    294,912       50.00%\nLayer 42             Conv2d          32,768     16,384        50.00%\nLayer 45             Conv2d          589,824    294,912       50.00%\nLayer 48             Conv2d          589,824    294,912       50.00%\nLayer 52             Conv2d          1,179,648  589,824       50.00%\nLayer 55             Conv2d          2,359,296  1,179,648     50.00%\nLayer 58             Conv2d          131,072    65,536        50.00%\nLayer 61             Conv2d          2,359,296  1,179,648     50.00%\nLayer 64             Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 5,583,456     50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B &gt; a_A\\) in the same training time.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "href": "tutorials/sparse/lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "title": "Lottery Ticket Hypothesis",
    "section": "Lottery Ticket Hypothesis with Rewinding",
    "text": "Lottery Ticket Hypothesis with Rewinding\nIn some case, LTH fails for deeper networks, author then propose a solution, which is to rewind the weights to a more advanced iteration instead of the initialization value.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nThis can be done in fasterai by passing the rewind_epoch parameter, that will save the weights at that epoch, then resetting the weights accordingly.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, rewind_epoch=1)\n\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.584980\n0.575644\n0.702977\n00:03\n\n\n1\n0.542651\n0.749559\n0.667118\n00:03\n\n\n2\n0.499945\n0.610964\n0.661705\n00:03\n\n\n3\n0.458487\n0.528082\n0.746955\n00:03\n\n\n4\n0.417654\n0.488568\n0.797700\n00:03\n\n\n5\n0.500780\n0.706954\n0.545332\n00:03\n\n\n6\n0.459245\n0.925210\n0.429635\n00:03\n\n\n7\n0.412464\n0.509823\n0.731394\n00:03\n\n\n8\n0.377349\n0.603450\n0.762517\n00:03\n\n\n9\n0.346884\n0.829551\n0.717185\n00:03\n\n\n10\n0.441250\n0.669327\n0.713802\n00:03\n\n\n11\n0.382227\n0.491025\n0.779432\n00:03\n\n\n12\n0.342169\n0.627792\n0.749662\n00:03\n\n\n13\n0.309651\n0.433117\n0.792963\n00:03\n\n\n14\n0.264981\n0.480634\n0.800406\n00:03\n\n\n15\n0.339205\n0.479244\n0.776049\n00:03\n\n\n16\n0.301984\n0.743302\n0.671854\n00:03\n\n\n17\n0.265180\n0.590050\n0.793640\n00:03\n\n\n18\n0.239833\n0.550390\n0.742896\n00:04\n\n\n19\n0.208132\n0.569378\n0.775372\n00:03\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSaving Weights at epoch 1\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      4,704         50.00%\nLayer 7              Conv2d          36,864     18,432        50.00%\nLayer 10             Conv2d          36,864     18,432        50.00%\nLayer 13             Conv2d          36,864     18,432        50.00%\nLayer 16             Conv2d          36,864     18,432        50.00%\nLayer 20             Conv2d          73,728     36,864        50.00%\nLayer 23             Conv2d          147,456    73,728        50.00%\nLayer 26             Conv2d          8,192      4,096         50.00%\nLayer 29             Conv2d          147,456    73,728        50.00%\nLayer 32             Conv2d          147,456    73,728        50.00%\nLayer 36             Conv2d          294,912    147,456       50.00%\nLayer 39             Conv2d          589,824    294,912       50.00%\nLayer 42             Conv2d          32,768     16,384        50.00%\nLayer 45             Conv2d          589,824    294,912       50.00%\nLayer 48             Conv2d          589,824    294,912       50.00%\nLayer 52             Conv2d          1,179,648  589,824       50.00%\nLayer 55             Conv2d          2,359,296  1,179,648     50.00%\nLayer 58             Conv2d          131,072    65,536        50.00%\nLayer 61             Conv2d          2,359,296  1,179,648     50.00%\nLayer 64             Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 5,583,456     50.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html#super-masks",
    "href": "tutorials/sparse/lottery_ticket.html#super-masks",
    "title": "Lottery Ticket Hypothesis",
    "section": "Super-Masks",
    "text": "Super-Masks\nResearchers from Uber AI investigated the LTH and found the existence of what they call “Super-Masks”, i.e. masks that, applied on a untrained neural network, allows to reach better-than-random results.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nTo find supermasks, authors perform the LTH method then apply the mask on the original, untrained network. In fasterai, you can pass the parameter reset_end=True, which will reset the weights to their original value at the end of the training, but keeping the pruned weights (i.e. the mask) unchanged.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, reset_end=True)\n\n\nlearn.fit(10, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.596297\n0.816344\n0.671177\n00:04\n\n\n1\n0.548523\n0.587115\n0.707713\n00:04\n\n\n2\n0.571606\n0.630654\n0.640054\n00:04\n\n\n3\n0.537812\n0.528453\n0.756428\n00:04\n\n\n4\n0.513824\n0.774966\n0.520298\n00:04\n\n\n5\n0.538326\n0.601889\n0.649526\n00:04\n\n\n6\n0.481332\n0.565981\n0.725981\n00:04\n\n\n7\n0.482080\n0.843460\n0.364005\n00:03\n\n\n8\n0.442359\n0.516618\n0.784844\n00:03\n\n\n9\n0.401376\n0.530230\n0.769283\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 2: [16.67]%\nSparsity at the end of epoch 3: [16.67]%\nSparsity at the end of epoch 4: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [33.33]%\nSparsity at the end of epoch 6: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 7: [50.0]%\nSparsity at the end of epoch 8: [50.0]%\nSparsity at the end of epoch 9: [50.0]%\nFinal Sparsity: [50.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      4,704         50.00%\nLayer 7              Conv2d          36,864     18,432        50.00%\nLayer 10             Conv2d          36,864     18,432        50.00%\nLayer 13             Conv2d          36,864     18,432        50.00%\nLayer 16             Conv2d          36,864     18,432        50.00%\nLayer 20             Conv2d          73,728     36,864        50.00%\nLayer 23             Conv2d          147,456    73,728        50.00%\nLayer 26             Conv2d          8,192      4,096         50.00%\nLayer 29             Conv2d          147,456    73,728        50.00%\nLayer 32             Conv2d          147,456    73,728        50.00%\nLayer 36             Conv2d          294,912    147,456       50.00%\nLayer 39             Conv2d          589,824    294,912       50.00%\nLayer 42             Conv2d          32,768     16,384        50.00%\nLayer 45             Conv2d          589,824    294,912       50.00%\nLayer 48             Conv2d          589,824    294,912       50.00%\nLayer 52             Conv2d          1,179,648  589,824       50.00%\nLayer 55             Conv2d          2,359,296  1,179,648     50.00%\nLayer 58             Conv2d          131,072    65,536        50.00%\nLayer 61             Conv2d          2,359,296  1,179,648     50.00%\nLayer 64             Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 5,583,456     50.00%\n\n\n\nlearn.model.conv1.weight\n\n\nl\n\n\nlearn.validate()\n\n\n\n\n\n\n\n\n(#2) [0.643438994884491,0.6576454639434814]",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html",
    "href": "tutorials/sparse/sparsify_callback.html",
    "title": "Sparsify Callback",
    "section": "",
    "text": "from fastai.vision.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.638129\n0.776416\n0.794317\n00:14\n\n\n1\n0.382736\n0.266816\n0.893775\n00:04\n\n\n2\n0.247629\n0.310128\n0.901218\n00:03\n\n\n3\n0.135562\n0.175250\n0.935724\n00:03\n\n\n4\n0.076362\n0.165934\n0.941813\n00:03\n\n\n\n\n\nLet’s now try adding some sparsity in our model\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_cycle)\n\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.672287\n1.130331\n0.778078\n00:05\n\n\n1\n0.401001\n0.315872\n0.860622\n00:04\n\n\n2\n0.233937\n0.191955\n0.912043\n00:03\n\n\n3\n0.130283\n0.208986\n0.921516\n00:03\n\n\n4\n0.071710\n0.202378\n0.930988\n00:03\n\n\n\n\n\nSparsity at the end of epoch 0: [1.96]%\nSparsity at the end of epoch 1: [20.07]%\nSparsity at the end of epoch 2: [45.86]%\nSparsity at the end of epoch 3: [49.74]%\nSparsity at the end of epoch 4: [50.0]%\nFinal Sparsity: [50.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 2              Conv2d          9,408      4,704         50.00%\nLayer 8              Conv2d          36,864     18,432        50.00%\nLayer 11             Conv2d          36,864     18,432        50.00%\nLayer 14             Conv2d          36,864     18,432        50.00%\nLayer 17             Conv2d          36,864     18,432        50.00%\nLayer 21             Conv2d          73,728     36,864        50.00%\nLayer 24             Conv2d          147,456    73,727        50.00%\nLayer 27             Conv2d          8,192      4,096         50.00%\nLayer 30             Conv2d          147,456    73,727        50.00%\nLayer 33             Conv2d          147,456    73,727        50.00%\nLayer 37             Conv2d          294,912    147,455       50.00%\nLayer 40             Conv2d          589,824    294,909       50.00%\nLayer 43             Conv2d          32,768     16,384        50.00%\nLayer 46             Conv2d          589,824    294,909       50.00%\nLayer 49             Conv2d          589,824    294,909       50.00%\nLayer 53             Conv2d          1,179,648  589,818       50.00%\nLayer 56             Conv2d          2,359,296  1,179,637     50.00%\nLayer 59             Conv2d          131,072    65,535        50.00%\nLayer 62             Conv2d          2,359,296  1,179,637     50.00%\nLayer 65             Conv2d          2,359,296  1,179,637     50.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 5,583,403     50.00%\n\n\nSurprisingly, our network that is composed of \\(50 \\%\\) of zeroes performs reasonnably well when compared to our plain and dense network.\nThe SparsifyCallback also accepts a list of sparsities, corresponding to each layer of layer_type to be pruned. Below, we show how to prune only the intermediate layers of ResNet-18.\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsparsities = [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]\n\n\nsp_cb = SparsifyCallback(sparsity=sparsities, granularity='weight', context='local', criteria=large_final, schedule=cos)\n\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.700158\n0.762811\n0.758457\n00:04\n\n\n1\n0.420924\n0.346230\n0.873478\n00:04\n\n\n2\n0.250773\n0.207668\n0.914073\n00:04\n\n\n3\n0.141221\n0.171472\n0.933018\n00:04\n\n\n4\n0.074364\n0.165237\n0.935724\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 4: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nFinal Sparsity: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 2              Conv2d          9,408      0              0.00%\nLayer 8              Conv2d          36,864     0              0.00%\nLayer 11             Conv2d          36,864     0              0.00%\nLayer 14             Conv2d          36,864     0              0.00%\nLayer 17             Conv2d          36,864     0              0.00%\nLayer 21             Conv2d          73,728     0              0.00%\nLayer 24             Conv2d          147,456    73,727        50.00%\nLayer 27             Conv2d          8,192      4,096         50.00%\nLayer 30             Conv2d          147,456    73,727        50.00%\nLayer 33             Conv2d          147,456    73,727        50.00%\nLayer 37             Conv2d          294,912    147,455       50.00%\nLayer 40             Conv2d          589,824    294,909       50.00%\nLayer 43             Conv2d          32,768     16,384        50.00%\nLayer 46             Conv2d          589,824    294,909       50.00%\nLayer 49             Conv2d          589,824    0              0.00%\nLayer 53             Conv2d          1,179,648  0              0.00%\nLayer 56             Conv2d          2,359,296  0              0.00%\nLayer 59             Conv2d          131,072    0              0.00%\nLayer 62             Conv2d          2,359,296  0              0.00%\nLayer 65             Conv2d          2,359,296  0              0.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 978,934        8.77%\n\n\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nlth: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you don’t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`\n\nFor example, we correctly pruned the convolution layers of our model, but we could imagine pruning the Linear Layers of even only the BatchNorm ones !",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html",
    "href": "tutorials/quantize/quantize_callback.html",
    "title": "Quantize Callback",
    "section": "",
    "text": "pretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\nlearn = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn.model.fc = nn.Linear(512, 2)\nlearn.fit_one_cycle(3, 1e-3, cbs=QuantizeCallback())\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.509876\n0.412579\n0.797023\n00:04\n\n\n1\n0.309299\n0.257415\n0.895129\n00:04\n\n\n2\n0.196601\n0.226849\n0.912720\n00:03\n\n\n\n\n\n\nfrom tqdm import tqdm\n\ndef get_model_size(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    size = os.path.getsize(\"temp.p\") / 1e6  # Size in MB\n    os.remove(\"temp.p\")\n    return size\n    \ndef compute_validation_accuracy(model, valid_dataloader, device=None):\n    # Set the model to evaluation mode\n    model.eval()\n    \n    # Use the model's device if no device is specified\n    \n    device = torch.device('cpu')\n    \n    # Move model to the specified device\n    model = model.to(device)\n    \n    # Tracking correct predictions and total samples\n    total_correct = 0\n    total_samples = 0\n    \n    # Disable gradient computation for efficiency\n    with torch.no_grad():\n        for batch in tqdm(valid_dataloader):\n            # Assuming batch is a tuple of (inputs, labels)\n            # Adjust this if your dataloader returns a different format\n            inputs, labels = batch\n            \n            # Move inputs and labels to the same device as the model\n            inputs = torch.Tensor(inputs).to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Get predictions (for classification tasks)\n            # Use argmax along the class dimension\n            _, predicted = torch.max(outputs, 1)\n            \n            # Update counters\n            total_samples += labels.size(0)\n            total_correct += (predicted == labels).sum().item()\n    \n    # Compute accuracy as a percentage\n    accuracy = (total_correct / total_samples) * 100\n    \n    return accuracy\n\n\npretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\nlearn_original = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn_original.model.fc = nn.Linear(512, 2)\n\n\nprint(f'Size of the original model: {get_model_size(learn_original.model):.2f} MB')\nprint(f'Size of the quantized model: {get_model_size(learn.model):.2f} MB')\n\nSize of the original model: 85.27 MB\nSize of the quantized model: 21.51 MB\n\n\n\ncompute_validation_accuracy(learn.model, dls.valid)\n\n100%|██████████| 24/24 [00:02&lt;00:00,  9.85it/s]\n\n\n90.73071718538566"
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html",
    "href": "tutorials/regularize/regularize_callback.html",
    "title": "Regularize Callback",
    "section": "",
    "text": "from fasterai.core.criteria import *\nfrom fasterai.core.schedule import *\nfrom fasterai.regularize.all import *\nfrom fastai.vision.all import *\n\nGet your data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nTrain a model without Regularization as a baseline\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.683390\n0.504752\n0.850474\n00:03\n\n\n1\n0.398581\n0.278983\n0.891746\n00:03\n\n\n2\n0.227765\n0.227970\n0.907984\n00:03\n\n\n3\n0.126593\n0.196543\n0.924899\n00:03\n\n\n4\n0.067882\n0.171512\n0.940460\n00:03\n\n\n\n\n\nCreate the RegularizeCallback\n\nreg_cb = RegularizeCallback(squared_final, 'weight', 3e-5, schedule=one_cycle)\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nlearn.fit_one_cycle(5, cbs=reg_cb)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.645172\n0.562196\n0.835589\n00:04\n\n\n1\n0.436420\n0.302934\n0.905954\n00:04\n\n\n2\n0.336652\n0.379853\n0.900541\n00:04\n\n\n3\n0.285935\n0.322683\n0.930988\n00:04\n\n\n4\n0.225295\n0.317049\n0.935724\n00:04",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "core/criteria.html",
    "href": "core/criteria.html",
    "title": "Criteria",
    "section": "",
    "text": "The criteria implemented come from this paper.",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core/criteria.html#magnitude-based-criteria",
    "href": "core/criteria.html#magnitude-based-criteria",
    "title": "Criteria",
    "section": "Magnitude Based Criteria",
    "text": "Magnitude Based Criteria\n\nRandom\n\ndemo_model(random)\n\n\n\n\n\n\n\n\n\n\nLarge Final Value\n\ndemo_model(large_final)\n\n\n\n\n\n\n\n\n\n\nSquared Final Value\n\ndemo_model(squared_final)\n\n\n\n\n\n\n\n\n\n\nSmall Final Value\n\ndemo_model(small_final)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core/criteria.html#init-based-criteria",
    "href": "core/criteria.html#init-based-criteria",
    "title": "Criteria",
    "section": "Init based criteria",
    "text": "Init based criteria\n\nLarge Init Value\n\ndemo_model(large_init)\n\n\n\n\n\n\n\n\n\n\nSmall Init Value\n\ndemo_model(small_init)\n\n\n\n\n\n\n\n\n\n\nLarge Init Large Final Value\n\ndemo_model(large_init_large_final, 80)\n\n\n\n\n\n\n\n\n\n\nSmall Init Small Final Value\n\ndemo_model(small_init_small_final)\n\n\n\n\n\n\n\n\n\n\nIncreasing Magnitude\n\ndemo_model(magnitude_increase, 60)\n\n\n\n\n\n\n\n\n\n\nMovement Pruning\n\ndemo_model(movement)\n\n\n\n\n\n\n\n\n\nmovmag = init_based_criteria(noop, output_fn=lambda x,y: torch.abs(torch.mul(x, torch.sub(x,y))))\n\n\ndemo_model(movmag)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core/criteria.html#update-based-criteria",
    "href": "core/criteria.html#update-based-criteria",
    "title": "Criteria",
    "section": "Update based criteria",
    "text": "Update based criteria\nThe following criteria use an updating value of the weights, i.e. the value from the previous iteration of training, instead of the initialization value to better capture the training dynamics.\n\nUpdating Magnitude Increase\n\ndemo_model(updating_magnitude_increase)\n\n\n\n\n\n\n\n\n\n\nUpdating Movement\n\ndemo_model(updating_movement, 50)\n\n\n\n\n\n\n\n\n\n\nUpdating mov-magnitude\n\ndemo_model(updating_movmag)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "distill/distillation_callback.html",
    "href": "distill/distillation_callback.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge Distillation, sometimes called teacher-student training, is a compression method in which a small (the student) model is trained to mimic the behaviour of a larger (the teacher) model.\nThe main goal is to reveal what is called the Dark Knowledge hidden in the teacher model.\nIf we take the same example provided by Geoffrey Hinton et al., we have\nThe main problem of classification is that the output activation function (softmax) will, by design, make a single value really high and squash others.\n\\[\np_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j} \\exp \\left(z_{j}\\right)}\n\\]\nWith \\(p_i\\) the probability of class \\(i\\), computed from the logits \\(z\\)\nHere is an example to illustrate this phenomenon:\nLet’s say that we have trained a model to discriminate between the following 5 classes: [cow, dog, plane, cat, car]\nAnd here is the output of the final layer (the logits) when the model is fed a new input image:\n\nlogits = torch.tensor([1.3, 3.1, 0.2, 1.9, -0.3])\n\nBy judging on the predictions, the model seems confident that the input data is a dog and quite confident that it is definitely not a plane nor a car, with predictions for cow and cat being moderately high.\nSo the model not only has learned to recognize a dog in the image, but also that a dog is very different from a car and a plane and share similarities with cats and cows. This information is what is called dark knowledge !\nWhen passing those predictions through a softmax, we have:\n\npredictions = F.softmax(logits, dim=-1); predictions\n\ntensor([0.1063, 0.6431, 0.0354, 0.1937, 0.0215])\n\n\nThis is accuenting the differences that we had earlier, discarding some of the dark knowledge acquired earlier. The way to keep this knowledge is to “soften” our softmax outputs, by adding a temperature parameter. The higher the temperature, the softer the predictions.\n\nsoft_predictions = F.softmax(logits/3, dim=-1); soft_predictions\n\ntensor([0.1879, 0.3423, 0.1302, 0.2294, 0.1102])\n\n\n\n\n\n\n\n\nNote\n\n\n\nif the Temperature is equal to 1, then we have regular softmax\n\n\nWhen applying Knowledge Distillation, we want to keep the Dark Knowledge that the teacher model has acquired during its training but not rely entirely on it. So we combine two losses:\n\nThe Teacher loss between the softened predictions of the teacher and the softened predictions of the student\nThe Classification loss, which is the regular loss between hard labels and hard predictions\n\nThe combination between those losses are weighted by an additional parameter α, as:\n\\[\nL_{K D}=\\alpha  * \\text { CrossEntropy }\\left(p_{S}^{\\tau}, p_{T}^{\\tau}\\right)+(1-\\alpha) * \\text { CrossEntropy }\\left(p_{S}, y_{\\text {true }}\\right)\n\\]\nWith \\(p^{\\tau}\\) being the softened predictions of the student and teacher\n\n\n\n\n\n\nNote\n\n\n\nIn practice, the distillation loss will be a bit different in the implementation\n\n\n\nThis can be done with fastai, using the Callback system !\n\nsource\n\nKnowledgeDistillationCallback\n\n KnowledgeDistillationCallback (teacher:torch.nn.modules.module.Module,\n                                loss:Callable, activations_student:Union[L\n                                ist[str],str,NoneType]=None, activations_t\n                                eacher:Union[List[str],str,NoneType]=None,\n                                weight:float=0.5)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nThe loss function that is used may depend on the use case. For classification, we usually use the one presented above, named SoftTarget in fasterai. But for regression cases, we may want to perform regression on the logits directly.",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Knowledge Distillation"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Methods • Features • Installation • Tutorials • Citing • License\nfasterai is a library created to make neural network smaller and faster. It essentially relies on common compression techniques for networks such as pruning, knowledge distillation, Lottery Ticket Hypothesis, …\nThe core feature of fasterai is its Sparsifying capabilities, constructed around 4 main modules: granularity, context, criteria, schedule. Each of these modules is highly customizable, allowing you to change them according to your needs or even to come up with your own !",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#project-documentation",
    "href": "overview.html#project-documentation",
    "title": "Overview",
    "section": "Project Documentation",
    "text": "Project Documentation\nVisit Read The Docs Project Page or read following README to know more about using fasterai.",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#features",
    "href": "overview.html#features",
    "title": "Overview",
    "section": "Features",
    "text": "Features\n\n1. Sparsifying\n\nMake your model sparse according to a:  - Sparsity:  the percentage of weights that will be replaced by 0  - Granularity:  the granularity at which you operate the pruning (removing weights, vectors, kernels, filters)  - Context:  prune either each layer independantly (local pruning) or the whole model (global pruning)  - Criteria:  the criteria used to select the weights to remove (magnitude, movement, …)  - Schedule:  which schedule you want to use for pruning (one shot, iterative, gradual, …) \nThis can be achieved by using the SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n2. Pruning\n\nOnce your model has useless nodes due to zero-weights, they can be removed to not be a part of the network anymore.\nThis can be achieved by using the PruneCallback(sparsity, context, criteria, schedule)\n\n\n3. Regularization\n\nInstead of explicitely make your network sparse, let it train towards sparse connections by pushing the weights to be as small as possible.\nRegularization can be applied to groups of weights, following the same granularities as for sparsifying, i.e.: - Granularity:  the granularity at which you operate the regularization (weights, vectors, kernels, filters, …)\nThis can be achieved by using the RegularizationCallback(granularity)\n\n\n4. Knowledge Distillation\n\n\n\nalt text\n\n\nDistill the knowledge acquired by a big model into a smaller one, by using the KnowledgeDistillation callback.\n\n\n5. Lottery Ticket Hypothesis\n\nFind the winning ticket in you network, i.e. the initial subnetwork able to attain at least similar performances than the network as a whole.",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#quick-start",
    "href": "overview.html#quick-start",
    "title": "Overview",
    "section": "Quick Start",
    "text": "Quick Start\n\n0. Import fasterai\nfrom fasterai.sparse.all import *\n\n\n1. Create your model with fastai\nlearn = cnn_learner(dls, model)\n\n\n2. Get you Fasterai Callback\nsp_cb=SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n3. Train you model to make it sparse !\nlearn.fit_one_cycle(n_epochs, cbs=sp_cb)",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#installation",
    "href": "overview.html#installation",
    "title": "Overview",
    "section": "Installation",
    "text": "Installation\npip install git+https://github.com/nathanhubens/fasterai.git\nor\npip install fasterai",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#tutorials",
    "href": "overview.html#tutorials",
    "title": "Overview",
    "section": "Tutorials",
    "text": "Tutorials\n\nGet Started with FasterAI\nCreate your own pruning schedule\nFind winning tickets using the Lottery Ticket Hypothesis\nUse Knowledge Distillation to help a student model to reach higher performance\nSparsify Transformers\nMore to come…",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#citing",
    "href": "overview.html#citing",
    "title": "Overview",
    "section": "Citing",
    "text": "Citing\n@software{Hubens,\n  author       = {Nathan Hubens},\n  title        = {fasterai},\n  year         = 2022,\n  publisher    = {Zenodo},\n  version      = {v0.1.6},\n  doi          = {10.5281/zenodo.6469868},\n  url          = {https://doi.org/10.5281/zenodo.6469868}\n}",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#license",
    "href": "overview.html#license",
    "title": "Overview",
    "section": "License",
    "text": "License\nApache-2.0 License.",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "misc/cpu_optimizer.html",
    "href": "misc/cpu_optimizer.html",
    "title": "Further optimize for CPU inference",
    "section": "",
    "text": "accelerate_model_for_cpu\n\n accelerate_model_for_cpu (model:torch.nn.modules.module.Module,\n                           example_input:torch.Tensor)"
  },
  {
    "objectID": "sparse/sparsifier.html",
    "href": "sparse/sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network’s weight are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\n\nsource\n\nSparsifier\n\n Sparsifier (model:torch.nn.modules.module.Module, granularity:str,\n             context:str, criteria:fasterai.core.criteria.Criteria,\n             nm:bool=False,\n             layer_type:Type[torch.nn.modules.module.Module]=&lt;class\n             'torch.nn.modules.conv.Conv2d'&gt;)\n\nClass providing sparsifying capabilities\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nModule\n\nThe model to sparsify\n\n\ngranularity\nstr\n\nGranularity of sparsification (e.g., ‘weight’, ‘filter’)\n\n\ncontext\nstr\n\nContext for sparsification (‘global’ or ‘local’)\n\n\ncriteria\nCriteria\n\nCriteria to determine which weights to keep\n\n\nnm\nbool\nFalse\nWhether to use N:M sparsity pattern (forces 2:4 sparsity)\n\n\nlayer_type\nType\nConv2d\nType of layers to apply sparsification to\n\n\n\nThe Sparsifier class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n\nThe granularity, i.e. the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\nThe context, i.e. if you want to consider each layer independently (local), or compare the parameters to remove across the whole network (global).\nThe criteria, i.e. the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful.\n\nUser can pass a single layer to prune by using the Sparsifier.sparsify_layer method.\n\nsource\n\n\nSparsifier.sparsify_layer\n\n Sparsifier.sparsify_layer (m:torch.nn.modules.module.Module,\n                            sparsity:float, round_to:Optional[int]=None)\n\nApply sparsification to a single layer\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nm\nModule\n\nThe layer to sparsify\n\n\nsparsity\nfloat\n\nTarget sparsity level (percentage)\n\n\nround_to\nOptional\nNone\nRound to a multiple of this value\n\n\nReturns\nNone\n\n\n\n\n\nMost of the time, we may want to prune the whole model at once, using the Sparsifier.prune_model method, indicating the percentage of sparsity to you want to apply.\n\nsource\n\n\nSparsifier.sparsify_model\n\n Sparsifier.sparsify_model (sparsity:Union[float,List[float]],\n                            round_to:Optional[int]=None)\n\nApply sparsification to all matching layers in the model\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsparsity\nUnion\n\nTarget sparsity level(s)\n\n\nround_to\nOptional\nNone\nRound to a multiple of this value\n\n\nReturns\nNone\n\n\n\n\n\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of a given number (e.g. 8), this can be done by passing the round_to parameter.\n\nAlso, instead of passing a single value of sparsity, a list of sparsities can also be provided. In that case, each value in the list is the sparsity that will be applied to all layers.\nExample: I have a 4-layer network and want to remove half of the parameters from the layers 2 and 3, I can provide the list: sparsity = [0, 50, 50, 0]",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "quantize/quantize_callback.html",
    "href": "quantize/quantize_callback.html",
    "title": "Quantize Callback",
    "section": "",
    "text": "source\n\nQuantizeCallback\n\n QuantizeCallback (quantizer=None, backend='x86', use_per_tensor=False,\n                   verbose=False)\n\nSimple callback for Quantization-Aware Training (QAT) in fastai. Uses the Quantizer class for configuration and conversion.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nquantizer\nNoneType\nNone\nProvide custom quantizer\n\n\nbackend\nstr\nx86\nTarget backend for quantization: ‘x86’, ‘qnnpack’\n\n\nuse_per_tensor\nbool\nFalse\nForce per-tensor quantization\n\n\nverbose\nbool\nFalse\nEnable verbose output",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "regularize/regularize_callback.html",
    "href": "regularize/regularize_callback.html",
    "title": "Regularize Callback",
    "section": "",
    "text": "RegularizeCallback\n\n RegularizeCallback (g, wd=0.01, layer_type=&lt;class\n                     'torch.nn.modules.conv.Conv2d'&gt;)\n\nCallback to apply grouped weight decay\nThe RegularizeCallbackcan be used to perform \\(l_1\\) regularization on any granularity available in the criteria class.",
    "crumbs": [
      "Contact Me",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "quantize/quantizer.html",
    "href": "quantize/quantizer.html",
    "title": "Quantizer",
    "section": "",
    "text": "source\n\nQuantizer\n\n Quantizer (backend:str='x86', method:str='static',\n            qconfig_mapping:Optional[Dict]=None,\n            custom_configs:Optional[Dict]=None, use_per_tensor:bool=False,\n            verbose:bool=False)\n\nInitialize a quantizer with specified backend and options.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbackend\nstr\nx86\nTarget backend for quantization\n\n\nmethod\nstr\nstatic\nQuantization method: ‘static’, ‘dynamic’, or ‘qat’\n\n\nqconfig_mapping\nOptional\nNone\nOptional custom quantization config\n\n\ncustom_configs\nOptional\nNone\nCustom module-specific configurations\n\n\nuse_per_tensor\nbool\nFalse\nForce per-tensor quantization\n\n\nverbose\nbool\nFalse\nEnable verbose output\n\n\n\n\nsource\n\n\nQuantizer.quantize\n\n Quantizer.quantize (model:torch.nn.modules.module.Module,\n                     calibration_dl:Any, max_calibration_samples:int=100,\n                     device:Union[str,torch.device]='cpu')\n\nQuantize a model using the specified method and settings.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nModule\n\nModel to quantize\n\n\ncalibration_dl\nAny\n\nDataloader for calibration\n\n\nmax_calibration_samples\nint\n100\nMaximum number of samples to use for calibration\n\n\ndevice\nUnion\ncpu\nDevice to use for calibration\n\n\nReturns\nModule",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantizer"
    ]
  },
  {
    "objectID": "sparse/sparsify_callback.html",
    "href": "sparse/sparsify_callback.html",
    "title": "Sparsify Callback",
    "section": "",
    "text": "source\n\nSparsifyCallback\n\n SparsifyCallback (sparsity:Union[float,List[float]], granularity:str,\n                   context:str, criteria:fasterai.core.criteria.Criteria,\n                   schedule:fasterai.core.schedule.Schedule,\n                   lth:bool=False, rewind_epoch:int=0,\n                   reset_end:bool=False, save_tickets:bool=False,\n                   model:Optional[torch.nn.modules.module.Module]=None,\n                   round_to:Optional[int]=None, nm:bool=False,\n                   layer_type:Type[torch.nn.modules.module.Module]=&lt;class\n                   'torch.nn.modules.conv.Conv2d'&gt;)\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsparsity\nUnion\n\nTarget sparsity level(s)\n\n\ngranularity\nstr\n\nType of pruning granularity (e.g., ‘weight’, ‘filter’)\n\n\ncontext\nstr\n\nPruning context (‘global’ or ‘local’)\n\n\ncriteria\nCriteria\n\nCriteria for determining weights to keep\n\n\nschedule\nSchedule\n\nPruning schedule to use\n\n\nlth\nbool\nFalse\nWhether to use Lottery Ticket Hypothesis approach\n\n\nrewind_epoch\nint\n0\nEpoch to rewind weights to for LTH\n\n\nreset_end\nbool\nFalse\nWhether to reset weights after pruning\n\n\nsave_tickets\nbool\nFalse\nWhether to save pruned models as “winning tickets”\n\n\nmodel\nOptional\nNone\nModel to sparsify (if None, uses learn.model)\n\n\nround_to\nOptional\nNone\nRound pruning to multiple of this value\n\n\nnm\nbool\nFalse\nWhether to use N:M structured sparsity\n\n\nlayer_type\nType\nConv2d\nLayer type to apply pruning to\n\n\n\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nlth: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you don’t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "misc/fc_decomposer.html",
    "href": "misc/fc_decomposer.html",
    "title": "Fully-Connected Layers Decomposer",
    "section": "",
    "text": "We can factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\) With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\n\n\nFC_Decomposer.decompose\n\n FC_Decomposer.decompose (model, percent_removed=0.5)\n\nA tutorial about how to use the FC_Decomposer functionalities can be found here",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Fully-Connected Layers Decomposer"
    ]
  },
  {
    "objectID": "misc/bn_folding.html",
    "href": "misc/bn_folding.html",
    "title": "Batch Norm Folding",
    "section": "",
    "text": "Batch Normalization is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity.\nIt consists of 2 steps:\n\nNormalize the batch by first subtracting its mean \\(\\mu\\), then dividing it by its standard deviation \\(\\sigma\\).\nFurther scale by a factor \\(\\gamma\\) and shift by a factor \\(\\beta\\). Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\\[\n\\begin{aligned}\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\ \\sigma_{\\mathcal{B}}^{2} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\ \\widehat{x}_{i} & \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\ y_{i} & \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) \\end{aligned}\\]\nDue to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?\nOnce the training has ended, each batch normalization layer possesses a specific set of \\(\\gamma\\) and \\(\\beta\\), but also \\(\\mu\\) and \\(\\sigma\\), the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\nAs a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!\nThis would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.\nWith a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.\nAs a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input \\(x\\), as:\n\\[\\begin{aligned} z &=W * x+b \\\\ \\mathrm{out} &=\\gamma \\cdot \\frac{z-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nSo, if we re-arrange the \\(W\\) and \\(b\\) of the convolution to take the parameters of the batch normalization into account, as such:\n\\[\\begin{aligned} w_{\\text {fold }} &=\\gamma \\cdot \\frac{W}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\ b_{\\text {fold }} &=\\gamma \\cdot \\frac{b-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nIn practice, this can be achieved in FasterAI with the BN_folder class\n\n\nBN_Folder.fold\n\n BN_Folder.fold (model)\n\nA tutorial about how to use the BN_Folder functionalities can be found here",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Batch Norm Folding"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quick Start",
    "section": "",
    "text": "Embark on a journey to supercharge your neural network models with FasterAI, a PyTorch-based library dedicated exclusively to advanced compression techniques. In today’s fast-paced world, where efficiency and performance are paramount, FasterAI stands out by providing cutting-edge solutions designed to make your neural networks not just lighter, but significantly faster.",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#why-choose-fasterai",
    "href": "quickstart.html#why-choose-fasterai",
    "title": "Quick Start",
    "section": "Why Choose FasterAI?",
    "text": "Why Choose FasterAI?\n\nStreamlined Efficiency: Dive into a suite of compression methodologies, including sparsification, pruning, quantization, and knowledge distillation, each tailored to enhance model efficiency without compromising on accuracy.\nEdge-Ready Models: With FasterAI, prepare your models for the edge, ensuring they run smoothly on devices with limited computational resources, from smartphones to IoT devices.\nCutting-edge Technology: Built on the latest research in data and model compression, FasterAI offers tools that are not just powerful but also easy to integrate into your existing workflows.\nVersatility: From image and video compression to deep learning model optimization, FasterAI is versatile enough to handle a wide range of compression needs, making it suitable for various industries and applications.\nOpen and Accessible: As a community-driven project, FasterAI encourages contributions and feedback, ensuring that the library continues to evolve to meet the needs of its users.",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#getting-started-with-fasterai",
    "href": "quickstart.html#getting-started-with-fasterai",
    "title": "Quick Start",
    "section": "Getting Started with FasterAI",
    "text": "Getting Started with FasterAI\nWhether you’re looking to optimize models for production, research, or hobby projects, FasterAI provides the tools and guidance to achieve your goals. Let’s make your neural networks faster and lighter, together.\n\nHow to use fasterai ?\nFasterAI’s integration with the callback system of fastai represents a significant advancement in how compression techniques can be applied to neural networks, particularly during the training phase. This approach allows for a more seamless and flexible implementation of compression strategies, making it possible to optimize models on-the-fly and potentially achieve better efficiency and performance.\n\n\nUnderstanding Callbacks in fastai\nBefore diving into how FasterAI leverages callbacks, it’s important to understand what callbacks are in the context of the fastai library. Callbacks are a programming pattern that allows users to inject custom behavior into certain stages of the training loop or model lifecycle without altering the core logic of the training process. They can be used for a variety of purposes, such as logging metrics, modifying learning rates, or implementing early stopping.\n\n\nFasterAI’s Use of Callbacks\nFasterAI takes advantage of the callback system in fastai to integrate neural network compression techniques directly into the training process. This integration means that instead of applying compression post-training as a separate step, FasterAI allows for compression techniques like pruning, quantization, and knowledge distillation to be applied dynamically as the model trains. Here’s how it enhances the training process:\n\nDynamic Compression: By using callbacks, FasterAI can dynamically adjust the compression parameters based on the model’s performance during training. For example, it can gradually increase the amount of pruning as the model becomes more stable, leading to a more efficient compression process that minimally impacts performance.\nReal-time Optimization: This approach enables real-time optimization of the model. As the model learns and adapts to the data, FasterAI can apply compression techniques in a way that’s informed by the model’s current state, potentially leading to more effective and efficient compression.\nSeamless Integration: Leveraging fastai’s callback system means that users of FasterAI can integrate compression into their training pipelines with minimal code changes. This seamless integration simplifies the process of applying advanced compression techniques, making it accessible even to those with limited experience in model optimization.\n\n\n\nPractical Implications\nFor practitioners, this means they can train models that are not only high-performing but also optimized for size and speed from the outset. It also opens up new possibilities for experimenting with compression techniques during training, which could lead to novel optimization strategies and more efficient models.\nIn essence, FasterAI’s use of the callback system in fastai democratizes the application of sophisticated neural network compression techniques, making them an integral part of the model development lifecycle rather than an afterthought. This approach aligns with the broader goal of developing AI models that are not just powerful but also efficient and adaptable to various deployment environments.\nTo illustrate the practical application of FasterAI’s integration with the fastai callback system for on-the-fly compression during the training phase, let’s walk through an example. This example will demonstrate how to apply dynamic pruning to a neural network model while it’s being trained, leveraging the callback system for seamless integration.\n\n\nSetting Up Your Environment\nFirst, ensure you have both fastai and FasterAI installed in your Python environment. If you haven’t installed these libraries yet, you can do so using pip:\npip install fastai fasterai\n\n\nImporting Necessary Libraries\nBegin by importing the required libraries from fasterai:\nfrom fasterai.sparse.all import *\n\n\nDefining the Dataloader and Learner\nJust get your favorite Dataloader and Learner as usual with fastai:\ndls = get_dls()\nlearner = get_learner()\n\n\nApplying Dynamic Sparsification with a Callback\nNow, integrate FasterAI’s dynamic sparsification into the training process by adding the SparsifyCallback to your model. This callback will apply sparsify dynamically based on the defined parameters:\nsparsify_callback = SparsifyCallback(sparsity, granularity, context, criteria, schedule)\nlearner.fit(n_epoch, max_lr, cbs=[sparsify_callback])\nIn this example, SparsifyCallback is initialized with different parameters (see the tutorial to better understand those parameters). The fit method trains the model for n_epochs, and the SparsifyCallback is passed through the cbs (callbacks) parameter, enabling dynamic sparsification during the training process.\n\n\nObserving the Effects\nAfter training, you can evaluate the model’s performance and size to observe the effects of dynamic sparsification. You should notice a reduction in the model size with minimal impact on accuracy, showcasing the efficiency of integrating compression techniques during training.\n\n\nConclusion\nThis example demonstrates how FasterAI’s integration with the fastai callback system allows for the application of compression techniques like sparsification directly within the training loop. By leveraging callbacks, you can dynamically optimize your neural network models, making them lighter and faster without a significant compromise on performance. This approach not only simplifies the compression process but also opens up new avenues for creating efficient AI models optimized for various deployment scenarios.",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "core/granularity.html",
    "href": "core/granularity.html",
    "title": "Granularity",
    "section": "",
    "text": "A Conv2d layer possess a 4d-tensor as weights. This means that there exist many ways of removing blocks from it.\n\n\nIn the case of convolution filters, removing 0-D elements is equivalent to removing individual weights.\n\nweight granularity\n\n\nget_pruned_conv('weight')\n\n\n\n\n\n\n\n\n\n\n\n1-D blocks of elements is equivalent to removing vectors from the convolution filters. There are several ways to chose the vectors, that will be represented below.\n\nshared_weight: this granularity is very particular as it removes individual weights from a filter, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_weight')\n\n\n\n\n\n\n\n\n\nchannel: remove vector of weights along the channel axis.\n\n\nget_pruned_conv('channel')\n\n\n\n\n\n\n\n\n\ncolumn: remove vector of weights along the height axis.\n\n\nget_pruned_conv('column')\n\n\n\n\n\n\n\n\n\nrow: remove vector of weights along the width axis.\n\n\nget_pruned_conv('row')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_channel: remove vector of weight along the channel axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_channel')\n\n\n\n\n\n\n\n\n\nshared_column: remove vector of weight along the height axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_column')\n\n\n\n\n\n\n\n\n\nshared_row: remove vector of weight along the width axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_row')\n\n\n\n\n\n\n\n\n\nvertical_slice: remove vertical slices of weight along the height axis.\n\n\nget_pruned_conv('vertical_slice')\n\n\n\n\n\n\n\n\n\nhorizontal_slice: remove vertical slices of weight along the width axis.\n\n\nget_pruned_conv('horizontal_slice')\n\n\n\n\n\n\n\n\n\nkernel: remove kernels of from the convolution filters.\n\n\nget_pruned_conv('kernel')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_vertical_slice: remove vertical slices of weight along the height axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_vertical_slice')\n\n\n\n\n\n\n\n\n\nshared_horizontal_slice: remove horizontal slices of weight along the width axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_horizontal_slice')\n\n\n\n\n\n\n\n\n\nshared_kernel: remove kernels of weight from the convolution filters, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_kernel')\n\n\n\n\n\n\n\n\n\nfilter: remove entire filters.\n\n\nget_pruned_conv('filter')",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/granularity.html#conv2d-pruning",
    "href": "core/granularity.html#conv2d-pruning",
    "title": "Granularity",
    "section": "",
    "text": "A Conv2d layer possess a 4d-tensor as weights. This means that there exist many ways of removing blocks from it.\n\n\nIn the case of convolution filters, removing 0-D elements is equivalent to removing individual weights.\n\nweight granularity\n\n\nget_pruned_conv('weight')\n\n\n\n\n\n\n\n\n\n\n\n1-D blocks of elements is equivalent to removing vectors from the convolution filters. There are several ways to chose the vectors, that will be represented below.\n\nshared_weight: this granularity is very particular as it removes individual weights from a filter, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_weight')\n\n\n\n\n\n\n\n\n\nchannel: remove vector of weights along the channel axis.\n\n\nget_pruned_conv('channel')\n\n\n\n\n\n\n\n\n\ncolumn: remove vector of weights along the height axis.\n\n\nget_pruned_conv('column')\n\n\n\n\n\n\n\n\n\nrow: remove vector of weights along the width axis.\n\n\nget_pruned_conv('row')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_channel: remove vector of weight along the channel axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_channel')\n\n\n\n\n\n\n\n\n\nshared_column: remove vector of weight along the height axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_column')\n\n\n\n\n\n\n\n\n\nshared_row: remove vector of weight along the width axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_row')\n\n\n\n\n\n\n\n\n\nvertical_slice: remove vertical slices of weight along the height axis.\n\n\nget_pruned_conv('vertical_slice')\n\n\n\n\n\n\n\n\n\nhorizontal_slice: remove vertical slices of weight along the width axis.\n\n\nget_pruned_conv('horizontal_slice')\n\n\n\n\n\n\n\n\n\nkernel: remove kernels of from the convolution filters.\n\n\nget_pruned_conv('kernel')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_vertical_slice: remove vertical slices of weight along the height axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_vertical_slice')\n\n\n\n\n\n\n\n\n\nshared_horizontal_slice: remove horizontal slices of weight along the width axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_horizontal_slice')\n\n\n\n\n\n\n\n\n\nshared_kernel: remove kernels of weight from the convolution filters, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_kernel')\n\n\n\n\n\n\n\n\n\nfilter: remove entire filters.\n\n\nget_pruned_conv('filter')",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/granularity.html#linear-pruning",
    "href": "core/granularity.html#linear-pruning",
    "title": "Granularity",
    "section": "Linear Pruning",
    "text": "Linear Pruning\n\n0-D Blocks\nAs for the convolution filters, weights from a Linear layer can be removed independently.\n\nweight: remove individual weights.\n\n\nget_pruned_linear('weight')\n\n\n\n\n\n\n\n\n\n\n1-D Blocks\n\ncolumn: remove column of weight, which corresponds to removing input neurons.\n\n\nget_pruned_linear('column')\n\n\n\n\n\n\n\n\n\nrow: remove rows of weight, which corresponds to removing output neurons.\n\n\nget_pruned_linear('row')",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/granularity.html#transformer-pruning",
    "href": "core/granularity.html#transformer-pruning",
    "title": "Granularity",
    "section": "Transformer Pruning",
    "text": "Transformer Pruning\n\n\n\n\n\n\nNote\n\n\n\nThis is an experimental part of the library",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/schedules.html",
    "href": "core/schedules.html",
    "title": "Schedules",
    "section": "",
    "text": "source",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#one-shot",
    "href": "core/schedules.html#one-shot",
    "title": "Schedules",
    "section": "One-Shot",
    "text": "One-Shot\nThe easiest schedule is the one-shot pruning, i.e. prune the network once. This can be done by simply returning the desired sparsity value. The moment when you want to prune will be controlled by the start_epoch argument in the SparsifyCallback.\n\nsource\n\nsched_oneshot\n\n sched_oneshot (start:float, end:float, pos:float)\n\nOne-shot pruning: jump directly to target sparsity\n\n\n\n\nType\nDetails\n\n\n\n\nstart\nfloat\nStarting sparsity level\n\n\nend\nfloat\nTarget sparsity level\n\n\npos\nfloat\nCurrent position in schedule (0-1)\n\n\nReturns\nfloat\n\n\n\n\n\none_shot.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#iterative",
    "href": "core/schedules.html#iterative",
    "title": "Schedules",
    "section": "Iterative",
    "text": "Iterative\nInstead of pruning the network to desired sparsity in one step, you can do it iteratively. In fasterai, you can change the amount of iterations\n\nsource\n\nsched_iterative\n\n sched_iterative (start:float, end:float, pos:float, n_steps:int=3)\n\nPerform iterative pruning in discrete steps\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nfloat\n\nStarting sparsity level\n\n\nend\nfloat\n\nTarget sparsity level\n\n\npos\nfloat\n\nCurrent position in schedule (0-1)\n\n\nn_steps\nint\n3\nNumber of pruning steps\n\n\nReturns\nfloat\n\n\n\n\n\n\niterative.plot(50)\n\n\n\n\n\n\n\n\n\n\nTo modify the default n_steps, you can use the partial function.\n\niterative = Schedule(partial(sched_iterative, n_steps=5), start_pct=0.2)\n\n\niterative.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#automated-gradual-pruning",
    "href": "core/schedules.html#automated-gradual-pruning",
    "title": "Schedules",
    "section": "Automated Gradual Pruning",
    "text": "Automated Gradual Pruning\nSome researchers have come up with more sophisticated schedules, such as the Automated Gradual Pruning.\n\nsource\n\nsched_agp\n\n sched_agp (start:float, end:float, pos:float)\n\nAutomated gradual pruning schedule with cubic decay\n\n\n\n\nType\nDetails\n\n\n\n\nstart\nfloat\nStarting sparsity level\n\n\nend\nfloat\nTarget sparsity level\n\n\npos\nfloat\nCurrent position in schedule (0-1)\n\n\nReturns\nfloat\n\n\n\n\n\nagp.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#one-cycle-pruning",
    "href": "core/schedules.html#one-cycle-pruning",
    "title": "Schedules",
    "section": "One-Cycle Pruning",
    "text": "One-Cycle Pruning\n\nsource\n\nsched_onecycle\n\n sched_onecycle (start:float, end:float, pos:float, α:float=14, β:float=6)\n\nOne-cycle schedule based on logistic function\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nfloat\n\nStarting sparsity level\n\n\nend\nfloat\n\nTarget sparsity level\n\n\npos\nfloat\n\nCurrent position in schedule (0-1)\n\n\nα\nfloat\n14\nSteepness parameter\n\n\nβ\nfloat\n6\nOffset parameter\n\n\nReturns\nfloat\n\n\n\n\n\n\none_cycle.plot(50)\n\n\n\n\n\n\n\n\n\n\nOn top of that, all of the schedules available in fastai by default are also available: - sched_cos - sched_linear\n\nfig, ax = plt.subplots(1, 1, figsize=(8,5), dpi=100)\nfig.patch.set_alpha(0.)\nax.patch.set_alpha(0.)\n\nprune = np.linspace(0, 1, 1000)\nsps = [cos([50], p) for p in prune]\nplt.plot(prune, sps, c='#89d6c9', linewidth=2)\nplt.xlabel('training iterations (Normalized)')\nplt.ylabel('sparsity')\n\n        \nax.spines['bottom'].set_color('#808080')\nax.spines['top'].set_color('#808080') \nax.spines['right'].set_color('#808080')\nax.spines['left'].set_color('#808080')\nax.tick_params(axis='x', colors='#808080')\nax.tick_params(axis='y', colors='#808080')\nax.yaxis.label.set_color('#808080')\nax.xaxis.label.set_color('#808080')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(8,5), dpi=100)\nfig.patch.set_alpha(0.)\nax.patch.set_alpha(0.)\n\nprune = np.linspace(0, 1, 1000)\nsps = [lin([50], p) for p in prune]\nplt.plot(prune, sps, c='#89d6c9', linewidth=2)\nplt.xlabel('training iterations (Normalized)')\nplt.ylabel('sparsity')\n\n        \nax.spines['bottom'].set_color('#808080')\nax.spines['top'].set_color('#808080') \nax.spines['right'].set_color('#808080')\nax.spines['left'].set_color('#808080')\nax.tick_params(axis='x', colors='#808080')\nax.tick_params(axis='y', colors='#808080')\nax.yaxis.label.set_color('#808080')\nax.xaxis.label.set_color('#808080')",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#dense-sparse-dense",
    "href": "core/schedules.html#dense-sparse-dense",
    "title": "Schedules",
    "section": "Dense-Sparse-Dense",
    "text": "Dense-Sparse-Dense\nYou can also create even more interesting behaviours such as the DSD method, where you prune the model in the first place, then re-grow it to its initial amount of parameter.\n\nsource\n\nsched_dsd\n\n sched_dsd (start:float, end:float, pos:float)\n\nDense-Sparse-Dense schedule: increase then decrease sparsity\n\n\n\n\nType\nDetails\n\n\n\n\nstart\nfloat\nStarting sparsity level\n\n\nend\nfloat\nTarget sparsity level\n\n\npos\nfloat\nCurrent position in schedule (0-1)\n\n\nReturns\nfloat\n\n\n\n\n\ndsd.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantizer.html",
    "href": "tutorials/quantize/quantizer.html",
    "title": "Quantization",
    "section": "",
    "text": "path = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\npretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\nlearn = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn.model.fc = nn.Linear(512, 2)\nlearn.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.536314\n0.409817\n0.817997\n00:03\n\n\n1\n0.327518\n0.273514\n0.889716\n00:03\n\n\n2\n0.203315\n0.206001\n0.916779\n00:03\n\n\n3\n0.116644\n0.180373\n0.932341\n00:03\n\n\n4\n0.073957\n0.182942\n0.925575\n00:03\n\n\n\n\n\n\nquantizer = Quantizer(\n    backend=\"x86\",\n    method=\"static\",    # Use dynamic quantization\n    verbose=True,       # See detailed output for debugging\n    use_per_tensor=False\n)\n\n# Quantize your model\nquantized_model = quantizer.quantize(\n    model=learn.model,\n    calibration_dl=dls.train,\n)\n\nPreparing model for static quantization with x86 backend\nCalibrating with up to 100 samples\n\n\nCalibrating: 100%|██████████| 1/1 [00:01&lt;00:00,  1.33s/it]\n\n\nConverting to quantized model\nQuantization complete\n\n\n\nprint(f'Size of the original model: {get_model_size(learn.model):.2f} MB')\nprint(f'Size of the quantized model: {get_model_size(quantized_model):.2f} MB')\n\nSize of the original model: 85.27 MB\nSize of the quantized model: 21.51 MB\n\n\n\ncompute_validation_accuracy(quantized_model, dls.valid)\n\n100%|██████████| 24/24 [00:02&lt;00:00,  9.64it/s]\n\n\n89.37753721244925"
  },
  {
    "objectID": "tutorials/walkthrough.html",
    "href": "tutorials/walkthrough.html",
    "title": "Walkthrough",
    "section": "",
    "text": "size, bs = 128, 32\ndls = get_dls(size, bs)\nLet’s start with a bit of context for the purpose of the demonstration. Imagine that we want to deploy a VGG16 model on a mobile device that has limited storage capacity and that our task requires our model to run sufficiently fast. It is known that parameters and speed efficiency are not the strong points of VGG16 but let’s see what we can do with it.\nLet’s first check the number of parameters and the inference time of VGG16.\nlearn = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nnum_parameters = get_num_parameters(learn.model)\ndisk_size = get_model_size(learn.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 537.30 MB (disk), 134309962 parameters\nSo our model has 134 millions parameters and needs 537MB of disk space in order to be stored\nmodel = learn.model.eval().to('cpu')\nx,y = dls.one_batch()\nprint(f'Inference Speed: {evaluate_cpu_speed(learn.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 27.51ms\nAnd it takes 26ms to perform inference on a single image.\nSnap ! This is more than we can afford for deployment, ideally we would like our model to take only half of that…but should we give up ? Nope, there are actually a lot of techniques that we can use to help reducing the size and improve the speed of our models! Let’s see how to apply them with FasterAI.\nWe will first train our VGG16 model to have a baseline of what performance we should expect from it.\nlearn.fit_one_cycle(10, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.058550\n1.862858\n0.379363\n00:18\n\n\n1\n1.721034\n1.621658\n0.476178\n00:15\n\n\n2\n1.404190\n1.387373\n0.561529\n00:15\n\n\n3\n1.245815\n1.127286\n0.628280\n00:15\n\n\n4\n1.068933\n1.300482\n0.630064\n00:15\n\n\n5\n0.920401\n0.852530\n0.724076\n00:15\n\n\n6\n0.807384\n0.737312\n0.762293\n00:15\n\n\n7\n0.723857\n0.658933\n0.787771\n00:15\n\n\n8\n0.642680\n0.607275\n0.802803\n00:15\n\n\n9\n0.600002\n0.591735\n0.808153\n00:15\nSo we would like our network to have comparable accuracy but fewer parameters and running faster… And the first technique that we will show how to use is called Knowledge Distillation",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#knowledge-distillation",
    "href": "tutorials/walkthrough.html#knowledge-distillation",
    "title": "Walkthrough",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation\nKnowledge distillation is a simple yet very efficient way to train a model. It was introduced in 2006 by Caruana et al.. The main idea behind is to use a small model (called the student) to approximate the function learned by a larger and high-performing model (called the teacher). This can be done by using the large model to pseudo-label the data. This idea has been used very recently to break the state-of-the-art accuracy on ImageNet.\nWhen we train our model for classification, we usually use a softmax as last layer. This softmax has the particularity to squish low value logits towards 0, and the highest logit towards 1. This has for effect to completely lose all the inter-class information, or what is sometimes called the dark knowledge. This is the information that is valuable and that we want to transfer from the teacher to the student.\nTo do so, we still use a regular classification loss but at the same time, we’ll use another loss, computed between the softened logits of the teacher (our soft labels) and the softened logits of the student (our soft predictions). Those soft values are obtained when you use a soft-softmax, that avoids squishing the values at its output. Our implementation follows this paper and the basic principle of training is represented in the figure below:\n\n\n\nTo use Knowledge Distillation with FasterAI, you only need to use this callback when training your student model:\n\n\n KnowledgeDistillation(teacher.model, loss) \n\n You only need to give to the callback function your teacher learner. Behind the scenes, FasterAI will take care of making your model train using knowledge distillation. \n\n\n\n\nfrom fasterai.distill.all import *\n\nThe first thing to do is to find a teacher, which can be any model, that preferrably performs well. We will chose VGG19 for our demonstration. To make sure it performs better than our VGG16 model, let’s start from a pretrained version.\n\nteacher = vision_learner(dls, models.vgg19_bn, metrics=[accuracy])\nteacher.fit_one_cycle(3, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.974149\n0.333519\n0.899618\n00:09\n\n\n1\n0.474079\n0.210869\n0.932484\n00:09\n\n\n2\n0.401034\n0.201098\n0.934777\n00:09\n\n\n\n\n\nOur teacher has 94% of accuracy which is pretty good, it is ready to take a student under its wing. So let’s create our student model and train it with the Knowledge Distillation callback:\n\nstudent = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nkd_cb = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-4, cbs=kd_cb)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n5.538533\n5.367633\n0.390064\n00:23\n\n\n1\n4.203467\n4.134189\n0.525605\n00:23\n\n\n2\n3.362392\n3.296359\n0.600764\n00:23\n\n\n3\n3.000877\n3.099703\n0.641529\n00:23\n\n\n4\n2.572030\n2.550671\n0.697325\n00:23\n\n\n5\n2.216439\n2.615297\n0.682038\n00:23\n\n\n6\n1.971665\n1.798345\n0.776560\n00:23\n\n\n7\n1.738369\n1.743116\n0.800764\n00:23\n\n\n8\n1.546372\n1.516451\n0.815287\n00:23\n\n\n9\n1.523020\n1.512000\n0.817325\n00:23\n\n\n\n\n\nAnd we can see that indeed, the knowledge of the teacher was useful for the student, as it is clearly overperforming the vanilla VGG16.\nOk, so now we are able to get more from a given model which is kind of cool ! With some experimentations we could come up with a model smaller than VGG16 but able to reach the same performance as our baseline! You can try to find it by yourself later, but for now let’s continue with the next technique !",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#sparsifying",
    "href": "tutorials/walkthrough.html#sparsifying",
    "title": "Walkthrough",
    "section": "Sparsifying",
    "text": "Sparsifying\nNow that we have a student model that is performing better than our baseline, we have some room to compress it. And we’ll start by making the network sparse. As explained in a previous article, there are many ways leading to a sparse network.\n\n\n\n\n\n\n\nNote\n\n\n\nUsually, the process of making a network sparse is called Pruning. I prefer using the term Pruning when parameters are actually removed from the network, which we will do in the next section.\n\n\n\n\n\nBy default, FasterAI uses the Automated Gradual Pruning paradigm as it removes parameters as the model trains and doesn’t require to pretrain the model, so it is usually much faster. In FasterAI, this is also managed by using a callback, that will replace the least important parameters of your model by zeroes during the training. The callback has a wide variety of parameters to tune your Sparsifying operation, let’s take a look at them:\n\n\nSparsifyCallback(learn, sparsity, granularity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ngranularity: on what granularity you want the sparsification to be operated\n\n\ncontext: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e linear, cosine, … and gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\n\nBut let’s come back to our example!\nHere, we will make our network 40% sparse, and remove entire filters, selected locally and based on L1 norm. We will train with a learning rate a bit smaller to be gentle with our network because it has already been trained. The scheduling selected is cosinusoidal, so the pruning starts and ends quite slowly.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='filter', context='global', criteria=large_final, schedule=cos)\nstudent.fit(10, 1e-5, cbs=sp_cb)\n\nPruning of filter until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.589216\n0.572886\n0.820382\n00:16\n\n\n1\n0.590776\n0.577411\n0.818089\n00:15\n\n\n2\n0.563901\n0.583462\n0.815287\n00:15\n\n\n3\n0.546637\n0.561217\n0.825732\n00:15\n\n\n4\n0.548417\n0.585929\n0.812229\n00:15\n\n\n5\n0.573570\n0.613701\n0.805605\n00:15\n\n\n6\n0.611797\n0.605457\n0.810955\n00:15\n\n\n7\n0.628342\n0.650856\n0.795669\n00:15\n\n\n8\n0.681669\n0.642906\n0.793631\n00:15\n\n\n9\n0.639112\n0.599782\n0.807389\n00:16\n\n\n\n\n\nSparsity at the end of epoch 0: [1.22]%\nSparsity at the end of epoch 1: [4.77]%\nSparsity at the end of epoch 2: [10.31]%\nSparsity at the end of epoch 3: [17.27]%\nSparsity at the end of epoch 4: [25.0]%\nSparsity at the end of epoch 5: [32.73]%\nSparsity at the end of epoch 6: [39.69]%\nSparsity at the end of epoch 7: [45.23]%\nSparsity at the end of epoch 8: [48.78]%\nSparsity at the end of epoch 9: [50.0]%\nFinal Sparsity: [50.0]%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 2              Conv2d          1,728      0              0.00%\nLayer 5              Conv2d          36,864     0              0.00%\nLayer 9              Conv2d          73,728     0              0.00%\nLayer 12             Conv2d          147,456    0              0.00%\nLayer 16             Conv2d          294,912    0              0.00%\nLayer 19             Conv2d          589,824    0              0.00%\nLayer 22             Conv2d          589,824    0              0.00%\nLayer 26             Conv2d          1,179,648  760,320       64.45%\nLayer 29             Conv2d          2,359,296  1,681,920     71.29%\nLayer 32             Conv2d          2,359,296  1,700,352     72.07%\nLayer 36             Conv2d          2,359,296  1,594,368     67.58%\nLayer 39             Conv2d          2,359,296  1,645,056     69.73%\nLayer 42             Conv2d          2,359,296  1,589,760     67.38%\n--------------------------------------------------------------------------------\nOverall              all             14,710,464 8,971,776     60.99%\n\n\nOur network now has 50% of its filters composed entirely of zeroes, without even losing accuracy. Obviously, choosing a higher sparsity makes it more difficult for the network to keep a similar accuracy. Other parameters can also widely change the behaviour of our sparsification process. For example choosing a more fine-grained sparsity usually leads to better results but is then more difficult to take advantage of in terms of speed.\n\nLet’s now see how much we gained in terms of speed. Because we removed 50% of convolution filters, we should expect crazy speed-up right ?\n\nprint(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 27.67ms\n\n\nWell actually, no. We didn’t remove any parameters, we just replaced some by zeroes, remember? The amount of parameters is still the same:\n\nnum_parameters = get_num_parameters(student.model)\ndisk_size = get_model_size(student.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 537.30 MB (disk), 134309962 parameters\n\n\nWhich leads us to the next section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#pruning",
    "href": "tutorials/walkthrough.html#pruning",
    "title": "Walkthrough",
    "section": "Pruning",
    "text": "Pruning\nWhy don’t we see any acceleration even though we removed half of the parameters? That’s because natively, our GPU does not know that our matrices are sparse and thus isn’t able to accelerate the computation. The easiest work around, is to physically remove the parameters we zeroed-out. But this operation requires to change the architecture of the network.\nThis pruning only works if we remove entire filters as it is the only case where we can change the architecture accordingly. Hopefully, sparse computations will soon be available on common deep learning librairies so this section will become useless in the future.\n\nHere is what it looks like with fasterai: \n\n\n\nPruneCallback(learn, sparsity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ncontext: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e linear, cosine, … and gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\nSo in the case of our example, it gives:\n\nfrom fasterai.prune.all import *\n\nLet’s now see what our model is capable of now:\n\npr_cb = PruneCallback(pruning_ratio=50, context='global', criteria=large_final, schedule=one_cycle)\nstudent.fit(5, 1e-5, cbs=pr_cb)\n\nIgnoring output layer: Linear(in_features=4096, out_features=10, bias=True)\nTotal ignored layers: 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.602215\n0.579023\n0.814268\n01:00\n\n\n1\n0.610252\n0.605600\n0.813248\n01:00\n\n\n2\n0.824139\n0.812066\n0.813758\n00:58\n\n\n3\n0.867410\n0.849190\n0.810955\n00:57\n\n\n4\n0.859486\n0.847288\n0.810191\n00:56\n\n\n\n\n\nSparsity at the end of epoch 0: 1.94%\nSparsity at the end of epoch 1: 19.96%\nSparsity at the end of epoch 2: 45.82%\nSparsity at the end of epoch 3: 49.74%\nSparsity at the end of epoch 4: 50.00%\n\n\n\nnum_parameters = get_num_parameters(student.model)\ndisk_size = get_model_size(student.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 181.41 MB (disk), 45339501 parameters\n\n\nAnd in terms of speed:\n\nprint(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 16.54ms\n\n\nYay ! Now we can talk ! Let’s just double check that our accuracy is unchanged and that we didn’t mess up somewhere:\n\nAnd there is actually more that we can do ! Let’s keep going !",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#batch-normalization-folding",
    "href": "tutorials/walkthrough.html#batch-normalization-folding",
    "title": "Walkthrough",
    "section": "Batch Normalization Folding",
    "text": "Batch Normalization Folding\nBatch Normalization Folding is a really easy to implement and straightforward idea. The gist is that batch normalization is nothing more than a normalization of the input data at each layer. Moreover, at inference time, the batch statistics used for this normalization are fixed. We can thus incorporate the normalization process directly in the convolution by changing its weights and completely remove the batch normalization layers, which is a gain both in terms of parameters and in terms of computations. For a more in-depth explaination, see this blog post.\nThis is how to use it with FasterAI:\n\nbn_folder = BN_Folder()\nbn_folder.fold(learn.model))\n\n Again, you only need to pass your model and FasterAI takes care of the rest. For models built using the nn.Sequential, you don’t need to change anything. For others, if you want to see speedup and compression, you actually need to subclass your model to remove the batch norm from the parameters and from the forward method of your network. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis operation should also be lossless as it redefines the convolution to take batch norm into account and is thus equivalent.\n\n\n\n\nfrom fasterai.misc.bn_folding import *\n\nLet’s do this with our model !\n\nbn_f = BN_Folder()\nfolded_model = bn_f.fold(student.model)\n\nThe parameters drop is generally not that significant, especially in a network such as VGG where almost all parameters are contained in the FC layers but, hey, any gain is good to take.\n\nnum_parameters = get_num_parameters(folded_model)\ndisk_size = get_model_size(folded_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 181.35 MB (disk), 45333867 parameters\n\n\nNow that we removed the batch normalization layers, we should again see a speedup.\n\nprint(f'Inference Speed: {evaluate_cpu_speed(folded_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 16.03ms\n\n\nAgain, let’s double check that we didn’t mess up somewhere:\n\nfolded_learner = Learner(dls, folded_model, metrics=[accuracy])\nfolded_learner.validate()\n\n\n\n\n\n\n\n\n(#2) [0.8472068905830383,0.8099362850189209]\n\n\nAnd we’re still not done yet ! As we know for VGG16, most of the parameters are comprised in the fully-connected layers so there should be something that we can do about it, right ?",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#fc-layers-factorization",
    "href": "tutorials/walkthrough.html#fc-layers-factorization",
    "title": "Walkthrough",
    "section": "FC Layers Factorization",
    "text": "FC Layers Factorization\nWe can indeed, factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\). With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\nIn FasterAI, to decompose the fully-connected layers of your model, here is what you need to do: \n\nFCD = FCDecomposer()\ndecomposed_model = FCD.decompose(model, percent_removed)\n\n The percent_removed corresponds to the percentage of singular values removed (k value above). \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis time, the decomposition is not exact, so we expect a drop in performance afterwards and further retraining will be needed.\n\n\nWhich gives with our example, if we only want to keep half of them:\n\nfrom fasterai.misc.fc_decomposer import *\n\n\nfc_decomposer = FC_Decomposer()\ndecomposed_model = fc_decomposer.decompose(folded_learner.model, percent_removed=0.5)\n\nHow many parameters do we have now ?\n\nnum_parameters = get_num_parameters(decomposed_model)\ndisk_size = get_model_size(decomposed_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 120.93 MB (disk), 28081814 parameters\n\n\nAnd how much time did we gain ?\n\nprint(f'Inference Speed: {evaluate_cpu_speed(decomposed_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 14.66ms\n\n\nWe actually get a network that is a little bit slower, but at the expense of reducing the by 10M the number of parameter. This is thus a matter of compromise between network weight and speed.\n\nHowever, this technique is an approximation so it is not lossless, so we should retrain our network a bit to recover its performance.\n\nfinal_learner = Learner(dls, decomposed_model, metrics=[accuracy])\nfinal_learner.fit_one_cycle(5, 1e-5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.003057\n1.004968\n0.603312\n00:09\n\n\n1\n0.870743\n0.923325\n0.660127\n00:09\n\n\n2\n0.813585\n0.798433\n0.718981\n00:09\n\n\n3\n0.760923\n0.776164\n0.763057\n00:09\n\n\n4\n0.706426\n0.766213\n0.766879\n00:09\n\n\n\n\n\nThis operation is usually less useful for more recent architectures as they usually do not have that many parameters in their fully-connected layers.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#quantization",
    "href": "tutorials/walkthrough.html#quantization",
    "title": "Walkthrough",
    "section": "Quantization",
    "text": "Quantization\n\nfrom fasterai.quantize.all import *\n\nNow that we have removed every superfluous parameter that we could, we can still continue to compress our model. A common way to do so is now to reduce the precision of each parameter in the network, making it considerably smaller. Such an approach is called Quantization and won’t affect the total number of parameter but will make each one of them smaller to store, on top of making computations faster.\nIn FasterAI, quantization can be done in a static way, i.e. apply quantization to the model, also called Post-Training Quantization. It also can be applied dynamically during the training, also called Quantization-Aware Training.\n\nfinal_learner.fit_one_cycle(5, 1e-5, cbs=QuantizeCallback())\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.800547\n0.817984\n0.743949\n00:13\n\n\n1\n0.759839\n0.769357\n0.785733\n00:13\n\n\n2\n0.725942\n0.796804\n0.767898\n00:13\n\n\n3\n0.644924\n0.713264\n0.801783\n00:13\n\n\n4\n0.627143\n0.706239\n0.802293\n00:13\n\n\n\n\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_learner.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 8.77ms\n\n\n\nnum_parameters = count_parameters_quantized(final_learner.model)\ndisk_size = get_model_size(final_learner.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")\n\nModel Size: 28.26 MB (disk), 28,081,814 parameters\n\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_learner.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 8.77ms",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#extra-acceleration",
    "href": "tutorials/walkthrough.html#extra-acceleration",
    "title": "Walkthrough",
    "section": "Extra Acceleration",
    "text": "Extra Acceleration\n\nfrom fasterai.misc.cpu_optimizer import accelerate_model_for_cpu\n\n\nfinal_model = accelerate_model_for_cpu(final_learner.model, x[0][None])\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 7.59ms\n\n\n\nnum_parameters = get_num_parameters(final_model)\ndisk_size = get_model_size(final_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")\n\nModel Size: 28.26 MB (disk), 0 parameters\n\n\n\nSo to recap, we saw in this article how to use fasterai to:  1. Make a student model learn from a teacher model (Knowledge Distillation)  2. Make our network sparse (Sparsifying)  3. Optionnaly physically remove the zero-filters (Pruning)  4. Remove the batch norm layers (Batch Normalization Folding)  5. Approximate our big fully-connected layers by smaller ones (Fully-Connected Layers Factorization)  6. Quantize the model to reduce the precision of the weights (Quantization)  7. Extra acceleration techniques to further optimize the speed of our network\n\nAnd we saw that by applying those, we could reduce our VGG16 model from 537 MB of parameters down to 26 MB (20x compression), and also speed-up the inference from 30.3ms to 7.9ms (3.8x speed-up) without any drop in accuracy compared to the baseline.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease keep in mind that the techniques presented above are not magic 🧙‍♂️, so do not expect to see a 200% speedup and compression everytime. What you can achieve highly depend on the architecture that you are using (some are already speed/parameter efficient by design) or the task it is doing (some datasets are so easy that you can remove almost all your network without seeing a drop in performance)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html",
    "href": "tutorials/sparse/sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network’s weight are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\nLet’s start by creating a model\nmodel = resnet18()\nAs you probably know, weights in a convolutional neural network have 4 dimensions ($ c_{out} c_{in} k_h k_w$)\nmodel.conv1.weight.ndim\n\n4\nIn the case of ResNet18, the dimension of the first layer weights is \\(64 \\times 3 \\times 7 \\times 7\\). We thus can plot each of the \\(64\\) filter as a \\(7 \\times 7\\) color image (because they contains \\(3\\) channels).\nplot_kernels(model.conv1)\nThe Sparsifier class allows us to remove some (part of) the filters, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\nUser can pass a single layer to prune by using the Sparsifier.sparsify_layer method.\nsource",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#granularity",
    "href": "tutorials/sparse/sparsifier.html#granularity",
    "title": "Sparsifier",
    "section": "Granularity",
    "text": "Granularity\nAs we said earlier, the granularity defines the structure of parameter that you will remove.\nIn the example below, we removed weight from each convolutional filter, meaning that we now have sparse filters, as can be seen in the image below:\n\nplot_kernels(model.conv1)\n\n\n\n\n\n\n\n\nAnother granularity is, for example, removing column vectors from the filters. To do so, just change the granularity parameter accordingly.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'column', 'local', large_final)\nsparsifier.sparsify_layer(model.conv1, 70)\n\n\nplot_kernels(model.conv1)\n\n\n\n\n\n\n\n\nFor more information and examples about the pruning granularities, I suggest you to take a look at the corresponding section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#context",
    "href": "tutorials/sparse/sparsifier.html#context",
    "title": "Sparsifier",
    "section": "Context",
    "text": "Context\nThe context defines where to look in the model, i.e. from where do we compare weight. The two basic contexts are: * local, i.e. we compare weight from each layer individually. This will lead to layers with similar levels of sparsity. * global, i.e. we compare weight from the whole model. This will lead to layers with different levels of sparsity\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'local', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      6,585         69.99%\nLayer 7              Conv2d          36,864     25,805        70.00%\nLayer 10             Conv2d          36,864     25,805        70.00%\nLayer 13             Conv2d          36,864     25,805        70.00%\nLayer 16             Conv2d          36,864     25,805        70.00%\nLayer 20             Conv2d          73,728     51,609        70.00%\nLayer 23             Conv2d          147,456    103,219       70.00%\nLayer 26             Conv2d          8,192      5,734         70.00%\nLayer 29             Conv2d          147,456    103,219       70.00%\nLayer 32             Conv2d          147,456    103,219       70.00%\nLayer 36             Conv2d          294,912    206,438       70.00%\nLayer 39             Conv2d          589,824    412,876       70.00%\nLayer 42             Conv2d          32,768     22,937        70.00%\nLayer 45             Conv2d          589,824    412,877       70.00%\nLayer 48             Conv2d          589,824    412,877       70.00%\nLayer 52             Conv2d          1,179,648  825,753       70.00%\nLayer 55             Conv2d          2,359,296  1,651,507     70.00%\nLayer 58             Conv2d          131,072    91,750        70.00%\nLayer 61             Conv2d          2,359,296  1,651,507     70.00%\nLayer 64             Conv2d          2,359,296  1,651,507     70.00%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 7,816,834     70.00%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      6,340         67.39%\nLayer 7              Conv2d          36,864     11,874        32.21%\nLayer 10             Conv2d          36,864     11,788        31.98%\nLayer 13             Conv2d          36,864     11,761        31.90%\nLayer 16             Conv2d          36,864     11,882        32.23%\nLayer 20             Conv2d          73,728     32,673        44.32%\nLayer 23             Conv2d          147,456    65,162        44.19%\nLayer 26             Conv2d          8,192      1,239         15.12%\nLayer 29             Conv2d          147,456    64,940        44.04%\nLayer 32             Conv2d          147,456    65,162        44.19%\nLayer 36             Conv2d          294,912    174,317       59.11%\nLayer 39             Conv2d          589,824    350,277       59.39%\nLayer 42             Conv2d          32,768     7,123         21.74%\nLayer 45             Conv2d          589,824    349,232       59.21%\nLayer 48             Conv2d          589,824    349,235       59.21%\nLayer 52             Conv2d          1,179,648  894,622       75.84%\nLayer 55             Conv2d          2,359,296  1,789,781     75.86%\nLayer 58             Conv2d          131,072    39,957        30.48%\nLayer 61             Conv2d          2,359,296  1,789,913     75.87%\nLayer 64             Conv2d          2,359,296  1,789,559     75.85%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 7,816,837     70.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#criteria",
    "href": "tutorials/sparse/sparsifier.html#criteria",
    "title": "Sparsifier",
    "section": "Criteria",
    "text": "Criteria\nThe criteria defines how we select the parameters to remove. It is usually given by a scoring method. The most common one is the large_final, i.e. select parameters with the highest absolute value as they are supposed to contribute the most to the final results of the model.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      6,340         67.39%\nLayer 7              Conv2d          36,864     11,870        32.20%\nLayer 10             Conv2d          36,864     11,904        32.29%\nLayer 13             Conv2d          36,864     11,784        31.97%\nLayer 16             Conv2d          36,864     11,769        31.93%\nLayer 20             Conv2d          73,728     32,861        44.57%\nLayer 23             Conv2d          147,456    64,797        43.94%\nLayer 26             Conv2d          8,192      1,265         15.44%\nLayer 29             Conv2d          147,456    65,236        44.24%\nLayer 32             Conv2d          147,456    64,946        44.04%\nLayer 36             Conv2d          294,912    174,580       59.20%\nLayer 39             Conv2d          589,824    349,429       59.24%\nLayer 42             Conv2d          32,768     6,896         21.04%\nLayer 45             Conv2d          589,824    349,440       59.24%\nLayer 48             Conv2d          589,824    350,151       59.37%\nLayer 52             Conv2d          1,179,648  894,264       75.81%\nLayer 55             Conv2d          2,359,296  1,790,074     75.87%\nLayer 58             Conv2d          131,072    40,049        30.55%\nLayer 61             Conv2d          2,359,296  1,789,386     75.84%\nLayer 64             Conv2d          2,359,296  1,789,797     75.86%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 7,816,838     70.00%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', small_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      9,407         99.99%\nLayer 7              Conv2d          36,864     702            1.90%\nLayer 10             Conv2d          36,864     1,177          3.19%\nLayer 13             Conv2d          36,864     281            0.76%\nLayer 16             Conv2d          36,864     142            0.39%\nLayer 20             Conv2d          73,728     4,066          5.51%\nLayer 23             Conv2d          147,456    3,290          2.23%\nLayer 26             Conv2d          8,192      9              0.11%\nLayer 29             Conv2d          147,456    8,264          5.60%\nLayer 32             Conv2d          147,456    1,489          1.01%\nLayer 36             Conv2d          294,912    46,798        15.87%\nLayer 39             Conv2d          589,824    85,884        14.56%\nLayer 42             Conv2d          32,768     94             0.29%\nLayer 45             Conv2d          589,824    101,028       17.13%\nLayer 48             Conv2d          589,824    107,154       18.17%\nLayer 52             Conv2d          1,179,648  1,059,786     89.84%\nLayer 55             Conv2d          2,359,296  2,171,266     92.03%\nLayer 58             Conv2d          131,072    99             0.08%\nLayer 61             Conv2d          2,359,296  2,084,975     88.37%\nLayer 64             Conv2d          2,359,296  2,130,925     90.32%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 7,816,836     70.00%\n\n\nFor more information and examples about the pruning criteria, I suggest you to take a look at the corresponding section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#remark",
    "href": "tutorials/sparse/sparsifier.html#remark",
    "title": "Sparsifier",
    "section": "Remark",
    "text": "Remark\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of 8, this can be done by passing the round_to parameter.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'filter', 'local', large_final)\nsparsifier.sparsify_model(70, round_to=8)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      5,880         62.50%\nLayer 7              Conv2d          36,864     23,040        62.50%\nLayer 10             Conv2d          36,864     23,040        62.50%\nLayer 13             Conv2d          36,864     23,040        62.50%\nLayer 16             Conv2d          36,864     23,040        62.50%\nLayer 20             Conv2d          73,728     50,688        68.75%\nLayer 23             Conv2d          147,456    101,376       68.75%\nLayer 26             Conv2d          8,192      5,632         68.75%\nLayer 29             Conv2d          147,456    101,376       68.75%\nLayer 32             Conv2d          147,456    101,376       68.75%\nLayer 36             Conv2d          294,912    202,752       68.75%\nLayer 39             Conv2d          589,824    405,504       68.75%\nLayer 42             Conv2d          32,768     22,528        68.75%\nLayer 45             Conv2d          589,824    405,504       68.75%\nLayer 48             Conv2d          589,824    405,504       68.75%\nLayer 52             Conv2d          1,179,648  811,008       68.75%\nLayer 55             Conv2d          2,359,296  1,622,016     68.75%\nLayer 58             Conv2d          131,072    90,112        68.75%\nLayer 61             Conv2d          2,359,296  1,622,016     68.75%\nLayer 64             Conv2d          2,359,296  1,622,017     68.75%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 7,667,449     68.66%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'filter', 'global', large_final)\nsparsifier.sparsify_model(70, round_to=8)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nLayer 1              Conv2d          9,408      8,232         87.50%\nLayer 7              Conv2d          36,864     0              0.00%\nLayer 10             Conv2d          36,864     0              0.00%\nLayer 13             Conv2d          36,864     0              0.00%\nLayer 16             Conv2d          36,864     0              0.00%\nLayer 20             Conv2d          73,728     69,120        93.75%\nLayer 23             Conv2d          147,456    138,240       93.75%\nLayer 26             Conv2d          8,192      0              0.00%\nLayer 29             Conv2d          147,456    138,240       93.75%\nLayer 32             Conv2d          147,456    129,024       87.50%\nLayer 36             Conv2d          294,912    285,696       96.88%\nLayer 39             Conv2d          589,824    571,392       96.88%\nLayer 42             Conv2d          32,768     0              0.00%\nLayer 45             Conv2d          589,824    571,392       96.88%\nLayer 48             Conv2d          589,824    571,392       96.88%\nLayer 52             Conv2d          1,179,648  1,161,216     98.44%\nLayer 55             Conv2d          2,359,296  2,322,432     98.44%\nLayer 58             Conv2d          131,072    0              0.00%\nLayer 61             Conv2d          2,359,296  2,322,432     98.44%\nLayer 64             Conv2d          2,359,296  2,322,432     98.44%\n--------------------------------------------------------------------------------\nOverall              all             11,166,912 10,611,240    95.02%\n\n\nFor more information about granularities at which you can operate, please check the related page.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/transformers.html",
    "href": "tutorials/sparse/transformers.html",
    "title": "Prune Transformers",
    "section": "",
    "text": "Note\n\n\n\nThis example code is taken from the fastai docs\npretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)\npath = untar_data(URLs.WIKITEXT_TINY)\nLet’s create our fastai Learner.\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())\nAnd let’s try to extend a given prompt with the pretrained model.\nprompt = \"\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn\"\npreds = learn.model.generate(inp, max_length=40, num_beams=5, temperature=1.5)\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\ntokenizer.decode(preds[0].cpu().numpy())\n\n'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn on its head.\\n\\nA unicorn is a magical creature with a rainbow tail and a horn'\nlearn.validate()\n\n\n\n\n\n\n\n\n(#2) [3.695716619491577,40.2744255065918]\nlearn.fit_one_cycle(1, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nperplexity\ntime\n\n\n\n\n0\n3.124115\n2.844266\n17.188944\n07:50\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None]\n\npreds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n\ntokenizer.decode(preds[0].cpu().numpy())\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_2855382/2352043074.py in &lt;cell line: 4&gt;()\n      2 inp = tensor(prompt_ids)[None]\n      3 \n----&gt; 4 preds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n      5 \n      6 tokenizer.decode(preds[0].cpu().numpy())\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\n     25         def decorate_context(*args, **kwargs):\n     26             with self.clone():\n---&gt; 27                 return func(*args, **kwargs)\n     28         return cast(F, decorate_context)\n     29 \n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/generation_utils.py in generate(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\n   1352             )\n   1353             # 12. run beam search\n-&gt; 1354             return self.beam_search(\n   1355                 input_ids,\n   1356                 beam_scorer,\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/generation_utils.py in beam_search(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\n   2203             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n   2204 \n-&gt; 2205             outputs = self(\n   2206                 **model_inputs,\n   2207                 return_dict=True,\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\n   1046         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n   1047 \n-&gt; 1048         transformer_outputs = self.transformer(\n   1049             input_ids,\n   1050             past_key_values=past_key_values,\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\n    832 \n    833         if inputs_embeds is None:\n--&gt; 834             inputs_embeds = self.wte(input_ids)\n    835         position_embeds = self.wpe(position_ids)\n    836         hidden_states = inputs_embeds + position_embeds\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n   1108         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1109                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1110             return forward_call(*input, **kwargs)\n   1111         # Do not call functions when jit is used\n   1112         full_backward_hooks, non_full_backward_hooks = [], []\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/modules/sparse.py in forward(self, input)\n    156 \n    157     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 158         return F.embedding(\n    159             input, self.weight, self.padding_idx, self.max_norm,\n    160             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\n~/miniconda3/envs/deep/lib/python3.8/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n   2181         # remove once script supports set_grad_enabled\n   2182         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n-&gt; 2183     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n   2184 \n   2185 \n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "tutorials/sparse/transformers.html#make-it-sparse",
    "href": "tutorials/sparse/transformers.html#make-it-sparse",
    "title": "Prune Transformers",
    "section": "Make it sparse !",
    "text": "Make it sparse !\nLet’s see now if we retrain our model, this time introducing sparsity\n\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())\n\nUnfortunately, the transformer model uses a custom layer: Conv1D, which is not a part of PyTorch. To overcome this problem, we have to add this layer to our Granularities class, so that it knows what to sparsify.\nHere, the Conv1D behaves like a Linear layer, i.e. the weights are defined by a matrix of dimension (nf,nx)\n\ndoc(Conv1D)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\nConv1D\nConv1D(nf, nx)1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n\nBasically works like a linear layer but the weights are transposed.\n\nArgs:\n    nf (`int`): The number of output features.\n    nx (`int`): The number of input features.\n\n\nWe can thus add the Conv1D granularity by using the add_granularity method, indicating the target module and the corresponding granularities that it can handle (the same as Linear so we can reuse it)\n\nGranularities.add_granularity(Conv1D, Granularities._granularities_Linear)\n\nLet’s now define our SparsifyCallback. Let’s say we want to make our model 30% sparse, by removing the highest-norm weight in each attention head.\n\nsp_cb = SparsifyCallback(sparsity=30, granularity='weight', context='local', criteria=large_final, schedule=one_cycle, layer_type=Conv1D)\n\nWe now only have to pass our callback to fastai\n\nlearn.fit_one_cycle(1, 1e-4, cbs=sp_cb)\n\nPruning of weight until a sparsity of [30]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nperplexity\ntime\n\n\n\n\n0\n3.151266\n2.882525\n17.859306\n09:44\n\n\n\n\n\nSparsity at the end of epoch 0: [30.0]%\nFinal Sparsity: [30.0]%\nSparsity in Conv1D 9: 30.00%\nSparsity in Conv1D 10: 30.00%\nSparsity in Conv1D 15: 30.00%\nSparsity in Conv1D 16: 30.00%\nSparsity in Conv1D 22: 30.00%\nSparsity in Conv1D 23: 30.00%\nSparsity in Conv1D 28: 30.00%\nSparsity in Conv1D 29: 30.00%\nSparsity in Conv1D 34: 30.00%\nSparsity in Conv1D 35: 30.00%\nSparsity in Conv1D 40: 30.00%\nSparsity in Conv1D 41: 30.00%\nSparsity in Conv1D 46: 30.00%\nSparsity in Conv1D 47: 30.00%\nSparsity in Conv1D 52: 30.00%\nSparsity in Conv1D 53: 30.00%\nSparsity in Conv1D 58: 30.00%\nSparsity in Conv1D 59: 30.00%\nSparsity in Conv1D 64: 30.00%\nSparsity in Conv1D 65: 30.00%\nSparsity in Conv1D 70: 30.00%\nSparsity in Conv1D 71: 30.00%\nSparsity in Conv1D 76: 30.00%\nSparsity in Conv1D 77: 30.00%\nSparsity in Conv1D 82: 30.00%\nSparsity in Conv1D 83: 30.00%\nSparsity in Conv1D 88: 30.00%\nSparsity in Conv1D 89: 30.00%\nSparsity in Conv1D 94: 30.00%\nSparsity in Conv1D 95: 30.00%\nSparsity in Conv1D 100: 30.00%\nSparsity in Conv1D 101: 30.00%\nSparsity in Conv1D 106: 30.00%\nSparsity in Conv1D 107: 30.00%\nSparsity in Conv1D 112: 30.00%\nSparsity in Conv1D 113: 30.00%\nSparsity in Conv1D 118: 30.00%\nSparsity in Conv1D 119: 30.00%\nSparsity in Conv1D 124: 30.00%\nSparsity in Conv1D 125: 30.00%\nSparsity in Conv1D 130: 30.00%\nSparsity in Conv1D 131: 30.00%\nSparsity in Conv1D 136: 30.00%\nSparsity in Conv1D 137: 30.00%\nSparsity in Conv1D 142: 30.00%\nSparsity in Conv1D 143: 30.00%\nSparsity in Conv1D 148: 30.00%\nSparsity in Conv1D 149: 30.00%\n\n\nAnd we can check the predicion to the same prompt as before\n\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None]\n\npreds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n\ntokenizer.decode(preds[0].cpu().numpy())\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn @-@ shaped head. The unicorn is a member of the &lt;unk&gt; &lt;unk&gt;'\n\n\nThat’s it ! You now have a sparse Transformer as performant as the whole model. However, this model is currently not more efficient speed and storage wise. To have such a speed-up, I suggest you to look at the granularity section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#get-the-data",
    "href": "tutorials/misc/fc_decomposer.html#get-the-data",
    "title": "Fully-Connected layers decomposition",
    "section": "1. Get the data",
    "text": "1. Get the data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#train-the-model",
    "href": "tutorials/misc/fc_decomposer.html#train-the-model",
    "title": "Fully-Connected layers decomposition",
    "section": "2. Train the model",
    "text": "2. Train the model\n\nlearn = Learner(dls, vgg16_bn(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(3)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.886644\n0.652151\n0.685386\n00:22\n\n\n1\n0.692583\n0.627857\n0.685386\n00:21\n\n\n2\n0.646516\n0.622866\n0.685386\n00:22\n\n\n\n\n\n\nDecompose !\n\n\nfc = FC_Decomposer()\nnew_model = fc.decompose(learn.model)\n\nThe fc layers have been factorized and replace by smaller ones.\n\nnew_model\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=25088, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Sequential(\n      (0): Linear(in_features=4096, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Sequential(\n      (0): Linear(in_features=4096, out_features=1, bias=False)\n      (1): Linear(in_features=1, out_features=2, bias=True)\n    )\n  )\n)\n\n\nWe can see compare the amount of parameters before/after:\n\ncount_parameters(learn.model)\n\n134277186\n\n\n\ncount_parameters(new_model)\n\n91281476\n\n\nThis represents a decrease of ~40M parameters !\nNow this is an approximation, so it isn’t really lossless and we should expect to see a performance drop, which will be bigger as we keep fewer singular values. Here we have:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\nnew_learn.validate()\n\n\n\n\n(#2) [0.6868855357170105,0.6853856444358826]",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html",
    "href": "tutorials/distill/distill_callback.html",
    "title": "KnowledgeDistillation Callback",
    "section": "",
    "text": "We’ll illustrate how to use Knowledge Distillation to distill the knowledge of a Resnet34 (the teacher), to a Resnet18 (the student)\nLet’s us grab some data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nThe first step is then to train the teacher model. We’ll start from a pretrained model, ensuring to get good results on our dataset.\n\nteacher = vision_learner(dls, resnet34, metrics=accuracy)\nteacher.unfreeze()\nteacher.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.712487\n0.780329\n0.826116\n00:04\n\n\n1\n0.426159\n0.454067\n0.895129\n00:04\n\n\n2\n0.306519\n0.290588\n0.897158\n00:04\n\n\n3\n0.196592\n0.349783\n0.875507\n00:04\n\n\n4\n0.172939\n0.191000\n0.925575\n00:04\n\n\n5\n0.131154\n0.193276\n0.926252\n00:04\n\n\n6\n0.107619\n0.802561\n0.884980\n00:04\n\n\n7\n0.070899\n0.199201\n0.936401\n00:04\n\n\n8\n0.039612\n0.191167\n0.937754\n00:04\n\n\n9\n0.024194\n0.194976\n0.937077\n00:04\n\n\n\n\n\n\nWithout KD\nWe’ll now train a Resnet18 from scratch, and without any help from the teacher model, to get that as a baseline\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n#student = vision_learner(dls, resnet18, metrics=accuracy)\nstudent.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.602736\n0.784080\n0.682003\n00:04\n\n\n1\n0.582019\n0.629800\n0.644790\n00:04\n\n\n2\n0.547411\n0.521493\n0.725981\n00:04\n\n\n3\n0.490268\n0.669058\n0.740189\n00:04\n\n\n4\n0.448316\n0.446682\n0.778078\n00:03\n\n\n5\n0.403792\n0.668784\n0.759811\n00:03\n\n\n6\n0.350714\n0.409201\n0.815291\n00:04\n\n\n7\n0.279282\n0.392315\n0.815968\n00:04\n\n\n8\n0.197490\n0.415861\n0.837618\n00:03\n\n\n9\n0.157046\n0.403317\n0.834235\n00:04\n\n\n\n\n\n\n\nWith KD\nAnd now we train the same model, but with the help of the teacher. The chosen loss is a combination of the regular classification loss (Cross-Entropy) and a loss pushing the student to learn from the teacher’s predictions.\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n#student = vision_learner(dls, resnet18, metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.874970\n2.434021\n0.709066\n00:04\n\n\n1\n2.619885\n2.321189\n0.737483\n00:04\n\n\n2\n2.381633\n2.690866\n0.730041\n00:04\n\n\n3\n2.101448\n1.772370\n0.771313\n00:04\n\n\n4\n1.824600\n1.707633\n0.793640\n00:04\n\n\n5\n1.588555\n1.433752\n0.814614\n00:04\n\n\n6\n1.273060\n1.264489\n0.843708\n00:04\n\n\n7\n0.979666\n1.169676\n0.849120\n00:04\n\n\n8\n0.768508\n1.047257\n0.862652\n00:04\n\n\n9\n0.630613\n1.043255\n0.861976\n00:04\n\n\n\n\n\nWhen helped, the student model performs better !\nThere exist more complicated KD losses, such as the one coming from Paying Attention to Attention, where the student tries to replicate the same attention maps of the teacher at intermediate layers.\nUsing such a loss requires to be able to specify from which layer we want to replicate those attention maps. To do so, we have to specify them from their string name, which can be obtained with the get_model_layers function.\nFor example, we set the loss to be applied after each Residual block of our models:\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, Attention, ['layer1', 'layer2', 'layer3', 'layer4'], ['0.4', '0.5', '0.6', '0.7'], weight=0.9)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.090439\n0.085387\n0.684709\n00:04\n\n\n1\n0.080193\n0.081185\n0.702300\n00:04\n\n\n2\n0.071975\n0.068845\n0.769959\n00:04\n\n\n3\n0.063899\n0.062546\n0.773342\n00:04\n\n\n4\n0.056500\n0.057492\n0.793640\n00:04\n\n\n5\n0.049420\n0.055552\n0.815968\n00:04\n\n\n6\n0.040951\n0.051518\n0.841678\n00:04\n\n\n7\n0.034474\n0.047924\n0.843708\n00:04\n\n\n8\n0.026169\n0.049825\n0.855210\n00:04\n\n\n9\n0.021952\n0.050935\n0.855210\n00:04",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/yolov8.html#training",
    "href": "tutorials/prune/yolov8.html#training",
    "title": "YOLOV8",
    "section": "Training",
    "text": "Training\n\nclass Args(argparse.Namespace):\n  model = 'yolov8l.pt'\n  cfg = 'default.yaml'\n  iterative_steps = 15\n  target_prune_rate = 0.15\n  max_map_drop = 0.2\n  sched = Schedule(partial(sched_onecycle,  α=10, β=4))\n\nargs=Args()\nprune(args)\n\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.731      0.768      0.828      0.658\n\nSpeed: 0.1ms preprocess, 7.5ms inference, 0.0ms loss, 0.8ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/val39\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nBefore Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.65799\n\n\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/train33\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/train33/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/train33\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10        12G     0.9535     0.9388      1.178    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.824      0.712      0.832      0.665\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.2G     0.9172     0.8093      1.161    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.836      0.755      0.842      0.682\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.6G     0.9013     0.7191      1.099    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.874      0.756      0.852      0.691\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      12.1G     0.9249     0.7495      1.118    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.858      0.793      0.863      0.699\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.8G     0.8319     0.6796      1.082    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.859       0.81      0.882      0.717\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.9G     0.8412     0.6843      1.096    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.848      0.828      0.892      0.729\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      12.1G      0.814     0.6248      1.067    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.88      0.825      0.898      0.737\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.9G     0.8093      0.615      1.067    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.905      0.819      0.897      0.748\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.8G     0.7573     0.5698      1.036    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.905       0.83      0.901      0.755\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      12.1G     0.7934     0.6052      1.075    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.894      0.835        0.9      0.758\n\n\n\n10 epochs completed in 0.029 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/train33/weights/last.pt, 175.3MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/train33/weights/best.pt, 175.3MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/train33/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.894      0.835        0.9      0.758\n\nSpeed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/train33\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.892      0.826      0.901      0.751\n\nSpeed: 0.2ms preprocess, 10.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/baseline_val28\n\n\n\n\nBefore Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.75140\nConv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.0027046189978777607\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 43081939 parameters, 74176 gradients, 162.7 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929        0.9      0.825      0.906      0.734\n\nSpeed: 0.2ms preprocess, 13.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_0_pre_val17\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_0_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_0_finetune16\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 1: MACs=81.5020432 G, #Params=43.105009 M, mAP=0.7335178809742455, speed up=1.0150224847369225\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_0_finetune16/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_0_finetune16\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      12.7G     0.7839       0.65      1.053    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.886      0.849      0.906      0.752\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.7G     0.7319      0.499      1.036    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.901      0.856      0.914      0.756\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      12.5G     0.7449     0.4985      1.014    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.896      0.864      0.915       0.76\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      12.6G     0.7679     0.5147      1.027    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.896      0.864      0.912      0.758\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      12.7G     0.6996     0.4846      1.015    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.909      0.863      0.915      0.761\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      12.8G     0.7258      0.516      1.026    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.908      0.869      0.916      0.766\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      12.8G     0.7541     0.5142      1.024    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.919      0.863      0.915      0.771\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      12.8G     0.7233     0.5094      1.014    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.912      0.868      0.917      0.776\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      12.8G       0.69     0.4918     0.9917    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.909      0.869      0.919      0.778\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      12.8G     0.7366     0.5463      1.035    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.919      0.865      0.918      0.779\n\n\n\n10 epochs completed in 0.029 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_0_finetune16/weights/last.pt, 173.0MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_0_finetune16/weights/best.pt, 173.0MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_0_finetune16/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.919      0.865      0.918      0.778\n\nSpeed: 0.1ms preprocess, 4.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_0_finetune16\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.909      0.865      0.915      0.776\n\nSpeed: 0.2ms preprocess, 13.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_0_post_val12\n\n\n\n\nAfter fine tuning mAP=0.775721307939776\nAfter post fine-tuning validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.005179586515491673\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 42712366 parameters, 74176 gradients, 161.3 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.923      0.859      0.916      0.767\n\nSpeed: 0.2ms preprocess, 13.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_1_pre_val9\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_1_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_1_finetune9\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 2: MACs=80.7933916 G, #Params=42.735334 M, mAP=0.7671079746246661, speed up=1.0239254072854147\n\n\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_1_finetune9/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_1_finetune9\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      13.6G     0.6788     0.5264     0.9988    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.91      0.873      0.918      0.777\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.7G     0.6175     0.4036     0.9705    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.93      0.863       0.92      0.783\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      12.6G      0.646     0.4189     0.9632    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.918      0.863      0.922      0.782\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      12.7G     0.6623     0.4434     0.9798    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.925       0.86      0.923      0.785\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      12.8G     0.6136     0.4164     0.9643    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.861      0.926      0.787\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      12.8G     0.6421     0.4466     0.9727    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.928      0.862      0.928      0.794\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      12.8G     0.6767      0.457     0.9847    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.934      0.865      0.931      0.795\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      12.9G     0.6599     0.4573     0.9827    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.877      0.934      0.797\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      12.8G     0.6616     0.4547     0.9732    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.928      0.872      0.935      0.802\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      12.7G     0.6991     0.5169      1.008    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.934      0.869      0.936      0.803\n\n\n\n10 epochs completed in 0.039 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_1_finetune9/weights/last.pt, 171.5MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_1_finetune9/weights/best.pt, 171.5MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_1_finetune9/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.933      0.871      0.935      0.803\n\nSpeed: 0.1ms preprocess, 4.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_1_finetune9\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.91      0.872       0.93      0.792\n\nSpeed: 0.2ms preprocess, 13.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_1_post_val9\n\n\n\n\nAfter fine tuning mAP=0.7924603103645905\nAfter post fine-tuning validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.009769531739708686\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 42094706 parameters, 74176 gradients, 158.8 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.908      0.868      0.924      0.779\n\nSpeed: 0.1ms preprocess, 13.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_2_pre_val9\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_2_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_2_finetune9\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 3: MACs=79.5541908 G, #Params=42.117503 M, mAP=0.7792330059497833, speed up=1.0398749024796818\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_2_finetune9/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_2_finetune9\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10        13G     0.6418     0.5222     0.9767    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.914      0.885      0.929      0.796\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.6G     0.5453      0.368     0.9336    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.92      0.884      0.938      0.802\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      12.5G     0.5814     0.3836     0.9312    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.925      0.884      0.937      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      12.6G     0.6058     0.3976     0.9452    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.949       0.87      0.937      0.807\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      12.6G     0.5516     0.3755     0.9373    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.948      0.867      0.939       0.81\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      12.7G     0.5859     0.4033     0.9496    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.929      0.887      0.942      0.811\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      12.6G      0.635     0.4225     0.9698    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.942      0.883      0.941      0.809\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      12.7G     0.6191     0.4212     0.9639    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.937      0.888      0.941      0.815\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      12.7G      0.631     0.4236     0.9608    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.936      0.889      0.942       0.82\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      12.6G     0.6808     0.5085      1.003    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.935      0.889      0.942      0.821\n\n\n\n10 epochs completed in 0.040 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_2_finetune9/weights/last.pt, 169.0MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_2_finetune9/weights/best.pt, 169.0MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_2_finetune9/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.935       0.89      0.942      0.821\n\nSpeed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_2_finetune9\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.883      0.942      0.811\n\nSpeed: 0.2ms preprocess, 13.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_2_post_val9\n\n\n\n\nAfter fine tuning mAP=0.8108994344622686\nAfter post fine-tuning validation\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.017924759478681728\nAfter Pruning\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 40919781 parameters, 74176 gradients, 154.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.911      0.866      0.928      0.771\n\nSpeed: 0.2ms preprocess, 13.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_3_pre_val8\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_3_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_3_finetune8\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 4: MACs=77.3600192 G, #Params=40.942254 M, mAP=0.7714194636333309, speed up=1.0693690003634333\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_3_finetune8/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_3_finetune8\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      12.6G     0.6354     0.4858     0.9676    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.931       0.87      0.936      0.788\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.6G     0.5377     0.3467     0.9207    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.933      0.878      0.939      0.795\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      12.5G      0.588     0.3751     0.9277    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.939      0.875      0.939      0.801\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      12.6G     0.5795     0.3872     0.9303    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.944       0.87      0.937      0.805\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      12.6G     0.5511     0.3675     0.9235    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.943      0.872      0.937      0.812\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      12.7G     0.5638     0.3974     0.9317    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.925      0.879      0.939      0.812\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      12.6G     0.6183     0.4048     0.9559    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.915      0.885      0.939      0.812\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      12.7G     0.6064     0.4156     0.9551    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.888       0.94       0.81\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      12.7G     0.6126     0.4111     0.9473    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.928      0.898      0.943      0.819\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      12.6G     0.6751     0.4922      1.001    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.898      0.942       0.82\n\n\n\n10 epochs completed in 0.027 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_3_finetune8/weights/last.pt, 164.3MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_3_finetune8/weights/best.pt, 164.3MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_3_finetune8/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.898      0.942       0.82\n\nSpeed: 0.1ms preprocess, 4.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_3_finetune8\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.919      0.891      0.941      0.813\n\nSpeed: 0.1ms preprocess, 13.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_3_post_val7\n\n\n\n\nAfter fine tuning mAP=0.813335254805048\nAfter post fine-tuning validation\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n0.03136884242508382\nAfter Pruning\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 39455305 parameters, 74176 gradients, 149.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.853       0.93      0.771\n\nSpeed: 0.2ms preprocess, 13.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_4_pre_val7\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_4_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_4_finetune7\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 5: MACs=74.8418608 G, #Params=39.477376 M, mAP=0.7709511018059185, speed up=1.1053494062777232\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_4_finetune7/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_4_finetune7\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      12.8G     0.6393     0.4996     0.9632    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.91      0.879      0.934      0.787\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.7G     0.5385     0.3464     0.9142    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.915      0.879      0.936      0.801\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      12.5G     0.5725     0.3758     0.9139    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.916      0.899      0.943      0.805\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      12.6G     0.5814     0.3863     0.9212    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.923        0.9      0.948      0.812\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      12.7G     0.5392     0.3637     0.9171    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.901      0.948      0.811\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      12.9G     0.5582     0.4023     0.9233    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.93      0.898       0.95      0.813\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      12.8G     0.6234     0.4091     0.9585    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.905      0.951      0.816\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      12.8G     0.6062     0.4115     0.9444    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.908      0.953      0.816\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      12.7G     0.6105     0.4111     0.9411    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.906      0.949      0.818\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      12.6G     0.6834      0.498      1.001    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.926      0.906       0.95      0.819\n\n\n\n10 epochs completed in 0.027 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_4_finetune7/weights/last.pt, 158.5MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_4_finetune7/weights/best.pt, 158.5MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_4_finetune7/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.905       0.95      0.819\n\nSpeed: 0.1ms preprocess, 4.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_4_finetune7\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.921      0.909      0.951      0.816\n\nSpeed: 0.2ms preprocess, 13.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_4_post_val6\n\n\n\n\nAfter fine tuning mAP=0.8155992309783842\nAfter post fine-tuning validation\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.051012679818528694\nAfter Pruning\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 37708749 parameters, 74176 gradients, 143.2 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.902      0.862      0.927      0.764\n\nSpeed: 0.2ms preprocess, 11.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_5_pre_val6\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_5_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_5_finetune6\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 6: MACs=71.732976 G, #Params=37.730325 M, mAP=0.7640629035267851, speed up=1.1532549046898597\n\n\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_5_finetune6/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_5_finetune6\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      13.1G     0.6622     0.5187     0.9815    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.911      0.878      0.936      0.782\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.7G     0.5412     0.3602       0.91    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.908      0.899       0.94      0.793\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      12.6G     0.5946     0.3883     0.9197    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.918      0.893      0.941      0.799\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      12.7G     0.5856      0.396     0.9263    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.937       0.88      0.942      0.805\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      12.7G     0.5495     0.3637     0.9176    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.93      0.873      0.944      0.808\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      12.8G     0.5562      0.396     0.9231    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.938      0.872      0.945      0.805\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      12.7G     0.6301     0.4115     0.9537    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.883      0.942      0.808\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      12.8G     0.6138     0.4093      0.944    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.919      0.894      0.946       0.81\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      12.8G     0.6297     0.4207     0.9478    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.926      0.898      0.949      0.813\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      12.7G      0.703     0.5027      1.009    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.898       0.95      0.814\n\n\n\n10 epochs completed in 0.037 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_5_finetune6/weights/last.pt, 151.5MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_5_finetune6/weights/best.pt, 151.5MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_5_finetune6/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.898       0.95      0.814\n\nSpeed: 0.1ms preprocess, 4.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_5_finetune6\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.888      0.946       0.81\n\nSpeed: 0.1ms preprocess, 11.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_5_post_val6\n\n\n\n\nAfter fine tuning mAP=0.8104708131787314\nAfter post fine-tuning validation\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.07518590641324997\nAfter Pruning\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 35995675 parameters, 74176 gradients, 136.7 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.878      0.799      0.907       0.74\n\nSpeed: 0.2ms preprocess, 12.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_6_pre_val6\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_6_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_6_finetune6\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 7: MACs=68.4860368 G, #Params=36.016747 M, mAP=0.7398590274182758, speed up=1.207930992438447\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_6_finetune6/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_6_finetune6\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      11.7G     0.6993     0.5584     0.9858    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.902      0.841      0.921      0.764\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.8G     0.5531     0.3752     0.9125    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.91      0.849      0.927      0.782\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.7G     0.6039     0.4089     0.9216    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.926       0.86       0.93      0.788\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.5G     0.6132      0.414     0.9289    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.906      0.882      0.933      0.796\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.5G     0.5718      0.383     0.9257    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.909      0.879      0.938      0.789\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.9G     0.5734     0.4089     0.9254    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.928      0.877      0.942      0.797\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.5G     0.6396     0.4206     0.9589    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.929      0.883      0.945        0.8\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.6G     0.6347     0.4307     0.9533    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.932      0.879      0.946      0.807\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.6G     0.6589     0.4376     0.9609    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.931      0.885      0.947      0.808\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.8G     0.7121     0.5232      1.016    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.93      0.886      0.945       0.81\n\n\n\n10 epochs completed in 0.025 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_6_finetune6/weights/last.pt, 144.6MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_6_finetune6/weights/best.pt, 144.6MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_6_finetune6/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.929      0.887      0.945      0.808\n\nSpeed: 0.1ms preprocess, 4.7ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_6_finetune6\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.887      0.943      0.804\n\nSpeed: 0.1ms preprocess, 12.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_6_post_val5\n\n\n\n\nAfter fine tuning mAP=0.8040926175515907\nAfter post fine-tuning validation\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.09935913300797124\nAfter Pruning\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 34583399 parameters, 74176 gradients, 131.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.915      0.829      0.917      0.733\n\nSpeed: 0.1ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_7_pre_val5\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_7_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_7_finetune5\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 8: MACs=65.8289424 G, #Params=34.604045 M, mAP=0.7333194226117398, speed up=1.2566874597092115\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_7_finetune5/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_7_finetune5\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      11.2G     0.6839     0.5488     0.9811    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.931      0.847      0.932      0.764\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.5G     0.5602     0.3693     0.9134    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.92      0.869      0.936      0.781\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.4G     0.5984     0.3895     0.9188    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.905      0.879      0.936      0.782\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.6G     0.6034      0.418     0.9248    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.899      0.886      0.936      0.782\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.4G     0.5658     0.3782      0.928    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.925      0.872      0.938      0.788\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.7G     0.5917     0.4156     0.9288    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.92      0.882      0.939      0.794\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.4G     0.6339     0.4252     0.9516    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.933      0.876      0.944      0.798\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.4G     0.6471     0.4304     0.9604    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.933      0.878      0.945        0.8\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.4G      0.668     0.4424      0.959    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.884      0.944      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.6G     0.7304     0.5445      1.031    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.919      0.887      0.943      0.802\n\n\n\n10 epochs completed in 0.024 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_7_finetune5/weights/last.pt, 139.0MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_7_finetune5/weights/best.pt, 139.0MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_7_finetune5/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.884      0.944      0.802\n\nSpeed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_7_finetune5\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.915       0.88       0.94      0.793\n\nSpeed: 0.2ms preprocess, 11.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_7_post_val5\n\n\n\n\nAfter fine tuning mAP=0.7931599917268005\nAfter post fine-tuning validation\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.11900297040141611\nAfter Pruning\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 33747610 parameters, 74176 gradients, 128.5 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.919      0.861       0.93      0.769\n\nSpeed: 0.2ms preprocess, 11.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_8_pre_val5\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_8_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_8_finetune5\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 9: MACs=64.3900056 G, #Params=33.768007 M, mAP=0.7690853683854553, speed up=1.2847709148203583\n\n\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_8_finetune5/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_8_finetune5\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10        12G     0.6504     0.5212     0.9696    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.934       0.87      0.935      0.782\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.6G     0.5047     0.3425     0.8995    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.908      0.889      0.936      0.791\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.5G     0.5622     0.3725     0.9046    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.913      0.892      0.938      0.788\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.7G     0.5558     0.3856     0.9139    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.925      0.894      0.941      0.791\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.7G     0.5481     0.3728       0.92    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.92      0.886      0.939       0.79\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.2G     0.5576     0.4045     0.9157    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.928      0.888      0.943      0.792\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.1G      0.635      0.429     0.9474    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.935      0.891      0.947      0.794\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.2G     0.6145     0.4019     0.9449    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.939      0.887      0.949        0.8\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.2G     0.6569     0.4243     0.9517    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.945      0.881      0.949      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.6G     0.7253     0.5226      1.024    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.945      0.883      0.948      0.802\n\n\n\n10 epochs completed in 0.034 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_8_finetune5/weights/last.pt, 135.6MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_8_finetune5/weights/best.pt, 135.6MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_8_finetune5/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.946      0.881      0.949      0.803\n\nSpeed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_8_finetune5\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.937      0.875      0.942      0.796\n\nSpeed: 0.2ms preprocess, 11.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_8_post_val5\n\n\n\n\nAfter fine tuning mAP=0.7963247276929302\nAfter post fine-tuning validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.1324470533478182\nAfter Pruning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 33209910 parameters, 74176 gradients, 126.7 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.935      0.847      0.931      0.768\n\nSpeed: 0.2ms preprocess, 11.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_9_pre_val3\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_9_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_9_finetune3\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 10: MACs=63.4942128 G, #Params=33.230145 M, mAP=0.7676251325156896, speed up=1.302896795658832\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_9_finetune3/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_9_finetune3\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      12.2G      0.627     0.5006     0.9614    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.92      0.879       0.94      0.792\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      12.4G     0.4757     0.3198     0.8867    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.935      0.878      0.944      0.798\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.7G     0.5402     0.3563     0.8958    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.931      0.877      0.941      0.798\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.8G     0.5459     0.3752     0.9083    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.934      0.882      0.945      0.796\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.8G     0.5164     0.3487     0.9079    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.923      0.893      0.943      0.795\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.9G     0.5421      0.385     0.9102    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.918      0.896      0.942      0.793\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.8G     0.6251     0.4038     0.9468    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.926       0.89      0.945      0.795\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.9G     0.5953     0.3998      0.934    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.918      0.905      0.946      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.9G     0.6388     0.4184     0.9476    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.92      0.898      0.947      0.802\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.7G     0.7187     0.5164      1.018    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.93      0.895      0.947      0.806\n\n\n\n10 epochs completed in 0.031 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_9_finetune3/weights/last.pt, 133.4MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_9_finetune3/weights/best.pt, 133.4MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_9_finetune3/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.932      0.893      0.947      0.805\n\nSpeed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_9_finetune3\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.936      0.889      0.945      0.801\n\nSpeed: 0.2ms preprocess, 11.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_9_post_val3\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine tuning mAP=0.8009899904343383\nAfter post fine-tuning validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.14060228108679124\nAfter Pruning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32703049 parameters, 74176 gradients, 124.6 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.847      0.937      0.771\n\nSpeed: 0.1ms preprocess, 12.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_10_pre_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_10_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_10_finetune2\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 11: MACs=62.4345712 G, #Params=32.723122 M, mAP=0.7711514639154989, speed up=1.3250096030130178\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_10_finetune2/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_10_finetune2\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10        12G     0.6334     0.5193     0.9605    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.931      0.852       0.94      0.788\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.5G      0.466     0.3067     0.8851    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.942      0.847      0.937      0.796\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.4G     0.5156      0.342     0.8852    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.946      0.868      0.938      0.793\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.5G     0.5392     0.3641     0.9066    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.934      0.884      0.937      0.791\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.5G     0.5117     0.3557     0.9081    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.909      0.894      0.941      0.796\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.6G      0.515      0.373     0.9027    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.911      0.889      0.939      0.796\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.5G     0.6053     0.4073     0.9379    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.905      0.884      0.939      0.797\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.6G     0.5829       0.39     0.9326    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.881      0.944      0.801\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.5G     0.6332     0.4096      0.942    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.929      0.886      0.945      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.5G     0.7178     0.5183      1.015    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.941      0.877      0.945      0.808\n\n\n\n10 epochs completed in 0.033 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_10_finetune2/weights/last.pt, 131.4MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_10_finetune2/weights/best.pt, 131.4MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_10_finetune2/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.941      0.876      0.945      0.809\n\nSpeed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_10_finetune2\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.92      0.896       0.95      0.808\n\nSpeed: 0.2ms preprocess, 12.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_10_post_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine tuning mAP=0.8081809745840371\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.14519222631100823\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32669140 parameters, 74176 gradients, 124.6 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.917      0.899       0.95       0.81\n\nSpeed: 0.2ms preprocess, 12.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_11_pre_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_11_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_11_finetune2\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 12: MACs=62.4070664 G, #Params=32.689204 M, mAP=0.8098376035512942, speed up=1.325593577332454\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_11_finetune2/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_11_finetune2\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      11.5G     0.5624     0.4578     0.9404    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.905      0.907      0.946      0.813\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.6G      0.399     0.2726      0.865    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.931      0.879      0.941      0.807\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.4G     0.4809     0.3212      0.879    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.929      0.892      0.941      0.811\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.5G     0.4806      0.334     0.8879    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.945      0.881       0.94      0.804\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.6G     0.4862     0.3288     0.8994    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.938      0.883      0.942      0.806\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.6G     0.4887     0.3584      0.893    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.929       0.89      0.943       0.81\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.6G     0.5702     0.3791     0.9254    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.937      0.888      0.943      0.815\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.6G     0.5531     0.3689     0.9184    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.933      0.896      0.946      0.812\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.6G      0.588     0.3884      0.923    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.926        0.9       0.95      0.815\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.5G     0.7003     0.4962      1.007    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.899      0.949      0.814\n\n\n\n10 epochs completed in 0.019 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_11_finetune2/weights/last.pt, 131.3MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_11_finetune2/weights/best.pt, 131.3MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_11_finetune2/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.925        0.9       0.95      0.815\n\nSpeed: 0.1ms preprocess, 4.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_11_finetune2\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.932      0.884      0.943      0.809\n\nSpeed: 0.2ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_11_post_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine tuning mAP=0.80867019892426\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.14766719382862217\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.884      0.943      0.793\n\nSpeed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_12_pre_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_12_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_12_finetune2\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 13: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.7929855954557918, speed up=1.3375568226839933\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_12_finetune2/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_12_finetune2\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      11.7G     0.5728     0.4693     0.9395    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.935      0.885      0.944      0.813\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.6G      0.403      0.267     0.8612    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.934      0.876      0.946      0.813\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.4G     0.4806     0.3161     0.8771    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.885      0.945      0.815\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.5G     0.4954     0.3358     0.8895    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.923      0.898      0.946       0.81\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.5G     0.4868     0.3256     0.8983    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.927      0.889      0.945      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.5G     0.5005     0.3485      0.893    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.923      0.901      0.946      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.5G     0.5637     0.3694     0.9171    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.923      0.898      0.946      0.803\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.6G     0.5525      0.361     0.9134    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.94      0.892      0.945      0.809\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.6G     0.5873     0.3731      0.923    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.94      0.897      0.945      0.815\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.5G     0.7004     0.4849      1.012    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.94      0.898      0.946      0.813\n\n\n\n10 epochs completed in 0.030 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_12_finetune2/weights/last.pt, 130.3MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_12_finetune2/weights/best.pt, 130.3MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_12_finetune2/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.921      0.886      0.945      0.814\n\nSpeed: 0.1ms preprocess, 4.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_12_finetune2\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.931      0.889      0.946       0.81\n\nSpeed: 0.2ms preprocess, 12.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_12_post_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine tuning mAP=0.81035824268819\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.14897095513156428\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.921      0.899      0.946      0.809\n\nSpeed: 0.1ms preprocess, 12.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_13_pre_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_13_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_13_finetune2\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 14: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8094708031863412, speed up=1.3375568226839933\n\n\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_13_finetune2/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_13_finetune2\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      11.2G     0.5243     0.4358     0.9284    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.922      0.904      0.948      0.817\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.3G     0.3797     0.2496     0.8509    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.926      0.903      0.949      0.819\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.2G     0.4293     0.2921     0.8642    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.939       0.89      0.948      0.821\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.2G     0.4573     0.3141     0.8773    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.936      0.899      0.949      0.816\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.3G     0.4667     0.3122     0.8898    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.929      0.897      0.947      0.811\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.4G     0.4877     0.3356     0.8894    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.932      0.889      0.945      0.802\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.3G     0.5567     0.3642     0.9154    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.937      0.882      0.947      0.807\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.4G      0.522     0.3407     0.9033    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.945      0.885      0.947      0.816\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.3G       0.59     0.3656     0.9174    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.946      0.884      0.947      0.818\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.3G     0.7007      0.488      1.009    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.941      0.887      0.949      0.824\n\n\n\n10 epochs completed in 0.020 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_13_finetune2/weights/last.pt, 130.3MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_13_finetune2/weights/best.pt, 130.3MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_13_finetune2/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.941      0.887      0.949      0.823\n\nSpeed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_13_finetune2\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.923      0.902      0.949      0.818\n\nSpeed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_13_post_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n\n\n\nAfter fine tuning mAP=0.8178296276387611\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n0.14964931342467439\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.926      0.897      0.949      0.817\n\nSpeed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_14_pre_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nyolo/engine/trainer: task=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_14_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=/home/HubensN/ultralytics/runs/detect/step_14_finetune2\n\nAMP: running Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 15: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8171226992562459, speed up=1.3375568226839933\n\n\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/utils/checks.py:373: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(True):\n\nAMP: checks passed ✅\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n\n  self.scaler = amp.GradScaler(enabled=self.amp)\n\ntrain: Scanning /home/HubensN/datasets/coco128/labels/train\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\nPlotting labels to /home/HubensN/ultralytics/runs/detect/step_14_finetune2/labels.jpg... \n\noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n\nImage sizes 640 train, 640 val\n\nUsing 8 dataloader workers\n\nLogging results to /home/HubensN/ultralytics/runs/detect/step_14_finetune2\n\nStarting training for 10 epochs...\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/yolo/engine/trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n\n  with torch.cuda.amp.autocast(self.amp):\n\n       1/10      11.3G     0.5084     0.4275     0.9241    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.943      0.886      0.945      0.824\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       2/10      11.3G     0.3449     0.2386     0.8452    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.939      0.889      0.948      0.826\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       3/10      11.2G     0.4181     0.2863     0.8581    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.945       0.89      0.948      0.821\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       4/10      11.5G     0.4197     0.3033     0.8692    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.954      0.903      0.952       0.82\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       5/10      11.3G     0.4552     0.3053     0.8846    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.949      0.905       0.95      0.819\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       6/10      11.4G     0.4572     0.3242      0.878    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.941      0.904      0.955      0.816\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       7/10      11.3G     0.5533     0.3546     0.9156    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.928      0.905      0.948      0.818\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       8/10      11.7G     0.5238     0.3388     0.9018    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.934      0.909      0.949       0.82\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n       9/10      11.6G     0.5513     0.3486     0.9081    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.943      0.912      0.952      0.821\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n      10/10      11.3G     0.6869      0.468     0.9985    \n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.944      0.913      0.952       0.82\n\n\n\n10 epochs completed in 0.018 hours.\n\n/tmp/ipykernel_4010773/2763320149.py:114: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  x = torch.load(f, map_location=torch.device('cpu'))\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/last.pt, 130.3MB\n\nOptimizer stripped from /home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/best.pt, 130.3MB\n\n\n\nValidating /home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/best.pt...\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929       0.94      0.889      0.948      0.826\n\nSpeed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_14_finetune2\n\n/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/nn/tasks.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  return torch.load(file, map_location='cpu'), file  # load\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.924      0.896      0.945       0.82\n\nSpeed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_14_post_val2\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CPU\n\n\n\n\nAfter fine tuning mAP=0.8199030883902761\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n\n\n\nYOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n\n\n\nPyTorch: starting from /home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (124.2 MB)\n\n\n\nONNX: starting export with onnx 1.17.0 opset 19...\n\nONNX: export success ✅ 2.2s, saved as /home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/best.onnx (123.9 MB)\n\n\n\nExport complete (2.9s)\n\nResults saved to /home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights\n\nPredict:         yolo predict task=detect model=/home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/best.onnx imgsz=640 \n\nValidate:        yolo val task=detect model=/home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/best.onnx imgsz=640 data=/home/HubensN/miniconda3/envs/fasterai20/lib/python3.10/site-packages/ultralytics/datasets/coco128.yaml \n\nVisualize:       https://netron.app",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "YOLOV8"
    ]
  },
  {
    "objectID": "tutorials/prune/yolov8.html#post-training-checks",
    "href": "tutorials/prune/yolov8.html#post-training-checks",
    "title": "YOLOV8",
    "section": "Post-Training Checks",
    "text": "Post-Training Checks\n\nmodel = YOLO('/home/HubensN/ultralytics/runs/detect/step_14_finetune2/weights/best.pt')\n\n\nexample_inputs = torch.randn(1, 3, 640, 640).to(model.device)\n\n\nbase_macs, base_nparams = tp.utils.count_ops_and_params(model.model, example_inputs); base_macs, base_nparams\n\n(61848891200.0, 32436843)\n\n\n\nresults = model.val(\n                data='coco128.yaml',\n                batch=1,\n                imgsz=640,\n                verbose=False,\n            )\n\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n\nval: Scanning /home/HubensN/datasets/coco128/labels/train20\n\n                 Class     Images  Instances      Box(P    \n\n                   all        128        929      0.941      0.891      0.948      0.828\n\nSpeed: 0.1ms preprocess, 9.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n\nResults saved to /home/HubensN/ultralytics/runs/detect/val41\n\n\n\n\n\nresults\n\nultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])\nbox: ultralytics.yolo.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.yolo.utils.metrics.ConfusionMatrix object&gt;\nfitness: 0.8398465728578197\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.80663,     0.56166,     0.46391,     0.97747,     0.97261,     0.92577,      0.9079,     0.78266,     0.69041,     0.39913,      0.8278,      0.8955,      0.8278,     0.86033,     0.85892,       0.995,       0.962,     0.92662,      0.8278,      0.8278,     0.93661,       0.995,     0.96237,     0.85834,\n           0.78541,     0.86574,     0.69715,     0.81464,     0.94304,      0.7515,      0.8955,     0.81395,     0.34834,     0.52181,     0.74658,     0.37593,     0.74195,      0.8278,     0.64171,     0.66705,     0.73098,     0.83937,      0.7376,     0.65567,     0.71183,     0.84817,       0.995,      0.8278,\n             0.995,      0.8408,     0.68827,      0.8293,     0.92344,      0.9785,       0.995,     0.92662,      0.8607,     0.91091,     0.86934,       0.995,     0.91683,      0.8603,      0.9501,     0.95359,     0.80117,     0.84734,      0.8278,     0.69016,     0.95773,     0.96025,      0.8278,     0.83127,\n             0.977,     0.58867,     0.88629,       0.995,       0.995,     0.93602,      0.8278,      0.9465])\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9410256301337474, 'metrics/recall(B)': 0.8911335314866058, 'metrics/mAP50(B)': 0.9482558527722713, 'metrics/mAP50-95(B)': 0.8278010973117694, 'fitness': 0.8398465728578197}\nsave_dir: Path('/home/HubensN/ultralytics/runs/detect/val41')\nspeed: {'preprocess': 0.14928914606571198, 'inference': 9.854648262262344, 'loss': 0.004881992936134338, 'postprocess': 0.43218769133090973}\n\n\n\nmodel.export(format = 'onnx', half = True)\n\nUltralytics YOLOv8.0.124 🚀 Python-3.10.15 torch-2.5.1+cu124 CPU\nWARNING ⚠️ half=True only compatible with GPU export, i.e. use device=0\n\nKeyboardInterrupt",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "YOLOV8"
    ]
  },
  {
    "objectID": "prune/pruner.html",
    "href": "prune/pruner.html",
    "title": "Pruner",
    "section": "",
    "text": "source\n\nPruner.prune_model\n\n Pruner.prune_model ()\n\nLet’s try the Pruner with a VGG16 model\n\nmodel = resnet18(); model\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\nThe Prunercan either remove filters based on local criteria (i.e. each layer will be trimmed of the same % of filters)\n\npruner = Pruner(model, 30, 'local', large_final)\npruner.prune_model()\nprint(model)\n\nIgnoring output layer: Linear(in_features=512, out_features=1000, bias=True)\nTotal ignored layers: 1\nResNet(\n  (conv1): Conv2d(3, 44, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(44, 89, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(44, 89, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(89, 179, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(89, 179, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(179, 179, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(179, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(179, 358, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(358, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(179, 358, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(358, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(358, 358, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(358, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=358, out_features=1000, bias=True)\n)\n\n\nThe Prunercan also remove filters based on global criteria (i.e. each layer will be trimmed of a different % of filters, but we specify the sparsity of the whole network)\n\nmodel = resnet18()\npruner = Pruner(model, 50, 'global', large_final)\npruner.prune_model()\nprint(model)\n\nIgnoring output layer: Linear(in_features=512, out_features=1000, bias=True)\nTotal ignored layers: 1\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 75, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(75, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(61, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 472, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 472, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(472, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 472, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(472, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=472, out_features=1000, bias=True)\n)",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Pruner"
    ]
  }
]