[
  {
    "objectID": "prune/prune_callback.html",
    "href": "prune/prune_callback.html",
    "title": "Prune Callback",
    "section": "",
    "text": "The PruneCallback integrates structured pruning into the fastai training loop. Unlike sparsification (which zeros weights), pruning physically removes network structures (filters, channels) to reduce model size and computation.\nKey Differences from SparsifyCallback: - Removes structures entirely (not just zeros) - Uses torch-pruning library for dependency handling - Supports various pruning criteria and schedules\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef PruneCallback(\n    pruning_ratio, schedule, context, criteria, args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nParameters:\n\npruning_ratio: Target ratio of parameters to remove (0-1 or 0-100). Values &gt;1 are treated as percentages.\nschedule: When to prune (from fasterai.core.schedule). Controls how pruning progresses over training.\ncontext: 'local' (per-layer pruning) or 'global' (across entire model).\ncriteria: How to select what to prune (from fasterai.core.criteria).",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "prune/prune_callback.html#overview",
    "href": "prune/prune_callback.html#overview",
    "title": "Prune Callback",
    "section": "",
    "text": "The PruneCallback integrates structured pruning into the fastai training loop. Unlike sparsification (which zeros weights), pruning physically removes network structures (filters, channels) to reduce model size and computation.\nKey Differences from SparsifyCallback: - Removes structures entirely (not just zeros) - Uses torch-pruning library for dependency handling - Supports various pruning criteria and schedules\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef PruneCallback(\n    pruning_ratio, schedule, context, criteria, args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nParameters:\n\npruning_ratio: Target ratio of parameters to remove (0-1 or 0-100). Values &gt;1 are treated as percentages.\nschedule: When to prune (from fasterai.core.schedule). Controls how pruning progresses over training.\ncontext: 'local' (per-layer pruning) or 'global' (across entire model).\ncriteria: How to select what to prune (from fasterai.core.criteria).",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "prune/prune_callback.html#usage-example",
    "href": "prune/prune_callback.html#usage-example",
    "title": "Prune Callback",
    "section": "Usage Example",
    "text": "Usage Example\nfrom fasterai.prune.prune_callback import PruneCallback\nfrom fasterai.core.schedule import agp\nfrom fasterai.core.criteria import large_final\n\n# Prune 30% of parameters using automated gradual pruning schedule\ncb = PruneCallback(\n    pruning_ratio=30,        # Remove 30% of parameters\n    schedule=agp,            # Gradual pruning (cubic decay)\n    context='global',        # Prune globally across all layers\n    criteria=large_final     # Keep weights with largest magnitude\n)\n\nlearn.fit(10, cbs=[cb])",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "prune/prune_callback.html#see-also",
    "href": "prune/prune_callback.html#see-also",
    "title": "Prune Callback",
    "section": "See Also",
    "text": "See Also\n\nPruner - Core structured pruning class used by this callback\nSchedules - Control pruning progression during training\nCriteria - Importance measures for selecting filters to prune\nSparsifyCallback - Unstructured pruning alternative",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/yolov8.html#training",
    "href": "tutorials/prune/yolov8.html#training",
    "title": "YOLOV8",
    "section": "Training",
    "text": "Training\n\nclass Args(argparse.Namespace):\n  model = 'yolov8l.pt'\n  cfg = 'default.yaml'\n  iterative_steps = 10\n  target_prune_rate = 0.15\n  max_map_drop = 0.2\n  sched = Schedule(partial(sched_onecycle,  Î±=10, Î²=4))\n\nargs=Args()\nprune(args)\n\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 43,668,288 parameters, 0 gradients, 165.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4049.2Â±1625.9 MB/s, size: 51.0 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.731      0.768      0.828       0.66\nSpeed: 0.7ms preprocess, 3.1ms inference, 0.0ms loss, 2.1ms postprocess per image\nResults saved to runs/detect/val8\nBefore Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.66035\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train7, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train7, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 3969.9Â±1494.9 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 505.5Â±201.7 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/train7/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/train7\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      17.6G     0.8369     0.7191      1.072        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.774      0.763      0.839      0.674\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      17.1G     0.8351      0.665      1.061        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.826      0.783       0.85      0.689\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      17.2G     0.8322     0.6222      1.066        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.858      0.794       0.86      0.704\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      17.1G     0.8023     0.5615      1.029         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.896      0.793       0.87      0.717\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      17.3G     0.7755      0.521      1.012         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.879      0.824       0.89      0.731\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      17.3G     0.7552     0.5039      1.011        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.869       0.84      0.892      0.738\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      16.8G     0.7342     0.4821     0.9817         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.885      0.835      0.896      0.749\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      17.2G     0.7389     0.4766     0.9989        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.884      0.855      0.904      0.762\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      17.2G     0.7197     0.4778     0.9785        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.875      0.866      0.909      0.767\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      17.2G     0.7149      0.457      1.007        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.882      0.867      0.911      0.768\n\n\n\n\n\n\n10 epochs completed in 0.010 hours.\nOptimizer stripped from runs/detect/train7/weights/last.pt, 175.3MB\nOptimizer stripped from runs/detect/train7/weights/best.pt, 175.3MB\n\nValidating runs/detect/train7/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 43,668,288 parameters, 0 gradients, 165.2 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.883      0.867      0.911      0.768\nSpeed: 0.1ms preprocess, 2.7ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 43,668,288 parameters, 0 gradients, 165.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5337.5Â±708.2 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.901      0.849      0.904      0.769\nSpeed: 0.1ms preprocess, 5.4ms inference, 0.0ms loss, 0.5ms postprocess per image\nResults saved to runs/detect/baseline_val4\nBefore Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.76904\nConv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 1: progress=0.018, ratio=0.003\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 43,043,386 parameters, 74,176 gradients, 162.6 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5560.8Â±1327.8 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.878      0.862      0.904      0.746\nSpeed: 0.2ms preprocess, 6.9ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_0_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 1: MACs=81.4709528 G, #Params=43.066447 M, mAP=0.7464191064783372, speed up=1.0154098308274602\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_0_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_0_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 3796.3Â±1197.1 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1819.0Â±382.7 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_0_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_0_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      17.4G       0.67     0.4225     0.9631        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.907      0.846      0.908      0.755\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      17.4G     0.6359     0.3913      0.947        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.888      0.861      0.914      0.757\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      17.3G     0.6677      0.427     0.9806        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.896      0.861      0.914      0.761\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      17.4G     0.6512     0.3957     0.9469         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.907      0.858      0.916      0.776\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      17.6G     0.6385     0.3909       0.94         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.852      0.919      0.779\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      17.3G     0.6406     0.4071     0.9522        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.942      0.846      0.917      0.781\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      17.4G     0.6228     0.3905     0.9324         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.877      0.883       0.92      0.788\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      17.4G     0.6583     0.4037     0.9571        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.866      0.923      0.793\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      17.4G     0.6465     0.4069      0.941        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.92      0.875      0.931      0.798\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      17.4G     0.6573     0.4086     0.9788        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914      0.884      0.932      0.799\n\n\n\n\n\n\n10 epochs completed in 0.010 hours.\nOptimizer stripped from runs/detect/step_0_finetune2/weights/last.pt, 172.8MB\nOptimizer stripped from runs/detect/step_0_finetune2/weights/best.pt, 172.8MB\n\nValidating runs/detect/step_0_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 43,043,386 parameters, 0 gradients, 162.6 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914      0.884      0.932      0.799\nSpeed: 0.1ms preprocess, 3.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 43,043,386 parameters, 0 gradients, 162.6 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4856.6Â±2095.6 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.916       0.87      0.922      0.791\nSpeed: 0.1ms preprocess, 7.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_0_post_val2\nAfter fine tuning mAP=0.7912829910872162\nAfter post fine-tuning validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 2: progress=0.048, ratio=0.007\nAfter Pruning\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 42,094,706 parameters, 74,160 gradients, 158.8 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5539.0Â±1668.1 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.926      0.845      0.912      0.769\nSpeed: 0.1ms preprocess, 6.9ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_1_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 2: MACs=79.5541908 G, #Params=42.117503 M, mAP=0.7685751155559024, speed up=1.0398749024796818\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_1_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_1_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 3717.7Â±1437.5 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1717.9Â±457.2 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_1_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_1_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10        17G     0.6016     0.3791     0.9319        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.859      0.921      0.783\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      17.1G     0.5765     0.3537     0.9187        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926      0.864      0.918      0.786\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      17.1G     0.5879     0.3755     0.9353        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.915      0.868      0.919      0.791\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      17.2G     0.5637     0.3453     0.9177         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.93       0.87      0.932      0.794\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      17.2G     0.5691     0.3553     0.9072         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.928      0.869      0.928      0.794\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      17.2G     0.5736     0.3496     0.9185        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.872      0.924      0.796\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      17.4G     0.5726     0.3525     0.9006         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926      0.873      0.924      0.794\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      17.3G     0.6045     0.3704     0.9303        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.927      0.882      0.932        0.8\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      17.3G     0.6179     0.3961     0.9203        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.938      0.883      0.932      0.804\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      17.1G     0.6393      0.416     0.9573        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.941      0.883      0.933      0.804\n\n\n\n\n\n\n10 epochs completed in 0.010 hours.\nOptimizer stripped from runs/detect/step_1_finetune2/weights/last.pt, 169.0MB\nOptimizer stripped from runs/detect/step_1_finetune2/weights/best.pt, 169.0MB\n\nValidating runs/detect/step_1_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 42,094,706 parameters, 0 gradients, 158.8 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.941      0.883      0.933      0.804\nSpeed: 0.1ms preprocess, 3.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 42,094,706 parameters, 0 gradients, 158.8 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5405.8Â±1076.0 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.931      0.882      0.931      0.795\nSpeed: 0.1ms preprocess, 7.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_1_post_val2\nAfter fine tuning mAP=0.7950947724666012\nAfter post fine-tuning validation\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 3: progress=0.119, ratio=0.018\nAfter Pruning\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 40,324,469 parameters, 74,160 gradients, 152.5 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5472.8Â±1324.6 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.858      0.851      0.907      0.743\nSpeed: 0.1ms preprocess, 7.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_2_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 3: MACs=76.3708784 G, #Params=40.34678 M, mAP=0.7432541366864699, speed up=1.0832192601833424\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_2_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_2_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4279.1Â±1583.2 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 975.8Â±228.6 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_2_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_2_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      16.8G     0.6389     0.3964     0.9263        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.922      0.836      0.912      0.764\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      17.3G     0.5608      0.361      0.903        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926      0.852      0.925       0.78\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      17.3G     0.5679      0.364     0.9166        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.903      0.876      0.931      0.783\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      17.1G      0.549      0.362     0.8975         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.905      0.885      0.934       0.79\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      16.9G     0.5402     0.3396     0.8914         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.932      0.873      0.929      0.793\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      17.1G     0.5511     0.3452     0.9006        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.925      0.872      0.933      0.797\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      17.1G     0.5463     0.3546     0.8866         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.92      0.882      0.932      0.797\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      16.9G     0.5963     0.3718     0.9195        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.894      0.936      0.801\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      16.9G     0.6017     0.3778     0.9098        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926      0.896      0.937      0.806\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      17.3G     0.6401     0.4083      0.954        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.93      0.892      0.937      0.808\n\n\n\n\n\n\n10 epochs completed in 0.010 hours.\nOptimizer stripped from runs/detect/step_2_finetune2/weights/last.pt, 161.9MB\nOptimizer stripped from runs/detect/step_2_finetune2/weights/best.pt, 161.9MB\n\nValidating runs/detect/step_2_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 40,324,469 parameters, 0 gradients, 152.5 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.93      0.892      0.937      0.808\nSpeed: 0.1ms preprocess, 3.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 40,324,469 parameters, 0 gradients, 152.5 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4201.8Â±2216.9 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.927      0.891      0.936        0.8\nSpeed: 0.1ms preprocess, 7.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_2_post_val2\nAfter fine tuning mAP=0.7996752102772763\nAfter post fine-tuning validation\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 4: progress=0.270, ratio=0.040\nAfter Pruning\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 37,708,749 parameters, 74,160 gradients, 143.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4881.8Â±1731.9 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.875      0.805      0.892      0.702\nSpeed: 0.1ms preprocess, 6.1ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_3_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 4: MACs=71.732976 G, #Params=37.730325 M, mAP=0.7020598286629811, speed up=1.1532549046898597\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_3_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_3_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4040.0Â±1479.4 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 898.4Â±215.3 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_3_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_3_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10        16G     0.6694     0.4281     0.9392        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.884      0.856      0.915      0.743\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      16.3G      0.596      0.378     0.9048        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.899      0.869      0.924      0.762\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      16.2G     0.5892      0.389     0.9174        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.901      0.868      0.922      0.775\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      16.3G     0.5714     0.3688     0.8989         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.923      0.875      0.933      0.779\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      16.3G     0.5768     0.3685     0.8988         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.93      0.875      0.935      0.788\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      16.3G     0.5769     0.3674     0.8972        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.902      0.892      0.937       0.79\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      16.3G     0.5726     0.3653     0.8875         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.929      0.876      0.937      0.796\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      16.4G     0.6152     0.3919     0.9235        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.918      0.883      0.939      0.802\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      16.4G     0.6269     0.3936       0.92        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.922      0.886      0.939      0.805\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      16.2G     0.6646     0.4099     0.9612        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.886      0.941      0.809\n\n\n\n\n\n\n10 epochs completed in 0.010 hours.\nOptimizer stripped from runs/detect/step_3_finetune2/weights/last.pt, 151.5MB\nOptimizer stripped from runs/detect/step_3_finetune2/weights/best.pt, 151.5MB\n\nValidating runs/detect/step_3_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 37,708,749 parameters, 0 gradients, 143.2 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.886      0.941      0.809\nSpeed: 0.1ms preprocess, 2.9ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 37,708,749 parameters, 0 gradients, 143.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 3800.7Â±1771.4 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.916      0.888      0.941      0.808\nSpeed: 0.1ms preprocess, 6.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_3_post_val2\nAfter fine tuning mAP=0.8076550755729582\nAfter post fine-tuning validation\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 5: progress=0.501, ratio=0.075\nAfter Pruning\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 35,132,671 parameters, 74,160 gradients, 133.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5237.1Â±1333.4 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.786      0.737       0.83      0.664\nSpeed: 0.1ms preprocess, 6.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_4_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 5: MACs=66.7424992 G, #Params=35.153479 M, mAP=0.6635248706814774, speed up=1.2394861953266503\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_4_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_4_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 3910.5Â±1562.2 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 745.1Â±159.7 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_4_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_4_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      15.6G     0.7302      0.482      0.966        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.844      0.814      0.887      0.718\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      15.5G     0.6487     0.4256     0.9334        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.891      0.835       0.91      0.753\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      15.7G     0.6402     0.4361     0.9413        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.889      0.848      0.919      0.761\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      15.7G     0.6214     0.3973     0.9185         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.893      0.862      0.924      0.776\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      15.7G     0.5974     0.3845     0.9032         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914       0.86      0.929      0.779\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      15.7G     0.6027     0.3936     0.9126        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926       0.86      0.932      0.786\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      15.8G     0.5974     0.3946     0.8942         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.922      0.872      0.934      0.791\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      15.8G     0.6442     0.4018     0.9322        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914      0.883      0.935        0.8\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      15.8G      0.659     0.4155     0.9267        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914      0.886      0.936      0.802\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      16.3G     0.6831     0.4327     0.9751        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.909      0.897      0.937      0.801\n\n\n\n\n\n\n10 epochs completed in 0.010 hours.\nOptimizer stripped from runs/detect/step_4_finetune2/weights/last.pt, 141.2MB\nOptimizer stripped from runs/detect/step_4_finetune2/weights/best.pt, 141.2MB\n\nValidating runs/detect/step_4_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 35,132,671 parameters, 0 gradients, 133.2 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914      0.886      0.936      0.802\nSpeed: 0.1ms preprocess, 2.7ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 35,132,671 parameters, 0 gradients, 133.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4520.2Â±1803.1 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.912      0.886      0.938        0.8\nSpeed: 0.1ms preprocess, 6.5ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_4_post_val2\nAfter fine tuning mAP=0.7996035449784826\nAfter post fine-tuning validation\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 6: progress=0.733, ratio=0.110\nAfter Pruning\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 33,747,610 parameters, 74,160 gradients, 128.5 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4015.0Â±1353.0 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.918      0.822      0.908      0.743\nSpeed: 0.2ms preprocess, 6.0ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_5_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 6: MACs=64.3900056 G, #Params=33.768007 M, mAP=0.7431841358762444, speed up=1.2847709148203583\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_5_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_5_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4044.3Â±1635.3 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 726.9Â±171.1 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_5_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_5_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      15.3G     0.6333     0.4011     0.9294        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926      0.828      0.922      0.757\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      15.3G     0.5444     0.3673     0.8873        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.93      0.842      0.925      0.769\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      15.2G     0.5664     0.3835     0.9134        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.917      0.867      0.929      0.771\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      15.4G     0.5632     0.3668     0.8936         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.915      0.867       0.93      0.782\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      15.4G     0.5594     0.3643     0.8994         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.924      0.856      0.929      0.792\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      15.4G     0.5635      0.359     0.8999        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.923      0.857       0.93      0.793\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      15.3G     0.5725     0.3679     0.8946         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.932      0.858      0.933      0.794\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      15.3G     0.6254     0.3951     0.9293        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.925      0.863      0.932      0.796\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      15.3G      0.642     0.4066     0.9224        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.871      0.906      0.932      0.797\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      15.2G     0.6799     0.4366     0.9771        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.896      0.894      0.932      0.797\n\n\n\n\n\n\n10 epochs completed in 0.018 hours.\nOptimizer stripped from runs/detect/step_5_finetune2/weights/last.pt, 135.6MB\nOptimizer stripped from runs/detect/step_5_finetune2/weights/best.pt, 135.6MB\n\nValidating runs/detect/step_5_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 33,747,610 parameters, 0 gradients, 128.5 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.871      0.906      0.932      0.797\nSpeed: 0.1ms preprocess, 3.4ms inference, 0.0ms loss, 1.5ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 33,747,610 parameters, 0 gradients, 128.5 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 3665.2Â±391.2 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.933      0.857      0.934      0.795\nSpeed: 1.5ms preprocess, 15.6ms inference, 0.0ms loss, 3.4ms postprocess per image\nResults saved to runs/detect/step_5_post_val2\nAfter fine tuning mAP=0.7951189977994946\nAfter post fine-tuning validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 7: progress=0.883, ratio=0.132\nAfter Pruning\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,913,682 parameters, 74,160 gradients, 125.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1738.3Â±815.1 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.824      0.863      0.908      0.742\nSpeed: 1.1ms preprocess, 13.4ms inference, 0.0ms loss, 2.8ms postprocess per image\nResults saved to runs/detect/step_6_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 7: MACs=62.7046164 G, #Params=32.933815 M, mAP=0.7416030070446816, speed up=1.319303284981104\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_6_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_6_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1812.0Â±691.5 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 881.6Â±250.0 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_6_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_6_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      15.1G     0.5837     0.3747     0.9064        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.895      0.851      0.923       0.76\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      15.1G     0.5111     0.3316     0.8736        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.905       0.87      0.929      0.777\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10        15G     0.5221     0.3494     0.8919        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.909      0.875      0.932      0.786\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10        15G      0.531     0.3318     0.8828         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.911      0.874      0.927      0.785\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      15.1G     0.5416     0.3462     0.8843         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.886      0.888       0.93      0.786\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      15.1G     0.5524      0.354       0.89        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.893      0.877      0.926      0.783\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      15.1G     0.5693     0.3642     0.8824         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.907      0.869      0.926      0.786\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      15.1G     0.6151     0.3821     0.9231        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.906      0.878      0.929      0.788\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      15.1G     0.6315     0.4039     0.9118        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.915      0.883      0.932      0.797\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10        15G     0.6669     0.4226     0.9674        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914      0.884      0.933      0.795\n\n\n\n\n\n\n10 epochs completed in 0.015 hours.\nOptimizer stripped from runs/detect/step_6_finetune2/weights/last.pt, 132.3MB\nOptimizer stripped from runs/detect/step_6_finetune2/weights/best.pt, 132.3MB\n\nValidating runs/detect/step_6_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,913,682 parameters, 0 gradients, 125.2 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.915      0.883      0.932      0.796\nSpeed: 0.1ms preprocess, 3.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,913,682 parameters, 0 gradients, 125.2 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 3042.0Â±594.6 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.915      0.877      0.931      0.794\nSpeed: 0.5ms preprocess, 9.4ms inference, 0.0ms loss, 1.6ms postprocess per image\nResults saved to runs/detect/step_6_post_val2\nAfter fine tuning mAP=0.7942548824738299\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 8: progress=0.955, ratio=0.143\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,669,140 parameters, 74,160 gradients, 124.6 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1360.5Â±623.7 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.916      0.867      0.927      0.789\nSpeed: 0.7ms preprocess, 9.3ms inference, 0.0ms loss, 1.6ms postprocess per image\nResults saved to runs/detect/step_7_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 8: MACs=62.4070664 G, #Params=32.689204 M, mAP=0.7892334700405261, speed up=1.325593577332454\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_7_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_7_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1005.4Â±526.3 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 626.8Â±111.3 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_7_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_7_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      14.9G      0.495     0.3205       0.88        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.923      0.874      0.929      0.799\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      15.1G     0.4323     0.2908     0.8485        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.917      0.884      0.936      0.794\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10        15G     0.4617     0.3107     0.8715        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.908      0.886      0.933      0.797\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10        15G     0.4587     0.3004     0.8575         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.929      0.878       0.93      0.795\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      14.9G     0.4739     0.3132     0.8601         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.929      0.878      0.932      0.796\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      14.9G     0.4952     0.3178     0.8655        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.934      0.864      0.929      0.795\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10        15G     0.4984      0.326     0.8585         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.92      0.877       0.93      0.796\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      15.1G     0.5569     0.3589      0.894        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926      0.872      0.929      0.798\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      15.1G     0.5973     0.3794      0.897        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.929      0.873      0.934      0.804\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10        15G     0.6558     0.4162     0.9558        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.929      0.878      0.936      0.805\n\n\n\n\n\n\n10 epochs completed in 0.012 hours.\nOptimizer stripped from runs/detect/step_7_finetune2/weights/last.pt, 131.3MB\nOptimizer stripped from runs/detect/step_7_finetune2/weights/best.pt, 131.3MB\n\nValidating runs/detect/step_7_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,669,140 parameters, 0 gradients, 124.6 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.929      0.878      0.936      0.805\nSpeed: 0.1ms preprocess, 2.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,669,140 parameters, 0 gradients, 124.6 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4798.0Â±2031.8 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.937      0.883      0.937      0.802\nSpeed: 0.2ms preprocess, 6.1ms inference, 0.0ms loss, 0.5ms postprocess per image\nResults saved to runs/detect/step_7_post_val2\nAfter fine tuning mAP=0.8021166946231042\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 9: progress=0.984, ratio=0.148\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 74,160 gradients, 123.4 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4707.7Â±1115.9 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.931      0.863      0.921      0.768\nSpeed: 0.1ms preprocess, 6.1ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_8_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 9: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.7680307574283493, speed up=1.3375568226839933\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_8_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_8_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4035.8Â±1487.2 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 1631.0Â±420.7 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_8_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_8_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      14.9G     0.4943     0.3212     0.8714        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929       0.93      0.869      0.926      0.789\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      14.9G     0.4371     0.2908     0.8475        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.935      0.869      0.931      0.802\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      14.9G      0.443     0.2951      0.864        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.933      0.873      0.934      0.801\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      14.9G     0.4433      0.295     0.8514         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.909      0.892      0.933      0.801\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10      14.9G     0.4481     0.2939      0.852         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.912      0.896      0.932      0.797\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      14.9G     0.4641     0.3056     0.8523        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.917       0.89      0.935      0.802\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      14.9G     0.4891     0.3107      0.858         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.931      0.886      0.937      0.802\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      14.9G      0.532      0.338     0.8835        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.942      0.887      0.936      0.802\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10        15G     0.5758     0.3629     0.8903        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.941      0.893      0.936      0.807\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      14.9G     0.6455     0.3983     0.9429        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.942      0.893      0.938      0.808\n\n\n\n\n\n\n10 epochs completed in 0.008 hours.\nOptimizer stripped from runs/detect/step_8_finetune2/weights/last.pt, 130.3MB\nOptimizer stripped from runs/detect/step_8_finetune2/weights/best.pt, 130.3MB\n\nValidating runs/detect/step_8_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 0 gradients, 123.4 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.942      0.893      0.938      0.808\nSpeed: 0.1ms preprocess, 2.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 0 gradients, 123.4 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 2456.1Â±440.7 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.933      0.893      0.935      0.806\nSpeed: 0.1ms preprocess, 6.2ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_8_post_val2\nAfter fine tuning mAP=0.8062525404490082\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruning step 10: progress=0.996, ratio=0.149\nAfter Pruning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 74,160 gradients, 123.4 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5303.3Â±1442.4 MB/s, size: 44.7 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.934      0.895      0.936      0.806\nSpeed: 0.1ms preprocess, 6.2ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_9_pre_val2\nAfter post-pruning Validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nAfter pruning iter 10: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8062440401624619, speed up=1.3375568226839933\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nengine/trainer: agnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco128.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=step_9_finetune2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/step_9_finetune2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nFreezing layer 'model.22.dfl.conv.weight'\ntrain: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 4503.1Â±1040.0 MB/s, size: 50.9 KB)\n\n\n\ntrain: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/lab\n\n\n\n\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 795.8Â±192.5 MB/s, size: 52.5 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n\n\n\nPlotting labels to runs/detect/step_9_finetune2/labels.jpg... \noptimizer: 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \noptimizer: AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 8 dataloader workers\nLogging results to runs/detect/step_9_finetune2\nStarting training for 10 epochs...\nClosing dataloader mosaic\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       1/10      15.3G      0.424     0.2844     0.8499        121        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.937      0.892      0.939      0.811\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       2/10      14.9G     0.3993     0.2626     0.8333        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.923      0.896      0.942      0.808\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       3/10      14.9G     0.4118     0.2764     0.8534        118        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.914      0.899      0.941      0.808\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       4/10      14.9G     0.4239     0.2808     0.8413         68        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.926      0.892      0.937      0.807\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       5/10        15G     0.4537     0.2909     0.8466         95        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.942      0.891      0.935      0.805\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       6/10      15.1G     0.4596      0.299     0.8484        122        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.947      0.885      0.938      0.807\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       7/10      14.9G     0.4647     0.3001     0.8475         75        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.948      0.887       0.94      0.807\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       8/10      14.9G     0.5177     0.3237     0.8788        142        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.947      0.891      0.942      0.807\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n       9/10      14.9G     0.5476     0.3486     0.8788        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.946      0.891      0.942      0.811\n\n\n\n\n\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\n\n      10/10      15.3G     0.6247     0.3905      0.942        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.944      0.889      0.941      0.811\n\n\n\n\n\n\n10 epochs completed in 0.008 hours.\nOptimizer stripped from runs/detect/step_9_finetune2/weights/last.pt, 130.3MB\nOptimizer stripped from runs/detect/step_9_finetune2/weights/best.pt, 130.3MB\n\nValidating runs/detect/step_9_finetune2/weights/best.pt...\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 0 gradients, 123.4 GFLOPs\n\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n                   all        128        929      0.946      0.891      0.942      0.811\nSpeed: 0.1ms preprocess, 2.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n\n\n\n\n\nAfter fine-tuning\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 0 gradients, 123.4 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5168.4Â±959.1 MB/s, size: 53.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.937      0.892      0.939      0.806\nSpeed: 0.1ms preprocess, 6.3ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/step_9_post_val2\nAfter fine tuning mAP=0.8059532806050649\nAfter post fine-tuning validation\nModel Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nPruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CPU (Intel Core(TM) i9-14900KS)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 0 gradients, 123.4 GFLOPs\n\nPyTorch: starting from 'runs/detect/step_9_finetune2/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (124.2 MB)\n\nONNX: starting export with onnx 1.17.0 opset 10...\n\n\nW0205 17:34:38.183000 260195 site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 10 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version &gt;=18 to leverage latest ONNX features\nThe model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 10).\nFailed to convert the model to the target version 10 using the ONNX C API. The model was not modified\nTraceback (most recent call last):\n  File \"/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n    converted_proto = _c_api_utils.call_onnx_api(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n    result = func(proto)\n             ^^^^^^^^^^^\n  File \"/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n    return onnx.version_converter.convert_version(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/onnx/version_converter.py\", line 38, in convert_version\n    converted_model_str = C.convert_version(model_str, target_version)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: /github/workspace/onnx/version_converter/BaseConverter.h:70: adapter_lookup: Assertion `false` failed: No Adapter To Version $17 for Resize\n\n\nApplied 1 of general pattern rewrite rules.\nONNX: slimming with onnxslim 0.1.59...\nONNX: export success âœ… 2.9s, saved as 'runs/detect/step_9_finetune2/weights/best.onnx' (123.8 MB)\n\nExport complete (3.4s)\nResults saved to /home/nathan/Developer/FasterAI-Labs/gh/fasterai/nbs/tutorials/prune/runs/detect/step_9_finetune2/weights\nPredict:         yolo predict task=detect model=runs/detect/step_9_finetune2/weights/best.onnx imgsz=640  \nValidate:        yolo val task=detect model=runs/detect/step_9_finetune2/weights/best.onnx imgsz=640 data=/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/ultralytics/cfg/datasets/coco128.yaml  \nVisualize:       https://netron.app",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "YOLOV8"
    ]
  },
  {
    "objectID": "tutorials/prune/yolov8.html#post-training-checks",
    "href": "tutorials/prune/yolov8.html#post-training-checks",
    "title": "YOLOV8",
    "section": "Post-Training Checks",
    "text": "Post-Training Checks\n\nmodel = YOLO('runs/detect/step_9_finetune2/weights/best.pt')\n\n\nexample_inputs = torch.randn(1, 3, 640, 640).to(model.device)\n\n\nbase_macs, base_nparams = tp.utils.count_ops_and_params(model.model, example_inputs); base_macs, base_nparams\n\n(61848891200.0, 32436843)\n\n\n\nresults = model.val(\n                data='coco128.yaml',\n                batch=1,\n                imgsz=640,\n                verbose=False,\n            )\n\nUltralytics 8.3.162 ðŸš€ Python-3.12.11 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 5090, 32109MiB)\nYOLOv8l summary (fused): 121 layers, 32,416,863 parameters, 0 gradients, 123.4 GFLOPs\nval: Fast image access âœ… (ping: 0.0Â±0.0 ms, read: 5274.3Â±1449.8 MB/s, size: 56.4 KB)\n\n\n\nval: Scanning /home/nathan/Developer/FasterAI-Labs/Projects/ALX Systems/datasets/coco128/label\n\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%\n\n\n\n\n                   all        128        929      0.949       0.89       0.94      0.813\nSpeed: 0.1ms preprocess, 5.9ms inference, 0.0ms loss, 0.4ms postprocess per image\nResults saved to runs/detect/val10\n\n\n\nresults\n\nultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: &lt;ultralytics.utils.metrics.ConfusionMatrix object&gt;\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,    0.011347,   0.0056734,           0],\n       [          1,           1,           1, ...,  0.00093844,  0.00046922,           0],\n       [          1,           1,           1, ...,   0.0010056,   0.0005028,           0],\n       ...,\n       [          1,           1,           1, ...,           1,           1,           0],\n       [          1,           1,           1, ...,           1,           1,           0],\n       [          1,           1,           1, ...,           1,           1,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.39068,     0.39068,     0.47539, ...,           0,           0,           0],\n       [    0.14286,     0.14286,     0.18083, ...,           0,           0,           0],\n       [      0.158,       0.158,     0.19265, ...,           0,           0,           0],\n       ...,\n       [    0.22222,     0.22222,     0.34654, ...,           0,           0,           0],\n       [    0.58333,     0.58333,     0.70817, ...,           0,           0,           0],\n       [    0.43478,     0.43478,     0.51359, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.24545,     0.24545,     0.31671, ...,           1,           1,           1],\n       [   0.078125,    0.078125,     0.10142, ...,           1,           1,           1],\n       [   0.087356,    0.087356,     0.10904, ...,           1,           1,           1],\n       ...,\n       [      0.125,       0.125,     0.20959, ...,           1,           1,           1],\n       [    0.41176,     0.41176,     0.54819, ...,           1,           1,           1],\n       [    0.27778,     0.27778,     0.34552, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.95669,     0.95669,     0.95276, ...,           0,           0,           0],\n       [    0.83333,     0.83333,     0.83333, ...,           0,           0,           0],\n       [    0.82609,     0.82609,     0.82609, ...,           0,           0,           0],\n       ...,\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0],\n       [          1,           1,           1, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: np.float64(0.8255287544201578)\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.75412,     0.63903,     0.41296,       0.995,     0.96812,     0.77275,      0.9432,     0.67183,     0.66633,     0.32049,     0.81284,     0.82439,     0.81284,     0.92504,     0.83701,       0.995,     0.95141,      0.8955,     0.81284,     0.81284,     0.92269,       0.995,       0.995,     0.98101,\n           0.66656,     0.86947,      0.6929,     0.78625,       0.902,     0.70888,      0.8955,     0.77263,     0.40951,     0.66091,     0.42614,     0.41974,     0.83395,     0.81284,     0.62183,      0.6069,     0.63543,     0.78458,     0.76411,     0.69596,     0.68787,     0.81765,       0.995,     0.81284,\n             0.995,      0.8744,     0.74271,     0.81827,       0.995,       0.945,     0.96893,       0.995,     0.82049,     0.94289,     0.83942,       0.995,     0.84753,     0.95775,       0.995,      0.9648,     0.65347,      0.7156,     0.81284,     0.76396,       0.995,     0.88804,     0.81284,     0.78227,\n            0.9159,     0.62052,     0.89885,     0.94712,      0.8955,      0.8924,     0.81284,     0.92639])\nnames: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\nnt_per_class: array([254,   6,  46,   5,   6,   7,   3,  12,   6,  14,   0,   2,   0,   9,  16,   4,   9,   2,   0,   0,  17,   1,   4,   9,   6,  18,  19,   7,   4,   5,   1,   7,   6,  10,   4,   7,   5,   0,   7,  18,  16,  36,   6,  16,  22,  28,   1,   0,   2,   4,  11,  24,   2,   5,  14,   4,  35,   6,  14,   3,  13,   2,\n         2,   3,   2,   8,   0,   8,   3,   5,   0,   6,   5,  29,   9,   2,   1,  21,   0,   5])\nnt_per_image: array([61,  3, 12,  4,  5,  5,  3,  5,  2,  4,  0,  2,  0,  5,  2,  4,  9,  1,  0,  0,  4,  1,  2,  4,  4,  4,  9,  6,  2,  5,  1,  2,  6,  2,  4,  4,  3,  0,  5,  6,  5, 10,  6,  7,  5,  9,  1,  0,  2,  1,  4,  3,  1,  5,  2,  4,  9,  5,  9,  3, 10,  2,  2,  2,  2,  5,  0,  5,  3,  5,  0,  4,  5,  6,  8,  2,  1,  6,\n        0,  2])\nresults_dict: {'metrics/precision(B)': np.float64(0.9490510981853786), 'metrics/recall(B)': np.float64(0.8897383245004401), 'metrics/mAP50(B)': np.float64(0.9396973902877623), 'metrics/mAP50-95(B)': np.float64(0.8128433504348684), 'fitness': np.float64(0.8255287544201578)}\nsave_dir: Path('runs/detect/val10')\nspeed: {'preprocess': 0.11946710401389282, 'inference': 5.948787069428363, 'loss': 0.0031694062272435986, 'postprocess': 0.4260614005033858}\nstats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\ntask: 'detect'",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "YOLOV8"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html",
    "href": "tutorials/prune/pruner.html",
    "title": "Pruner Tutorial",
    "section": "",
    "text": "The Pruner class performs structured pruning - physically removing entire filters and channels from your neural network. Unlike sparsification (which zeros weights but keeps the architecture), structured pruning creates a genuinely smaller model that runs faster on standard hardware.\n\n\n\n\n\n\n\n\n\n\nAspect\nSparsifier\nPruner\n\n\n\n\nWhat it removes\nIndividual weights â†’ zeros\nEntire filters â†’ gone\n\n\nArchitecture\nUnchanged (same shapes)\nSmaller (fewer channels)\n\n\nSpeedup\nNeeds sparse hardware\nImmediate on any hardware\n\n\nUse case\nResearch, sparse accelerators\nProduction deployment\n\n\n\nWhen to use Pruner: - You need a smaller model file - You want faster inference without special hardware - Youâ€™re deploying to edge devices or mobile\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\nfrom fasterai.prune.pruner import Pruner\nfrom fasterai.core.criteria import large_final, random",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#overview",
    "href": "tutorials/prune/pruner.html#overview",
    "title": "Pruner Tutorial",
    "section": "",
    "text": "The Pruner class performs structured pruning - physically removing entire filters and channels from your neural network. Unlike sparsification (which zeros weights but keeps the architecture), structured pruning creates a genuinely smaller model that runs faster on standard hardware.\n\n\n\n\n\n\n\n\n\n\nAspect\nSparsifier\nPruner\n\n\n\n\nWhat it removes\nIndividual weights â†’ zeros\nEntire filters â†’ gone\n\n\nArchitecture\nUnchanged (same shapes)\nSmaller (fewer channels)\n\n\nSpeedup\nNeeds sparse hardware\nImmediate on any hardware\n\n\nUse case\nResearch, sparse accelerators\nProduction deployment\n\n\n\nWhen to use Pruner: - You need a smaller model file - You want faster inference without special hardware - Youâ€™re deploying to edge devices or mobile\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\nfrom fasterai.prune.pruner import Pruner\nfrom fasterai.core.criteria import large_final, random",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#basic-pruning",
    "href": "tutorials/prune/pruner.html#basic-pruning",
    "title": "Pruner Tutorial",
    "section": "1. Basic Pruning",
    "text": "1. Basic Pruning\nLetâ€™s start with a ResNet18 and prune 30% of its filters:\n\nmodel = resnet18(weights=None)\n\nprint('Before pruning:')\nprint(f'  conv1: {model.conv1}')\nprint(f'  layer1[0].conv1: {model.layer1[0].conv1}')\nprint(f'  Parameters: {sum(p.numel() for p in model.parameters()):,}')\n\nBefore pruning:\n  conv1: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  layer1[0].conv1: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  Parameters: 11,689,512\n\n\n\npruner = Pruner(\n    model, \n    pruning_ratio=30,      # Remove 30% of filters\n    context='local',       # Prune each layer independently\n    criteria=large_final   # Keep filters with largest weights\n)\npruner.prune_model()\n\nprint('\\nAfter pruning:')\nprint(f'  conv1: {model.conv1}')\nprint(f'  layer1[0].conv1: {model.layer1[0].conv1}')\nparams_after = sum(p.numel() for p in model.parameters())\nprint(f'  Parameters: {params_after:,}')\nprint(f'  Reduction: {100*(1 - params_after/11689512):.1f}%')\n\nIgnoring output layer: fc\nTotal ignored layers: 1\n\nAfter pruning:\n  conv1: Conv2d(3, 44, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  layer1[0].conv1: Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  Parameters: 5,820,556\n  Reduction: 50.2%\n\n\nNotice the channel counts changed: Conv2d(3, 64, ...) became Conv2d(3, 44, ...). The model is genuinely smaller!\nKey point: The Pruner automatically handles layer dependencies. When you remove output channels from one layer, it removes the corresponding input channels from the next layer.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#local-vs-global-pruning",
    "href": "tutorials/prune/pruner.html#local-vs-global-pruning",
    "title": "Pruner Tutorial",
    "section": "2. Local vs Global Pruning",
    "text": "2. Local vs Global Pruning\nThe context parameter controls how filters are selected for pruning:\n\n\n\n\n\n\n\n\nContext\nBehavior\nBest for\n\n\n\n\n'local'\nEach layer loses same % of filters\nUniform compression\n\n\n'global'\nCompare importance across all layers\nMaximum accuracy retention\n\n\n\n\n# Local: each layer pruned independently to 50%\nmodel_local = resnet18(weights=None)\npruner = Pruner(model_local, 50, 'local', large_final)\npruner.prune_model()\n\nprint('\\nLocal pruning (each layer loses 50%):')\nprint(f'  layer1[0].conv1: Conv2d({model_local.layer1[0].conv1.in_channels}, {model_local.layer1[0].conv1.out_channels}, ...)')\nprint(f'  layer2[0].conv1: Conv2d({model_local.layer2[0].conv1.in_channels}, {model_local.layer2[0].conv1.out_channels}, ...)')\nprint(f'  layer3[0].conv1: Conv2d({model_local.layer3[0].conv1.in_channels}, {model_local.layer3[0].conv1.out_channels}, ...)')\nprint(f'  layer4[0].conv1: Conv2d({model_local.layer4[0].conv1.in_channels}, {model_local.layer4[0].conv1.out_channels}, ...)')\n\nIgnoring output layer: fc\nTotal ignored layers: 1\n\nLocal pruning (each layer loses 50%):\n  layer1[0].conv1: Conv2d(32, 32, ...)\n  layer2[0].conv1: Conv2d(32, 64, ...)\n  layer3[0].conv1: Conv2d(64, 128, ...)\n  layer4[0].conv1: Conv2d(128, 256, ...)\n\n\n\n# Global: least important filters across entire network\nmodel_global = resnet18(weights=None)\npruner = Pruner(model_global, 50, 'global', large_final)\npruner.prune_model()\n\nprint('\\nGlobal pruning (importance compared across layers):')\nprint(f'  layer1[0].conv1: Conv2d({model_global.layer1[0].conv1.in_channels}, {model_global.layer1[0].conv1.out_channels}, ...)')\nprint(f'  layer2[0].conv1: Conv2d({model_global.layer2[0].conv1.in_channels}, {model_global.layer2[0].conv1.out_channels}, ...)')\nprint(f'  layer3[0].conv1: Conv2d({model_global.layer3[0].conv1.in_channels}, {model_global.layer3[0].conv1.out_channels}, ...)')\nprint(f'  layer4[0].conv1: Conv2d({model_global.layer4[0].conv1.in_channels}, {model_global.layer4[0].conv1.out_channels}, ...)')\n\nIgnoring output layer: fc\nTotal ignored layers: 1\n\nGlobal pruning (importance compared across layers):\n  layer1[0].conv1: Conv2d(64, 64, ...)\n  layer2[0].conv1: Conv2d(64, 128, ...)\n  layer3[0].conv1: Conv2d(128, 69, ...)\n  layer4[0].conv1: Conv2d(256, 512, ...)\n\n\nWith global pruning, early layers often keep more filters (theyâ€™re more important) while later layers with redundant features get pruned more aggressively.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#iterative-pruning",
    "href": "tutorials/prune/pruner.html#iterative-pruning",
    "title": "Pruner Tutorial",
    "section": "3. Iterative Pruning",
    "text": "3. Iterative Pruning\nFor high compression ratios, iterative pruning works better than one-shot. The model gradually adapts to having fewer parameters:\n\nmodel = resnet18(weights=None)\nparams_orig = sum(p.numel() for p in model.parameters())\n\n# Iterative pruning: 5 steps to reach 50% pruning\npruner = Pruner(\n    model, \n    pruning_ratio=50,\n    context='local', \n    criteria=large_final,\n    iterative_steps=5  # Spread pruning over 5 steps\n)\n\nprint('Iterative pruning (5 steps to reach 50%):')\nfor i in range(5):\n    pruner.prune_model()\n    params = sum(p.numel() for p in model.parameters())\n    print(f'  Step {i+1}: {params:,} params ({100*(1-params/params_orig):.1f}% reduction)')\n\nIgnoring output layer: fc\nTotal ignored layers: 1\nIterative pruning (5 steps to reach 50%):\n  Step 1: 9,481,588 params (18.9% reduction)\n  Step 2: 7,534,380 params (35.5% reduction)\n  Step 3: 5,820,556 params (50.2% reduction)\n  Step 4: 4,318,898 params (63.1% reduction)\n  Step 5: 3,055,880 params (73.9% reduction)\n\n\nIn practice: When using PruneCallback during training, iterative pruning happens automatically - the model is pruned a little bit after each batch, allowing it to recover between steps.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#per-layer-pruning-ratios",
    "href": "tutorials/prune/pruner.html#per-layer-pruning-ratios",
    "title": "Pruner Tutorial",
    "section": "4. Per-Layer Pruning Ratios",
    "text": "4. Per-Layer Pruning Ratios\nDifferent layers have different sensitivity to pruning. You can specify custom ratios using a dictionary:\n\nmodel = resnet18(weights=None)\n\n# Conservative on early layers, aggressive on later layers\nper_layer_ratios = {\n    'layer1.0.conv1': 20,  'layer1.0.conv2': 20,  # 20% pruning\n    'layer2.0.conv1': 40,  'layer2.0.conv2': 40,  # 40% pruning  \n    'layer3.0.conv1': 60,  'layer3.0.conv2': 60,  # 60% pruning\n    'layer4.0.conv1': 80,  'layer4.0.conv2': 80,  # 80% pruning\n}\n\npruner = Pruner(model, per_layer_ratios, 'local', large_final)\npruner.prune_model()\n\nprint('\\nPer-layer pruning results:')\nprint(f'  layer1.0.conv1: {model.layer1[0].conv1.out_channels} channels (20% pruned from 64)')\nprint(f'  layer2.0.conv1: {model.layer2[0].conv1.out_channels} channels (40% pruned from 128)')\nprint(f'  layer3.0.conv1: {model.layer3[0].conv1.out_channels} channels (60% pruned from 256)')\nprint(f'  layer4.0.conv1: {model.layer4[0].conv1.out_channels} channels (80% pruned from 512)')\n\nIgnoring output layer: fc\nTotal ignored layers: 1\nUsing per-layer pruning with 8 layer-specific ratios\n\nPer-layer pruning results:\n  layer1.0.conv1: 51 channels (20% pruned from 64)\n  layer2.0.conv1: 76 channels (40% pruned from 128)\n  layer3.0.conv1: 102 channels (60% pruned from 256)\n  layer4.0.conv1: 102 channels (80% pruned from 512)\n\n\nTip: Use sensitivity analysis to determine which layers can tolerate more pruning. See the Sensitivity Tutorial for details.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#verifying-the-pruned-model",
    "href": "tutorials/prune/pruner.html#verifying-the-pruned-model",
    "title": "Pruner Tutorial",
    "section": "5. Verifying the Pruned Model",
    "text": "5. Verifying the Pruned Model\nAfter pruning, the model remains fully functional - it just has fewer parameters:\n\nmodel = resnet18(weights=None)\nmodel.eval()\n\n# Prune 50%\npruner = Pruner(model, 50, 'global', large_final)\npruner.prune_model()\n\n# Verify forward pass works\nx = torch.randn(1, 3, 224, 224)\nwith torch.no_grad():\n    output = model(x)\n\nprint('\\nForward pass verification:')\nprint(f'  Input shape:  {x.shape}')\nprint(f'  Output shape: {output.shape}')\nprint('  Model works correctly after pruning!')\n\nIgnoring output layer: fc\nTotal ignored layers: 1\n\nForward pass verification:\n  Input shape:  torch.Size([1, 3, 224, 224])\n  Output shape: torch.Size([1, 1000])\n  Model works correctly after pruning!",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#importance-criteria",
    "href": "tutorials/prune/pruner.html#importance-criteria",
    "title": "Pruner Tutorial",
    "section": "6. Importance Criteria",
    "text": "6. Importance Criteria\nThe criteria parameter determines how filter importance is calculated:\n\n\n\n\n\n\n\n\nCriteria\nMethod\nBest for\n\n\n\n\nlarge_final\nKeep filters with largest L1 norm\nGeneral use, most common\n\n\nsmall_final\nKeep filters with smallest L1 norm\nUnusual, for experimentation\n\n\nrandom\nRandom selection\nBaseline comparison\n\n\n\n\nresults = {}\nfor name, criteria in [('large_final', large_final), ('random', random)]:\n    model = resnet18(weights=None)\n    pruner = Pruner(model, 30, 'local', criteria)\n    pruner.prune_model()\n    results[name] = sum(p.numel() for p in model.parameters())\n\nprint('\\nSame pruning ratio, different criteria:')\nfor name, params in results.items():\n    print(f'  {name}: {params:,} parameters')\nprint('\\nNote: Parameter counts are similar, but accuracy differs!')\nprint('large_final preserves important filters, random does not.')\n\nIgnoring output layer: fc\nTotal ignored layers: 1\nIgnoring output layer: fc\nTotal ignored layers: 1\n\nSame pruning ratio, different criteria:\n  large_final: 5,820,556 parameters\n  random: 5,820,556 parameters\n\nNote: Parameter counts are similar, but accuracy differs!\nlarge_final preserves important filters, random does not.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#summary",
    "href": "tutorials/prune/pruner.html#summary",
    "title": "Pruner Tutorial",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nStructured pruning\nRemoves entire filters, creating genuinely smaller models\n\n\nLocal context\nEach layer pruned by same percentage\n\n\nGlobal context\nCompare importance across all layers\n\n\nIterative pruning\nGradual pruning for better accuracy retention\n\n\nPer-layer ratios\nDictionary of custom ratios per layer\n\n\nAuto dependency\nHandles layer connections automatically\n\n\n\n\nTypical Workflow\n# 1. One-shot pruning for quick experiments\npruner = Pruner(model, 30, 'local', large_final)\npruner.prune_model()\n\n# 2. During training with PruneCallback (recommended)\ncb = PruneCallback(pruning_ratio=50, schedule=agp, context='global', criteria=large_final)\nlearn.fit(10, cbs=[cb])",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/prune/pruner.html#see-also",
    "href": "tutorials/prune/pruner.html#see-also",
    "title": "Pruner Tutorial",
    "section": "See Also",
    "text": "See Also\n\nPruneCallback Tutorial - Apply pruning during fastai training\nSparsifier Tutorial - Unstructured pruning alternative\nCriteria - Importance measures for filter selection\nSchedules - Control pruning progression",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Pruner Tutorial"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html",
    "href": "tutorials/misc/bn_folding.html",
    "title": "BatchNorm Folding",
    "section": "",
    "text": "BatchNorm Folding is an optimization technique that merges batch normalization layers into preceding convolutional layers. This eliminates the batch norm computation entirely while maintaining mathematically equivalent results.\n\n\n\n\n\nAspect\nBefore Folding\nAfter Folding\n\n\n\n\nLayers\nConv â†’ BN â†’ ReLU\nConv â†’ ReLU\n\n\nParameters\nConv weights + BN params\nModified Conv weights only\n\n\nInference Speed\nSlower (extra ops)\nFaster (no BN overhead)\n\n\nAccuracy\nBaseline\nIdentical (mathematically equivalent)\n\n\n\n\n\n\nDuring inference, batch normalization applies a linear transformation: \\[y = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\\]\nSince convolution is also linear, we can fold BN parameters into the conv weights: \\[W_{new} = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot W_{old}\\] \\[b_{new} = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot (b_{old} - \\mu) + \\beta\\]\nThe result is identical outputs with fewer operations.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html#overview",
    "href": "tutorials/misc/bn_folding.html#overview",
    "title": "BatchNorm Folding",
    "section": "",
    "text": "BatchNorm Folding is an optimization technique that merges batch normalization layers into preceding convolutional layers. This eliminates the batch norm computation entirely while maintaining mathematically equivalent results.\n\n\n\n\n\nAspect\nBefore Folding\nAfter Folding\n\n\n\n\nLayers\nConv â†’ BN â†’ ReLU\nConv â†’ ReLU\n\n\nParameters\nConv weights + BN params\nModified Conv weights only\n\n\nInference Speed\nSlower (extra ops)\nFaster (no BN overhead)\n\n\nAccuracy\nBaseline\nIdentical (mathematically equivalent)\n\n\n\n\n\n\nDuring inference, batch normalization applies a linear transformation: \\[y = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\\]\nSince convolution is also linear, we can fold BN parameters into the conv weights: \\[W_{new} = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot W_{old}\\] \\[b_{new} = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot (b_{old} - \\mu) + \\beta\\]\nThe result is identical outputs with fewer operations.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html#setup-and-training",
    "href": "tutorials/misc/bn_folding.html#setup-and-training",
    "title": "BatchNorm Folding",
    "section": "1. Setup and Training",
    "text": "1. Setup and Training\nFirst, letâ€™s train a model with batch normalization layers.\n\nLoad Data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\n\nTrain the Model\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.604740\n0.685939\n0.685386\n00:02\n\n\n1\n0.565022\n0.724329\n0.694858\n00:02\n\n\n2\n0.512418\n0.516759\n0.736807\n00:02\n\n\n3\n0.445161\n0.466733\n0.763193\n00:02\n\n\n4\n0.362070\n0.433802\n0.792963\n00:02",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html#fold-batchnorm-layers",
    "href": "tutorials/misc/bn_folding.html#fold-batchnorm-layers",
    "title": "BatchNorm Folding",
    "section": "2. Fold BatchNorm Layers",
    "text": "2. Fold BatchNorm Layers\nUse BN_Folder to fold all batch normalization layers into their preceding convolutions:\n\nbn = BN_Folder()\nnew_model = bn.fold(learn.model)\n\nThe batch norm layers have been replaced by Identity layers, and the convolution weights have been modified to incorporate the batch norm transformation.\n\nnew_model\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n  (bn1): Identity()\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html#comparing-results",
    "href": "tutorials/misc/bn_folding.html#comparing-results",
    "title": "BatchNorm Folding",
    "section": "3. Comparing Results",
    "text": "3. Comparing Results\n\nParameter Count\nThe folded model has fewer parameters (BN parameters are absorbed into conv weights):\n\ncount_parameters(learn.model)\n\n11177538\n\n\n\ncount_parameters(new_model)\n\n11172738\n\n\n\n\nInference Speed\nThe folded model is faster because batch norm operations are eliminated:\n\nx,y = dls.one_batch()\n\n\nlearn.model(x[0][None].cuda())\n\n1.19 ms Â± 4.31 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n\n\n\nnew_model(x[0][None].cuda())\n\n768 Î¼s Â± 1.79 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nAccuracy Verification\nMost importantly, the folded model produces identical results to the original:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\n\n\nnew_learn.validate()\n\n\n\n\n\n\n\n\n[0.4338044822216034, 0.792963445186615]",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html#summary",
    "href": "tutorials/misc/bn_folding.html#summary",
    "title": "BatchNorm Folding",
    "section": "Summary",
    "text": "Summary\n\n\n\nMetric\nOriginal\nFolded\nImprovement\n\n\n\n\nParameters\n11,177,538\n11,172,738\n~5K fewer\n\n\nInference (single image)\n1.19 ms\n0.77 ms\n~35% faster\n\n\nAccuracy\nBaseline\nIdentical\nNo change\n\n\n\n\nWhen to Use BN Folding\n\n\n\nScenario\nRecommendation\n\n\n\n\nInference/deployment\nâœ… Always fold - free speedup\n\n\nBefore quantization\nâœ… Fold first - cleaner quantization\n\n\nDuring training\nâŒ Donâ€™t fold - BN helps training\n\n\nModels without BN\nN/A - Nothing to fold",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/misc/bn_folding.html#see-also",
    "href": "tutorials/misc/bn_folding.html#see-also",
    "title": "BatchNorm Folding",
    "section": "See Also",
    "text": "See Also\n\nQuantize Callback - Apply quantization after folding for maximum compression\nONNX Exporter - Export folded models to ONNX for deployment\nPruner - Combine with pruning for smaller, faster models",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "BatchNorm Folding"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html",
    "href": "tutorials/sparse/schedules.html",
    "title": "Schedules",
    "section": "",
    "text": "Neural Network Pruning usually follows one of the next 3 schedules:\nIn fasterai, all those 3 schedules can be applied from the same callback. Weâ€™ll cover each below\nIn the SparsifyCallback, there are several parameters to â€˜shapeâ€™ our pruning schedule: * start_sparsity: the initial sparsity of our model, generally kept at 0 as after initialization, our weights are generally non-zero. * end_sparsity: the target sparsity at the end of the training * start_epoch: we can decide to start pruning right from the beginning or let it train a bit before removing weights. * sched_func: this is where the general shape of the schedule is specified as it specifies how the sparsity evolves along the training. You can either use a schedule available in fastai our even coming with your own !\npath = untar_data(URLs.PETS)\n\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\nWe will first train a network without any pruning, which will serve as a baseline.\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.773334\n0.612048\n0.803112\n00:03\n\n\n1\n0.484909\n0.651635\n0.823410\n00:04\n\n\n2\n0.275744\n0.470824\n0.819350\n00:04\n\n\n3\n0.210518\n0.255528\n0.896482\n00:04\n\n\n4\n0.187754\n0.402981\n0.848444\n00:04\n\n\n5\n0.168017\n0.282024\n0.878214\n00:04\n\n\n6\n0.104045\n0.215567\n0.923545\n00:04\n\n\n7\n0.063283\n0.211051\n0.928281\n00:04\n\n\n8\n0.032446\n0.187703\n0.937754\n00:04\n\n\n9\n0.021794\n0.190525\n0.938430\n00:04",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#one-shot-pruning",
    "href": "tutorials/sparse/schedules.html#one-shot-pruning",
    "title": "Schedules",
    "section": "One-Shot Pruning",
    "text": "One-Shot Pruning\nThe simplest way to perform pruning is called One-Shot Pruning. It consists of the following three steps:\n\nYou first need to train a network\nYou then need to remove some weights (depending on your criteria, needs,â€¦)\nYou fine-tune the remaining weights to recover from the loss of parameters.\n\nWith fasterai, this is really easy to do. Letâ€™s illustrate it by an example:\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning. This training can be done independently from the pruning callback, or simulated by the start_epoch that will delay the pruning process.\nYou thus only need to create the Callback with the one_shot schedule and set the start_epoch argument, i.e.Â how many epochs you want to train your network before pruning it.\n\nsp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=one_shot)\n\nLetâ€™s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of 90%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.579104\n0.326434\n0.863329\n00:04\n\n\n1\n0.365433\n0.696673\n0.757104\n00:04\n\n\n2\n0.272263\n0.507063\n0.830853\n00:04\n\n\n3\n0.222548\n0.264393\n0.892422\n00:05\n\n\n4\n0.180488\n0.251536\n0.896482\n00:04\n\n\n5\n0.225195\n0.280325\n0.879567\n00:04\n\n\n6\n0.163019\n0.266747\n0.906631\n00:04\n\n\n7\n0.122460\n0.255492\n0.905277\n00:04\n\n\n8\n0.100369\n0.287000\n0.895805\n00:04\n\n\n9\n0.073793\n0.272262\n0.908660\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: 0.00%\nSparsity at the end of epoch 1: 0.00%\nSparsity at the end of epoch 2: 0.00%\nSparsity at the end of epoch 3: 0.00%\nSparsity at the end of epoch 4: 90.00%\nSparsity at the end of epoch 5: 90.00%\nSparsity at the end of epoch 6: 90.00%\nSparsity at the end of epoch 7: 90.00%\nSparsity at the end of epoch 8: 90.00%\nSparsity at the end of epoch 9: 90.00%\nFinal Sparsity: 90.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\n0.0                            Conv2d          9,408      8,467         90.00%\n0.4.0.conv1                    Conv2d          36,864     33,177        90.00%\n0.4.0.conv2                    Conv2d          36,864     33,177        90.00%\n0.4.1.conv1                    Conv2d          36,864     33,177        90.00%\n0.4.1.conv2                    Conv2d          36,864     33,177        90.00%\n0.5.0.conv1                    Conv2d          73,728     66,355        90.00%\n0.5.0.conv2                    Conv2d          147,456    132,710       90.00%\n0.5.0.downsample.0             Conv2d          8,192      7,372         89.99%\n0.5.1.conv1                    Conv2d          147,456    132,710       90.00%\n0.5.1.conv2                    Conv2d          147,456    132,710       90.00%\n0.6.0.conv1                    Conv2d          294,912    265,420       90.00%\n0.6.0.conv2                    Conv2d          589,824    530,841       90.00%\n0.6.0.downsample.0             Conv2d          32,768     29,491        90.00%\n0.6.1.conv1                    Conv2d          589,824    530,841       90.00%\n0.6.1.conv2                    Conv2d          589,824    530,841       90.00%\n0.7.0.conv1                    Conv2d          1,179,648  1,061,683     90.00%\n0.7.0.conv2                    Conv2d          2,359,296  2,123,366     90.00%\n0.7.0.downsample.0             Conv2d          131,072    117,964       90.00%\n0.7.1.conv1                    Conv2d          2,359,296  2,123,366     90.00%\n0.7.1.conv2                    Conv2d          2,359,296  2,123,366     90.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 10,050,211    90.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#iterative-pruning",
    "href": "tutorials/sparse/schedules.html#iterative-pruning",
    "title": "Schedules",
    "section": "Iterative Pruning",
    "text": "Iterative Pruning\nResearchers have come up with a better way to do pruning than pruning all the weigths in once (as in One-Shot Pruning). The idea is to perform several iterations of pruning and fine-tuning and is thus called Iterative Pruning.\n\nYou first need to train a network\nYou then need to remove a part of the weights weights (depending on your criteria, needs,â€¦)\nYou fine-tune the remaining weights to recover from the loss of parameters.\nBack to step 2.\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning.\nYou only need to create the Callback with the iterative schedule and set the start_epoch argument, i.e.Â how many epochs you want to train your network before pruning it.\nThe iterative schedules has a n_stepsparameter, i.e.Â how many iterations of pruning/fine-tuning you want to perform. To modify its value, we can use the partial function like this:\niterative = partial(iterative, n_steps=5)\n\nsp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=iterative)\n\nLetâ€™s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of 90%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.540534\n0.429058\n0.818674\n00:04\n\n\n1\n0.343884\n0.275153\n0.872124\n00:04\n\n\n2\n0.246214\n0.325832\n0.870771\n00:03\n\n\n3\n0.188388\n0.760273\n0.748985\n00:03\n\n\n4\n0.161098\n0.326967\n0.879567\n00:03\n\n\n5\n0.120742\n0.337820\n0.878890\n00:03\n\n\n6\n0.108090\n0.260159\n0.901894\n00:03\n\n\n7\n0.255249\n0.327174\n0.863329\n00:03\n\n\n8\n0.163483\n0.272866\n0.888363\n00:03\n\n\n9\n0.112936\n0.243970\n0.911367\n00:03\n\n\n\n\n\nSparsity at the end of epoch 0: 0.00%\nSparsity at the end of epoch 1: 0.00%\nSparsity at the end of epoch 2: 30.00%\nSparsity at the end of epoch 3: 30.00%\nSparsity at the end of epoch 4: 60.00%\nSparsity at the end of epoch 5: 60.00%\nSparsity at the end of epoch 6: 60.00%\nSparsity at the end of epoch 7: 90.00%\nSparsity at the end of epoch 8: 90.00%\nSparsity at the end of epoch 9: 90.00%\nFinal Sparsity: 90.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\n0.0                            Conv2d          9,408      8,467         90.00%\n0.4.0.conv1                    Conv2d          36,864     33,177        90.00%\n0.4.0.conv2                    Conv2d          36,864     33,177        90.00%\n0.4.1.conv1                    Conv2d          36,864     33,177        90.00%\n0.4.1.conv2                    Conv2d          36,864     33,177        90.00%\n0.5.0.conv1                    Conv2d          73,728     66,355        90.00%\n0.5.0.conv2                    Conv2d          147,456    132,710       90.00%\n0.5.0.downsample.0             Conv2d          8,192      7,372         89.99%\n0.5.1.conv1                    Conv2d          147,456    132,710       90.00%\n0.5.1.conv2                    Conv2d          147,456    132,710       90.00%\n0.6.0.conv1                    Conv2d          294,912    265,420       90.00%\n0.6.0.conv2                    Conv2d          589,824    530,841       90.00%\n0.6.0.downsample.0             Conv2d          32,768     29,491        90.00%\n0.6.1.conv1                    Conv2d          589,824    530,841       90.00%\n0.6.1.conv2                    Conv2d          589,824    530,841       90.00%\n0.7.0.conv1                    Conv2d          1,179,648  1,061,682     90.00%\n0.7.0.conv2                    Conv2d          2,359,296  2,123,365     90.00%\n0.7.0.downsample.0             Conv2d          131,072    117,964       90.00%\n0.7.1.conv1                    Conv2d          2,359,296  2,123,366     90.00%\n0.7.1.conv2                    Conv2d          2,359,296  2,123,366     90.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 10,050,209    90.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#gradual-pruning",
    "href": "tutorials/sparse/schedules.html#gradual-pruning",
    "title": "Schedules",
    "section": "Gradual Pruning",
    "text": "Gradual Pruning\nHere is for example how to implement the Automated Gradual Pruning schedule.\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=agp)\n\nLetâ€™s start pruning after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of 90%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.614568\n0.392339\n0.836265\n00:05\n\n\n1\n0.381756\n0.379407\n0.847091\n00:04\n\n\n2\n0.273133\n0.325403\n0.844384\n00:04\n\n\n3\n0.225977\n0.302057\n0.874154\n00:04\n\n\n4\n0.186390\n0.251494\n0.891746\n00:04\n\n\n5\n0.155119\n0.322763\n0.865359\n00:04\n\n\n6\n0.158356\n0.303624\n0.878890\n00:04\n\n\n7\n0.131190\n0.241102\n0.902571\n00:06\n\n\n8\n0.121991\n0.238827\n0.912043\n00:06\n\n\n9\n0.081199\n0.237045\n0.918133\n00:06\n\n\n\n\n\nSparsity at the end of epoch 0: 0.00%\nSparsity at the end of epoch 1: 0.00%\nSparsity at the end of epoch 2: 29.71%\nSparsity at the end of epoch 3: 52.03%\nSparsity at the end of epoch 4: 68.03%\nSparsity at the end of epoch 5: 78.75%\nSparsity at the end of epoch 6: 85.25%\nSparsity at the end of epoch 7: 88.59%\nSparsity at the end of epoch 8: 89.82%\nSparsity at the end of epoch 9: 90.00%\nFinal Sparsity: 90.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\n0.0                            Conv2d          9,408      8,467         90.00%\n0.4.0.conv1                    Conv2d          36,864     33,177        90.00%\n0.4.0.conv2                    Conv2d          36,864     33,177        90.00%\n0.4.1.conv1                    Conv2d          36,864     33,177        90.00%\n0.4.1.conv2                    Conv2d          36,864     33,177        90.00%\n0.5.0.conv1                    Conv2d          73,728     66,355        90.00%\n0.5.0.conv2                    Conv2d          147,456    132,710       90.00%\n0.5.0.downsample.0             Conv2d          8,192      7,372         89.99%\n0.5.1.conv1                    Conv2d          147,456    132,710       90.00%\n0.5.1.conv2                    Conv2d          147,456    132,710       90.00%\n0.6.0.conv1                    Conv2d          294,912    265,420       90.00%\n0.6.0.conv2                    Conv2d          589,824    530,841       90.00%\n0.6.0.downsample.0             Conv2d          32,768     29,491        90.00%\n0.6.1.conv1                    Conv2d          589,824    530,841       90.00%\n0.6.1.conv2                    Conv2d          589,824    530,841       90.00%\n0.7.0.conv1                    Conv2d          1,179,648  1,061,683     90.00%\n0.7.0.conv2                    Conv2d          2,359,296  2,123,366     90.00%\n0.7.0.downsample.0             Conv2d          131,072    117,964       90.00%\n0.7.1.conv1                    Conv2d          2,359,296  2,123,366     90.00%\n0.7.1.conv2                    Conv2d          2,359,296  2,123,366     90.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 10,050,211    90.00%\n\n\nEven though they are often considered as different pruning methods, those 3 schedules can be captured by the same Callback. Here is how the sparsity in the network evolves for those methods;\nLetâ€™s take an example here. Letâ€™s say that we want to train our network for 3 epochs without pruning and then 7 epochs with pruning.\nThen this is what our different pruning schedules will look like:\n\n\n\n\n\n\n\n\n\nYou can also come up with your own pruning schedule !",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#summary",
    "href": "tutorials/sparse/schedules.html#summary",
    "title": "Schedules",
    "section": "Summary",
    "text": "Summary\n\n\n\nSchedule\nBehavior\n\n\n\n\none_shot\nApply once at a set point\n\n\niterative\nStep-wise pruning in N discrete steps\n\n\nagp\nAutomated Gradual Pruning (cubic decay)\n\n\none_cycle\nLogistic-function based single cycle\n\n\ncos\nCosine annealing\n\n\nlin\nLinear ramp\n\n\ndsd\nDense-Sparse-Dense (prune then regrow)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/schedules.html#see-also",
    "href": "tutorials/sparse/schedules.html#see-also",
    "title": "Schedules",
    "section": "See Also",
    "text": "See Also\n\nSchedules API - Full schedule API reference\nSparsifyCallback - Use schedules with sparsification\nPruneCallback - Use schedules with structured pruning",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html",
    "href": "tutorials/sparse/lottery_ticket.html",
    "title": "Lottery Ticket Hypothesis",
    "section": "",
    "text": "The Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that â€” when trained in isolation â€” it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B &gt; a_A\\), in a training time \\(t_B &lt; t_A\\).\nLetâ€™s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLetâ€™s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.602693\n0.557460\n0.710419\n00:03\n\n\n1\n0.538421\n0.569738\n0.694858\n00:02\n\n\n2\n0.515489\n0.555442\n0.728687\n00:02\n\n\n3\n0.470217\n0.560604\n0.691475\n00:03\n\n\n4\n0.444224\n0.617595\n0.652233\n00:03\n\n\n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e.Â before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e.Â step 4) of the LTH. To empirically validate the LTH, we need to retrain the found â€œlottery ticketâ€ after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of 50%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.591790\n0.597947\n0.722598\n00:04\n\n\n1\n0.539337\n0.593441\n0.705683\n00:05\n\n\n2\n0.508178\n0.581475\n0.708390\n00:04\n\n\n3\n0.476695\n0.577352\n0.718539\n00:04\n\n\n4\n0.433314\n0.499315\n0.753721\n00:04\n\n\n5\n0.555141\n0.545766\n0.713126\n00:04\n\n\n6\n0.520976\n0.575154\n0.718539\n00:04\n\n\n7\n0.488574\n0.636114\n0.631258\n00:04\n\n\n8\n0.457624\n0.484251\n0.767253\n00:04\n\n\n9\n0.417182\n0.444555\n0.788904\n00:04\n\n\n10\n0.494244\n0.641547\n0.703654\n00:04\n\n\n11\n0.438013\n0.482871\n0.760487\n00:04\n\n\n12\n0.399567\n0.506693\n0.748985\n00:04\n\n\n13\n0.375169\n0.413867\n0.803789\n00:04\n\n\n14\n0.343471\n0.522576\n0.732747\n00:04\n\n\n15\n0.407545\n0.486564\n0.761164\n00:04\n\n\n16\n0.359361\n0.480845\n0.783491\n00:04\n\n\n17\n0.321976\n0.413148\n0.805142\n00:04\n\n\n18\n0.283537\n0.439845\n0.814614\n00:04\n\n\n19\n0.252996\n0.424548\n0.835589\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: 0.00%\nSparsity at the end of epoch 1: 0.00%\nSparsity at the end of epoch 2: 0.00%\nSparsity at the end of epoch 3: 0.00%\nSparsity at the end of epoch 4: 0.00%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: 16.67%\nSparsity at the end of epoch 6: 16.67%\nSparsity at the end of epoch 7: 16.67%\nSparsity at the end of epoch 8: 16.67%\nSparsity at the end of epoch 9: 16.67%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: 33.33%\nSparsity at the end of epoch 11: 33.33%\nSparsity at the end of epoch 12: 33.33%\nSparsity at the end of epoch 13: 33.33%\nSparsity at the end of epoch 14: 33.33%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: 50.00%\nSparsity at the end of epoch 16: 50.00%\nSparsity at the end of epoch 17: 50.00%\nSparsity at the end of epoch 18: 50.00%\nSparsity at the end of epoch 19: 50.00%\nFinal Sparsity: 50.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      4,704         50.00%\nlayer1.0.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.0.conv2                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv2                 Conv2d          36,864     18,432        50.00%\nlayer2.0.conv1                 Conv2d          73,728     36,864        50.00%\nlayer2.0.conv2                 Conv2d          147,456    73,728        50.00%\nlayer2.0.downsample.0          Conv2d          8,192      4,096         50.00%\nlayer2.1.conv1                 Conv2d          147,456    73,728        50.00%\nlayer2.1.conv2                 Conv2d          147,456    73,728        50.00%\nlayer3.0.conv1                 Conv2d          294,912    147,456       50.00%\nlayer3.0.conv2                 Conv2d          589,824    294,912       50.00%\nlayer3.0.downsample.0          Conv2d          32,768     16,384        50.00%\nlayer3.1.conv1                 Conv2d          589,824    294,912       50.00%\nlayer3.1.conv2                 Conv2d          589,824    294,912       50.00%\nlayer4.0.conv1                 Conv2d          1,179,648  589,824       50.00%\nlayer4.0.conv2                 Conv2d          2,359,296  1,179,647     50.00%\nlayer4.0.downsample.0          Conv2d          131,072    65,536        50.00%\nlayer4.1.conv1                 Conv2d          2,359,296  1,179,648     50.00%\nlayer4.1.conv2                 Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 5,583,455     50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B &gt; a_A\\) in the same training time.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html#the-lottery-ticket-hypothesis",
    "href": "tutorials/sparse/lottery_ticket.html#the-lottery-ticket-hypothesis",
    "title": "Lottery Ticket Hypothesis",
    "section": "",
    "text": "The Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that â€” when trained in isolation â€” it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B &gt; a_A\\), in a training time \\(t_B &lt; t_A\\).\nLetâ€™s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLetâ€™s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.602693\n0.557460\n0.710419\n00:03\n\n\n1\n0.538421\n0.569738\n0.694858\n00:02\n\n\n2\n0.515489\n0.555442\n0.728687\n00:02\n\n\n3\n0.470217\n0.560604\n0.691475\n00:03\n\n\n4\n0.444224\n0.617595\n0.652233\n00:03\n\n\n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e.Â before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e.Â step 4) of the LTH. To empirically validate the LTH, we need to retrain the found â€œlottery ticketâ€ after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of 50%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.591790\n0.597947\n0.722598\n00:04\n\n\n1\n0.539337\n0.593441\n0.705683\n00:05\n\n\n2\n0.508178\n0.581475\n0.708390\n00:04\n\n\n3\n0.476695\n0.577352\n0.718539\n00:04\n\n\n4\n0.433314\n0.499315\n0.753721\n00:04\n\n\n5\n0.555141\n0.545766\n0.713126\n00:04\n\n\n6\n0.520976\n0.575154\n0.718539\n00:04\n\n\n7\n0.488574\n0.636114\n0.631258\n00:04\n\n\n8\n0.457624\n0.484251\n0.767253\n00:04\n\n\n9\n0.417182\n0.444555\n0.788904\n00:04\n\n\n10\n0.494244\n0.641547\n0.703654\n00:04\n\n\n11\n0.438013\n0.482871\n0.760487\n00:04\n\n\n12\n0.399567\n0.506693\n0.748985\n00:04\n\n\n13\n0.375169\n0.413867\n0.803789\n00:04\n\n\n14\n0.343471\n0.522576\n0.732747\n00:04\n\n\n15\n0.407545\n0.486564\n0.761164\n00:04\n\n\n16\n0.359361\n0.480845\n0.783491\n00:04\n\n\n17\n0.321976\n0.413148\n0.805142\n00:04\n\n\n18\n0.283537\n0.439845\n0.814614\n00:04\n\n\n19\n0.252996\n0.424548\n0.835589\n00:04\n\n\n\n\n\nSparsity at the end of epoch 0: 0.00%\nSparsity at the end of epoch 1: 0.00%\nSparsity at the end of epoch 2: 0.00%\nSparsity at the end of epoch 3: 0.00%\nSparsity at the end of epoch 4: 0.00%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: 16.67%\nSparsity at the end of epoch 6: 16.67%\nSparsity at the end of epoch 7: 16.67%\nSparsity at the end of epoch 8: 16.67%\nSparsity at the end of epoch 9: 16.67%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: 33.33%\nSparsity at the end of epoch 11: 33.33%\nSparsity at the end of epoch 12: 33.33%\nSparsity at the end of epoch 13: 33.33%\nSparsity at the end of epoch 14: 33.33%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: 50.00%\nSparsity at the end of epoch 16: 50.00%\nSparsity at the end of epoch 17: 50.00%\nSparsity at the end of epoch 18: 50.00%\nSparsity at the end of epoch 19: 50.00%\nFinal Sparsity: 50.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      4,704         50.00%\nlayer1.0.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.0.conv2                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv2                 Conv2d          36,864     18,432        50.00%\nlayer2.0.conv1                 Conv2d          73,728     36,864        50.00%\nlayer2.0.conv2                 Conv2d          147,456    73,728        50.00%\nlayer2.0.downsample.0          Conv2d          8,192      4,096         50.00%\nlayer2.1.conv1                 Conv2d          147,456    73,728        50.00%\nlayer2.1.conv2                 Conv2d          147,456    73,728        50.00%\nlayer3.0.conv1                 Conv2d          294,912    147,456       50.00%\nlayer3.0.conv2                 Conv2d          589,824    294,912       50.00%\nlayer3.0.downsample.0          Conv2d          32,768     16,384        50.00%\nlayer3.1.conv1                 Conv2d          589,824    294,912       50.00%\nlayer3.1.conv2                 Conv2d          589,824    294,912       50.00%\nlayer4.0.conv1                 Conv2d          1,179,648  589,824       50.00%\nlayer4.0.conv2                 Conv2d          2,359,296  1,179,647     50.00%\nlayer4.0.downsample.0          Conv2d          131,072    65,536        50.00%\nlayer4.1.conv1                 Conv2d          2,359,296  1,179,648     50.00%\nlayer4.1.conv2                 Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 5,583,455     50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B &gt; a_A\\) in the same training time.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "href": "tutorials/sparse/lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "title": "Lottery Ticket Hypothesis",
    "section": "Lottery Ticket Hypothesis with Rewinding",
    "text": "Lottery Ticket Hypothesis with Rewinding\nIn some case, LTH fails for deeper networks, author then propose a solution, which is to rewind the weights to a more advanced iteration instead of the initialization value.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nThis can be done in fasterai by passing the rewind_epoch parameter, that will save the weights at that epoch, then resetting the weights accordingly.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, rewind_epoch=1)\n\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of 50%\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.585928\n0.599651\n0.695535\n00:03\n\n\n1\n0.567199\n0.571735\n0.725304\n00:03\n\n\n2\n0.520258\n0.527225\n0.738160\n00:04\n\n\n3\n0.487749\n0.526812\n0.746279\n00:03\n\n\n4\n0.455229\n0.509876\n0.732747\n00:03\n\n\n5\n0.502195\n0.805385\n0.704330\n00:03\n\n\n6\n0.465073\n0.489304\n0.769959\n00:03\n\n\n7\n0.424250\n1.209978\n0.462111\n00:03\n\n\n8\n0.393979\n0.475942\n0.756428\n00:03\n\n\n9\n0.361146\n0.519064\n0.751015\n00:03\n\n\n10\n0.433051\n0.483362\n0.756428\n00:03\n\n\n11\n0.400122\n0.484240\n0.786198\n00:03\n\n\n12\n0.360677\n0.424378\n0.807848\n00:03\n\n\n13\n0.323188\n0.448461\n0.805819\n00:03\n\n\n14\n0.291294\n0.436022\n0.820027\n00:03\n\n\n15\n0.360917\n0.482217\n0.803112\n00:03\n\n\n16\n0.320696\n0.397914\n0.816644\n00:03\n\n\n17\n0.269400\n0.516504\n0.771989\n00:03\n\n\n18\n0.256572\n0.471974\n0.799053\n00:02\n\n\n19\n0.222436\n0.480946\n0.779432\n00:03\n\n\n\n\n\nSparsity at the end of epoch 0: 0.00%\nSaving Weights at epoch 1\nSparsity at the end of epoch 1: 0.00%\nSparsity at the end of epoch 2: 0.00%\nSparsity at the end of epoch 3: 0.00%\nSparsity at the end of epoch 4: 0.00%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 5: 16.67%\nSparsity at the end of epoch 6: 16.67%\nSparsity at the end of epoch 7: 16.67%\nSparsity at the end of epoch 8: 16.67%\nSparsity at the end of epoch 9: 16.67%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 10: 33.33%\nSparsity at the end of epoch 11: 33.33%\nSparsity at the end of epoch 12: 33.33%\nSparsity at the end of epoch 13: 33.33%\nSparsity at the end of epoch 14: 33.33%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 15: 50.00%\nSparsity at the end of epoch 16: 50.00%\nSparsity at the end of epoch 17: 50.00%\nSparsity at the end of epoch 18: 50.00%\nSparsity at the end of epoch 19: 50.00%\nFinal Sparsity: 50.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      4,704         50.00%\nlayer1.0.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.0.conv2                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv2                 Conv2d          36,864     18,432        50.00%\nlayer2.0.conv1                 Conv2d          73,728     36,864        50.00%\nlayer2.0.conv2                 Conv2d          147,456    73,728        50.00%\nlayer2.0.downsample.0          Conv2d          8,192      4,096         50.00%\nlayer2.1.conv1                 Conv2d          147,456    73,728        50.00%\nlayer2.1.conv2                 Conv2d          147,456    73,728        50.00%\nlayer3.0.conv1                 Conv2d          294,912    147,456       50.00%\nlayer3.0.conv2                 Conv2d          589,824    294,912       50.00%\nlayer3.0.downsample.0          Conv2d          32,768     16,384        50.00%\nlayer3.1.conv1                 Conv2d          589,824    294,912       50.00%\nlayer3.1.conv2                 Conv2d          589,824    294,912       50.00%\nlayer4.0.conv1                 Conv2d          1,179,648  589,824       50.00%\nlayer4.0.conv2                 Conv2d          2,359,296  1,179,648     50.00%\nlayer4.0.downsample.0          Conv2d          131,072    65,536        50.00%\nlayer4.1.conv1                 Conv2d          2,359,296  1,179,648     50.00%\nlayer4.1.conv2                 Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 5,583,456     50.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/lottery_ticket.html#super-masks",
    "href": "tutorials/sparse/lottery_ticket.html#super-masks",
    "title": "Lottery Ticket Hypothesis",
    "section": "Super-Masks",
    "text": "Super-Masks\nResearchers from Uber AI investigated the LTH and found the existence of what they call â€œSuper-Masksâ€, i.e.Â masks that, applied on a untrained neural network, allows to reach better-than-random results.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n&lt;All keys matched successfully&gt;\n\n\nTo find supermasks, authors perform the LTH method then apply the mask on the original, untrained network. In fasterai, you can pass the parameter reset_end=True, which will reset the weights to their original value at the end of the training, but keeping the pruned weights (i.e.Â the mask) unchanged.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, reset_end=True)\n\n\nlearn.fit(10, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of 50%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.594171\n0.610432\n0.649526\n00:04\n\n\n1\n0.561133\n0.920322\n0.684709\n00:03\n\n\n2\n0.574408\n0.606864\n0.659675\n00:03\n\n\n3\n0.567595\n0.552262\n0.714479\n00:02\n\n\n4\n0.516829\n0.581788\n0.694858\n00:03\n\n\n5\n0.553904\n0.525427\n0.740866\n00:02\n\n\n6\n0.490872\n0.523433\n0.715156\n00:02\n\n\n7\n0.494422\n0.939405\n0.379567\n00:02\n\n\n8\n0.438296\n0.471724\n0.783491\n00:02\n\n\n9\n0.402010\n0.540082\n0.765900\n00:02\n\n\n\n\n\nSparsity at the end of epoch 0: 0.00%\nSparsity at the end of epoch 1: 0.00%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 2: 16.67%\nSparsity at the end of epoch 3: 16.67%\nSparsity at the end of epoch 4: 16.67%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: 33.33%\nSparsity at the end of epoch 6: 33.33%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 7: 50.00%\nSparsity at the end of epoch 8: 50.00%\nSparsity at the end of epoch 9: 50.00%\nFinal Sparsity: 50.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      4,704         50.00%\nlayer1.0.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.0.conv2                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv1                 Conv2d          36,864     18,432        50.00%\nlayer1.1.conv2                 Conv2d          36,864     18,432        50.00%\nlayer2.0.conv1                 Conv2d          73,728     36,864        50.00%\nlayer2.0.conv2                 Conv2d          147,456    73,728        50.00%\nlayer2.0.downsample.0          Conv2d          8,192      4,096         50.00%\nlayer2.1.conv1                 Conv2d          147,456    73,728        50.00%\nlayer2.1.conv2                 Conv2d          147,456    73,728        50.00%\nlayer3.0.conv1                 Conv2d          294,912    147,456       50.00%\nlayer3.0.conv2                 Conv2d          589,824    294,912       50.00%\nlayer3.0.downsample.0          Conv2d          32,768     16,384        50.00%\nlayer3.1.conv1                 Conv2d          589,824    294,912       50.00%\nlayer3.1.conv2                 Conv2d          589,824    294,911       50.00%\nlayer4.0.conv1                 Conv2d          1,179,648  589,824       50.00%\nlayer4.0.conv2                 Conv2d          2,359,296  1,179,648     50.00%\nlayer4.0.downsample.0          Conv2d          131,072    65,536        50.00%\nlayer4.1.conv1                 Conv2d          2,359,296  1,179,648     50.00%\nlayer4.1.conv2                 Conv2d          2,359,296  1,179,648     50.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 5,583,455     50.00%\n\n\n\nlearn.validate()\n\n\n\n\n\n\n\n\n[0.6421075463294983, 0.6617050170898438]",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Lottery Ticket Hypothesis"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html",
    "href": "tutorials/sparse/sparsify_callback.html",
    "title": "Sparsify Callback",
    "section": "",
    "text": "from fastai.vision.all import *",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#overview",
    "href": "tutorials/sparse/sparsify_callback.html#overview",
    "title": "Sparsify Callback",
    "section": "Overview",
    "text": "Overview\nSparsification sets individual weights to zero during training, creating sparse networks that can be more efficient for inference. Unlike structured pruning (which removes entire filters), sparsification maintains the original architecture while introducing zeros.\n\nWhy Use Sparsification?\n\n\n\n\n\n\n\n\n\nApproach\nWhatâ€™s Removed\nArchitecture\nHardware Support\n\n\n\n\nSparsification\nIndividual weights\nUnchanged\nSparse accelerators\n\n\nStructured Pruning\nEntire filters/channels\nChanged\nStandard hardware\n\n\n\n\n\nKey Benefits\n\nGradual sparsity - Weights are progressively zeroed during training\nMaintained accuracy - Network adapts to sparsity during training\nFlexible targeting - Choose which layers and how much to sparsify\nSchedule control - Use one-cycle, cosine, or custom schedules",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#setup-and-data",
    "href": "tutorials/sparse/sparsify_callback.html#setup-and-data",
    "title": "Sparsify Callback",
    "section": "1. Setup and Data",
    "text": "1. Setup and Data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#baseline-dense-model",
    "href": "tutorials/sparse/sparsify_callback.html#baseline-dense-model",
    "title": "Sparsify Callback",
    "section": "2. Baseline: Dense Model",
    "text": "2. Baseline: Dense Model\nFirst, letâ€™s train a standard dense model to establish baseline accuracy:\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.732612\n0.397222\n0.839648\n00:04\n\n\n1\n0.394582\n0.260210\n0.887686\n00:04\n\n\n2\n0.218636\n0.235590\n0.907307\n00:04\n\n\n3\n0.118740\n0.200626\n0.922869\n00:04\n\n\n4\n0.078772\n0.187712\n0.922869\n00:04",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#training-with-sparsifycallback",
    "href": "tutorials/sparse/sparsify_callback.html#training-with-sparsifycallback",
    "title": "Sparsify Callback",
    "section": "3. Training with SparsifyCallback",
    "text": "3. Training with SparsifyCallback\nNow letâ€™s train with 50% sparsity. The SparsifyCallback gradually introduces zeros during training according to the specified schedule:\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nThe callback requires a schedule parameter that controls how sparsity increases over training. You can use any fastai annealing function or define your own.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_cycle)\n\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of 50%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.662926\n1.296763\n0.810555\n00:07\n\n\n1\n0.376402\n0.278251\n0.883627\n00:06\n\n\n2\n0.243227\n0.213432\n0.911367\n00:07\n\n\n3\n0.130433\n0.186261\n0.930311\n00:07\n\n\n4\n0.079553\n0.165558\n0.934371\n00:06\n\n\n\n\n\nSparsity at the end of epoch 0: 1.96%\nSparsity at the end of epoch 1: 20.07%\nSparsity at the end of epoch 2: 45.86%\nSparsity at the end of epoch 3: 49.74%\nSparsity at the end of epoch 4: 50.00%\nFinal Sparsity: 50.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\n0.0                            Conv2d          9,408      4,704         50.00%\n0.4.0.conv1                    Conv2d          36,864     18,432        50.00%\n0.4.0.conv2                    Conv2d          36,864     18,432        50.00%\n0.4.1.conv1                    Conv2d          36,864     18,432        50.00%\n0.4.1.conv2                    Conv2d          36,864     18,432        50.00%\n0.5.0.conv1                    Conv2d          73,728     36,864        50.00%\n0.5.0.conv2                    Conv2d          147,456    73,727        50.00%\n0.5.0.downsample.0             Conv2d          8,192      4,096         50.00%\n0.5.1.conv1                    Conv2d          147,456    73,727        50.00%\n0.5.1.conv2                    Conv2d          147,456    73,727        50.00%\n0.6.0.conv1                    Conv2d          294,912    147,455       50.00%\n0.6.0.conv2                    Conv2d          589,824    294,909       50.00%\n0.6.0.downsample.0             Conv2d          32,768     16,384        50.00%\n0.6.1.conv1                    Conv2d          589,824    294,909       50.00%\n0.6.1.conv2                    Conv2d          589,824    294,909       50.00%\n0.7.0.conv1                    Conv2d          1,179,648  589,818       50.00%\n0.7.0.conv2                    Conv2d          2,359,296  1,179,637     50.00%\n0.7.0.downsample.0             Conv2d          131,072    65,535        50.00%\n0.7.1.conv1                    Conv2d          2,359,296  1,179,637     50.00%\n0.7.1.conv2                    Conv2d          2,359,296  1,179,637     50.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 5,583,403     50.00%\n\n\nDespite having 50% of weights set to zero, the sparse model performs comparably to the dense baseline!",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#b.-per-layer-sparsity",
    "href": "tutorials/sparse/sparsify_callback.html#b.-per-layer-sparsity",
    "title": "Sparsify Callback",
    "section": "3b. Per-Layer Sparsity",
    "text": "3b. Per-Layer Sparsity\nDifferent layers have different sensitivities to sparsification. Early layers often need more weights to preserve low-level features, while deeper layers can tolerate higher sparsity. You can specify per-layer targets using a dictionary:\n\n# Define different sparsity targets for different layers\nper_layer_sparsity = {\n    '0.4.0.conv1': 30,   # Early layers: lower sparsity (more sensitive)\n    '0.4.0.conv2': 30,\n    '0.4.1.conv1': 30,\n    '0.4.1.conv2': 30,\n    '0.5.0.conv1': 50,   # Middle layers: medium sparsity\n    '0.5.0.conv2': 50,\n    '0.5.1.conv1': 50,\n    '0.5.1.conv2': 50,\n    '0.6.0.conv1': 70,   # Deeper layers: higher sparsity (more redundant)\n    '0.6.0.conv2': 70,\n    '0.6.1.conv1': 70,\n    '0.6.1.conv2': 70,\n    '0.7.0.conv1': 80,   # Deepest layers: highest sparsity\n    '0.7.0.conv2': 80,\n    '0.7.1.conv1': 80,\n    '0.7.1.conv2': 80,\n}\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n# Use dict for per-layer sparsity - requires 'local' context\nsp_cb = SparsifyCallback(\n    sparsity=per_layer_sparsity, \n    granularity='weight', \n    context='local',  # Required for per-layer sparsity\n    criteria=large_final, \n    schedule=cos\n)\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of {'0.4.0.conv1': 30, '0.4.0.conv2': 30, '0.4.1.conv1': 30, '0.4.1.conv2': 30, '0.5.0.conv1': 50, '0.5.0.conv2': 50, '0.5.1.conv1': 50, '0.5.1.conv2': 50, '0.6.0.conv1': 70, '0.6.0.conv2': 70, '0.6.1.conv1': 70, '0.6.1.conv2': 70, '0.7.0.conv1': 80, '0.7.0.conv2': 80, '0.7.1.conv1': 80, '0.7.1.conv2': 80}%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.702893\n0.432825\n0.829499\n00:06\n\n\n1\n0.395077\n0.314297\n0.887010\n00:06\n\n\n2\n0.229694\n0.263221\n0.892422\n00:05\n\n\n3\n0.132596\n0.182942\n0.930311\n00:06\n\n\n4\n0.077698\n0.172972\n0.935724\n00:07\n\n\n\n\n\nSparsity at the end of epoch 0: avg=5.49%\nSparsity at the end of epoch 1: avg=19.87%\nSparsity at the end of epoch 2: avg=37.63%\nSparsity at the end of epoch 3: avg=52.01%\nSparsity at the end of epoch 4: avg=57.50%\nFinal Sparsity: {'0.4.0.conv1': 30.0, '0.4.0.conv2': 30.0, '0.4.1.conv1': 30.0, '0.4.1.conv2': 30.0, '0.5.0.conv1': 50.0, '0.5.0.conv2': 50.0, '0.5.1.conv1': 50.0, '0.5.1.conv2': 50.0, '0.6.0.conv1': 70.0, '0.6.0.conv2': 70.0, '0.6.1.conv1': 70.0, '0.6.1.conv2': 70.0, '0.7.0.conv1': 80.0, '0.7.0.conv2': 80.0, '0.7.1.conv1': 80.0, '0.7.1.conv2': 80.0}\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\n0.0                            Conv2d          9,408      0              0.00%\n0.4.0.conv1                    Conv2d          36,864     11,059        30.00%\n0.4.0.conv2                    Conv2d          36,864     11,059        30.00%\n0.4.1.conv1                    Conv2d          36,864     11,059        30.00%\n0.4.1.conv2                    Conv2d          36,864     11,059        30.00%\n0.5.0.conv1                    Conv2d          73,728     36,864        50.00%\n0.5.0.conv2                    Conv2d          147,456    73,727        50.00%\n0.5.0.downsample.0             Conv2d          8,192      0              0.00%\n0.5.1.conv1                    Conv2d          147,456    73,727        50.00%\n0.5.1.conv2                    Conv2d          147,456    73,727        50.00%\n0.6.0.conv1                    Conv2d          294,912    206,436       70.00%\n0.6.0.conv2                    Conv2d          589,824    412,872       70.00%\n0.6.0.downsample.0             Conv2d          32,768     0              0.00%\n0.6.1.conv1                    Conv2d          589,824    412,872       70.00%\n0.6.1.conv2                    Conv2d          589,824    412,872       70.00%\n0.7.0.conv1                    Conv2d          1,179,648  943,709       80.00%\n0.7.0.conv2                    Conv2d          2,359,296  1,887,418     80.00%\n0.7.0.downsample.0             Conv2d          131,072    0              0.00%\n0.7.1.conv1                    Conv2d          2,359,296  1,887,418     80.00%\n0.7.1.conv2                    Conv2d          2,359,296  1,887,418     80.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 8,353,296     74.80%\n\n\nKey points about per-layer sparsity:\n\nUse a dict mapping layer names to sparsity percentages\nRequires context='local' (global context doesnâ€™t support non-uniform sparsity)\nLayer names match those shown in the Sparsity Report (e.g., '0.4.0.conv1')\nLayers not in the dict are left dense (0% sparsity)\nThe schedule applies uniformly - all layers progress from 0% to their target together\n\nTip: Use learn.model to explore layer names, or run a uniform sparsity first to see the Sparsity Report with all layer names.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#parameter-reference",
    "href": "tutorials/sparse/sparsify_callback.html#parameter-reference",
    "title": "Sparsify Callback",
    "section": "4. Parameter Reference",
    "text": "4. Parameter Reference\n\nCore Parameters\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample\n\n\n\n\nsparsity\nTarget sparsity % (float or dict for per-layer)\n50 or {'layer1': 30, 'layer2': 70}\n\n\ngranularity\nLevel of sparsification\n'weight', 'vector', 'kernel', 'filter'\n\n\ncontext\nHow to compute importance\n'local' (per-layer) or 'global' (whole model)\n\n\ncriteria\nImportance measure\nlarge_final, small_final, magnitude\n\n\nschedule\nHow sparsity increases over training\none_cycle, cos, lin\n\n\n\n\n\nAdvanced Parameters\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nlth\nEnable Lottery Ticket Hypothesis (reset weights after pruning)\n\n\nrewind_epoch\nEpoch to rewind weights to (for LTH)\n\n\nreset_end\nReset weights to original values after training\n\n\nsave_tickets\nSave intermediate winning tickets\n\n\nmodel\nApply to specific submodule instead of whole model\n\n\nround_to\nRound sparsity to nearest multiple\n\n\nlayer_type\nType of layers to sparsify (default: nn.Conv2d)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#summary",
    "href": "tutorials/sparse/sparsify_callback.html#summary",
    "title": "Sparsify Callback",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nSparsification\nSetting individual weights to zero while maintaining architecture\n\n\nSparsifyCallback\nfastai callback for gradual sparsification during training\n\n\nSchedule\nControls how sparsity increases over training (one_cycle, cos, etc.)\n\n\nPer-layer sparsity\nDifferent sparsity targets for different layers\n\n\nTypical result\n50%+ sparsity with minimal accuracy loss",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsify_callback.html#see-also",
    "href": "tutorials/sparse/sparsify_callback.html#see-also",
    "title": "Sparsify Callback",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Lower-level API for one-shot sparsification\nSchedules - Available sparsity schedules\nCriteria - Weight importance measures\nLottery Ticket Tutorial - Using LTH with sparsification\nPruner - For structured pruning (removing entire filters)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html",
    "href": "tutorials/walkthrough.html",
    "title": "Walkthrough",
    "section": "",
    "text": "size, bs = 128, 32\ndls = get_dls(size, bs)\nLetâ€™s start with a bit of context for the purpose of the demonstration. Imagine that we want to deploy a VGG16 model on a mobile device that has limited storage capacity and that our task requires our model to run sufficiently fast. It is known that parameters and speed efficiency are not the strong points of VGG16 but letâ€™s see what we can do with it.\nLetâ€™s first check the number of parameters and the inference time of VGG16.\nlearn = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nnum_parameters = get_num_parameters(learn.model)\ndisk_size = get_model_size(learn.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 537.30 MB (disk), 134309962 parameters\nSo our model has 134 millions parameters and needs 537MB of disk space in order to be stored\nmodel = learn.model.eval().to('cpu')\nx,y = dls.one_batch()\nprint(f'Inference Speed: {evaluate_cpu_speed(learn.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 80.86ms\nAnd it takes 26ms to perform inference on a single image.\nSnap ! This is more than we can afford for deployment, ideally we would like our model to take only half of thatâ€¦but should we give up ? Nope, there are actually a lot of techniques that we can use to help reducing the size and improve the speed of our models! Letâ€™s see how to apply them with FasterAI.\nWe will first train our VGG16 model to have a baseline of what performance we should expect from it.\nlearn.fit_one_cycle(10, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.084260\n1.887413\n0.333758\n00:11\n\n\n1\n1.658447\n1.451500\n0.524076\n00:10\n\n\n2\n1.379249\n1.647492\n0.514140\n00:10\n\n\n3\n1.235636\n1.450754\n0.536561\n00:10\n\n\n4\n1.096547\n0.920276\n0.708025\n00:10\n\n\n5\n0.939336\n0.904787\n0.713885\n00:10\n\n\n6\n0.820493\n0.774724\n0.752357\n00:10\n\n\n7\n0.675887\n0.635337\n0.791592\n00:10\n\n\n8\n0.636166\n0.582814\n0.811210\n00:10\n\n\n9\n0.581690\n0.588991\n0.811720\n00:10\nSo we would like our network to have comparable accuracy but fewer parameters and running fasterâ€¦ And the first technique that we will show how to use is called Knowledge Distillation",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#knowledge-distillation",
    "href": "tutorials/walkthrough.html#knowledge-distillation",
    "title": "Walkthrough",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation\nKnowledge distillation is a simple yet very efficient way to train a model. It was introduced in 2006 by Caruana et al.. The main idea behind is to use a small model (called the student) to approximate the function learned by a larger and high-performing model (called the teacher). This can be done by using the large model to pseudo-label the data. This idea has been used very recently to break the state-of-the-art accuracy on ImageNet.\nWhen we train our model for classification, we usually use a softmax as last layer. This softmax has the particularity to squish low value logits towards 0, and the highest logit towards 1. This has for effect to completely lose all the inter-class information, or what is sometimes called the dark knowledge. This is the information that is valuable and that we want to transfer from the teacher to the student.\nTo do so, we still use a regular classification loss but at the same time, weâ€™ll use another loss, computed between the softened logits of the teacher (our soft labels) and the softened logits of the student (our soft predictions). Those soft values are obtained when you use a soft-softmax, that avoids squishing the values at its output. Our implementation follows this paper and the basic principle of training is represented in the figure below:\n\n\n\nTo use Knowledge Distillation with FasterAI, you only need to use this callback when training your student model:\n\n\n KnowledgeDistillation(teacher.model, loss) \n\n You only need to give to the callback function your teacher learner. Behind the scenes, FasterAI will take care of making your model train using knowledge distillation. \n\n\n\n\nfrom fasterai.distill.all import *\n\nThe first thing to do is to find a teacher, which can be any model, that preferrably performs well. We will chose VGG19 for our demonstration. To make sure it performs better than our VGG16 model, letâ€™s start from a pretrained version.\n\nteacher = vision_learner(dls, models.vgg19_bn, metrics=[accuracy])\nteacher.fit_one_cycle(3, 1e-4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.941329\n0.353027\n0.896051\n00:06\n\n\n1\n0.467530\n0.222822\n0.929936\n00:06\n\n\n2\n0.443237\n0.213563\n0.935032\n00:06\n\n\n\n\n\nOur teacher has 94% of accuracy which is pretty good, it is ready to take a student under its wing. So letâ€™s create our student model and train it with the Knowledge Distillation callback:\n\nstudent = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nkd_cb = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-4, cbs=kd_cb)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n5.864407\n5.476386\n0.411465\n00:16\n\n\n1\n4.312775\n5.342780\n0.501911\n00:16\n\n\n2\n3.596799\n4.517756\n0.507006\n00:16\n\n\n3\n3.102537\n3.239754\n0.644841\n00:16\n\n\n4\n2.596609\n2.465116\n0.717707\n00:16\n\n\n5\n2.418236\n2.516353\n0.702420\n00:16\n\n\n6\n2.024942\n2.050347\n0.771465\n00:16\n\n\n7\n1.850588\n1.784052\n0.803567\n00:16\n\n\n8\n1.625190\n1.574969\n0.818089\n00:16\n\n\n9\n1.565640\n1.578238\n0.813758\n00:16\n\n\n\n\n\nAnd we can see that indeed, the knowledge of the teacher was useful for the student, as it is clearly overperforming the vanilla VGG16.\nOk, so now we are able to get more from a given model which is kind of cool ! With some experimentations we could come up with a model smaller than VGG16 but able to reach the same performance as our baseline! You can try to find it by yourself later, but for now letâ€™s continue with the next technique !",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#sparsifying",
    "href": "tutorials/walkthrough.html#sparsifying",
    "title": "Walkthrough",
    "section": "Sparsifying",
    "text": "Sparsifying\nNow that we have a student model that is performing better than our baseline, we have some room to compress it. And weâ€™ll start by making the network sparse. As explained in a previous article, there are many ways leading to a sparse network.\n\n\n\n\n\n\n\nNote\n\n\n\nUsually, the process of making a network sparse is called Pruning. I prefer using the term Pruning when parameters are actually removed from the network, which we will do in the next section.\n\n\n\n\n\nBy default, FasterAI uses the Automated Gradual Pruning paradigm as it removes parameters as the model trains and doesnâ€™t require to pretrain the model, so it is usually much faster. In FasterAI, this is also managed by using a callback, that will replace the least important parameters of your model by zeroes during the training. The callback has a wide variety of parameters to tune your Sparsifying operation, letâ€™s take a look at them:\n\n\nSparsifyCallback(learn, sparsity, granularity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ngranularity: on what granularity you want the sparsification to be operated\n\n\ncontext: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e linear, cosine, â€¦ and gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\n\nBut letâ€™s come back to our example!\nHere, we will make our network 40% sparse, and remove entire filters, selected locally and based on L1 norm. We will train with a learning rate a bit smaller to be gentle with our network because it has already been trained. The scheduling selected is cosinusoidal, so the pruning starts and ends quite slowly.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='filter', context='global', criteria=large_final, schedule=cos)\nstudent.fit(10, 1e-5, cbs=sp_cb)\n\nPruning of filter until a sparsity of 50%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.590607\n0.572063\n0.820637\n00:11\n\n\n1\n0.621573\n0.576351\n0.814522\n00:11\n\n\n2\n0.569623\n0.564671\n0.812229\n00:11\n\n\n3\n0.575295\n0.585963\n0.813503\n00:11\n\n\n4\n0.557072\n0.572918\n0.818599\n00:11\n\n\n5\n0.565852\n0.592840\n0.812229\n00:11\n\n\n6\n0.621907\n0.611757\n0.807389\n00:11\n\n\n7\n0.656858\n0.608958\n0.802803\n00:11\n\n\n8\n0.624567\n0.648065\n0.788535\n00:11\n\n\n9\n0.620338\n0.608264\n0.800255\n00:11\n\n\n\n\n\nSparsity at the end of epoch 0: 1.22%\nSparsity at the end of epoch 1: 4.77%\nSparsity at the end of epoch 2: 10.31%\nSparsity at the end of epoch 3: 17.27%\nSparsity at the end of epoch 4: 25.00%\nSparsity at the end of epoch 5: 32.73%\nSparsity at the end of epoch 6: 39.69%\nSparsity at the end of epoch 7: 45.23%\nSparsity at the end of epoch 8: 48.78%\nSparsity at the end of epoch 9: 50.00%\nFinal Sparsity: 50.00%\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nfeatures.0                     Conv2d          1,728      0              0.00%\nfeatures.3                     Conv2d          36,864     0              0.00%\nfeatures.7                     Conv2d          73,728     0              0.00%\nfeatures.10                    Conv2d          147,456    0              0.00%\nfeatures.14                    Conv2d          294,912    0              0.00%\nfeatures.17                    Conv2d          589,824    0              0.00%\nfeatures.20                    Conv2d          589,824    0              0.00%\nfeatures.24                    Conv2d          1,179,648  755,712       64.06%\nfeatures.27                    Conv2d          2,359,296  1,700,352     72.07%\nfeatures.30                    Conv2d          2,359,296  1,714,176     72.66%\nfeatures.34                    Conv2d          2,359,296  1,612,800     68.36%\nfeatures.37                    Conv2d          2,359,296  1,529,856     64.84%\nfeatures.40                    Conv2d          2,359,296  1,663,488     70.51%\n--------------------------------------------------------------------------------\nOverall                        all             14,710,464 8,976,384     61.02%\n\n\nOur network now has 50% of its filters composed entirely of zeroes, without even losing accuracy. Obviously, choosing a higher sparsity makes it more difficult for the network to keep a similar accuracy. Other parameters can also widely change the behaviour of our sparsification process. For example choosing a more fine-grained sparsity usually leads to better results but is then more difficult to take advantage of in terms of speed.\n\nLetâ€™s now see how much we gained in terms of speed. Because we removed 50% of convolution filters, we should expect crazy speed-up right ?\n\nprint(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 83.03ms\n\n\nWell actually, no. We didnâ€™t remove any parameters, we just replaced some by zeroes, remember? The amount of parameters is still the same:\n\nnum_parameters = get_num_parameters(student.model)\ndisk_size = get_model_size(student.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 537.30 MB (disk), 134309962 parameters\n\n\nWhich leads us to the next section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#pruning",
    "href": "tutorials/walkthrough.html#pruning",
    "title": "Walkthrough",
    "section": "Pruning",
    "text": "Pruning\nWhy donâ€™t we see any acceleration even though we removed half of the parameters? Thatâ€™s because natively, our GPU does not know that our matrices are sparse and thus isnâ€™t able to accelerate the computation. The easiest work around, is to physically remove the parameters we zeroed-out. But this operation requires to change the architecture of the network.\nThis pruning only works if we remove entire filters as it is the only case where we can change the architecture accordingly. Hopefully, sparse computations will soon be available on common deep learning librairies so this section will become useless in the future.\n\nHere is what it looks like with fasterai: \n\n\n\nPruneCallback(learn, sparsity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ncontext: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e linear, cosine, â€¦ and gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\nSo in the case of our example, it gives:\n\nfrom fasterai.prune.all import *\n\nLetâ€™s now see what our model is capable of now:\n\npr_cb = PruneCallback(pruning_ratio=50, context='global', criteria=large_final, schedule=one_cycle)\nstudent.fit(5, 1e-5, cbs=pr_cb)\n\nIgnoring output layer: classifier.6\nTotal ignored layers: 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.600658\n0.603180\n0.802293\n01:04\n\n\n1\n0.606881\n0.634970\n0.804076\n01:22\n\n\n2\n0.827685\n0.815733\n0.803057\n01:00\n\n\n3\n0.873774\n0.878398\n0.803567\n00:57\n\n\n4\n0.889905\n0.876898\n0.803312\n00:56\n\n\n\n\n\nSparsity at the end of epoch 0: 1.94%\nSparsity at the end of epoch 1: 19.96%\nSparsity at the end of epoch 2: 45.82%\nSparsity at the end of epoch 3: 49.74%\nSparsity at the end of epoch 4: 50.00%\n\n\n\nnum_parameters = get_num_parameters(student.model)\ndisk_size = get_model_size(student.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 148.86 MB (disk), 37200724 parameters\n\n\nAnd in terms of speed:\n\nprint(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 53.93ms\n\n\nYay ! Now we can talk ! Letâ€™s just double check that our accuracy is unchanged and that we didnâ€™t mess up somewhere:\n\nAnd there is actually more that we can do ! Letâ€™s keep going !",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#batch-normalization-folding",
    "href": "tutorials/walkthrough.html#batch-normalization-folding",
    "title": "Walkthrough",
    "section": "Batch Normalization Folding",
    "text": "Batch Normalization Folding\nBatch Normalization Folding is a really easy to implement and straightforward idea. The gist is that batch normalization is nothing more than a normalization of the input data at each layer. Moreover, at inference time, the batch statistics used for this normalization are fixed. We can thus incorporate the normalization process directly in the convolution by changing its weights and completely remove the batch normalization layers, which is a gain both in terms of parameters and in terms of computations. For a more in-depth explaination, see this blog post.\nThis is how to use it with FasterAI:\n\nbn_folder = BN_Folder()\nbn_folder.fold(learn.model))\n\n Again, you only need to pass your model and FasterAI takes care of the rest. For models built using the nn.Sequential, you donâ€™t need to change anything. For others, if you want to see speedup and compression, you actually need to subclass your model to remove the batch norm from the parameters and from the forward method of your network. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis operation should also be lossless as it redefines the convolution to take batch norm into account and is thus equivalent.\n\n\n\n\nfrom fasterai.misc.bn_folding import *\n\nLetâ€™s do this with our model !\n\nbn_f = BN_Folder()\nfolded_model = bn_f.fold(student.model)\n\nThe parameters drop is generally not that significant, especially in a network such as VGG where almost all parameters are contained in the FC layers but, hey, any gain is good to take.\n\nnum_parameters = get_num_parameters(folded_model)\ndisk_size = get_model_size(folded_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 148.79 MB (disk), 37194430 parameters\n\n\nNow that we removed the batch normalization layers, we should again see a speedup.\n\nprint(f'Inference Speed: {evaluate_cpu_speed(folded_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 54.40ms\n\n\nAgain, letâ€™s double check that we didnâ€™t mess up somewhere:\n\nfolded_learner = Learner(dls, folded_model, metrics=[accuracy])\nfolded_learner.validate()\n\n\n\n\n\n\n\n\n[0.8769694566726685, 0.8033121228218079]\n\n\nAnd weâ€™re still not done yet ! As we know for VGG16, most of the parameters are comprised in the fully-connected layers so there should be something that we can do about it, right ?",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#fc-layers-factorization",
    "href": "tutorials/walkthrough.html#fc-layers-factorization",
    "title": "Walkthrough",
    "section": "FC Layers Factorization",
    "text": "FC Layers Factorization\nWe can indeed, factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\). With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\nIn FasterAI, to decompose the fully-connected layers of your model, here is what you need to do: \n\nFCD = FCDecomposer()\ndecomposed_model = FCD.decompose(model, percent_removed)\n\n The percent_removed corresponds to the percentage of singular values removed (k value above). \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis time, the decomposition is not exact, so we expect a drop in performance afterwards and further retraining will be needed.\n\n\nWhich gives with our example, if we only want to keep half of them:\n\nfrom fasterai.misc.fc_decomposer import *\n\n\nfc_decomposer = FC_Decomposer()\ndecomposed_model = fc_decomposer.decompose(folded_learner.model, percent_removed=0.5)\n\nHow many parameters do we have now ?\n\nnum_parameters = get_num_parameters(decomposed_model)\ndisk_size = get_model_size(decomposed_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")\n\nModel Size: 98.82 MB (disk), 22936504 parameters\n\n\nAnd how much time did we gain ?\n\nprint(f'Inference Speed: {evaluate_cpu_speed(decomposed_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 51.96ms\n\n\nWe actually get a network that is a little bit slower, but at the expense of reducing the by 10M the number of parameter. This is thus a matter of compromise between network weight and speed.\n\nHowever, this technique is an approximation so it is not lossless, so we should retrain our network a bit to recover its performance.\n\nfinal_learner = Learner(dls, decomposed_model, metrics=[accuracy])\nfinal_learner.fit_one_cycle(5, 1e-5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.912265\n0.920241\n0.714140\n00:06\n\n\n1\n0.747366\n0.758958\n0.774267\n00:06\n\n\n2\n0.686810\n0.702474\n0.789809\n00:07\n\n\n3\n0.653998\n0.683656\n0.800000\n00:06\n\n\n4\n0.611083\n0.677875\n0.800510\n00:06\n\n\n\n\n\nThis operation is usually less useful for more recent architectures as they usually do not have that many parameters in their fully-connected layers.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#quantization",
    "href": "tutorials/walkthrough.html#quantization",
    "title": "Walkthrough",
    "section": "Quantization",
    "text": "Quantization\n\nfrom fasterai.quantize.all import *\n\nNow that we have removed every superfluous parameter that we could, we can still continue to compress our model. A common way to do so is now to reduce the precision of each parameter in the network, making it considerably smaller. Such an approach is called Quantization and wonâ€™t affect the total number of parameter but will make each one of them smaller to store, on top of making computations faster.\nIn FasterAI, quantization can be done in a static way, i.e.Â apply quantization to the model, also called Post-Training Quantization. It also can be applied dynamically during the training, also called Quantization-Aware Training.\n\nfinal_learner.fit_one_cycle(5, 1e-5, cbs=QuantizeCallback())\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.673032\n0.713057\n0.784968\n00:09\n\n\n1\n0.659847\n0.752605\n0.785733\n00:09\n\n\n2\n0.635560\n0.689459\n0.795159\n00:09\n\n\n3\n0.613627\n0.643876\n0.812739\n00:09\n\n\n4\n0.573815\n0.643116\n0.809172\n00:09\n\n\n\n\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_learner.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 29.35ms\n\n\n\nnum_parameters = count_parameters_quantized(final_learner.model)\ndisk_size = get_model_size(final_learner.model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")\n\nModel Size: 23.11 MB (disk), 22,936,504 parameters\n\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_learner.model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 29.57ms",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/walkthrough.html#extra-acceleration",
    "href": "tutorials/walkthrough.html#extra-acceleration",
    "title": "Walkthrough",
    "section": "Extra Acceleration",
    "text": "Extra Acceleration\n\nfrom fasterai.misc.cpu_optimizer import accelerate_model_for_cpu\n\n\nfinal_model = accelerate_model_for_cpu(final_learner.model, x[0][None])\n\n\nprint(f'Inference Speed: {evaluate_cpu_speed(final_model, x[0][None])[0]:.2f}ms')\n\nInference Speed: 28.94ms\n\n\n\nnum_parameters = get_num_parameters(final_model)\ndisk_size = get_model_size(final_model)\nprint(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")\n\nModel Size: 0.00 MB (disk), 0 parameters\n\n\n\nSo to recap, we saw in this article how to use fasterai to:  1. Make a student model learn from a teacher model (Knowledge Distillation)  2. Make our network sparse (Sparsifying)  3. Optionnaly physically remove the zero-filters (Pruning)  4. Remove the batch norm layers (Batch Normalization Folding)  5. Approximate our big fully-connected layers by smaller ones (Fully-Connected Layers Factorization)  6. Quantize the model to reduce the precision of the weights (Quantization)  7. Extra acceleration techniques to further optimize the speed of our network\n\nAnd we saw that by applying those, we could reduce our VGG16 model from 537 MB of parameters down to 26 MB (20x compression), and also speed-up the inference from 30.3ms to 7.9ms (3.8x speed-up) without any drop in accuracy compared to the baseline.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease keep in mind that the techniques presented above are not magic ðŸ§™â€â™‚ï¸, so do not expect to see a 200% speedup and compression everytime. What you can achieve highly depend on the architecture that you are using (some are already speed/parameter efficient by design) or the task it is doing (some datasets are so easy that you can remove almost all your network without seeing a drop in performance)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Walkthrough"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html",
    "href": "tutorials/quantize/quantize_callback.html",
    "title": "Quantize Callback",
    "section": "",
    "text": "Quantization-Aware Training (QAT) simulates low-precision inference during training, allowing the model to adapt to quantization effects. This produces more accurate quantized models than post-training quantization alone.\n\n\n\n\n\n\n\n\n\n\n\nApproach\nAccuracy\nSpeed\nWhen to Use\n\n\n\n\nPost-Training Quantization\nLower\nFast (no training)\nQuick deployment, accuracy tolerant\n\n\nQuantization-Aware Training\nHigher\nSlower (requires training)\nProduction models, accuracy critical\n\n\n\n\n\n\n\n4x smaller models - FP32 â†’ INT8 reduces model size by ~75%\nFaster inference - Integer operations are faster on most hardware\nMaintained accuracy - QAT minimizes accuracy degradation\nHardware compatibility - INT8 is widely supported (CPU, mobile, edge)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html#overview",
    "href": "tutorials/quantize/quantize_callback.html#overview",
    "title": "Quantize Callback",
    "section": "",
    "text": "Quantization-Aware Training (QAT) simulates low-precision inference during training, allowing the model to adapt to quantization effects. This produces more accurate quantized models than post-training quantization alone.\n\n\n\n\n\n\n\n\n\n\n\nApproach\nAccuracy\nSpeed\nWhen to Use\n\n\n\n\nPost-Training Quantization\nLower\nFast (no training)\nQuick deployment, accuracy tolerant\n\n\nQuantization-Aware Training\nHigher\nSlower (requires training)\nProduction models, accuracy critical\n\n\n\n\n\n\n\n4x smaller models - FP32 â†’ INT8 reduces model size by ~75%\nFaster inference - Integer operations are faster on most hardware\nMaintained accuracy - QAT minimizes accuracy degradation\nHardware compatibility - INT8 is widely supported (CPU, mobile, edge)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html#setup-and-data",
    "href": "tutorials/quantize/quantize_callback.html#setup-and-data",
    "title": "Quantize Callback",
    "section": "1. Setup and Data",
    "text": "1. Setup and Data\nFirst, letâ€™s load a dataset and create a model. Weâ€™ll use a pretrained ResNet-34 from timm.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html#training-with-quantizecallback",
    "href": "tutorials/quantize/quantize_callback.html#training-with-quantizecallback",
    "title": "Quantize Callback",
    "section": "2. Training with QuantizeCallback",
    "text": "2. Training with QuantizeCallback\nThe QuantizeCallback automatically: 1. Prepares the model for QAT by inserting fake quantization modules 2. Trains with simulated INT8 precision 3. Converts the model to actual INT8 after training\nSimply add the callback to your training loop:\n\npretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\nlearn = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn.model.fc = nn.Linear(512, 2)\nlearn.fit_one_cycle(3, 1e-3, cbs=QuantizeCallback())\n\n/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.497828\n0.383409\n0.817321\n00:05\n\n\n1\n0.291996\n0.257519\n0.891746\n00:05\n\n\n2\n0.180348\n0.229320\n0.905277\n00:04",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html#evaluating-the-quantized-model",
    "href": "tutorials/quantize/quantize_callback.html#evaluating-the-quantized-model",
    "title": "Quantize Callback",
    "section": "3. Evaluating the Quantized Model",
    "text": "3. Evaluating the Quantized Model\nLetâ€™s compare the original and quantized models in terms of size and accuracy.\n\nfrom tqdm import tqdm\n\ndef get_model_size(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    size = os.path.getsize(\"temp.p\") / 1e6  # Size in MB\n    os.remove(\"temp.p\")\n    return size\n    \ndef compute_validation_accuracy(model, valid_dataloader, device=None):\n    # Set the model to evaluation mode\n    model.eval()\n    \n    # Use the model's device if no device is specified\n    \n    device = torch.device('cpu')\n    \n    # Move model to the specified device\n    model = model.to(device)\n    \n    # Tracking correct predictions and total samples\n    total_correct = 0\n    total_samples = 0\n    \n    # Disable gradient computation for efficiency\n    with torch.no_grad():\n        for batch in tqdm(valid_dataloader):\n            # Assuming batch is a tuple of (inputs, labels)\n            # Adjust this if your dataloader returns a different format\n            inputs, labels = batch\n            \n            # Move inputs and labels to the same device as the model\n            inputs = torch.Tensor(inputs).to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Get predictions (for classification tasks)\n            # Use argmax along the class dimension\n            _, predicted = torch.max(outputs, 1)\n            \n            # Update counters\n            total_samples += labels.size(0)\n            total_correct += (predicted == labels).sum().item()\n    \n    # Compute accuracy as a percentage\n    accuracy = (total_correct / total_samples) * 100\n    \n    return accuracy\n\n\npretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\nlearn_original = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn_original.model.fc = nn.Linear(512, 2)\n\n\nSize Comparison\nCreate an original (non-quantized) model for comparison:\n\nprint(f'Size of the original model: {get_model_size(learn_original.model):.2f} MB')\nprint(f'Size of the quantized model: {get_model_size(learn.model):.2f} MB')\n\nSize of the original model: 85.27 MB\nSize of the quantized model: 21.51 MB\n\n\n\n\nAccuracy Verification\nDespite the 4x size reduction, the quantized model maintains good accuracy:\n\ncompute_validation_accuracy(learn.model, dls.valid)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:01&lt;00:00, 15.47it/s]\n\n\n90.73071718538566",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html#parameter-guide",
    "href": "tutorials/quantize/quantize_callback.html#parameter-guide",
    "title": "Quantize Callback",
    "section": "4. Parameter Guide",
    "text": "4. Parameter Guide\n\nQuantizeCallback Parameters\n\n\n\n\n\n\n\n\nParameter\nDefault\nDescription\n\n\n\n\nbackend\n'x86'\nQuantization backend: 'x86' (Intel/AMD), 'qnnpack' (ARM/mobile), 'fbgemm' (server)\n\n\nqconfig\nNone\nCustom quantization config. If None, uses backend default\n\n\n\n\n\nBackend Selection Guide\n\n\n\nBackend\nBest For\nHardware\n\n\n\n\n'x86'\nDesktop/server CPUs\nIntel, AMD processors\n\n\n'qnnpack'\nMobile deployment\nARM processors, Android, iOS\n\n\n'fbgemm'\nServer inference\nFacebookâ€™s optimized backend\n\n\n\n\n\nTips for Best Results\n\nTrain longer - QAT benefits from more epochs to adapt to quantization noise\nLower learning rate - Use 1/10th the normal LR for fine-tuning\nCalibrate batch norm - Run a few batches through the model before final conversion\nTest on target hardware - Quantization benefits vary by platform",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html#summary",
    "href": "tutorials/quantize/quantize_callback.html#summary",
    "title": "Quantize Callback",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nQuantization-Aware Training\nTraining with simulated low-precision to prepare model for INT8 inference\n\n\nQuantizeCallback\nfastai callback that handles QAT preparation, training, and conversion\n\n\nSize Reduction\n~4x smaller model (FP32 â†’ INT8)\n\n\nBackend\nTarget hardware platform ('x86', 'qnnpack', 'fbgemm')\n\n\nTypical Use\nProduction deployment where model size and inference speed matter",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantize_callback.html#see-also",
    "href": "tutorials/quantize/quantize_callback.html#see-also",
    "title": "Quantize Callback",
    "section": "See Also",
    "text": "See Also\n\nQuantizer - Lower-level quantization API with more control\nSparsifier - Combine with sparsification for even smaller models\nBN Folding - Fold batch norm layers before quantization for best results\nPyTorch Quantization Docs - Official PyTorch quantization documentation",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html",
    "href": "tutorials/regularize/regularize_callback.html",
    "title": "Regularize Callback",
    "section": "",
    "text": "from fasterai.core.criteria import *\nfrom fasterai.core.schedule import *\nfrom fasterai.regularize.all import *\nfrom fastai.vision.all import *",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#overview",
    "href": "tutorials/regularize/regularize_callback.html#overview",
    "title": "Regularize Callback",
    "section": "Overview",
    "text": "Overview\nGroup Regularization is a technique that encourages structured sparsity in neural networks during training. Unlike standard L2 regularization (weight decay) which penalizes individual weights, group regularization penalizes groups of weights togetherâ€”such as entire filters, kernels, or channels.\n\nWhy Use Group Regularization?\nWhen preparing a model for structured pruning, you want entire structures (filters, channels) to become unimportant, not just individual weights. Group regularization pushes these structures toward zero during training, making subsequent pruning:\n\nMore effective - Pruned structures are already near-zero, minimizing accuracy loss\nCleaner - Clear separation between important and unimportant structures\nHardware-friendly - Structured sparsity maps well to GPU/CPU acceleration\n\n\n\nThe RegularizeCallback\nThe RegularizeCallback adds a regularization term to the loss function:\n\\[\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\lambda \\sum_{g \\in \\text{groups}} \\|W_g\\|_p\\]\nWhere \\(\\lambda\\) is the regularization weight and \\(W_g\\) are weight groups at your chosen granularity.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#setup-and-data",
    "href": "tutorials/regularize/regularize_callback.html#setup-and-data",
    "title": "Regularize Callback",
    "section": "1. Setup and Data",
    "text": "1. Setup and Data\nLetâ€™s start by loading a dataset and establishing a baseline model without regularization.\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#baseline-training-no-regularization",
    "href": "tutorials/regularize/regularize_callback.html#baseline-training-no-regularization",
    "title": "Regularize Callback",
    "section": "2. Baseline Training (No Regularization)",
    "text": "2. Baseline Training (No Regularization)\nFirst, we train a model without any regularization to establish a baseline accuracy.\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.695834\n0.946244\n0.740866\n00:03\n\n\n1\n0.383204\n0.314028\n0.847091\n00:03\n\n\n2\n0.222566\n0.240774\n0.899865\n00:04\n\n\n3\n0.123860\n0.220786\n0.914073\n00:03\n\n\n4\n0.070046\n0.203204\n0.920162\n00:03",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#training-with-group-regularization",
    "href": "tutorials/regularize/regularize_callback.html#training-with-group-regularization",
    "title": "Regularize Callback",
    "section": "3. Training with Group Regularization",
    "text": "3. Training with Group Regularization\nNow letâ€™s train with RegularizeCallback. Weâ€™ll configure it with:\n\ncriteria=squared_final: Uses squared weight magnitudes for regularization\ngranularity='weight': Regularizes at individual weight level (try 'filter' for structured pruning prep)\nweight=3e-5: Regularization strength (higher = more aggressive)\nschedule=one_cycle: Varies regularization strength during training\n\n\nreg_cb = RegularizeCallback(squared_final, 'weight', 1e-3, schedule=one_cycle)\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nlearn.fit_one_cycle(5, cbs=reg_cb)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.776885\n1.002960\n0.771989\n00:03\n\n\n1\n1.337281\n2.333170\n0.866712\n00:03\n\n\n2\n3.507138\n4.474680\n0.876861\n00:03\n\n\n3\n4.233951\n4.420142\n0.905277\n00:03\n\n\n4\n4.203538\n4.318278\n0.926928\n00:03",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#comparing-results",
    "href": "tutorials/regularize/regularize_callback.html#comparing-results",
    "title": "Regularize Callback",
    "section": "4. Comparing Results",
    "text": "4. Comparing Results\nAfter training, you should observe: - Similar or slightly lower accuracy (regularization adds a constraint) - Weights that are more concentrated around zero - Cleaner weight distribution for subsequent pruning\nTo visualize the effect, you can plot weight histograms:\nimport matplotlib.pyplot as plt\n\n# Get all conv weights\nweights = torch.cat([m.weight.data.flatten() for m in learn.model.modules() \n                     if isinstance(m, nn.Conv2d)])\n\nplt.hist(weights.cpu().numpy(), bins=100, alpha=0.7)\nplt.xlabel('Weight Value')\nplt.ylabel('Count')\nplt.title('Weight Distribution After Group Regularization')\nplt.show()",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#parameter-guide",
    "href": "tutorials/regularize/regularize_callback.html#parameter-guide",
    "title": "Regularize Callback",
    "section": "5. Parameter Guide",
    "text": "5. Parameter Guide\n\nChoosing Granularity\n\n\n\n\n\n\n\n\nGranularity\nEffect\nBest For\n\n\n\n\n'weight'\nRegularizes individual weights\nUnstructured pruning, general sparsity\n\n\n'filter'\nRegularizes entire Conv2d filters\nStructured pruning (recommended)\n\n\n'kernel'\nRegularizes 2D kernels within filters\nModerate structure\n\n\n'channel'\nRegularizes input channels\nChannel pruning\n\n\n\n\n\nChoosing Regularization Weight\n\n\n\nWeight Range\nEffect\n\n\n\n\n1e-6 - 1e-5\nVery light regularization, minimal accuracy impact\n\n\n1e-5 - 1e-4\nModerate regularization, good balance\n\n\n1e-4 - 1e-3\nStrong regularization, may reduce accuracy\n\n\n&gt; 1e-3\nVery aggressive, use with caution\n\n\n\nTip: Start with 1e-5 and increase if weights donâ€™t concentrate toward zero.\n\n\nRecommended Workflow\n# 1. Train with filter-level regularization\nreg_cb = RegularizeCallback(\n    criteria=large_final,      # or squared_final\n    granularity='filter',      # for structured pruning\n    weight=1e-4,\n    schedule=one_cycle,\n    verbose=True\n)\nlearn.fit(epochs, cbs=[reg_cb])\n\n# 2. Prune the regularized model\nfrom fasterai.prune.all import *\npruner = Pruner(learn.model, sparsity=0.3, context='local', criteria=large_final)\npruner.prune_model()\n\n# 3. Fine-tune\nlearn.fit(fine_tune_epochs)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#summary",
    "href": "tutorials/regularize/regularize_callback.html#summary",
    "title": "Regularize Callback",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nGroup Regularization\nPenalizes groups of weights to encourage structured sparsity\n\n\nRegularizeCallback\nfastai callback that adds regularization term to loss\n\n\nGranularity\nLevel at which to group weights ('weight', 'filter', 'kernel')\n\n\nSchedule\nVaries regularization strength during training\n\n\nTypical Use\nPre-pruning preparation to make structured pruning more effective",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "tutorials/regularize/regularize_callback.html#see-also",
    "href": "tutorials/regularize/regularize_callback.html#see-also",
    "title": "Regularize Callback",
    "section": "See Also",
    "text": "See Also\n\nCriteria - Importance measures used for regularization\nSchedules - Control regularization strength over training\nPruner - Apply structured pruning after regularization\nSparsifier - Apply unstructured sparsification",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "core/criteria.html",
    "href": "core/criteria.html",
    "title": "Criteria",
    "section": "",
    "text": "The criteria implemented come from this paper.\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core/criteria.html#magnitude-based-criteria",
    "href": "core/criteria.html#magnitude-based-criteria",
    "title": "Criteria",
    "section": "Magnitude Based Criteria",
    "text": "Magnitude Based Criteria\n\nRandom\n\ndemo_model(random)\n\n\n\n\n\n\n\n\n\n\nLarge Final Value\n\ndemo_model(large_final)\n\n\n\n\n\n\n\n\n\n\nSquared Final Value\n\ndemo_model(squared_final)\n\n\n\n\n\n\n\n\n\n\nSmall Final Value\n\ndemo_model(small_final)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core/criteria.html#init-based-criteria",
    "href": "core/criteria.html#init-based-criteria",
    "title": "Criteria",
    "section": "Init based criteria",
    "text": "Init based criteria\n\nLarge Init Value\n\ndemo_model(large_init)\n\n\n\n\n\n\n\n\n\n\nSmall Init Value\n\ndemo_model(small_init)\n\n\n\n\n\n\n\n\n\n\nLarge Init Large Final Value\n\ndemo_model(large_init_large_final, 80)\n\n\n\n\n\n\n\n\n\n\nSmall Init Small Final Value\n\ndemo_model(small_init_small_final)\n\n\n\n\n\n\n\n\n\n\nIncreasing Magnitude\n\ndemo_model(magnitude_increase, 60)\n\n\n\n\n\n\n\n\n\n\nMovement Pruning\n\ndemo_model(movement)\n\n\n\n\n\n\n\n\n\nmovmag = init_based_criteria(noop, output_fn=lambda x,y: torch.abs(torch.mul(x, torch.sub(x,y))))\n\n\ndemo_model(movmag)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core/criteria.html#update-based-criteria",
    "href": "core/criteria.html#update-based-criteria",
    "title": "Criteria",
    "section": "Update based criteria",
    "text": "Update based criteria\nThe following criteria use an updating value of the weights, i.e.Â the value from the previous iteration of training, instead of the initialization value to better capture the training dynamics.\n\nUpdating Magnitude Increase\n\ndemo_model(updating_magnitude_increase)\n\n\n\n\n\n\n\n\n\n\nUpdating Movement\n\ndemo_model(updating_movement, 50)\n\n\n\n\n\n\n\n\n\n\nUpdating mov-magnitude\n\ndemo_model(updating_movmag)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "core/criteria.html#see-also",
    "href": "core/criteria.html#see-also",
    "title": "Criteria",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Apply sparsification using these criteria\nPruner - Structured pruning with importance scoring\nGranularity - Control what gets pruned (weights, filters, etc.)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Criteria"
    ]
  },
  {
    "objectID": "distill/losses.html",
    "href": "distill/losses.html",
    "title": "Distillation Losses",
    "section": "",
    "text": "This module provides loss functions for knowledge distillation. These losses enable training a smaller â€œstudentâ€ network to mimic a larger â€œteacherâ€ network.\nLoss Categories: - Output-based: SoftTarget, Logits, Mutual - compare final predictions - Feature-based: Attention, FitNet, Similarity, ActivationBoundaries - compare intermediate representations",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Distillation Losses"
    ]
  },
  {
    "objectID": "distill/losses.html#overview",
    "href": "distill/losses.html#overview",
    "title": "Distillation Losses",
    "section": "",
    "text": "This module provides loss functions for knowledge distillation. These losses enable training a smaller â€œstudentâ€ network to mimic a larger â€œteacherâ€ network.\nLoss Categories: - Output-based: SoftTarget, Logits, Mutual - compare final predictions - Feature-based: Attention, FitNet, Similarity, ActivationBoundaries - compare intermediate representations",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Distillation Losses"
    ]
  },
  {
    "objectID": "distill/losses.html#output-based-losses",
    "href": "distill/losses.html#output-based-losses",
    "title": "Distillation Losses",
    "section": "Output-Based Losses",
    "text": "Output-Based Losses\nThese losses compare the final output predictions between student and teacher networks.\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\nSoftTarget\n\ndef SoftTarget(\n    pred:torch.Tensor, # Student predictions\n    teacher_pred:torch.Tensor, # Teacher predictions\n    T:float=5, # Temperature for softening\n    kwargs:VAR_KEYWORD\n)-&gt;torch.Tensor:\n\nKnowledge distillation with softened distributions (Hinton et al.)\n\nsource\n\n\nLogits\n\ndef Logits(\n    pred:torch.Tensor, # Student predictions\n    teacher_pred:torch.Tensor, # Teacher predictions\n    kwargs:VAR_KEYWORD\n)-&gt;torch.Tensor:\n\nDirect logit matching between student and teacher\n\nsource\n\n\nMutual\n\ndef Mutual(\n    pred:torch.Tensor, # Student predictions\n    teacher_pred:torch.Tensor, # Teacher predictions\n    kwargs:VAR_KEYWORD\n)-&gt;torch.Tensor:\n\nKL divergence between student and teacher",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Distillation Losses"
    ]
  },
  {
    "objectID": "distill/losses.html#feature-based-losses",
    "href": "distill/losses.html#feature-based-losses",
    "title": "Distillation Losses",
    "section": "Feature-Based Losses",
    "text": "Feature-Based Losses\nThese losses compare intermediate feature representations, enabling the student to learn internal representations similar to the teacher.\n\nsource\n\nAttention\n\ndef Attention(\n    fm_s:dict[str, torch.Tensor], # Student feature maps {name: tensor}\n    fm_t:dict[str, torch.Tensor], # Teacher feature maps {name: tensor}\n    p:int=2, # Power for attention computation\n    kwargs:VAR_KEYWORD\n)-&gt;torch.Tensor:\n\nAttention transfer loss (Zagoruyko & Komodakis)\n\nsource\n\n\nActivationBoundaries\n\ndef ActivationBoundaries(\n    fm_s:dict[str, torch.Tensor], # Student feature maps\n    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n    m:float=2, # Boundary margin\n    kwargs:VAR_KEYWORD\n)-&gt;torch.Tensor:\n\nBoundary-based knowledge distillation (Heo et al.)\n\nsource\n\n\nFitNet\n\ndef FitNet(\n    fm_s:dict[str, torch.Tensor], # Student feature maps\n    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n    kwargs:VAR_KEYWORD\n)-&gt;torch.Tensor:\n\nFitNets: direct feature map matching (Romero et al.)\n\nsource\n\n\nSimilarity\n\ndef Similarity(\n    fm_s:dict[str, torch.Tensor], # Student feature maps\n    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n    pred:torch.Tensor, # Student predictions (unused, for API consistency)\n    p:int=2, # Normalization power\n    kwargs:VAR_KEYWORD\n)-&gt;torch.Tensor:\n\nSimilarity-preserving knowledge distillation (Tung & Mori)",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Distillation Losses"
    ]
  },
  {
    "objectID": "distill/losses.html#see-also",
    "href": "distill/losses.html#see-also",
    "title": "Distillation Losses",
    "section": "See Also",
    "text": "See Also\n\nKnowledgeDistillationCallback - Apply these losses during training\nDistillation Tutorial - Practical examples with different losses\n\n\nLoss Selection Guide\n\n\n\nLoss\nBest For\nComplexity\n\n\n\n\nSoftTarget\nGeneral distillation, logit matching\nLow\n\n\nAttention\nWhen attention patterns matter\nLow\n\n\nFitNet\nIntermediate feature matching\nMedium\n\n\nPKT\nProbability distribution matching\nMedium\n\n\nRKD\nRelational knowledge transfer\nHigh",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Distillation Losses"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quick Start",
    "section": "",
    "text": "Embark on a journey to supercharge your neural network models with FasterAI, a PyTorch-based library dedicated exclusively to advanced compression techniques. In todayâ€™s fast-paced world, where efficiency and performance are paramount, FasterAI stands out by providing cutting-edge solutions designed to make your neural networks not just lighter, but significantly faster.",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#why-choose-fasterai",
    "href": "quickstart.html#why-choose-fasterai",
    "title": "Quick Start",
    "section": "Why Choose FasterAI?",
    "text": "Why Choose FasterAI?\n\nStreamlined Efficiency: Dive into a suite of compression methodologies, including sparsification, pruning, quantization, and knowledge distillation, each tailored to enhance model efficiency without compromising on accuracy.\nEdge-Ready Models: With FasterAI, prepare your models for the edge, ensuring they run smoothly on devices with limited computational resources, from smartphones to IoT devices.\nCutting-edge Technology: Built on the latest research in data and model compression, FasterAI offers tools that are not just powerful but also easy to integrate into your existing workflows.\nVersatility: From image and video compression to deep learning model optimization, FasterAI is versatile enough to handle a wide range of compression needs, making it suitable for various industries and applications.\nOpen and Accessible: As a community-driven project, FasterAI encourages contributions and feedback, ensuring that the library continues to evolve to meet the needs of its users.",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#getting-started-with-fasterai",
    "href": "quickstart.html#getting-started-with-fasterai",
    "title": "Quick Start",
    "section": "Getting Started with FasterAI",
    "text": "Getting Started with FasterAI\nWhether youâ€™re looking to optimize models for production, research, or hobby projects, FasterAI provides the tools and guidance to achieve your goals. Letâ€™s make your neural networks faster and lighter, together.\n\nHow to use fasterai ?\nFasterAIâ€™s integration with the callback system of fastai represents a significant advancement in how compression techniques can be applied to neural networks, particularly during the training phase. This approach allows for a more seamless and flexible implementation of compression strategies, making it possible to optimize models on-the-fly and potentially achieve better efficiency and performance.\n\n\nUnderstanding Callbacks in fastai\nBefore diving into how FasterAI leverages callbacks, itâ€™s important to understand what callbacks are in the context of the fastai library. Callbacks are a programming pattern that allows users to inject custom behavior into certain stages of the training loop or model lifecycle without altering the core logic of the training process. They can be used for a variety of purposes, such as logging metrics, modifying learning rates, or implementing early stopping.\n\n\nFasterAIâ€™s Use of Callbacks\nFasterAI takes advantage of the callback system in fastai to integrate neural network compression techniques directly into the training process. This integration means that instead of applying compression post-training as a separate step, FasterAI allows for compression techniques like pruning, quantization, and knowledge distillation to be applied dynamically as the model trains. Hereâ€™s how it enhances the training process:\n\nDynamic Compression: By using callbacks, FasterAI can dynamically adjust the compression parameters based on the modelâ€™s performance during training. For example, it can gradually increase the amount of pruning as the model becomes more stable, leading to a more efficient compression process that minimally impacts performance.\nReal-time Optimization: This approach enables real-time optimization of the model. As the model learns and adapts to the data, FasterAI can apply compression techniques in a way thatâ€™s informed by the modelâ€™s current state, potentially leading to more effective and efficient compression.\nSeamless Integration: Leveraging fastaiâ€™s callback system means that users of FasterAI can integrate compression into their training pipelines with minimal code changes. This seamless integration simplifies the process of applying advanced compression techniques, making it accessible even to those with limited experience in model optimization.\n\n\n\nPractical Implications\nFor practitioners, this means they can train models that are not only high-performing but also optimized for size and speed from the outset. It also opens up new possibilities for experimenting with compression techniques during training, which could lead to novel optimization strategies and more efficient models.\nIn essence, FasterAIâ€™s use of the callback system in fastai democratizes the application of sophisticated neural network compression techniques, making them an integral part of the model development lifecycle rather than an afterthought. This approach aligns with the broader goal of developing AI models that are not just powerful but also efficient and adaptable to various deployment environments.\nTo illustrate the practical application of FasterAIâ€™s integration with the fastai callback system for on-the-fly compression during the training phase, letâ€™s walk through an example. This example will demonstrate how to apply dynamic pruning to a neural network model while itâ€™s being trained, leveraging the callback system for seamless integration.\n\n\nSetting Up Your Environment\nFirst, ensure you have both fastai and FasterAI installed in your Python environment. If you havenâ€™t installed these libraries yet, you can do so using pip:\npip install fastai fasterai\n\n\nImporting Necessary Libraries\nBegin by importing the required libraries from fasterai:\nfrom fasterai.sparse.all import *\n\n\nDefining the Dataloader and Learner\nJust get your favorite Dataloader and Learner as usual with fastai:\ndls = get_dls()\nlearner = get_learner()\n\n\nApplying Dynamic Sparsification with a Callback\nNow, integrate FasterAIâ€™s dynamic sparsification into the training process by adding the SparsifyCallback to your model. This callback will apply sparsify dynamically based on the defined parameters:\nsparsify_callback = SparsifyCallback(sparsity, granularity, context, criteria, schedule)\nlearner.fit(n_epoch, max_lr, cbs=[sparsify_callback])\nIn this example, SparsifyCallback is initialized with different parameters (see the tutorial to better understand those parameters). The fit method trains the model for n_epochs, and the SparsifyCallback is passed through the cbs (callbacks) parameter, enabling dynamic sparsification during the training process.\n\n\nObserving the Effects\nAfter training, you can evaluate the modelâ€™s performance and size to observe the effects of dynamic sparsification. You should notice a reduction in the model size with minimal impact on accuracy, showcasing the efficiency of integrating compression techniques during training.\n\n\nConclusion\nThis example demonstrates how FasterAIâ€™s integration with the fastai callback system allows for the application of compression techniques like sparsification directly within the training loop. By leveraging callbacks, you can dynamically optimize your neural network models, making them lighter and faster without a significant compromise on performance. This approach not only simplifies the compression process but also opens up new avenues for creating efficient AI models optimized for various deployment scenarios.",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#summary",
    "href": "quickstart.html#summary",
    "title": "Quick Start",
    "section": "Summary",
    "text": "Summary\n\n\n\nTool\nPurpose\n\n\n\n\nSparsifier\nZero out weights (unstructured compression)\n\n\nSparsifyCallback\nSparsification during fastai training\n\n\nPruner\nRemove entire filters (structured compression)\n\n\nPruneCallback\nStructured pruning during fastai training\n\n\nQuantizer\nReduce weight precision to INT8\n\n\nKnowledgeDistillationCallback\nTransfer knowledge from teacher to student\n\n\nbenchmark()\nMeasure model size, speed, compute, memory",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "quickstart.html#see-also",
    "href": "quickstart.html#see-also",
    "title": "Quick Start",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Detailed sparsification API\nPruner - Structured pruning API\nQuantizer - Quantization API\nKnowledge Distillation - Teacher-student training\nSchedules - Control compression progression\nSensitivity Analysis - Find optimal per-layer compression",
    "crumbs": [
      "Contact Me",
      "Quick Start"
    ]
  },
  {
    "objectID": "misc/bn_folding.html",
    "href": "misc/bn_folding.html",
    "title": "Batch Norm Folding",
    "section": "",
    "text": "Batch Norm Folding is an inference optimization technique that merges BatchNorm layers into their preceding Conv2d layers. This eliminates the BatchNorm computation entirely without changing the modelâ€™s output.\nBenefits: - Removes BatchNorm layers completely from the computation graph - Reduces inference latency (one less operation per Conv-BN pair) - Decreases model size slightly (removes BN parameters) - Zero accuracy loss (mathematically equivalent transformation)\n\n\n\n\n\nModel\nOriginal Layers\nAfter Folding\nParam Reduction\n\n\n\n\nResNet-18\nConv + BN pairs\nConv only\n~2% fewer params\n\n\nVGG-16 BN\nConv + BN pairs\nConv only\n~1% fewer params\n\n\n\nNote: The model must be in eval() mode for folding to work correctly, as BatchNorm uses running statistics in inference.\nBatch Normalization is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity.\nIt consists of 2 steps:\n\nNormalize the batch by first subtracting its mean \\(\\mu\\), then dividing it by its standard deviation \\(\\sigma\\).\nFurther scale by a factor \\(\\gamma\\) and shift by a factor \\(\\beta\\). Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\\[\n\\begin{aligned}\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\ \\sigma_{\\mathcal{B}}^{2} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\ \\widehat{x}_{i} & \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\ y_{i} & \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) \\end{aligned}\\]\nDue to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?\nOnce the training has ended, each batch normalization layer possesses a specific set of \\(\\gamma\\) and \\(\\beta\\), but also \\(\\mu\\) and \\(\\sigma\\), the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\nAs a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!\nThis would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.\nWith a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.\nAs a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input \\(x\\), as:\n\\[\\begin{aligned} z &=W * x+b \\\\ \\mathrm{out} &=\\gamma \\cdot \\frac{z-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nSo, if we re-arrange the \\(W\\) and \\(b\\) of the convolution to take the parameters of the batch normalization into account, as such:\n\\[\\begin{aligned} w_{\\text {fold }} &=\\gamma \\cdot \\frac{W}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\ b_{\\text {fold }} &=\\gamma \\cdot \\frac{b-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nIn practice, this can be achieved in FasterAI with the BN_folder class\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\n\ndef fold(\n    model:Module, # The model to fold (must be in eval mode)\n)-&gt;Module:\n\nRecursively fold all BatchNorm2d layers into their preceding Conv2d layers",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Batch Norm Folding"
    ]
  },
  {
    "objectID": "misc/bn_folding.html#overview",
    "href": "misc/bn_folding.html#overview",
    "title": "Batch Norm Folding",
    "section": "",
    "text": "Batch Norm Folding is an inference optimization technique that merges BatchNorm layers into their preceding Conv2d layers. This eliminates the BatchNorm computation entirely without changing the modelâ€™s output.\nBenefits: - Removes BatchNorm layers completely from the computation graph - Reduces inference latency (one less operation per Conv-BN pair) - Decreases model size slightly (removes BN parameters) - Zero accuracy loss (mathematically equivalent transformation)\n\n\n\n\n\nModel\nOriginal Layers\nAfter Folding\nParam Reduction\n\n\n\n\nResNet-18\nConv + BN pairs\nConv only\n~2% fewer params\n\n\nVGG-16 BN\nConv + BN pairs\nConv only\n~1% fewer params\n\n\n\nNote: The model must be in eval() mode for folding to work correctly, as BatchNorm uses running statistics in inference.\nBatch Normalization is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity.\nIt consists of 2 steps:\n\nNormalize the batch by first subtracting its mean \\(\\mu\\), then dividing it by its standard deviation \\(\\sigma\\).\nFurther scale by a factor \\(\\gamma\\) and shift by a factor \\(\\beta\\). Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\\[\n\\begin{aligned}\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\ \\sigma_{\\mathcal{B}}^{2} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\ \\widehat{x}_{i} & \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\ y_{i} & \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) \\end{aligned}\\]\nDue to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?\nOnce the training has ended, each batch normalization layer possesses a specific set of \\(\\gamma\\) and \\(\\beta\\), but also \\(\\mu\\) and \\(\\sigma\\), the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\nAs a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!\nThis would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.\nWith a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.\nAs a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input \\(x\\), as:\n\\[\\begin{aligned} z &=W * x+b \\\\ \\mathrm{out} &=\\gamma \\cdot \\frac{z-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nSo, if we re-arrange the \\(W\\) and \\(b\\) of the convolution to take the parameters of the batch normalization into account, as such:\n\\[\\begin{aligned} w_{\\text {fold }} &=\\gamma \\cdot \\frac{W}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\ b_{\\text {fold }} &=\\gamma \\cdot \\frac{b-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nIn practice, this can be achieved in FasterAI with the BN_folder class\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\n\ndef fold(\n    model:Module, # The model to fold (must be in eval mode)\n)-&gt;Module:\n\nRecursively fold all BatchNorm2d layers into their preceding Conv2d layers",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Batch Norm Folding"
    ]
  },
  {
    "objectID": "misc/bn_folding.html#usage-example",
    "href": "misc/bn_folding.html#usage-example",
    "title": "Batch Norm Folding",
    "section": "Usage Example",
    "text": "Usage Example\nfrom fasterai.misc.bn_folding import BN_Folder\nfrom torchvision.models import resnet18\n\n# Load model and set to eval mode (required!)\nmodel = resnet18(pretrained=True).eval()\n\n# Count parameters before\nparams_before = sum(p.numel() for p in model.parameters())\n\n# Fold BatchNorm layers\nfolder = BN_Folder()\nfolded_model = folder.fold(model)\n\n# Count parameters after\nparams_after = sum(p.numel() for p in folded_model.parameters())\n\nprint(f\"Before: {params_before:,} params\")\nprint(f\"After:  {params_after:,} params\")\nprint(f\"Removed: {params_before - params_after:,} params\")\nA detailed tutorial can be found here.",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Batch Norm Folding"
    ]
  },
  {
    "objectID": "misc/bn_folding.html#see-also",
    "href": "misc/bn_folding.html#see-also",
    "title": "Batch Norm Folding",
    "section": "See Also",
    "text": "See Also\n\nFC Decomposer - Another optimization technique for reducing model size\nONNX Exporter - Export optimized models for deployment\nQuantizer - Apply quantization after batch norm folding for additional compression",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Batch Norm Folding"
    ]
  },
  {
    "objectID": "misc/fc_decomposer.html",
    "href": "misc/fc_decomposer.html",
    "title": "Fully-Connected Layers Decomposer",
    "section": "",
    "text": "The FC_Decomposer class reduces model size by factorizing large fully-connected (Linear) layers into two smaller layers using Singular Value Decomposition (SVD). This is particularly effective for models with large FC layers like VGG or older architectures with big classifier heads.\nKey Benefits: - Reduces parameter count without changing model architecture externally - No retraining required (though fine-tuning may improve accuracy) - Works on any model with Linear layers\n\n\n\n\n\n\n\n\n\nScenario\nRecommendation\n\n\n\n\nLarge classifier heads (e.g., VGGâ€™s 4096â†’4096â†’1000)\nHighly recommended - significant savings\n\n\nModern architectures (ResNet, EfficientNet)\nLimited benefit - already efficient\n\n\nTransformer attention layers\nUse with caution - may hurt performance\n\n\nPre-deployment optimization\nGood complement to pruning/quantization\n\n\n\n\n\n\nFor a Linear layer with shape (out_features, in_features): - Original parameters: out_features Ã— in_features + out_features (with bias) - After decomposition (keeping k singular values): k Ã— in_features + out_features Ã— k + out_features - Compression ratio: roughly 1 / (1 - percent_removed) for square layers",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Fully-Connected Layers Decomposer"
    ]
  },
  {
    "objectID": "misc/fc_decomposer.html#overview",
    "href": "misc/fc_decomposer.html#overview",
    "title": "Fully-Connected Layers Decomposer",
    "section": "",
    "text": "The FC_Decomposer class reduces model size by factorizing large fully-connected (Linear) layers into two smaller layers using Singular Value Decomposition (SVD). This is particularly effective for models with large FC layers like VGG or older architectures with big classifier heads.\nKey Benefits: - Reduces parameter count without changing model architecture externally - No retraining required (though fine-tuning may improve accuracy) - Works on any model with Linear layers\n\n\n\n\n\n\n\n\n\nScenario\nRecommendation\n\n\n\n\nLarge classifier heads (e.g., VGGâ€™s 4096â†’4096â†’1000)\nHighly recommended - significant savings\n\n\nModern architectures (ResNet, EfficientNet)\nLimited benefit - already efficient\n\n\nTransformer attention layers\nUse with caution - may hurt performance\n\n\nPre-deployment optimization\nGood complement to pruning/quantization\n\n\n\n\n\n\nFor a Linear layer with shape (out_features, in_features): - Original parameters: out_features Ã— in_features + out_features (with bias) - After decomposition (keeping k singular values): k Ã— in_features + out_features Ã— k + out_features - Compression ratio: roughly 1 / (1 - percent_removed) for square layers",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Fully-Connected Layers Decomposer"
    ]
  },
  {
    "objectID": "misc/fc_decomposer.html#how-it-works",
    "href": "misc/fc_decomposer.html#how-it-works",
    "title": "Fully-Connected Layers Decomposer",
    "section": "How It Works",
    "text": "How It Works\nSVD decomposes a weight matrix into three matrices: \\(W = U \\Sigma V^T\\)\nWhere: - \\(U\\) contains left singular vectors (output features) - \\(\\Sigma\\) is diagonal with singular values (importance scores) - \\(V^T\\) contains right singular vectors (input features)\nBy keeping only the top \\(k\\) singular values, we approximate \\(W\\) with two smaller matrices, trading accuracy for compression.\n\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\nFC_Decomposer.decompose\n\ndef decompose(\n    model:Module, # The model to decompose\n    percent_removed:float=0.5, # Fraction of singular values to remove [0, 1)\n)-&gt;Module:\n\nRecursively decompose all Linear layers in the model using SVD",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Fully-Connected Layers Decomposer"
    ]
  },
  {
    "objectID": "misc/fc_decomposer.html#usage-example",
    "href": "misc/fc_decomposer.html#usage-example",
    "title": "Fully-Connected Layers Decomposer",
    "section": "Usage Example",
    "text": "Usage Example\nfrom fasterai.misc.fc_decomposer import FC_Decomposer\nfrom torchvision.models import vgg16\n\n# Load a model with large FC layers\nmodel = vgg16(pretrained=True)\n\n# Decompose, removing 50% of singular values\ndecomposer = FC_Decomposer()\ncompressed_model = decomposer.decompose(model, percent_removed=0.5)\n\n# Check parameter reduction\noriginal_params = sum(p.numel() for p in model.parameters())\ncompressed_params = sum(p.numel() for p in compressed_model.parameters())\nprint(f\"Compression: {original_params/compressed_params:.2f}x\")",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Fully-Connected Layers Decomposer"
    ]
  },
  {
    "objectID": "misc/fc_decomposer.html#see-also",
    "href": "misc/fc_decomposer.html#see-also",
    "title": "Fully-Connected Layers Decomposer",
    "section": "See Also",
    "text": "See Also\n\nFC Decomposer Tutorial - Step-by-step walkthrough with examples\nBN Folding - Another optimization technique to reduce inference overhead\nPruner - Remove entire filters for structured compression",
    "crumbs": [
      "Contact Me",
      "Misc",
      "Fully-Connected Layers Decomposer"
    ]
  },
  {
    "objectID": "sparse/sparsify_callback.html",
    "href": "sparse/sparsify_callback.html",
    "title": "Sparsify Callback",
    "section": "",
    "text": "The SparsifyCallback integrates weight sparsification into the fastai training loop. Unlike pruning (which removes structures), sparsification zeros out individual weights while maintaining the original network shape.\nKey Features: - Gradual sparsification according to a schedule - Support for Lottery Ticket Hypothesis (LTH) training - Multiple granularity levels (weight, vector, kernel, filter) - Global or local sparsification context\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef SparsifyCallback(\n    sparsity:float | dict[str, float], # Target sparsity (float) or per-layer dict\n    granularity:str, # Type of pruning granularity (e.g., 'weight', 'filter')\n    context:str, # Pruning context ('global' or 'local')\n    criteria:Criteria, # Criteria for determining weights to keep\n    schedule:Schedule, # Pruning schedule to use\n    lth:bool=False, # Whether to use Lottery Ticket Hypothesis approach\n    rewind_epoch:int=0, # Epoch to rewind weights to for LTH\n    reset_end:bool=False, # Whether to reset weights after pruning\n    save_tickets:bool=False, # Whether to save pruned models as \"winning tickets\"\n    model:nn.Module | None=None, # Model to sparsify (if None, uses learn.model)\n    round_to:int | None=None, # Round pruning to multiple of this value\n    nm:bool=False, # Whether to use N:M structured sparsity\n    layer_type:Type[nn.Module]=&lt;class 'torch.nn.modules.conv.Conv2d'&gt;, # Layer type to apply pruning to\n):\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nlth: whether training using the Lottery Ticket Hypothesis, i.e.Â reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you donâ€™t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "sparse/sparsify_callback.html#overview",
    "href": "sparse/sparsify_callback.html#overview",
    "title": "Sparsify Callback",
    "section": "",
    "text": "The SparsifyCallback integrates weight sparsification into the fastai training loop. Unlike pruning (which removes structures), sparsification zeros out individual weights while maintaining the original network shape.\nKey Features: - Gradual sparsification according to a schedule - Support for Lottery Ticket Hypothesis (LTH) training - Multiple granularity levels (weight, vector, kernel, filter) - Global or local sparsification context\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef SparsifyCallback(\n    sparsity:float | dict[str, float], # Target sparsity (float) or per-layer dict\n    granularity:str, # Type of pruning granularity (e.g., 'weight', 'filter')\n    context:str, # Pruning context ('global' or 'local')\n    criteria:Criteria, # Criteria for determining weights to keep\n    schedule:Schedule, # Pruning schedule to use\n    lth:bool=False, # Whether to use Lottery Ticket Hypothesis approach\n    rewind_epoch:int=0, # Epoch to rewind weights to for LTH\n    reset_end:bool=False, # Whether to reset weights after pruning\n    save_tickets:bool=False, # Whether to save pruned models as \"winning tickets\"\n    model:nn.Module | None=None, # Model to sparsify (if None, uses learn.model)\n    round_to:int | None=None, # Round pruning to multiple of this value\n    nm:bool=False, # Whether to use N:M structured sparsity\n    layer_type:Type[nn.Module]=&lt;class 'torch.nn.modules.conv.Conv2d'&gt;, # Layer type to apply pruning to\n):\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nlth: whether training using the Lottery Ticket Hypothesis, i.e.Â reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you donâ€™t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "sparse/sparsify_callback.html#usage-example",
    "href": "sparse/sparsify_callback.html#usage-example",
    "title": "Sparsify Callback",
    "section": "Usage Example",
    "text": "Usage Example\nfrom fasterai.sparse.sparsify_callback import SparsifyCallback\nfrom fasterai.core.schedule import cos\nfrom fasterai.core.criteria import large_final\n\n# Gradually sparsify to 50% using cosine schedule\ncb = SparsifyCallback(\n    sparsity=50,\n    granularity='weight',\n    context='global',\n    criteria=large_final,\n    schedule=cos\n)\n\nlearn.fit(10, cbs=[cb])\n\nPer-Layer Sparsity with Dict\n# Different sparsity targets for different layers\ncb = SparsifyCallback(\n    sparsity={'conv1': 30, 'layer1': 50, 'layer2': 70},\n    granularity='weight',\n    context='local',\n    criteria=large_final,\n    schedule=cos\n)\n\n\nWith Lottery Ticket Hypothesis\n# Train with LTH - rewind weights to epoch 2 values after each pruning step\ncb = SparsifyCallback(\n    sparsity=90,\n    granularity='weight',\n    context='global', \n    criteria=large_final,\n    schedule=one_cycle,\n    lth=True,\n    rewind_epoch=2\n)\n\nlearn.fit(20, cbs=[cb])",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "sparse/sparsify_callback.html#see-also",
    "href": "sparse/sparsify_callback.html#see-also",
    "title": "Sparsify Callback",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Core sparsification class used by this callback\nSchedules - Control sparsification progression (one_shot, agp, etc.)\nCriteria - Importance measures (large_final, movement, etc.)\nLottery Ticket Tutorial - Finding winning tickets with sparsification",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsify Callback"
    ]
  },
  {
    "objectID": "analyze/sensitivity.html",
    "href": "analyze/sensitivity.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "Not all layers in a neural network are equally important. Some are fragile â€” compressing them even slightly degrades performance â€” while others are robust and can be heavily compressed with minimal impact. Sensitivity analysis measures this per-layer fragility, enabling smarter compression strategies.\nThe SensitivityAnalyzer works by compressing one layer at a time, measuring the impact on a user-provided evaluation metric, and ranking layers by their sensitivity (delta from baseline).\nKey Features:\n\nSupports three compression types: sparsity (weight zeroing), pruning (structural filter removal), and quantization (precision reduction)\nGenerates non-uniform per-layer targets via to_layer_targets() â€” fragile layers get less compression, robust layers get more\nUses fasteraiâ€™s Sparsifier and Pruner internally for consistent results\nVisualization with plot() and export to pandas with to_dataframe()\n\n\n\n\n\n\n\n\n\n\n\n\nCompression Type\nlevel parameter\nWhat it tests\nBest for\n\n\n\n\n\"sparsity\"\n% of weights zeroed (e.g., 50)\nUnstructured weight removal\nGenerating per-layer sparsity targets\n\n\n\"pruning\"\n% of filters removed (e.g., 30)\nStructural filter pruning\nIdentifying layers to protect via ignored_layers\n\n\n\"quantization\"\nBit width (e.g., 8)\nPrecision reduction\nFinding layers that need higher precision",
    "crumbs": [
      "Contact Me",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "analyze/sensitivity.html#overview",
    "href": "analyze/sensitivity.html#overview",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "Not all layers in a neural network are equally important. Some are fragile â€” compressing them even slightly degrades performance â€” while others are robust and can be heavily compressed with minimal impact. Sensitivity analysis measures this per-layer fragility, enabling smarter compression strategies.\nThe SensitivityAnalyzer works by compressing one layer at a time, measuring the impact on a user-provided evaluation metric, and ranking layers by their sensitivity (delta from baseline).\nKey Features:\n\nSupports three compression types: sparsity (weight zeroing), pruning (structural filter removal), and quantization (precision reduction)\nGenerates non-uniform per-layer targets via to_layer_targets() â€” fragile layers get less compression, robust layers get more\nUses fasteraiâ€™s Sparsifier and Pruner internally for consistent results\nVisualization with plot() and export to pandas with to_dataframe()\n\n\n\n\n\n\n\n\n\n\n\n\nCompression Type\nlevel parameter\nWhat it tests\nBest for\n\n\n\n\n\"sparsity\"\n% of weights zeroed (e.g., 50)\nUnstructured weight removal\nGenerating per-layer sparsity targets\n\n\n\"pruning\"\n% of filters removed (e.g., 30)\nStructural filter pruning\nIdentifying layers to protect via ignored_layers\n\n\n\"quantization\"\nBit width (e.g., 8)\nPrecision reduction\nFinding layers that need higher precision",
    "crumbs": [
      "Contact Me",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "analyze/sensitivity.html#data-classes",
    "href": "analyze/sensitivity.html#data-classes",
    "title": "Sensitivity Analysis",
    "section": "Data Classes",
    "text": "Data Classes\nSensitivity results are returned as structured dataclasses for easy inspection, sorting, and export.\nLayerSensitivity holds the result for a single layer: the metric before and after compression, the delta (positive = degradation), and layer metadata. Use as_dict() to serialize.\n\nSensitivityResult aggregates all per-layer results. It provides methods to inspect, rank, visualize, and convert sensitivity data into actionable compression targets.\n\nKey Methods\nThe to_layer_targets() method converts sensitivity scores into a dict[str, float] mapping layer names to compression percentages. This dict can be passed directly to Sparsifier.sparsify_model() or SparsifyCallback(sparsity=...) for non-uniform compression.\nThe gamma parameter controls differentiation: higher values protect fragile layers more aggressively.\ntargets = result.to_layer_targets(model, target_pct=50, min_pct=10, max_pct=80, gamma=1.5)\n# {'conv1': 62.5, 'layer1.0.conv1': 65.3, 'layer1.1.conv2': 10.0, ...}",
    "crumbs": [
      "Contact Me",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "analyze/sensitivity.html#sensitivityanalyzer",
    "href": "analyze/sensitivity.html#sensitivityanalyzer",
    "title": "Sensitivity Analysis",
    "section": "SensitivityAnalyzer",
    "text": "SensitivityAnalyzer\nThe SensitivityAnalyzer class provides full control over the analysis process. For quick one-off analysis, see analyze_sensitivity() below.\nThe eval_fn should take a nn.Module and return a scalar metric (e.g., accuracy, loss). The analyzer will call it once per layer, so it should be reasonably fast â€” a forward pass on a small validation batch is typical.\n\nKey Methods\nFor quantization analysis, additional parameters control the behavior:\n\nquant_per_channel=True â€” per-channel quantization (more accurate, standard for weights)\nquant_activations=False â€” set to True to also quantize activations (slower but more realistic)\nlevel is interpreted as bit width (e.g., 8 for INT8) instead of percentage",
    "crumbs": [
      "Contact Me",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "analyze/sensitivity.html#convenience-function",
    "href": "analyze/sensitivity.html#convenience-function",
    "title": "Sensitivity Analysis",
    "section": "Convenience Function",
    "text": "Convenience Function\nFor quick one-off analysis without creating an analyzer instance:\n\nUsage Example\nfrom fasterai.analyze.sensitivity import analyze_sensitivity\n\nresult = analyze_sensitivity(model, sample, eval_fn, compression=\"sparsity\", level=50)\n\n# Inspect results\nresult.summary()                          # formatted console output\nfragile = result.top(5, most_sensitive=True)  # most sensitive layers\n\n# Generate per-layer targets for non-uniform compression\ntargets = result.to_layer_targets(model, target_pct=50, min_pct=10, max_pct=80)\n# Pass directly to Sparsifier: sparsifier.sparsify_model(sparsity=targets)",
    "crumbs": [
      "Contact Me",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "analyze/sensitivity.html#see-also",
    "href": "analyze/sensitivity.html#see-also",
    "title": "Sensitivity Analysis",
    "section": "See Also",
    "text": "See Also\n\nSensitivity Tutorial - Step-by-step guide with real examples on ResNet18\nSparsifier - Apply non-uniform sparsity using to_layer_targets() output\nPruner - Structural pruning (use top() to find layers to protect)\nCriteria - Importance scoring methods used during analysis\nSchedules - Control compression progression during training",
    "crumbs": [
      "Contact Me",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "quantize/quantizer.html",
    "href": "quantize/quantizer.html",
    "title": "Quantizer",
    "section": "",
    "text": "The Quantizer class provides model quantization capabilities to reduce model size and improve inference speed. Quantization converts floating-point weights and activations to lower precision integers (typically int8).\nSupported Backends: - 'x86': Optimized for Intel CPUs (default) - 'qnnpack': Optimized for ARM CPUs (mobile devices) - 'fbgemm': Facebookâ€™s quantization backend\nQuantization Methods: - 'static': Post-training quantization with calibration data - best accuracy, requires representative data - 'dynamic': Runtime quantization without calibration - easier to use, slightly lower accuracy - 'qat': Quantization-aware training - highest accuracy, requires retraining\nNote: PyTorch quantization produces CPU-only models. The quantized model will always run on CPU regardless of original device.\n\n\n\n\n\n\n\n\n\n\n\nMethod\nAccuracy\nSetup Effort\nWhen to Use\n\n\n\n\nStatic\nHigh\nMedium (needs calibration data)\nProduction with representative dataset available\n\n\nDynamic\nMedium\nLow (no calibration)\nQuick experiments, NLP models with variable input\n\n\nQAT\nHighest\nHigh (requires retraining)\nMaximum accuracy critical, have training resources\n\n\n\n\n\n\n\n\n\nBackend\nTarget Hardware\nBest For\n\n\n\n\n'x86'\nIntel/AMD CPUs\nDesktop/server deployment\n\n\n'qnnpack'\nARM CPUs\nMobile (iOS/Android), Raspberry Pi\n\n\n'fbgemm'\nIntel CPUs\nServer-side with batch inference\n\n\n\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\n\ndef Quantizer(\n    backend:str='x86', # Target backend for quantization\n    method:str='static', # Quantization method: 'static', 'dynamic', or 'qat'\n    qconfig_mapping:dict | None=None, # Optional custom quantization config\n    custom_configs:dict | None=None, # Custom module-specific configurations\n    use_per_tensor:bool=False, # Force per-tensor quantization\n    verbose:bool=False, # Enable verbose output\n):\n\nInitialize a quantizer with specified backend and options.\nParameters:\n\nbackend: Target hardware backend ('x86', 'qnnpack', 'fbgemm')\nmethod: Quantization approach ('static', 'dynamic', 'qat')\nqconfig_mapping: Optional custom quantization configuration\ncustom_configs: Dict of module-specific configurations\nuse_per_tensor: Force per-tensor quantization (may help with conversion issues)\nverbose: Enable detailed output during quantization\n\n\n\nsource\n\n\n\n\ndef quantize(\n    model:Module, # Model to quantize\n    calibration_dl:Any, # Dataloader for calibration\n    max_calibration_samples:int=100, # Maximum number of samples to use for calibration\n    device:str | torch.device='cpu', # Device to use for calibration\n)-&gt;Module:\n\nQuantize a model using the specified method and settings.\nNote: PyTorch quantization produces CPU-only models. The returned model will always be on CPU regardless of the input modelâ€™s device.",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantizer"
    ]
  },
  {
    "objectID": "quantize/quantizer.html#overview",
    "href": "quantize/quantizer.html#overview",
    "title": "Quantizer",
    "section": "",
    "text": "The Quantizer class provides model quantization capabilities to reduce model size and improve inference speed. Quantization converts floating-point weights and activations to lower precision integers (typically int8).\nSupported Backends: - 'x86': Optimized for Intel CPUs (default) - 'qnnpack': Optimized for ARM CPUs (mobile devices) - 'fbgemm': Facebookâ€™s quantization backend\nQuantization Methods: - 'static': Post-training quantization with calibration data - best accuracy, requires representative data - 'dynamic': Runtime quantization without calibration - easier to use, slightly lower accuracy - 'qat': Quantization-aware training - highest accuracy, requires retraining\nNote: PyTorch quantization produces CPU-only models. The quantized model will always run on CPU regardless of original device.\n\n\n\n\n\n\n\n\n\n\n\nMethod\nAccuracy\nSetup Effort\nWhen to Use\n\n\n\n\nStatic\nHigh\nMedium (needs calibration data)\nProduction with representative dataset available\n\n\nDynamic\nMedium\nLow (no calibration)\nQuick experiments, NLP models with variable input\n\n\nQAT\nHighest\nHigh (requires retraining)\nMaximum accuracy critical, have training resources\n\n\n\n\n\n\n\n\n\nBackend\nTarget Hardware\nBest For\n\n\n\n\n'x86'\nIntel/AMD CPUs\nDesktop/server deployment\n\n\n'qnnpack'\nARM CPUs\nMobile (iOS/Android), Raspberry Pi\n\n\n'fbgemm'\nIntel CPUs\nServer-side with batch inference\n\n\n\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\n\ndef Quantizer(\n    backend:str='x86', # Target backend for quantization\n    method:str='static', # Quantization method: 'static', 'dynamic', or 'qat'\n    qconfig_mapping:dict | None=None, # Optional custom quantization config\n    custom_configs:dict | None=None, # Custom module-specific configurations\n    use_per_tensor:bool=False, # Force per-tensor quantization\n    verbose:bool=False, # Enable verbose output\n):\n\nInitialize a quantizer with specified backend and options.\nParameters:\n\nbackend: Target hardware backend ('x86', 'qnnpack', 'fbgemm')\nmethod: Quantization approach ('static', 'dynamic', 'qat')\nqconfig_mapping: Optional custom quantization configuration\ncustom_configs: Dict of module-specific configurations\nuse_per_tensor: Force per-tensor quantization (may help with conversion issues)\nverbose: Enable detailed output during quantization\n\n\n\nsource\n\n\n\n\ndef quantize(\n    model:Module, # Model to quantize\n    calibration_dl:Any, # Dataloader for calibration\n    max_calibration_samples:int=100, # Maximum number of samples to use for calibration\n    device:str | torch.device='cpu', # Device to use for calibration\n)-&gt;Module:\n\nQuantize a model using the specified method and settings.\nNote: PyTorch quantization produces CPU-only models. The returned model will always be on CPU regardless of the input modelâ€™s device.",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantizer"
    ]
  },
  {
    "objectID": "quantize/quantizer.html#usage-examples",
    "href": "quantize/quantizer.html#usage-examples",
    "title": "Quantizer",
    "section": "Usage Examples",
    "text": "Usage Examples\n\nStatic Quantization (Recommended for best accuracy)\nfrom fasterai.quantize.quantizer import Quantizer\n\n# Create quantizer for static quantization\nquantizer = Quantizer(\n    backend='x86',\n    method='static',\n    verbose=True\n)\n\n# Quantize with calibration data\nquantized_model = quantizer.quantize(\n    model,\n    calibration_dl=dls.valid,\n    max_calibration_samples=100\n)\n\n\nDynamic Quantization (No calibration needed)\nfrom fasterai.quantize.quantizer import Quantizer\n\n# Create quantizer for dynamic quantization\nquantizer = Quantizer(\n    backend='x86',\n    method='dynamic'\n)\n\n# Quantize - no dataloader needed\nquantized_model = quantizer.quantize(model, calibration_dl=dls.valid)\n\n\nMobile Deployment (ARM devices)\n```python from fasterai.quantize.quantizer import Quantizer",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantizer"
    ]
  },
  {
    "objectID": "quantize/quantizer.html#see-also",
    "href": "quantize/quantizer.html#see-also",
    "title": "Quantizer",
    "section": "See Also",
    "text": "See Also\n\nQuantizeCallback - Apply quantization during fastai training\nPyTorch Quantization Documentation - Official PyTorch quantization guide\nONNX Exporter - Export models for cross-platform deployment",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantizer"
    ]
  },
  {
    "objectID": "regularize/regularize_callback.html",
    "href": "regularize/regularize_callback.html",
    "title": "Regularize Callback",
    "section": "",
    "text": "The RegularizeCallback applies structured regularization during training to encourage weight sparsity at various granularities. This is useful as a pre-pruning step: by regularizing groups of weights toward zero during training, subsequent pruning can remove more parameters with less accuracy loss.\nKey Features: - Supports multiple granularity levels ('weight', 'vector', 'kernel', 'filter') - Compatible with any criteria from fasterai.core.criteria - Optional scheduling to vary regularization strength over training\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef RegularizeCallback(\n    criteria:Criteria | list[Criteria], # Criteria(s) to use for regularization\n    granularity:str | list[str], # Granularity level(s) for grouping\n    weight:float=0.01, # Regularization weight\n    layer_types:Type | list[Type]=&lt;class 'torch.nn.modules.conv.Conv2d'&gt;, # Layer types to apply regularization to\n    schedule:Schedule | None=None, # Optional schedule for regularization weight\n    verbose:bool=False, # Whether to report regularization weight\n):\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nParameters: - criteria: Importance criteria to use for computing regularization (e.g., large_final) - granularity: Level at which to group weights ('weight', 'vector', 'kernel', 'filter') - weight: Regularization coefficient (higher = stronger regularization) - layer_types: Module types to regularize (default: nn.Conv2d) - schedule: Optional schedule to vary regularization strength over training - verbose: Print regularization weight after each epoch",
    "crumbs": [
      "Contact Me",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "regularize/regularize_callback.html#overview",
    "href": "regularize/regularize_callback.html#overview",
    "title": "Regularize Callback",
    "section": "",
    "text": "The RegularizeCallback applies structured regularization during training to encourage weight sparsity at various granularities. This is useful as a pre-pruning step: by regularizing groups of weights toward zero during training, subsequent pruning can remove more parameters with less accuracy loss.\nKey Features: - Supports multiple granularity levels ('weight', 'vector', 'kernel', 'filter') - Compatible with any criteria from fasterai.core.criteria - Optional scheduling to vary regularization strength over training\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef RegularizeCallback(\n    criteria:Criteria | list[Criteria], # Criteria(s) to use for regularization\n    granularity:str | list[str], # Granularity level(s) for grouping\n    weight:float=0.01, # Regularization weight\n    layer_types:Type | list[Type]=&lt;class 'torch.nn.modules.conv.Conv2d'&gt;, # Layer types to apply regularization to\n    schedule:Schedule | None=None, # Optional schedule for regularization weight\n    verbose:bool=False, # Whether to report regularization weight\n):\n\nBasic class handling tweaks of the training loop by changing a Learner in various events\nParameters: - criteria: Importance criteria to use for computing regularization (e.g., large_final) - granularity: Level at which to group weights ('weight', 'vector', 'kernel', 'filter') - weight: Regularization coefficient (higher = stronger regularization) - layer_types: Module types to regularize (default: nn.Conv2d) - schedule: Optional schedule to vary regularization strength over training - verbose: Print regularization weight after each epoch",
    "crumbs": [
      "Contact Me",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "regularize/regularize_callback.html#usage-example",
    "href": "regularize/regularize_callback.html#usage-example",
    "title": "Regularize Callback",
    "section": "Usage Example",
    "text": "Usage Example\nApply filter-level L1 regularization to encourage entire filters to become unimportant (making them easier to prune later):\nfrom fasterai.regularize.regularize_callback import RegularizeCallback\nfrom fasterai.core.criteria import large_final\n\n# Apply L1 regularization at filter granularity\ncb = RegularizeCallback(\n    criteria=large_final,\n    granularity='filter',\n    weight=0.01,\n    verbose=True\n)\n\nlearn.fit(10, cbs=[cb])\nTypical Workflow: 1. Train with RegularizeCallback to push unimportant filter groups toward zero 2. After training, use PruneCallback or Pruner to remove the zeroed-out structures 3. Fine-tune the pruned model to recover any lost accuracy",
    "crumbs": [
      "Contact Me",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "regularize/regularize_callback.html#see-also",
    "href": "regularize/regularize_callback.html#see-also",
    "title": "Regularize Callback",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Apply sparsification after regularization pushes weights to zero\nCriteria - Importance measures that can leverage regularized weights\nSparsifyCallback - Combine with sparsification for gradual pruning",
    "crumbs": [
      "Contact Me",
      "Regularize",
      "Regularize Callback"
    ]
  },
  {
    "objectID": "quantize/quantize_callback.html",
    "href": "quantize/quantize_callback.html",
    "title": "Quantize Callback",
    "section": "",
    "text": "The QuantizeCallback enables Quantization-Aware Training (QAT) within the fastai training loop. QAT simulates quantization effects during training, allowing the model to adapt its weights for better accuracy after quantization.\nWhy use QAT over post-training quantization? - Higher accuracy on the quantized model - Model learns to be robust to quantization noise - Especially beneficial for models sensitive to precision loss\nTrade-offs: - Requires retraining (not just calibration) - Training is slower due to simulated quantization - Only for situations where you can afford additional training time\nParameters:\n\nquantizer: Optional custom Quantizer instance for advanced configuration\nbackend: Target backend ('x86', 'qnnpack') - only used if quantizer not provided\nuse_per_tensor: Force per-tensor quantization to avoid conversion issues\nverbose: Enable detailed output during QAT",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "quantize/quantize_callback.html#overview",
    "href": "quantize/quantize_callback.html#overview",
    "title": "Quantize Callback",
    "section": "",
    "text": "The QuantizeCallback enables Quantization-Aware Training (QAT) within the fastai training loop. QAT simulates quantization effects during training, allowing the model to adapt its weights for better accuracy after quantization.\nWhy use QAT over post-training quantization? - Higher accuracy on the quantized model - Model learns to be robust to quantization noise - Especially beneficial for models sensitive to precision loss\nTrade-offs: - Requires retraining (not just calibration) - Training is slower due to simulated quantization - Only for situations where you can afford additional training time\nParameters:\n\nquantizer: Optional custom Quantizer instance for advanced configuration\nbackend: Target backend ('x86', 'qnnpack') - only used if quantizer not provided\nuse_per_tensor: Force per-tensor quantization to avoid conversion issues\nverbose: Enable detailed output during QAT",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "quantize/quantize_callback.html#usage-example",
    "href": "quantize/quantize_callback.html#usage-example",
    "title": "Quantize Callback",
    "section": "Usage Example",
    "text": "Usage Example\nfrom fasterai.quantize.quantize_callback import QuantizeCallback\n\n# Basic QAT with default settings\ncb = QuantizeCallback(backend='x86', verbose=True)\n\n# Train with QAT\nlearn.fit(5, cbs=[cb])\n\n# After training, the quantized model is available at:\nquantized_model = learn.quantized_model\n\nQAT Workflow\n\nbefore_fit: Model is prepared for QAT (fake quantization nodes inserted)\nTraining: Model trains with simulated quantization effects\nafter_fit: Model is converted to fully quantized form\n\nThe final learn.model is the quantized model ready for CPU inference.",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "quantize/quantize_callback.html#see-also",
    "href": "quantize/quantize_callback.html#see-also",
    "title": "Quantize Callback",
    "section": "See Also",
    "text": "See Also\n\nQuantizer - Core quantization class with backend/method options\nONNX Exporter - Export quantized models for deployment\nPyTorch Quantization Docs - Official PyTorch guide",
    "crumbs": [
      "Contact Me",
      "Quantize",
      "Quantize Callback"
    ]
  },
  {
    "objectID": "export/onnx_exporter.html",
    "href": "export/onnx_exporter.html",
    "title": "ONNX Exporter",
    "section": "",
    "text": "Export PyTorch models to ONNX format for deployment. Supports: - Basic ONNX export with graph optimization - Dynamic INT8 quantization (no calibration needed) - Static INT8 quantization (with calibration data) - Output verification against original model\n\n\n\n\n\n\n\n\n\n\n\nfrom fasterai.export.all import export_onnx, ONNXModel, verify_onnx\n\n# Basic export\npath = export_onnx(model, sample, \"model.onnx\")\n\n# With quantization\npath = export_onnx(model, sample, \"model.onnx\", quantize=True)\n\n# Inference\nonnx_model = ONNXModel(\"model.onnx\")\noutput = onnx_model(input_tensor)\n\n# Verify\nassert verify_onnx(model, \"model.onnx\", sample)\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef export_onnx(\n    model:nn.Module, # PyTorch model to export\n    sample:torch.Tensor, # Example input for tracing (with batch dim)\n    output_path:str | Path, # Output .onnx file path\n    opset_version:int=17, # ONNX opset version (17 recommended for compatibility)\n    quantize:bool=False, # Apply INT8 quantization after export\n    quantize_mode:str='dynamic', # \"dynamic\" (no calibration) or \"static\"\n    calibration_data:Iterable | None=None, # DataLoader for static quantization\n    optimize:bool=True, # Run ONNX graph optimizer\n    dynamic_batch:bool=True, # Allow variable batch size at runtime\n    input_names:list[str] | None=None, # Names for input tensors\n    output_names:list[str] | None=None, # Names for output tensors\n)-&gt;Path:\n\nExport a PyTorch model to ONNX format with optional quantization\n\nsource\n\n\n\n\ndef ONNXModel(\n    path:str | Path, device:str='cpu'\n):\n\nWrapper for ONNX Runtime inference with PyTorch-like interface\n\nsource\n\n\n\n\ndef verify_onnx(\n    model:nn.Module, # Original PyTorch model\n    onnx_path:str | Path, # Path to exported ONNX model\n    sample:torch.Tensor, # Test input tensor\n    rtol:float=0.001, # Relative tolerance\n    atol:float=1e-05, # Absolute tolerance\n)-&gt;bool:\n\nVerify ONNX model outputs match PyTorch model within tolerance",
    "crumbs": [
      "Contact Me",
      "Export",
      "ONNX Exporter"
    ]
  },
  {
    "objectID": "export/onnx_exporter.html#usage-examples",
    "href": "export/onnx_exporter.html#usage-examples",
    "title": "ONNX Exporter",
    "section": "",
    "text": "from fasterai.export.all import export_onnx, ONNXModel, verify_onnx\n\n# Basic export\npath = export_onnx(model, sample, \"model.onnx\")\n\n# With quantization\npath = export_onnx(model, sample, \"model.onnx\", quantize=True)\n\n# Inference\nonnx_model = ONNXModel(\"model.onnx\")\noutput = onnx_model(input_tensor)\n\n# Verify\nassert verify_onnx(model, \"model.onnx\", sample)\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef export_onnx(\n    model:nn.Module, # PyTorch model to export\n    sample:torch.Tensor, # Example input for tracing (with batch dim)\n    output_path:str | Path, # Output .onnx file path\n    opset_version:int=17, # ONNX opset version (17 recommended for compatibility)\n    quantize:bool=False, # Apply INT8 quantization after export\n    quantize_mode:str='dynamic', # \"dynamic\" (no calibration) or \"static\"\n    calibration_data:Iterable | None=None, # DataLoader for static quantization\n    optimize:bool=True, # Run ONNX graph optimizer\n    dynamic_batch:bool=True, # Allow variable batch size at runtime\n    input_names:list[str] | None=None, # Names for input tensors\n    output_names:list[str] | None=None, # Names for output tensors\n)-&gt;Path:\n\nExport a PyTorch model to ONNX format with optional quantization\n\nsource\n\n\n\n\ndef ONNXModel(\n    path:str | Path, device:str='cpu'\n):\n\nWrapper for ONNX Runtime inference with PyTorch-like interface\n\nsource\n\n\n\n\ndef verify_onnx(\n    model:nn.Module, # Original PyTorch model\n    onnx_path:str | Path, # Path to exported ONNX model\n    sample:torch.Tensor, # Test input tensor\n    rtol:float=0.001, # Relative tolerance\n    atol:float=1e-05, # Absolute tolerance\n)-&gt;bool:\n\nVerify ONNX model outputs match PyTorch model within tolerance",
    "crumbs": [
      "Contact Me",
      "Export",
      "ONNX Exporter"
    ]
  },
  {
    "objectID": "sparse/sparsifier.html",
    "href": "sparse/sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the networkâ€™s weights are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef Sparsifier(\n    model:nn.Module, # The model to sparsify\n    granularity:str, # Granularity of sparsification (e.g., 'weight', 'filter')\n    context:str, # Context for sparsification ('global' or 'local')\n    criteria:Criteria, # Criteria to determine which weights to keep\n    nm:bool=False, # Whether to use N:M sparsity pattern (forces 2:4 sparsity)\n    layer_type:Type[nn.Module]=&lt;class 'torch.nn.modules.conv.Conv2d'&gt;, # Type of layers to apply sparsification to\n):\n\nClass providing sparsifying capabilities\nThe Sparsifier class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n\nThe granularity, i.e.Â the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\nThe context, i.e.Â if you want to consider each layer independently (local), or compare the parameters to remove across the whole network (global).\nThe criteria, i.e.Â the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful.",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "sparse/sparsifier.html#overview",
    "href": "sparse/sparsifier.html#overview",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the networkâ€™s weights are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef Sparsifier(\n    model:nn.Module, # The model to sparsify\n    granularity:str, # Granularity of sparsification (e.g., 'weight', 'filter')\n    context:str, # Context for sparsification ('global' or 'local')\n    criteria:Criteria, # Criteria to determine which weights to keep\n    nm:bool=False, # Whether to use N:M sparsity pattern (forces 2:4 sparsity)\n    layer_type:Type[nn.Module]=&lt;class 'torch.nn.modules.conv.Conv2d'&gt;, # Type of layers to apply sparsification to\n):\n\nClass providing sparsifying capabilities\nThe Sparsifier class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n\nThe granularity, i.e.Â the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\nThe context, i.e.Â if you want to consider each layer independently (local), or compare the parameters to remove across the whole network (global).\nThe criteria, i.e.Â the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful.",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "sparse/sparsifier.html#key-methods",
    "href": "sparse/sparsifier.html#key-methods",
    "title": "Sparsifier",
    "section": "Key Methods",
    "text": "Key Methods\nUser can pass a single layer to sparsify by using the Sparsifier.sparsify_layer method.\n\nsource\n\nSparsifier.sparsify_layer\n\ndef sparsify_layer(\n    m:nn.Module, # The layer to sparsify\n    sparsity:float, # Target sparsity level (percentage)\n    round_to:int | None=None, # Round to a multiple of this value\n)-&gt;None:\n\nApply sparsification to a single layer\n\nMost of the time, we may want to sparsify the whole model at once, using the Sparsifier.sparsify_model method, indicating the percentage of sparsity you want to apply.\n\nsource\n\n\nSparsifier.sparsify_model\n\ndef sparsify_model(\n    sparsity:float | dict, # Target sparsity level or per-layer dict\n    round_to:int | None=None, # Round to a multiple of this value\n)-&gt;None:\n\nApply sparsification to all matching layers in the model",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "sparse/sparsifier.html#advanced-options",
    "href": "sparse/sparsifier.html#advanced-options",
    "title": "Sparsifier",
    "section": "Advanced Options",
    "text": "Advanced Options\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of a given number (e.g.Â 8), this can be done by passing the round_to parameter.\nInstead of passing a single value of sparsity, a dictionary of per-layer sparsities can be provided. This allows fine-grained control over which layers get sparsified and by how much.\nExample: Apply different sparsity levels to specific layers:\nsparsity_levels = {\n    'conv1': 30,           # 30% sparsity on first conv\n    'layer1.0.conv1': 50,  # 50% sparsity\n    'layer2.0.conv1': 70,  # 70% sparsity (more aggressive)\n}\nsparsifier.sparsify_model(sparsity=sparsity_levels)\nThis works seamlessly with SensitivityResult.to_layer_targets() to apply sensitivity-aware compression.",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "sparse/sparsifier.html#see-also",
    "href": "sparse/sparsifier.html#see-also",
    "title": "Sparsifier",
    "section": "See Also",
    "text": "See Also\n\nSparsifyCallback - Apply sparsification during fastai training\nCriteria - Different importance measures for selecting what to sparsify\nGranularity - Control what gets sparsified (weights, filters, etc.)\nSchedules - Control sparsification progression during training\nPruner - Structured pruning that removes filters entirely",
    "crumbs": [
      "Contact Me",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "misc/cpu_optimizer.html",
    "href": "misc/cpu_optimizer.html",
    "title": "Further optimize for CPU inference",
    "section": "",
    "text": "The accelerate_model_for_cpu function applies optimizations to prepare a PyTorch model for efficient CPU inference. It combines several techniques:\n\nChannels-last memory format: Optimizes memory layout for CNN operations on CPU\nTorchScript compilation: JIT compiles the model for faster execution\nMobile optimization: Applies optimize_for_mobile for operator fusion and other optimizations\n\nWhen to use: - Deploying models on CPU-only servers - Edge deployment without GPU - After quantization for maximum CPU performance\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef accelerate_model_for_cpu(\n    model:Module, example_input:Tensor\n):\n\nParameters:\n\nmodel: The PyTorch model to optimize\nexample_input: A sample input tensor (used for tracing)\n\nReturns: An optimized TorchScript model"
  },
  {
    "objectID": "misc/cpu_optimizer.html#overview",
    "href": "misc/cpu_optimizer.html#overview",
    "title": "Further optimize for CPU inference",
    "section": "",
    "text": "The accelerate_model_for_cpu function applies optimizations to prepare a PyTorch model for efficient CPU inference. It combines several techniques:\n\nChannels-last memory format: Optimizes memory layout for CNN operations on CPU\nTorchScript compilation: JIT compiles the model for faster execution\nMobile optimization: Applies optimize_for_mobile for operator fusion and other optimizations\n\nWhen to use: - Deploying models on CPU-only servers - Edge deployment without GPU - After quantization for maximum CPU performance\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource\n\n\n\ndef accelerate_model_for_cpu(\n    model:Module, example_input:Tensor\n):\n\nParameters:\n\nmodel: The PyTorch model to optimize\nexample_input: A sample input tensor (used for tracing)\n\nReturns: An optimized TorchScript model"
  },
  {
    "objectID": "misc/cpu_optimizer.html#usage-example",
    "href": "misc/cpu_optimizer.html#usage-example",
    "title": "Further optimize for CPU inference",
    "section": "Usage Example",
    "text": "Usage Example\nfrom fasterai.misc.cpu_optimizer import accelerate_model_for_cpu\nimport torch\n\n# Create example input matching your model's expected shape\nexample_input = torch.randn(1, 3, 224, 224)\n\n# Optimize model for CPU inference\noptimized_model = accelerate_model_for_cpu(model, example_input)\n\n# Use the optimized model\nwith torch.no_grad():\n    output = optimized_model(input_tensor)\nNote: The returned model is a TorchScript model. Some dynamic Python features may not be supported."
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Methods â€¢ Features â€¢ Installation â€¢ Tutorials â€¢ Citing â€¢ License\nfasterai is a library created to make neural network smaller and faster. It essentially relies on common compression techniques for networks such as pruning, knowledge distillation, Lottery Ticket Hypothesis, â€¦\nThe core feature of fasterai is its Sparsifying capabilities, constructed around 4 main modules: granularity, context, criteria, schedule. Each of these modules is highly customizable, allowing you to change them according to your needs or even to come up with your own!",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#project-documentation",
    "href": "overview.html#project-documentation",
    "title": "Overview",
    "section": "Project Documentation",
    "text": "Project Documentation\nVisit Read The Docs Project Page or read following README to know more about using fasterai.",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#features",
    "href": "overview.html#features",
    "title": "Overview",
    "section": "Features",
    "text": "Features\n\n1. Sparsifying\n\nMake your model sparse according to a:  - Sparsity:  the percentage of weights that will be replaced by 0  - Granularity:  the granularity at which you operate the pruning (removing weights, vectors, kernels, filters)  - Context:  prune either each layer independantly (local pruning) or the whole model (global pruning)  - Criteria:  the criteria used to select the weights to remove (magnitude, movement, â€¦)  - Schedule:  which schedule you want to use for pruning (one shot, iterative, gradual, â€¦) \nThis can be achieved by using the SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n2. Pruning\n\nOnce your model has useless nodes due to zero-weights, they can be removed to not be a part of the network anymore.\nThis can be achieved by using the PruneCallback(sparsity, context, criteria, schedule)\n\n\n3. Regularization\n\nInstead of explicitely make your network sparse, let it train towards sparse connections by pushing the weights to be as small as possible.\nRegularization can be applied to groups of weights, following the same granularities as for sparsifying, i.e.: - Granularity:  the granularity at which you operate the regularization (weights, vectors, kernels, filters, â€¦)\nThis can be achieved by using the RegularizationCallback(granularity)\n\n\n4. Knowledge Distillation\n\n\n\nalt text\n\n\nDistill the knowledge acquired by a big model into a smaller one, by using the KnowledgeDistillation callback.\n\n\n5. Lottery Ticket Hypothesis\n\nFind the winning ticket in you network, i.e. the initial subnetwork able to attain at least similar performances than the network as a whole.",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#the-fasterai-ecosystem",
    "href": "overview.html#the-fasterai-ecosystem",
    "title": "Overview",
    "section": "The FasterAI Ecosystem",
    "text": "The FasterAI Ecosystem\nfasterai is part of a family of libraries designed to make neural network optimization accessible:\n\n\n\n\n\n\n\n\nPackage\nPurpose\nWhen to Use\n\n\n\n\nfasterai\nCompression techniques\nPruning, sparsification, distillation, quantization during training\n\n\nfasterbench\nBenchmarking\nMeasuring model size, speed, memory, compute, and energy\n\n\nfasterlatency\nLatency prediction\nHardware-aware neural architecture search\n\n\nfasterrecipes\nHigh-level workflows\nQuick compression with sensible defaults\n\n\n\n\nTypical Workflow\n\nBenchmark your model with fasterbench to identify bottlenecks\nCompress using fasterai techniques (pruning, distillation, quantization)\nValidate compression impact with fasterbench\nDeploy the optimized model\n\nOr use fasterrecipes for one-line compression with sensible defaults!",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#quick-start",
    "href": "overview.html#quick-start",
    "title": "Overview",
    "section": "Quick Start",
    "text": "Quick Start\n\n0. Import fasterai\nfrom fasterai.sparse.all import *\n\n\n1. Create your model with fastai\nlearn = cnn_learner(dls, model)\n\n\n2. Get you Fasterai Callback\nsp_cb=SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n3. Train you model to make it sparse !\nlearn.fit_one_cycle(n_epochs, cbs=sp_cb)",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#installation",
    "href": "overview.html#installation",
    "title": "Overview",
    "section": "Installation",
    "text": "Installation\npip install git+https://github.com/nathanhubens/fasterai.git\nor\npip install fasterai",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#tutorials",
    "href": "overview.html#tutorials",
    "title": "Overview",
    "section": "Tutorials",
    "text": "Tutorials\n\nGet Started with FasterAI\nCreate your own pruning schedule\nFind winning tickets using the Lottery Ticket Hypothesis\nUse Knowledge Distillation to help a student model to reach higher performance\nSparsify Transformers\nMore to comeâ€¦",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#citing",
    "href": "overview.html#citing",
    "title": "Overview",
    "section": "Citing",
    "text": "Citing\n@software{Hubens,\n  author       = {Nathan Hubens},\n  title        = {fasterai},\n  year         = 2022,\n  publisher    = {Zenodo},\n  version      = {v0.1.6},\n  doi          = {10.5281/zenodo.6469868},\n  url          = {https://doi.org/10.5281/zenodo.6469868}\n}",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#license",
    "href": "overview.html#license",
    "title": "Overview",
    "section": "License",
    "text": "License\nApache-2.0 License.",
    "crumbs": [
      "Contact Me",
      "Overview"
    ]
  },
  {
    "objectID": "distill/distillation_callback.html",
    "href": "distill/distillation_callback.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge Distillation, sometimes called teacher-student training, is a compression method in which a small (the student) model is trained to mimic the behaviour of a larger (the teacher) model.\nThe main goal is to reveal what is called the Dark Knowledge hidden in the teacher model.\nIf we take the same example provided by Geoffrey Hinton et al., we have\nThe main problem of classification is that the output activation function (softmax) will, by design, make a single value really high and squash others.\n\\[\np_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j} \\exp \\left(z_{j}\\right)}\n\\]\nWith \\(p_i\\) the probability of class \\(i\\), computed from the logits \\(z\\)\nHere is an example to illustrate this phenomenon:\nLetâ€™s say that we have trained a model to discriminate between the following 5 classes: [cow, dog, plane, cat, car]\nAnd here is the output of the final layer (the logits) when the model is fed a new input image:\nlogits = torch.tensor([1.3, 3.1, 0.2, 1.9, -0.3])\nBy judging on the predictions, the model seems confident that the input data is a dog and quite confident that it is definitely not a plane nor a car, with predictions for cow and cat being moderately high.\nSo the model not only has learned to recognize a dog in the image, but also that a dog is very different from a car and a plane and share similarities with cats and cows. This information is what is called dark knowledge !\nWhen passing those predictions through a softmax, we have:\npredictions = F.softmax(logits, dim=-1); predictions\n\ntensor([0.1063, 0.6431, 0.0354, 0.1937, 0.0215])\nThis is accuenting the differences that we had earlier, discarding some of the dark knowledge acquired earlier. The way to keep this knowledge is to â€œsoftenâ€ our softmax outputs, by adding a temperature parameter. The higher the temperature, the softer the predictions.\nsoft_predictions = F.softmax(logits/3, dim=-1); soft_predictions\n\ntensor([0.1879, 0.3423, 0.1302, 0.2294, 0.1102])\nWhen applying Knowledge Distillation, we want to keep the Dark Knowledge that the teacher model has acquired during its training but not rely entirely on it. So we combine two losses:\nThe combination between those losses are weighted by an additional parameter Î±, as:\n\\[\nL_{K D}=\\alpha  * \\text { CrossEntropy }\\left(p_{S}^{\\tau}, p_{T}^{\\tau}\\right)+(1-\\alpha) * \\text { CrossEntropy }\\left(p_{S}, y_{\\text {true }}\\right)\n\\]\nWith \\(p^{\\tau}\\) being the softened predictions of the student and teacher\nThis can be done with fastai, using the Callback system !\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Knowledge Distillation"
    ]
  },
  {
    "objectID": "distill/distillation_callback.html#usage-with-schedule",
    "href": "distill/distillation_callback.html#usage-with-schedule",
    "title": "Knowledge Distillation",
    "section": "Usage with Schedule",
    "text": "Usage with Schedule\nYou can now gradually increase the distillation weight during training:\nfrom fasterai.core.schedule import cos\n\n# Gradually increase teacher influence from 0 to 0.8 using cosine schedule\ncb = KnowledgeDistillationCallback(\n    teacher=teacher_model,\n    loss=SoftTarget,\n    weight=0.8,\n    schedule=cos\n)\n\nlearn.fit(10, cbs=[cb])",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Knowledge Distillation"
    ]
  },
  {
    "objectID": "distill/distillation_callback.html#see-also",
    "href": "distill/distillation_callback.html#see-also",
    "title": "Knowledge Distillation",
    "section": "See Also",
    "text": "See Also\n\nDistillation Losses - Available loss functions (Attention, FitNet, PKT, etc.)\nDistillation Tutorial - Step-by-step guide to knowledge distillation\nSchedules - Control distillation weight progression\nPruner - Combine with pruning for maximum compression",
    "crumbs": [
      "Contact Me",
      "Distill",
      "Knowledge Distillation"
    ]
  },
  {
    "objectID": "core/granularity.html",
    "href": "core/granularity.html",
    "title": "Granularity",
    "section": "",
    "text": "A Conv2d layer possess a 4d-tensor as weights. This means that there exist many ways of removing blocks from it.\n\n\nIn the case of convolution filters, removing 0-D elements is equivalent to removing individual weights.\n\nweight granularity\n\n\nget_pruned_conv('weight')\n\n\n\n\n\n\n\n\n\n\n\n1-D blocks of elements is equivalent to removing vectors from the convolution filters. There are several ways to chose the vectors, that will be represented below.\n\nshared_weight: this granularity is very particular as it removes individual weights from a filter, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_weight')\n\n\n\n\n\n\n\n\n\nchannel: remove vector of weights along the channel axis.\n\n\nget_pruned_conv('channel')\n\n\n\n\n\n\n\n\n\ncolumn: remove vector of weights along the height axis.\n\n\nget_pruned_conv('column')\n\n\n\n\n\n\n\n\n\nrow: remove vector of weights along the width axis.\n\n\nget_pruned_conv('row')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_channel: remove vector of weight along the channel axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_channel')\n\n\n\n\n\n\n\n\n\nshared_column: remove vector of weight along the height axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_column')\n\n\n\n\n\n\n\n\n\nshared_row: remove vector of weight along the width axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_row')\n\n\n\n\n\n\n\n\n\nvertical_slice: remove vertical slices of weight along the height axis.\n\n\nget_pruned_conv('vertical_slice')\n\n\n\n\n\n\n\n\n\nhorizontal_slice: remove vertical slices of weight along the width axis.\n\n\nget_pruned_conv('horizontal_slice')\n\n\n\n\n\n\n\n\n\nkernel: remove kernels of from the convolution filters.\n\n\nget_pruned_conv('kernel')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_vertical_slice: remove vertical slices of weight along the height axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_vertical_slice')\n\n\n\n\n\n\n\n\n\nshared_horizontal_slice: remove horizontal slices of weight along the width axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_horizontal_slice')\n\n\n\n\n\n\n\n\n\nshared_kernel: remove kernels of weight from the convolution filters, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_kernel')\n\n\n\n\n\n\n\n\n\nfilter: remove entire filters.\n\n\nget_pruned_conv('filter')",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/granularity.html#conv2d-pruning",
    "href": "core/granularity.html#conv2d-pruning",
    "title": "Granularity",
    "section": "",
    "text": "A Conv2d layer possess a 4d-tensor as weights. This means that there exist many ways of removing blocks from it.\n\n\nIn the case of convolution filters, removing 0-D elements is equivalent to removing individual weights.\n\nweight granularity\n\n\nget_pruned_conv('weight')\n\n\n\n\n\n\n\n\n\n\n\n1-D blocks of elements is equivalent to removing vectors from the convolution filters. There are several ways to chose the vectors, that will be represented below.\n\nshared_weight: this granularity is very particular as it removes individual weights from a filter, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_weight')\n\n\n\n\n\n\n\n\n\nchannel: remove vector of weights along the channel axis.\n\n\nget_pruned_conv('channel')\n\n\n\n\n\n\n\n\n\ncolumn: remove vector of weights along the height axis.\n\n\nget_pruned_conv('column')\n\n\n\n\n\n\n\n\n\nrow: remove vector of weights along the width axis.\n\n\nget_pruned_conv('row')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_channel: remove vector of weight along the channel axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_channel')\n\n\n\n\n\n\n\n\n\nshared_column: remove vector of weight along the height axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_column')\n\n\n\n\n\n\n\n\n\nshared_row: remove vector of weight along the width axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_row')\n\n\n\n\n\n\n\n\n\nvertical_slice: remove vertical slices of weight along the height axis.\n\n\nget_pruned_conv('vertical_slice')\n\n\n\n\n\n\n\n\n\nhorizontal_slice: remove vertical slices of weight along the width axis.\n\n\nget_pruned_conv('horizontal_slice')\n\n\n\n\n\n\n\n\n\nkernel: remove kernels of from the convolution filters.\n\n\nget_pruned_conv('kernel')\n\n\n\n\n\n\n\n\n\n\n\n\nshared_vertical_slice: remove vertical slices of weight along the height axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_vertical_slice')\n\n\n\n\n\n\n\n\n\nshared_horizontal_slice: remove horizontal slices of weight along the width axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_horizontal_slice')\n\n\n\n\n\n\n\n\n\nshared_kernel: remove kernels of weight from the convolution filters, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_kernel')\n\n\n\n\n\n\n\n\n\nfilter: remove entire filters.\n\n\nget_pruned_conv('filter')",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/granularity.html#linear-pruning",
    "href": "core/granularity.html#linear-pruning",
    "title": "Granularity",
    "section": "Linear Pruning",
    "text": "Linear Pruning\n\n0-D Blocks\nAs for the convolution filters, weights from a Linear layer can be removed independently.\n\nweight: remove individual weights.\n\n\nget_pruned_linear('weight')\n\n\n\n\n\n\n\n\n\n\n1-D Blocks\n\ncolumn: remove column of weight, which corresponds to removing input neurons.\n\n\nget_pruned_linear('column')\n\n\n\n\n\n\n\n\n\nrow: remove rows of weight, which corresponds to removing output neurons.\n\n\nget_pruned_linear('row')",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/granularity.html#transformer-pruning",
    "href": "core/granularity.html#transformer-pruning",
    "title": "Granularity",
    "section": "Transformer Pruning",
    "text": "Transformer Pruning\n\n\n\n\n\n\nNote\n\n\n\nThis is an experimental part of the library",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/granularity.html#see-also",
    "href": "core/granularity.html#see-also",
    "title": "Granularity",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Apply sparsification at different granularities\nCriteria - Different importance measures for selecting what to prune\nSchedules - Control when pruning happens during training\n\n\nGranularity Selection Guide\n\n\n\nUse Case\nRecommended Granularity\n\n\n\n\nMaximum compression\nweight (unstructured)\n\n\nHardware speedup on CPU/GPU\nfilter (structured)\n\n\nBalanced compression/speed\nkernel\n\n\nN:M sparsity (e.g., 2:4)\nweight with nm=True",
    "crumbs": [
      "Contact Me",
      "Core",
      "Granularity"
    ]
  },
  {
    "objectID": "core/schedules.html",
    "href": "core/schedules.html",
    "title": "Schedules",
    "section": "",
    "text": "Found permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#one-shot",
    "href": "core/schedules.html#one-shot",
    "title": "Schedules",
    "section": "One-Shot",
    "text": "One-Shot\nThe easiest schedule is the one-shot pruning, i.e.Â prune the network once. This can be done by simply returning the desired sparsity value. The moment when you want to prune will be controlled by the start_epoch argument in the SparsifyCallback.\n\nsource\n\nsched_oneshot\n\ndef sched_oneshot(\n    start:float, # Starting sparsity level\n    end:float, # Target sparsity level\n    pos:float, # Current position in schedule (0-1)\n)-&gt;float:\n\nOne-shot pruning: jump directly to target sparsity\n\none_shot.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#iterative",
    "href": "core/schedules.html#iterative",
    "title": "Schedules",
    "section": "Iterative",
    "text": "Iterative\nInstead of pruning the network to desired sparsity in one step, you can do it iteratively. In fasterai, you can change the amount of iterations\n\nsource\n\nsched_iterative\n\ndef sched_iterative(\n    start:float, # Starting sparsity level\n    end:float, # Target sparsity level\n    pos:float, # Current position in schedule (0-1)\n    n_steps:int=3, # Number of pruning steps\n)-&gt;float:\n\nPerform iterative pruning in discrete steps\n\niterative.plot(50)\n\n\n\n\n\n\n\n\n\n\nTo modify the default n_steps, you can use the partial function.\n\niterative = Schedule(partial(sched_iterative, n_steps=5), start_pct=0.2)\n\n\niterative.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#automated-gradual-pruning",
    "href": "core/schedules.html#automated-gradual-pruning",
    "title": "Schedules",
    "section": "Automated Gradual Pruning",
    "text": "Automated Gradual Pruning\nSome researchers have come up with more sophisticated schedules, such as the Automated Gradual Pruning.\n\nsource\n\nsched_agp\n\ndef sched_agp(\n    start:float, # Starting sparsity level\n    end:float, # Target sparsity level\n    pos:float, # Current position in schedule (0-1)\n)-&gt;float:\n\nAutomated gradual pruning schedule with cubic decay\n\nagp.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#one-cycle-pruning",
    "href": "core/schedules.html#one-cycle-pruning",
    "title": "Schedules",
    "section": "One-Cycle Pruning",
    "text": "One-Cycle Pruning\n\nsource\n\nsched_onecycle\n\ndef sched_onecycle(\n    start:float, # Starting sparsity level\n    end:float, # Target sparsity level\n    pos:float, # Current position in schedule (0-1)\n    Î±:float=14, # Steepness parameter\n    Î²:float=6, # Offset parameter\n)-&gt;float:\n\nOne-cycle schedule based on logistic function\n\none_cycle.plot(50)\n\n\n\n\n\n\n\n\n\n\nOn top of that, all of the schedules available in fastai by default are also available: - sched_cos - sched_linear\n\ncos.plot(50)\n\n\n\n\n\n\n\n\n\n\n\nlin.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#dense-sparse-dense",
    "href": "core/schedules.html#dense-sparse-dense",
    "title": "Schedules",
    "section": "Dense-Sparse-Dense",
    "text": "Dense-Sparse-Dense\nYou can also create even more interesting behaviours such as the DSD method, where you prune the model in the first place, then re-grow it to its initial amount of parameter.\n\nsource\n\nsched_dsd\n\ndef sched_dsd(\n    start:float, # Starting sparsity level\n    end:float, # Target sparsity level\n    pos:float, # Current position in schedule (0-1)\n)-&gt;float:\n\nDense-Sparse-Dense schedule: increase then decrease sparsity\n\ndsd.plot(50)",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#composing-schedules",
    "href": "core/schedules.html#composing-schedules",
    "title": "Schedules",
    "section": "Composing Schedules",
    "text": "Composing Schedules\nBy default, progress() returns values in [0, 1]. But with start_val and end_val, you can control the output range of each schedule, making it easy to chain them together for multi-phase training.\nFor example, say you want to:\n\nPhase 1 (0%â€“40% of training): ramp sparsity from 0% to 30% using AGP\nPhase 2 (40%â€“70% of training): ramp sparsity from 30% to 50% using cosine\nPhase 3 (70%â€“100% of training): hold at 50%\n\nEach schedule maps its [start_val, end_val] to a portion of the overall progress. The callback still just computes target * progress â€” the composition is entirely in the schedule definitions.\n\ncomposed = [\n    Schedule(sched_agp, start_pct=0.0, end_pct=0.4, start_val=0.0, end_val=0.6),  # 0â†’60% of target\n    Schedule(sched_cos, start_pct=0.4, end_pct=0.7, start_val=0.6, end_val=1.0),  # 60â†’100% of target\n    # Phase 3: no schedule needed â€” last schedule holds at end_val after end_pct\n]\n\n\n\n\n\n\n\n\n\n\nThe first schedule (AGP) ramps progress from 0.0 to 0.6, so target * progress goes from 0% to 30%. The second schedule (cosine) picks up at 0.6 and continues to 1.0, taking sparsity from 30% to 50%. After the last scheduleâ€™s end_pct, the progress holds at end_val â€” giving us the hold phase for free.",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "core/schedules.html#see-also",
    "href": "core/schedules.html#see-also",
    "title": "Schedules",
    "section": "See Also",
    "text": "See Also\n\nSparsifyCallback - Apply sparsification during training using these schedules\nPruneCallback - Apply structured pruning during training\nCriteria - Different importance measures for selecting what to prune",
    "crumbs": [
      "Contact Me",
      "Core",
      "Schedules"
    ]
  },
  {
    "objectID": "tutorials/quantize/quantizer.html",
    "href": "tutorials/quantize/quantizer.html",
    "title": "Quantization",
    "section": "",
    "text": "path = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\npretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)\nlearn = Learner(dls, pretrained_resnet_34, metrics=accuracy)\nlearn.model.fc = nn.Linear(512, 2)\nlearn.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.510738\n0.345193\n0.846414\n00:02\n\n\n1\n0.323801\n0.279681\n0.868065\n00:03\n\n\n2\n0.193122\n0.247449\n0.905277\n00:02\n\n\n3\n0.107039\n0.210729\n0.917456\n00:03\n\n\n4\n0.068932\n0.214874\n0.921516\n00:02\n\n\n\n\n\n\nquantizer = Quantizer(\n    backend=\"x86\",\n    method=\"static\",    # Use dynamic quantization\n    verbose=True,       # See detailed output for debugging\n    use_per_tensor=False\n)\n\n# Quantize your model\nquantized_model = quantizer.quantize(\n    model=learn.model,\n    calibration_dl=dls.train,\n    max_calibration_samples= 200,\n)\n\nPreparing model for static quantization with x86 backend\n\n\n/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n  warnings.warn(\n\n\nCalibrating with up to 200 samples\n\n\nCalibrating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02&lt;00:00,  1.10it/s]\n\n\nConverting to quantized model\nQuantization complete\n\n\n\nprint(f'Size of the original model: {get_model_size(learn.model):.2f} MB')\nprint(f'Size of the quantized model: {get_model_size(quantized_model):.2f} MB')\n\nSize of the original model: 85.27 MB\nSize of the quantized model: 21.51 MB\n\n\n\ncompute_validation_accuracy(quantized_model, dls.valid)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:03&lt;00:00,  6.01it/s]\n\n\n92.2192151556157"
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html",
    "href": "tutorials/analyze/sensitivity.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "Not all layers in a neural network respond equally to compression. Some layers are robust and can be heavily compressed with minimal impact, while others are fragile and degrade quickly. Sensitivity analysis helps you identify which layers fall into each category.\nThis tutorial shows you how to: 1. Analyze layer sensitivity to different compression methods 2. Identify fragile vs robust layers 3. Generate non-uniform per-layer compression targets based on sensitivity",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#overview",
    "href": "tutorials/analyze/sensitivity.html#overview",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "Not all layers in a neural network respond equally to compression. Some layers are robust and can be heavily compressed with minimal impact, while others are fragile and degrade quickly. Sensitivity analysis helps you identify which layers fall into each category.\nThis tutorial shows you how to: 1. Analyze layer sensitivity to different compression methods 2. Identify fragile vs robust layers 3. Generate non-uniform per-layer compression targets based on sensitivity",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#setup",
    "href": "tutorials/analyze/sensitivity.html#setup",
    "title": "Sensitivity Analysis",
    "section": "1. Setup",
    "text": "1. Setup\nFirst, we need a model and an evaluation function. The evaluation function takes a model and returns a metric (e.g., accuracy, loss).\n\n# Load a pretrained model\nmodel = resnet18(pretrained=True)\nmodel.eval()\n\n# Create sample input\nsample = torch.randn(1, 3, 224, 224)\n\n# Define an evaluation function\n# In practice, this would evaluate on your validation set\ndef eval_fn(m):\n    \"\"\"Simple proxy: measure output magnitude (replace with real accuracy)\"\"\"\n    with torch.no_grad():\n        out = m(sample)\n        return out.abs().mean().item()\n\n/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#basic-sensitivity-analysis",
    "href": "tutorials/analyze/sensitivity.html#basic-sensitivity-analysis",
    "title": "Sensitivity Analysis",
    "section": "2. Basic Sensitivity Analysis",
    "text": "2. Basic Sensitivity Analysis\nThe simplest way to run sensitivity analysis is with the analyze_sensitivity() function:\n\nresult = analyze_sensitivity(\n    model, \n    sample, \n    eval_fn,\n    compression=\"sparsity\",  # \"sparsity\", \"pruning\", or \"quantization\"\n    level=50,                 # 50% sparsity\n)\n\nComputing baseline accuracy... 1.1848\nAnalyzing 21 layers for sparsity @ 50% (granularity=weight, criteria=abs)\n  [1/21] conv1... Î”=+0.0064\n  [2/21] layer1.0.conv1... Î”=-0.0987\n  [3/21] layer1.0.conv2... Î”=-0.0649\n  [4/21] layer1.1.conv1... Î”=-0.0779\n  [5/21] layer1.1.conv2... Î”=+0.0648\n  [6/21] layer2.0.conv1... Î”=-0.1699\n  [7/21] layer2.0.conv2... Î”=-0.1183\n  [8/21] layer2.0.downsample.0... Î”=-0.2051\n  [9/21] layer2.1.conv1... Î”=-0.1259\n  [10/21] layer2.1.conv2... Î”=+0.0256\n  [11/21] layer3.0.conv1... Î”=-0.0785\n  [12/21] layer3.0.conv2... Î”=+0.0398\n  [13/21] layer3.0.downsample.0... Î”=-0.0064\n  [14/21] layer3.1.conv1... Î”=-0.0353\n  [15/21] layer3.1.conv2... Î”=-0.0569\n  [16/21] layer4.0.conv1... Î”=-0.0521\n  [17/21] layer4.0.conv2... Î”=+0.0452\n  [18/21] layer4.0.downsample.0... Î”=+0.0108\n  [19/21] layer4.1.conv1... Î”=-0.0230\n  [20/21] layer4.1.conv2... Î”=-0.0772\n  [21/21] fc... Î”=+0.0573\nâœ“ Analysis complete",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#understanding-the-results",
    "href": "tutorials/analyze/sensitivity.html#understanding-the-results",
    "title": "Sensitivity Analysis",
    "section": "3. Understanding the Results",
    "text": "3. Understanding the Results\nThe SensitivityResult object provides several ways to inspect the results:\n\n# Print a formatted summary\nresult.summary()\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nSensitivity Analysis: sparsity @ 50%\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  Baseline accuracy: 1.1848\n  Layers analyzed: 21\n\n  ðŸ”´ Most Sensitive (fragile):\n     1. layer1.1.conv2                 Î”=+0.0648\n     2. fc                             Î”=+0.0573\n     3. layer4.0.conv2                 Î”=+0.0452\n     4. layer3.0.conv2                 Î”=+0.0398\n     5. layer2.1.conv2                 Î”=+0.0256\n\n  ðŸŸ¢ Most Robust (compressible):\n     1. layer2.0.downsample.0          Î”=-0.2051\n     2. layer2.0.conv1                 Î”=-0.1699\n     3. layer2.1.conv1                 Î”=-0.1259\n     4. layer2.0.conv2                 Î”=-0.1183\n     5. layer1.0.conv1                 Î”=-0.0987\n\n\n\n# Get the top 3 most sensitive layers\nfragile = result.top(3, most_sensitive=True)\nfor layer in fragile:\n    print(f\"{layer.name}: Î”={layer.delta:.4f}\")\n\nlayer1.1.conv2: Î”=0.0648\nfc: Î”=0.0573\nlayer4.0.conv2: Î”=0.0452\n\n\n\n# Get the top 3 most robust layers (safe to compress heavily)\nrobust = result.top(3, most_sensitive=False)\nfor layer in robust:\n    print(f\"{layer.name}: Î”={layer.delta:.4f}\")\n\nlayer2.0.downsample.0: Î”=-0.2051\nlayer2.0.conv1: Î”=-0.1699\nlayer2.1.conv1: Î”=-0.1259",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#visualizing-sensitivity",
    "href": "tutorials/analyze/sensitivity.html#visualizing-sensitivity",
    "title": "Sensitivity Analysis",
    "section": "4. Visualizing Sensitivity",
    "text": "4. Visualizing Sensitivity\nA bar chart helps visualize which layers are most sensitive:\n\nresult.plot(figsize=(14, 5))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#creating-non-uniform-per-layer-targets",
    "href": "tutorials/analyze/sensitivity.html#creating-non-uniform-per-layer-targets",
    "title": "Sensitivity Analysis",
    "section": "5. Creating Non-Uniform Per-Layer Targets",
    "text": "5. Creating Non-Uniform Per-Layer Targets\nThe key insight from sensitivity analysis is that you shouldnâ€™t compress all layers equally. Use to_layer_targets() to generate non-uniform per-layer compression targets:\n\ntargets = result.to_layer_targets(\n    model,\n    target_pct=50,   # Target 50% average compression\n    min_pct=10,      # No layer below 10%\n    max_pct=80,      # No layer above 80%\n    gamma=1.5,       # Higher = more differentiation between layers\n)\n\n# Show a few entries\nfor name, sparsity in list(targets.items())[:5]:\n    print(f\"{name}: {sparsity}%\")\n\nconv1: 57.59%\nlayer1.0.conv1: 67.72%\nlayer1.0.conv2: 67.72%\nlayer1.1.conv1: 67.72%\nlayer1.1.conv2: 10.0%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#applying-per-layer-targets-with-sparsifier",
    "href": "tutorials/analyze/sensitivity.html#applying-per-layer-targets-with-sparsifier",
    "title": "Sensitivity Analysis",
    "section": "6. Applying Per-Layer Targets with Sparsifier",
    "text": "6. Applying Per-Layer Targets with Sparsifier\nThe targets generated by to_layer_targets() can be directly used with fasteraiâ€™s Sparsifier to apply non-uniform compression:\n\nfrom fasterai.sparse.all import Sparsifier\n\n# 1. Run sensitivity analysis\nresult = analyze_sensitivity(model, sample, eval_fn, compression=\"sparsity\", level=50)\n\n# 2. Generate per-layer targets (layer_name -&gt; sparsity %)\ntargets = result.to_layer_targets(model, target_pct=50, min_pct=10, max_pct=80)\n\n# 3. Create a fresh model and Sparsifier\nmodel = resnet18(pretrained=True)\nsparsifier = Sparsifier(model, granularity='weight', context='local', criteria=large_final)\n\n# 4. Apply non-uniform sparsity using the targets\nfor name, module in model.named_modules():\n    if name in targets:\n        sparsifier.sparsify_layer(module, targets[name])\n        \n# 5. Check the results\nsparsifier.print_sparsity()\n\nComputing baseline accuracy... 1.1848\nAnalyzing 21 layers for sparsity @ 50% (granularity=weight, criteria=abs)\n  [1/21] conv1... Î”=+0.0064\n  [2/21] layer1.0.conv1... Î”=-0.0987\n  [3/21] layer1.0.conv2... Î”=-0.0649\n  [4/21] layer1.1.conv1... Î”=-0.0779\n  [5/21] layer1.1.conv2... Î”=+0.0648\n  [6/21] layer2.0.conv1... Î”=-0.1699\n  [7/21] layer2.0.conv2... Î”=-0.1183\n  [8/21] layer2.0.downsample.0... Î”=-0.2051\n  [9/21] layer2.1.conv1... Î”=-0.1259\n  [10/21] layer2.1.conv2... Î”=+0.0256\n  [11/21] layer3.0.conv1... Î”=-0.0785\n  [12/21] layer3.0.conv2... Î”=+0.0398\n  [13/21] layer3.0.downsample.0... Î”=-0.0064\n  [14/21] layer3.1.conv1... Î”=-0.0353\n  [15/21] layer3.1.conv2... Î”=-0.0569\n  [16/21] layer4.0.conv1... Î”=-0.0521\n  [17/21] layer4.0.conv2... Î”=+0.0452\n  [18/21] layer4.0.downsample.0... Î”=+0.0108\n  [19/21] layer4.1.conv1... Î”=-0.0230\n  [20/21] layer4.1.conv2... Î”=-0.0772\n  [21/21] fc... Î”=+0.0573\nâœ“ Analysis complete\n\n\n/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      5,473         58.17%\nlayer1.0.conv1                 Conv2d          36,864     23,998        65.10%\nlayer1.0.conv2                 Conv2d          36,864     23,998        65.10%\nlayer1.1.conv1                 Conv2d          36,864     23,998        65.10%\nlayer1.1.conv2                 Conv2d          36,864     3,687         10.00%\nlayer2.0.conv1                 Conv2d          73,728     47,997        65.10%\nlayer2.0.conv2                 Conv2d          147,456    95,994        65.10%\nlayer2.0.downsample.0          Conv2d          8,192      5,333         65.10%\nlayer2.1.conv1                 Conv2d          147,456    95,994        65.10%\nlayer2.1.conv2                 Conv2d          147,456    55,267        37.48%\nlayer3.0.conv1                 Conv2d          294,912    191,988       65.10%\nlayer3.0.conv2                 Conv2d          589,824    130,646       22.15%\nlayer3.0.downsample.0          Conv2d          32,768     21,332        65.10%\nlayer3.1.conv1                 Conv2d          589,824    383,975       65.10%\nlayer3.1.conv2                 Conv2d          589,824    383,975       65.10%\nlayer4.0.conv1                 Conv2d          1,179,648  767,951       65.10%\nlayer4.0.conv2                 Conv2d          2,359,296  385,037       16.32%\nlayer4.0.downsample.0          Conv2d          131,072    70,071        53.46%\nlayer4.1.conv1                 Conv2d          2,359,296  1,535,902     65.10%\nlayer4.1.conv2                 Conv2d          2,359,296  1,535,902     65.10%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 5,788,518     51.84%\n\n\n\nNote on Structural Pruning\nThe to_layer_targets() method generates per-layer compression targets, which works directly with Sparsifier.\nFor structural pruning with Pruner, the current API uses a uniform ratio across all layers (torch-pruning handles dependency graphs automatically). You can still use sensitivity analysis to: 1. Identify which layers to exclude from pruning (fragile layers) 2. Choose an appropriate global pruning ratio based on the most sensitive layers\n\nfrom fasterai.prune.all import Pruner\n\n# Use sensitivity to identify layers to EXCLUDE from pruning\nfragile_layers = [layer.name for layer in result.top(3, most_sensitive=True)]\nprint(f\"Fragile layers to protect: {fragile_layers}\")\n\n# Get the actual module references for ignored_layers\nmodel = resnet18(pretrained=True)\nignored = []\nfor name, module in model.named_modules():\n    if name in fragile_layers:\n        ignored.append(module)\n\n# Create Pruner with uniform ratio, but protect fragile layers\npruner = Pruner(\n    model,\n    example_inputs=sample,\n    pruning_ratio=30,           # Uniform 30% pruning\n    ignored_layers=ignored,     # Protect sensitive layers!\n    context='global',\n    criteria=large_final,\n)\n\npruner.prune_model()\npruner.print_sparsity()\n\nFragile layers to protect: ['layer1.1.conv2', 'fc', 'layer4.0.conv2']\n\nPruning Report:\n-------------------------------------------------------------------------------------\nLayer                               Type         In Ch    Out Ch   Params      \n-------------------------------------------------------------------------------------\nconv1                               Conv2d       3        64       9,408       \nlayer1.0.conv1                      Conv2d       64       64       36,864      \nlayer1.0.conv2                      Conv2d       64       64       36,864      \nlayer1.1.conv1                      Conv2d       64       64       36,864      \nlayer1.1.conv2                      Conv2d       64       64       36,864      \nlayer2.0.conv1                      Conv2d       64       128      73,728      \nlayer2.0.conv2                      Conv2d       128      128      147,456     \nlayer2.0.downsample.0               Conv2d       64       128      8,192       \nlayer2.1.conv1                      Conv2d       128      128      147,456     \nlayer2.1.conv2                      Conv2d       128      128      147,456     \nlayer3.0.conv1                      Conv2d       128      256      294,912     \nlayer3.0.conv2                      Conv2d       256      256      589,824     \nlayer3.0.downsample.0               Conv2d       128      256      32,768      \nlayer3.1.conv1                      Conv2d       256      249      573,696     \nlayer3.1.conv2                      Conv2d       249      256      573,696     \nlayer4.0.conv1                      Conv2d       256      338      778,752     \nlayer4.0.conv2                      Conv2d       338      512      1,557,504   \nlayer4.0.downsample.0               Conv2d       256      512      131,072     \nlayer4.1.conv1                      Conv2d       512      1        4,608       \nlayer4.1.conv2                      Conv2d       1        512      4,608       \nfc                                  Linear       512      1000     513,000     \n-------------------------------------------------------------------------------------\nTotal                                                              5,735,592   \nOriginal                                                           11,689,512  \nReduction                                                               50.93%\n\n\n\nWorkflow Summary: - Sparsifier: Use to_layer_targets() for per-layer sparsity targets - Pruner: Use top(most_sensitive=True) to identify layers to protect via ignored_layers - Both: Fine-tune after compression to recover accuracy",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#using-sensitivityanalyzer-for-more-control",
    "href": "tutorials/analyze/sensitivity.html#using-sensitivityanalyzer-for-more-control",
    "title": "Sensitivity Analysis",
    "section": "7. Using SensitivityAnalyzer for More Control",
    "text": "7. Using SensitivityAnalyzer for More Control\n\nanalyzer = SensitivityAnalyzer(\n    model,\n    sample,\n    eval_fn,\n    criteria=large_final,       # Importance scoring method\n    higher_is_better=True,      # Higher metric = better\n    metric_name=\"accuracy\",     # For display\n)\n\n# Analyze sparsity sensitivity\nsparsity_result = analyzer.analyze(\n    compression=\"sparsity\",\n    level=50,\n    granularity=\"weight\",  # or \"filter\", \"kernel\", etc.\n)\n\nComputing baseline accuracy... 0.6909\nAnalyzing 21 layers for sparsity @ 50% (granularity=weight, criteria=abs)\n  [1/21] conv1... Î”=+0.0001\n  [2/21] layer1.0.conv1... Î”=-0.0006\n  [3/21] layer1.0.conv2... Î”=-0.0046\n  [4/21] layer1.1.conv1... Î”=-0.0052\n  [5/21] layer1.1.conv2... Î”=+0.0020\n  [6/21] layer2.0.conv1... Î”=-0.0021\n  [7/21] layer2.0.conv2... Î”=-0.0031\n  [8/21] layer2.0.downsample.0... Î”=-0.0055\n  [9/21] layer2.1.conv1... Î”=-0.0014\n  [10/21] layer2.1.conv2... Î”=-0.0021\n  [11/21] layer3.0.conv1... Î”=+0.0017\n  [12/21] layer3.0.conv2... Î”=+0.0051\n  [13/21] layer3.0.downsample.0... Î”=+0.0010\n  [14/21] layer3.1.conv1... Î”=+0.0031\n  [15/21] layer3.1.conv2... Î”=+0.0005\n  [16/21] layer4.0.conv1... Î”=+0.0053\n  [17/21] layer4.0.conv2... Î”=-0.0020\n  [18/21] layer4.0.downsample.0... Î”=-0.0050\n  [19/21] layer4.1.conv1... Î”=+0.0036\n  [20/21] layer4.1.conv2... Î”=-0.0001\n  [21/21] fc... Î”=-0.5034\nâœ“ Analysis complete",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#different-compression-types",
    "href": "tutorials/analyze/sensitivity.html#different-compression-types",
    "title": "Sensitivity Analysis",
    "section": "8. Different Compression Types",
    "text": "8. Different Compression Types\n\nsparsity_result = analyzer.analyze(\n    compression=\"sparsity\",\n    level=50,                # 50% of weights zeroed\n    granularity=\"weight\",    # Unstructured sparsity\n)\n\nComputing baseline accuracy... 0.6909\nAnalyzing 21 layers for sparsity @ 50% (granularity=weight, criteria=abs)\n  [1/21] conv1... Î”=+0.0001\n  [2/21] layer1.0.conv1... Î”=-0.0006\n  [3/21] layer1.0.conv2... Î”=-0.0046\n  [4/21] layer1.1.conv1... Î”=-0.0052\n  [5/21] layer1.1.conv2... Î”=+0.0020\n  [6/21] layer2.0.conv1... Î”=-0.0021\n  [7/21] layer2.0.conv2... Î”=-0.0031\n  [8/21] layer2.0.downsample.0... Î”=-0.0055\n  [9/21] layer2.1.conv1... Î”=-0.0014\n  [10/21] layer2.1.conv2... Î”=-0.0021\n  [11/21] layer3.0.conv1... Î”=+0.0017\n  [12/21] layer3.0.conv2... Î”=+0.0051\n  [13/21] layer3.0.downsample.0... Î”=+0.0010\n  [14/21] layer3.1.conv1... Î”=+0.0031\n  [15/21] layer3.1.conv2... Î”=+0.0005\n  [16/21] layer4.0.conv1... Î”=+0.0053\n  [17/21] layer4.0.conv2... Î”=-0.0020\n  [18/21] layer4.0.downsample.0... Î”=-0.0050\n  [19/21] layer4.1.conv1... Î”=+0.0036\n  [20/21] layer4.1.conv2... Î”=-0.0001\n  [21/21] fc... Î”=-0.5034\nâœ“ Analysis complete\n\n\n\nStructural Pruning (Filter Removal)\n\npruning_result = analyzer.analyze(\n    compression=\"pruning\",\n    level=30,                # 30% of filters removed\n)\n\nComputing baseline accuracy... 0.6909\nAnalyzing 21 layers for pruning @ 30% (structural, criteria=abs)\n  [1/21] conv1... Î”=0.0000\n  [2/21] layer1.0.conv1... Î”=-0.0016\n  [3/21] layer1.0.conv2... Î”=0.0000\n  [4/21] layer1.1.conv1... Î”=-0.0001\n  [5/21] layer1.1.conv2... Î”=0.0000\n  [6/21] layer2.0.conv1... Î”=-0.0045\n  [7/21] layer2.0.conv2... Î”=0.0000\n  [8/21] layer2.0.downsample.0... Î”=0.0000\n  [9/21] layer2.1.conv1... Î”=-0.0187\n  [10/21] layer2.1.conv2... Î”=0.0000\n  [11/21] layer3.0.conv1... Î”=+0.0001\n  [12/21] layer3.0.conv2... Î”=0.0000\n  [13/21] layer3.0.downsample.0... Î”=0.0000\n  [14/21] layer3.1.conv1... Î”=-0.0012\n  [15/21] layer3.1.conv2... Î”=0.0000\n  [16/21] layer4.0.conv1... Î”=-0.0063\n  [17/21] layer4.0.conv2... Î”=0.0000\n  [18/21] layer4.0.downsample.0... Î”=0.0000\n  [19/21] layer4.1.conv1... Î”=0.0000\n  [20/21] layer4.1.conv2... Î”=0.0000\n  [21/21] fc... Î”=+0.0301\nâœ“ Analysis complete\n\n\n\n\nQuantization (Precision Reduction)\n\nquant_result = analyzer.analyze(\n    compression=\"quantization\",\n    level=8,                     # 8-bit quantization\n    quant_per_channel=True,      # Per-channel quantization\n    quant_activations=False,     # Weights only\n)\n\nComputing baseline accuracy... 0.6909\nAnalyzing 21 layers for quantization @ 8bits (per-channel, weights only)\n  [1/21] conv1... Î”=-0.0002\n  [2/21] layer1.0.conv1... Î”=+0.0003\n  [3/21] layer1.0.conv2... Î”=+0.0001\n  [4/21] layer1.1.conv1... Î”=-0.0002\n  [5/21] layer1.1.conv2... Î”=-0.0003\n  [6/21] layer2.0.conv1... Î”=-0.0001\n  [7/21] layer2.0.conv2... Î”=-0.0002\n  [8/21] layer2.0.downsample.0... Î”=-0.0002\n  [9/21] layer2.1.conv1... Î”=-0.0004\n  [10/21] layer2.1.conv2... Î”=-0.0000\n  [11/21] layer3.0.conv1... Î”=+0.0000\n  [12/21] layer3.0.conv2... Î”=+0.0001\n  [13/21] layer3.0.downsample.0... Î”=-0.0000\n  [14/21] layer3.1.conv1... Î”=-0.0001\n  [15/21] layer3.1.conv2... Î”=+0.0002\n  [16/21] layer4.0.conv1... Î”=-0.0002\n  [17/21] layer4.0.conv2... Î”=-0.0000\n  [18/21] layer4.0.downsample.0... Î”=-0.0002\n  [19/21] layer4.1.conv1... Î”=+0.0001\n  [20/21] layer4.1.conv2... Î”=+0.0000\n  [21/21] fc... Î”=-0.0009\nâœ“ Analysis complete",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#sweeping-multiple-compression-levels",
    "href": "tutorials/analyze/sensitivity.html#sweeping-multiple-compression-levels",
    "title": "Sensitivity Analysis",
    "section": "9. Sweeping Multiple Compression Levels",
    "text": "9. Sweeping Multiple Compression Levels\n\nresults = analyzer.sweep(\n    compression=\"sparsity\",\n    levels=[25, 50, 75],\n)\n\n\n============================================================\nSweep: sparsity @ 25%\n============================================================\nComputing baseline accuracy... 0.6909\nAnalyzing 21 layers for sparsity @ 25% (granularity=weight, criteria=abs)\n  [1/21] conv1... Î”=-0.0001\n  [2/21] layer1.0.conv1... Î”=-0.0003\n  [3/21] layer1.0.conv2... Î”=+0.0005\n  [4/21] layer1.1.conv1... Î”=-0.0006\n  [5/21] layer1.1.conv2... Î”=+0.0015\n  [6/21] layer2.0.conv1... Î”=+0.0010\n  [7/21] layer2.0.conv2... Î”=+0.0019\n  [8/21] layer2.0.downsample.0... Î”=-0.0014\n  [9/21] layer2.1.conv1... Î”=+0.0009\n  [10/21] layer2.1.conv2... Î”=+0.0000\n  [11/21] layer3.0.conv1... Î”=-0.0012\n  [12/21] layer3.0.conv2... Î”=+0.0007\n  [13/21] layer3.0.downsample.0... Î”=+0.0008\n  [14/21] layer3.1.conv1... Î”=+0.0004\n  [15/21] layer3.1.conv2... Î”=-0.0005\n  [16/21] layer4.0.conv1... Î”=+0.0010\n  [17/21] layer4.0.conv2... Î”=-0.0006\n  [18/21] layer4.0.downsample.0... Î”=-0.0011\n  [19/21] layer4.1.conv1... Î”=+0.0011\n  [20/21] layer4.1.conv2... Î”=-0.0000\n  [21/21] fc... Î”=-0.0011\nâœ“ Analysis complete\n\n============================================================\nSweep: sparsity @ 50%\n============================================================\nComputing baseline accuracy... 0.6909\nAnalyzing 21 layers for sparsity @ 50% (granularity=weight, criteria=abs)\n  [1/21] conv1... Î”=+0.0001\n  [2/21] layer1.0.conv1... Î”=-0.0006\n  [3/21] layer1.0.conv2... Î”=-0.0046\n  [4/21] layer1.1.conv1... Î”=-0.0052\n  [5/21] layer1.1.conv2... Î”=+0.0020\n  [6/21] layer2.0.conv1... Î”=-0.0021\n  [7/21] layer2.0.conv2... Î”=-0.0031\n  [8/21] layer2.0.downsample.0... Î”=-0.0055\n  [9/21] layer2.1.conv1... Î”=-0.0014\n  [10/21] layer2.1.conv2... Î”=-0.0021\n  [11/21] layer3.0.conv1... Î”=+0.0017\n  [12/21] layer3.0.conv2... Î”=+0.0051\n  [13/21] layer3.0.downsample.0... Î”=+0.0010\n  [14/21] layer3.1.conv1... Î”=+0.0031\n  [15/21] layer3.1.conv2... Î”=+0.0005\n  [16/21] layer4.0.conv1... Î”=+0.0053\n  [17/21] layer4.0.conv2... Î”=-0.0020\n  [18/21] layer4.0.downsample.0... Î”=-0.0050\n  [19/21] layer4.1.conv1... Î”=+0.0036\n  [20/21] layer4.1.conv2... Î”=-0.0001\n  [21/21] fc... Î”=-0.5034\nâœ“ Analysis complete\n\n============================================================\nSweep: sparsity @ 75%\n============================================================\nComputing baseline accuracy... 0.6909\nAnalyzing 21 layers for sparsity @ 75% (granularity=weight, criteria=abs)\n  [1/21] conv1... Î”=+0.0020\n  [2/21] layer1.0.conv1... Î”=-0.0142\n  [3/21] layer1.0.conv2... Î”=-0.0076\n  [4/21] layer1.1.conv1... Î”=-0.0119\n  [5/21] layer1.1.conv2... Î”=-0.0053\n  [6/21] layer2.0.conv1... Î”=-0.0100\n  [7/21] layer2.0.conv2... Î”=-0.0100\n  [8/21] layer2.0.downsample.0... Î”=-0.0090\n  [9/21] layer2.1.conv1... Î”=-0.0107\n  [10/21] layer2.1.conv2... Î”=-0.0007\n  [11/21] layer3.0.conv1... Î”=-0.0200\n  [12/21] layer3.0.conv2... Î”=+0.0011\n  [13/21] layer3.0.downsample.0... Î”=+0.0008\n  [14/21] layer3.1.conv1... Î”=+0.0050\n  [15/21] layer3.1.conv2... Î”=-0.0010\n  [16/21] layer4.0.conv1... Î”=+0.0087\n  [17/21] layer4.0.conv2... Î”=-0.0084\n  [18/21] layer4.0.downsample.0... Î”=-0.0028\n  [19/21] layer4.1.conv1... Î”=+0.0074\n  [20/21] layer4.1.conv2... Î”=-0.0006\n  [21/21] fc... Î”=-2.6806\nâœ“ Analysis complete",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#exporting-results",
    "href": "tutorials/analyze/sensitivity.html#exporting-results",
    "title": "Sensitivity Analysis",
    "section": "10. Exporting Results",
    "text": "10. Exporting Results\n\ndf = result.to_dataframe()\ndf.head()\n\n\n\n\n\n\n\n\nname\nlayer_type\nparams\nbaseline_metric\ncompressed_metric\ndelta\n\n\n\n\n0\nconv1\nConv2d\n9408\n1.184846\n1.178427\n0.006419\n\n\n1\nlayer1.0.conv1\nConv2d\n36864\n1.184846\n1.283498\n-0.098653\n\n\n2\nlayer1.0.conv2\nConv2d\n36864\n1.184846\n1.249760\n-0.064914\n\n\n3\nlayer1.1.conv1\nConv2d\n36864\n1.184846\n1.262704\n-0.077859\n\n\n4\nlayer1.1.conv2\nConv2d\n36864\n1.184846\n1.120000\n0.064846",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#summary",
    "href": "tutorials/analyze/sensitivity.html#summary",
    "title": "Sensitivity Analysis",
    "section": "Summary",
    "text": "Summary\n\n\n\nFunction/Method\nPurpose\n\n\n\n\nanalyze_sensitivity()\nOne-line sensitivity analysis\n\n\nSensitivityAnalyzer\nFull control over analysis\n\n\nresult.summary()\nPrint formatted summary\n\n\nresult.top(n, most_sensitive)\nGet top N layers\n\n\nresult.plot()\nVisualize sensitivity\n\n\nresult.to_layer_targets()\nGenerate non-uniform per-layer compression targets\n\n\nresult.to_dataframe()\nExport to pandas\n\n\nanalyzer.sweep()\nTest multiple compression levels",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/analyze/sensitivity.html#see-also",
    "href": "tutorials/analyze/sensitivity.html#see-also",
    "title": "Sensitivity Analysis",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Apply sparsity to models\nPruner - Structural pruning\nCriteria - Importance scoring methods\nSchedules - Gradual compression schedules",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Analyze",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html",
    "href": "tutorials/export/onnx_export.html",
    "title": "ONNX Export Tutorial",
    "section": "",
    "text": "After compressing a model with fasterai, youâ€™ll want to deploy it. ONNX (Open Neural Network Exchange) is the standard format for deploying models across different platforms and runtimes.\n\n\n\n\n\n\n\n\n\nBenefit\nDescription\n\n\n\n\nPortability\nRun on any platform: servers, mobile, edge devices, browsers\n\n\nPerformance\nONNX Runtime is highly optimized for inference\n\n\nQuantization\nApply additional INT8 quantization during export\n\n\nNo Python needed\nDeploy without Python dependencies\n\n\n\n\n\n\nTrain â†’ Compress (prune/sparsify/quantize) â†’ Fold BN â†’ Export ONNX â†’ Deploy\nThis tutorial walks through the complete pipeline.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#overview",
    "href": "tutorials/export/onnx_export.html#overview",
    "title": "ONNX Export Tutorial",
    "section": "",
    "text": "After compressing a model with fasterai, youâ€™ll want to deploy it. ONNX (Open Neural Network Exchange) is the standard format for deploying models across different platforms and runtimes.\n\n\n\n\n\n\n\n\n\nBenefit\nDescription\n\n\n\n\nPortability\nRun on any platform: servers, mobile, edge devices, browsers\n\n\nPerformance\nONNX Runtime is highly optimized for inference\n\n\nQuantization\nApply additional INT8 quantization during export\n\n\nNo Python needed\nDeploy without Python dependencies\n\n\n\n\n\n\nTrain â†’ Compress (prune/sparsify/quantize) â†’ Fold BN â†’ Export ONNX â†’ Deploy\nThis tutorial walks through the complete pipeline.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#setup-and-training",
    "href": "tutorials/export/onnx_export.html#setup-and-training",
    "title": "ONNX Export Tutorial",
    "section": "1. Setup and Training",
    "text": "1. Setup and Training\nFirst, letâ€™s train a model that weâ€™ll later compress and export.\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nCould not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\n\n\n\n---------------------------------------------------------------------------\nAcceleratorError                          Traceback (most recent call last)\nCell In[2], line 6\n      2 files = get_image_files(path/\"images\")\n      4 def label_func(f): return f[0].isupper()\n----&gt; 6 dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/vision/data.py:150, in ImageDataLoaders.from_name_func(cls, path, fnames, label_func, **kwargs)\n    148     raise ValueError(\"label_func couldn't be lambda function on Windows\")\n    149 f = using_attr(label_func, 'name')\n--&gt; 150 return cls.from_path_func(path, fnames, f, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/vision/data.py:136, in ImageDataLoaders.from_path_func(cls, path, fnames, label_func, valid_pct, seed, item_tfms, batch_tfms, img_cls, **kwargs)\n    130 \"Create from list of `fnames` in `path`s with `label_func`\"\n    131 dblock = DataBlock(blocks=(ImageBlock(img_cls), CategoryBlock),\n    132                    splitter=RandomSplitter(valid_pct, seed=seed),\n    133                    get_y=label_func,\n    134                    item_tfms=item_tfms,\n    135                    batch_tfms=batch_tfms)\n--&gt; 136 return cls.from_dblock(dblock, fnames, path=path, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:280, in DataLoaders.from_dblock(cls, dblock, source, path, bs, val_bs, shuffle, device, **kwargs)\n    269 @classmethod\n    270 def from_dblock(cls, \n    271     dblock, # `DataBlock` object\n   (...)    278     **kwargs\n    279 ):\n--&gt; 280     return dblock.dataloaders(source, path=path, bs=bs, val_bs=val_bs, shuffle=shuffle, device=device, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/block.py:159, in DataBlock.dataloaders(self, source, path, verbose, **kwargs)\n    157 dsets = self.datasets(source, verbose=verbose)\n    158 kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n--&gt; 159 return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:333, in FilteredBase.dataloaders(self, bs, shuffle_train, shuffle, val_shuffle, n, path, dl_type, dl_kwargs, device, drop_last, val_bs, **kwargs)\n    331 dl = dl_type(self.subset(0), **merge(kwargs,def_kwargs, dl_kwargs[0]))\n    332 def_kwargs = {'bs':bs if val_bs is None else val_bs,'shuffle':val_shuffle,'n':None,'drop_last':False}\n--&gt; 333 dls = [dl] + [dl.new(self.subset(i), **merge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n    334               for i in range(1, self.n_subsets)]\n    335 return self._dbunch_type(*dls, path=path, device=device)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:104, in TfmdDL.new(self, dataset, cls, **kwargs)\n    102 if not hasattr(self, '_n_inp') or not hasattr(self, '_types'):\n    103     try:\n--&gt; 104         self._one_pass()\n    105         res._n_inp,res._types = self._n_inp,self._types\n    106     except Exception as e: \n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:86, in TfmdDL._one_pass(self)\n     84 def _one_pass(self):\n     85     b = self.do_batch([self.do_item(None)])\n---&gt; 86     if self.device is not None: b = to_device(b, self.device)\n     87     its = self.after_batch(b)\n     88     self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:287, in to_device(b, device, non_blocking)\n    285     if isinstance(o,Tensor): return o.to(device, non_blocking=non_blocking)\n    286     return o\n--&gt; 287 return apply(_inner, b)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:224, in apply(func, x, *args, **kwargs)\n    222 def apply(func, x, *args, **kwargs):\n    223     \"Apply `func` recursively to `x`, passing on args\"\n--&gt; 224     if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n    225     if isinstance(x,(dict,MutableMapping)): return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n    226     res = func(x, *args, **kwargs)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:226, in apply(func, x, *args, **kwargs)\n    224 if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n    225 if isinstance(x,(dict,MutableMapping)): return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n--&gt; 226 res = func(x, *args, **kwargs)\n    227 return res if x is None else retain_type(res, x)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:285, in to_device.&lt;locals&gt;._inner(o)\n    283 def _inner(o):\n    284     # ToDo: add TensorDict when released\n--&gt; 285     if isinstance(o,Tensor): return o.to(device, non_blocking=non_blocking)\n    286     return o\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:384, in TensorBase.__torch_function__(cls, func, types, args, kwargs)\n    382 if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n    383 if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n--&gt; 384 res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n    385 dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n    386 if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n\nFile ~/miniconda3/envs/dev/lib/python3.12/site-packages/torch/_tensor.py:1654, in Tensor.__torch_function__(cls, func, types, args, kwargs)\n   1651     return NotImplemented\n   1653 with _C.DisableTorchFunctionSubclass():\n-&gt; 1654     ret = func(*args, **kwargs)\n   1655     if func in get_default_nowrap_functions():\n   1656         return ret\n\nAcceleratorError: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\nlearn.fit_one_cycle(3)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#compress-the-model",
    "href": "tutorials/export/onnx_export.html#compress-the-model",
    "title": "ONNX Export Tutorial",
    "section": "2. Compress the Model",
    "text": "2. Compress the Model\nApply sparsification to reduce model size. You could also use pruning, quantization, or any combination.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_cycle)\nlearn.fit_one_cycle(2, cbs=sp_cb)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#fold-batchnorm-layers",
    "href": "tutorials/export/onnx_export.html#fold-batchnorm-layers",
    "title": "ONNX Export Tutorial",
    "section": "3. Fold BatchNorm Layers",
    "text": "3. Fold BatchNorm Layers\nBefore export, fold batch normalization layers into convolutions for faster inference:\n\nbn_folder = BN_Folder()\nmodel = bn_folder.fold(learn.model)\nmodel.eval();",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#export-to-onnx",
    "href": "tutorials/export/onnx_export.html#export-to-onnx",
    "title": "ONNX Export Tutorial",
    "section": "4. Export to ONNX",
    "text": "4. Export to ONNX\nNow export the optimized model to ONNX format:\n\n# Create example input (batch_size=1, channels=3, height=64, width=64)\nsample = torch.randn(1, 3, 64, 64)\n\n# Export to ONNX\nonnx_path = export_onnx(model.cpu(), sample, \"model.onnx\")\nprint(f\"Exported to: {onnx_path}\")\n\n\nVerify the Export\nAlways verify that the ONNX model produces the same outputs as the PyTorch model:\n\nis_valid = verify_onnx(model, onnx_path, sample)\nprint(f\"Verification {'passed' if is_valid else 'FAILED'}: ONNX outputs {'match' if is_valid else 'do not match'} PyTorch!\")",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#export-with-int8-quantization",
    "href": "tutorials/export/onnx_export.html#export-with-int8-quantization",
    "title": "ONNX Export Tutorial",
    "section": "5. Export with INT8 Quantization",
    "text": "5. Export with INT8 Quantization\nFor even smaller models and faster inference, apply INT8 quantization during export:\n\n# Dynamic quantization (no calibration data needed)\nquantized_path = export_onnx(\n    model.cpu(), sample, \"model_int8.onnx\",\n    quantize=True,\n    quantize_mode=\"dynamic\"\n)\nprint(f\"Exported quantized model to: {quantized_path}\")\n\nFor better accuracy, use static quantization with calibration data:\n# Static quantization with calibration\nquantized_path = export_onnx(\n    model, sample, \"model_int8_static.onnx\",\n    quantize=True,\n    quantize_mode=\"static\",\n    calibration_data=dls.train  # Use training data for calibration\n)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#compare-model-sizes",
    "href": "tutorials/export/onnx_export.html#compare-model-sizes",
    "title": "ONNX Export Tutorial",
    "section": "6. Compare Model Sizes",
    "text": "6. Compare Model Sizes\n\nimport os\n\ndef get_size_mb(path):\n    return os.path.getsize(path) / 1e6\n\n# Save PyTorch model for comparison\ntorch.save(model.state_dict(), \"model.pt\")\n\npt_size = get_size_mb(\"model.pt\")\nonnx_size = get_size_mb(\"model.onnx\")\nint8_size = get_size_mb(quantized_path)\n\nprint(f\"PyTorch model:    {pt_size:.2f} MB\")\nprint(f\"ONNX model:       {onnx_size:.2f} MB\")\nprint(f\"ONNX INT8 model:  {int8_size:.2f} MB ({pt_size/int8_size:.1f}x smaller)\")",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#running-inference-with-onnx-runtime",
    "href": "tutorials/export/onnx_export.html#running-inference-with-onnx-runtime",
    "title": "ONNX Export Tutorial",
    "section": "7. Running Inference with ONNX Runtime",
    "text": "7. Running Inference with ONNX Runtime\nUse the ONNXModel wrapper for easy inference:\n\n# Load the ONNX model\nonnx_model = ONNXModel(\"model.onnx\", device=\"cpu\")\n\n# Run inference\ntest_input = torch.randn(1, 3, 64, 64)\noutput = onnx_model(test_input)\n\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Predictions: {output}\")\n\n\nBenchmark Inference Speed\n\nimport time\n\ndef benchmark(fn, input_tensor, warmup=10, runs=100):\n    # Warmup\n    for _ in range(warmup):\n        fn(input_tensor)\n    \n    # Benchmark\n    start = time.perf_counter()\n    for _ in range(runs):\n        fn(input_tensor)\n    elapsed = (time.perf_counter() - start) / runs * 1000\n    return elapsed\n\ntest_input = torch.randn(1, 3, 64, 64)\n\n# PyTorch\nmodel.eval()\nwith torch.no_grad():\n    pt_time = benchmark(model, test_input)\n\n# ONNX\nonnx_model = ONNXModel(\"model.onnx\")\nonnx_time = benchmark(onnx_model, test_input)\n\n# ONNX INT8\nonnx_int8 = ONNXModel(quantized_path)\nint8_time = benchmark(onnx_int8, test_input)\n\nprint(f\"PyTorch inference: {pt_time:.2f} ms\")\nprint(f\"ONNX inference:    {onnx_time:.2f} ms ({pt_time/onnx_time:.1f}x faster)\")\nprint(f\"ONNX INT8:         {int8_time:.2f} ms ({pt_time/int8_time:.1f}x faster)\")",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#parameter-reference",
    "href": "tutorials/export/onnx_export.html#parameter-reference",
    "title": "ONNX Export Tutorial",
    "section": "8. Parameter Reference",
    "text": "8. Parameter Reference\n\nexport_onnx Parameters\n\n\n\n\n\n\n\n\nParameter\nDefault\nDescription\n\n\n\n\nmodel\nRequired\nPyTorch model to export\n\n\nsample\nRequired\nExample input tensor (with batch dimension)\n\n\noutput_path\nRequired\nOutput .onnx file path\n\n\nopset_version\n18\nONNX opset version\n\n\nquantize\nFalse\nApply INT8 quantization after export\n\n\nquantize_mode\n\"dynamic\"\n\"dynamic\" (no calibration) or \"static\"\n\n\ncalibration_data\nNone\nDataLoader for static quantization\n\n\noptimize\nTrue\nRun ONNX graph optimizer\n\n\ndynamic_batch\nTrue\nAllow variable batch size at runtime\n\n\n\n\n\nQuantization Mode Comparison\n\n\n\nMode\nCalibration\nAccuracy\nSpeed\nUse Case\n\n\n\n\ndynamic\nNot needed\nGood\nFast export\nQuick deployment\n\n\nstatic\nRequired\nBetter\nSlower export\nProduction models",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#summary",
    "href": "tutorials/export/onnx_export.html#summary",
    "title": "ONNX Export Tutorial",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\nStep\nTool\nPurpose\n\n\n\n\nCompress\nSparsifyCallback, PruneCallback, etc.\nReduce model complexity\n\n\nFold BN\nBN_Folder\nEliminate batch norm overhead\n\n\nExport\nexport_onnx\nConvert to deployment format\n\n\nVerify\nverify_onnx\nEnsure correctness\n\n\nQuantize\nquantize=True\nFurther reduce size (4x)\n\n\nDeploy\nONNXModel\nRun inference\n\n\n\n\nComplete Pipeline Example\nfrom fasterai.sparse.all import *\nfrom fasterai.misc.all import *\nfrom fasterai.export.all import *\n\n# 1. Train with compression\nsp_cb = SparsifyCallback(sparsity=50, granularity='weight', ...)\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\n# 2. Fold batch norm\nmodel = BN_Folder().fold(learn.model)\n\n# 3. Export with quantization\nsample = torch.randn(1, 3, 224, 224)\npath = export_onnx(model, sample, \"model_int8.onnx\", quantize=True)\n\n# 4. Verify\nassert verify_onnx(model, path, sample)\n\n# 5. Deploy\nonnx_model = ONNXModel(path)\noutput = onnx_model(input_tensor)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/export/onnx_export.html#see-also",
    "href": "tutorials/export/onnx_export.html#see-also",
    "title": "ONNX Export Tutorial",
    "section": "See Also",
    "text": "See Also\n\nONNX Exporter API - Detailed API reference\nBN Folding - Fold batch norm before export\nCPU Optimizer - Alternative: TorchScript for CPU deployment\nSparsify Callback - Compress before export\nQuantize Callback - QAT before export",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Export",
      "ONNX Export Tutorial"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html",
    "href": "tutorials/sparse/sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the networkâ€™s weight are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\nLetâ€™s start by creating a model\nmodel = resnet18()\nAs you probably know, weights in a convolutional neural network have 4 dimensions ($ c_{out} c_{in} k_h k_w$)\nmodel.conv1.weight.ndim\n\n4\nIn the case of ResNet18, the dimension of the first layer weights is \\(64 \\times 3 \\times 7 \\times 7\\). We thus can plot each of the \\(64\\) filter as a \\(7 \\times 7\\) color image (because they contains \\(3\\) channels).\nplot_kernels(model.conv1)\nThe Sparsifier class allows us to remove some (part of) the filters, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\nUser can pass a single layer to prune by using the Sparsifier.sparsify_layer method.\nFound permutation search CUDA kernels [ASP][Info] permutation_search_kernels can be imported. â€”\nsource",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#granularity",
    "href": "tutorials/sparse/sparsifier.html#granularity",
    "title": "Sparsifier",
    "section": "Granularity",
    "text": "Granularity\nAs we said earlier, the granularity defines the structure of parameter that you will remove.\nIn the example below, we removed weight from each convolutional filter, meaning that we now have sparse filters, as can be seen in the image below:\n\nplot_kernels(model.conv1)\n\n\n\n\n\n\n\n\nAnother granularity is, for example, removing column vectors from the filters. To do so, just change the granularity parameter accordingly.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'column', 'local', large_final)\nsparsifier.sparsify_layer(model.conv1, 70)\n\n\nplot_kernels(model.conv1)\n\n\n\n\n\n\n\n\nFor more information and examples about the pruning granularities, I suggest you to take a look at the corresponding section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#context",
    "href": "tutorials/sparse/sparsifier.html#context",
    "title": "Sparsifier",
    "section": "Context",
    "text": "Context\nThe context defines where to look in the model, i.e.Â from where do we compare weight. The two basic contexts are: * local, i.e.Â we compare weight from each layer individually. This will lead to layers with similar levels of sparsity. * global, i.e.Â we compare weight from the whole model. This will lead to layers with different levels of sparsity\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'local', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      6,585         69.99%\nlayer1.0.conv1                 Conv2d          36,864     25,805        70.00%\nlayer1.0.conv2                 Conv2d          36,864     25,805        70.00%\nlayer1.1.conv1                 Conv2d          36,864     25,805        70.00%\nlayer1.1.conv2                 Conv2d          36,864     25,805        70.00%\nlayer2.0.conv1                 Conv2d          73,728     51,609        70.00%\nlayer2.0.conv2                 Conv2d          147,456    103,219       70.00%\nlayer2.0.downsample.0          Conv2d          8,192      5,734         70.00%\nlayer2.1.conv1                 Conv2d          147,456    103,219       70.00%\nlayer2.1.conv2                 Conv2d          147,456    103,219       70.00%\nlayer3.0.conv1                 Conv2d          294,912    206,438       70.00%\nlayer3.0.conv2                 Conv2d          589,824    412,877       70.00%\nlayer3.0.downsample.0          Conv2d          32,768     22,937        70.00%\nlayer3.1.conv1                 Conv2d          589,824    412,877       70.00%\nlayer3.1.conv2                 Conv2d          589,824    412,877       70.00%\nlayer4.0.conv1                 Conv2d          1,179,648  825,753       70.00%\nlayer4.0.conv2                 Conv2d          2,359,296  1,651,506     70.00%\nlayer4.0.downsample.0          Conv2d          131,072    91,750        70.00%\nlayer4.1.conv1                 Conv2d          2,359,296  1,651,507     70.00%\nlayer4.1.conv2                 Conv2d          2,359,296  1,651,507     70.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 7,816,834     70.00%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      6,214         66.05%\nlayer1.0.conv1                 Conv2d          36,864     11,786        31.97%\nlayer1.0.conv2                 Conv2d          36,864     11,864        32.18%\nlayer1.1.conv1                 Conv2d          36,864     11,806        32.03%\nlayer1.1.conv2                 Conv2d          36,864     11,831        32.09%\nlayer2.0.conv1                 Conv2d          73,728     32,757        44.43%\nlayer2.0.conv2                 Conv2d          147,456    64,894        44.01%\nlayer2.0.downsample.0          Conv2d          8,192      1,234         15.06%\nlayer2.1.conv1                 Conv2d          147,456    64,982        44.07%\nlayer2.1.conv2                 Conv2d          147,456    65,301        44.29%\nlayer3.0.conv1                 Conv2d          294,912    174,570       59.19%\nlayer3.0.conv2                 Conv2d          589,824    349,497       59.25%\nlayer3.0.downsample.0          Conv2d          32,768     7,208         22.00%\nlayer3.1.conv1                 Conv2d          589,824    349,981       59.34%\nlayer3.1.conv2                 Conv2d          589,824    349,240       59.21%\nlayer4.0.conv1                 Conv2d          1,179,648  894,898       75.86%\nlayer4.0.conv2                 Conv2d          2,359,296  1,788,755     75.82%\nlayer4.0.downsample.0          Conv2d          131,072    39,958        30.49%\nlayer4.1.conv1                 Conv2d          2,359,296  1,790,109     75.87%\nlayer4.1.conv2                 Conv2d          2,359,296  1,789,953     75.87%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 7,816,838     70.00%",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#criteria",
    "href": "tutorials/sparse/sparsifier.html#criteria",
    "title": "Sparsifier",
    "section": "Criteria",
    "text": "Criteria\nThe criteria defines how we select the parameters to remove. It is usually given by a scoring method. The most common one is the large_final, i.e.Â select parameters with the highest absolute value as they are supposed to contribute the most to the final results of the model.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', large_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      6,325         67.23%\nlayer1.0.conv1                 Conv2d          36,864     11,915        32.32%\nlayer1.0.conv2                 Conv2d          36,864     11,815        32.05%\nlayer1.1.conv1                 Conv2d          36,864     11,965        32.46%\nlayer1.1.conv2                 Conv2d          36,864     11,990        32.52%\nlayer2.0.conv1                 Conv2d          73,728     32,395        43.94%\nlayer2.0.conv2                 Conv2d          147,456    65,275        44.27%\nlayer2.0.downsample.0          Conv2d          8,192      1,279         15.61%\nlayer2.1.conv1                 Conv2d          147,456    64,888        44.00%\nlayer2.1.conv2                 Conv2d          147,456    65,148        44.18%\nlayer3.0.conv1                 Conv2d          294,912    174,785       59.27%\nlayer3.0.conv2                 Conv2d          589,824    349,838       59.31%\nlayer3.0.downsample.0          Conv2d          32,768     7,069         21.57%\nlayer3.1.conv1                 Conv2d          589,824    350,378       59.40%\nlayer3.1.conv2                 Conv2d          589,824    349,638       59.28%\nlayer4.0.conv1                 Conv2d          1,179,648  894,232       75.80%\nlayer4.0.conv2                 Conv2d          2,359,296  1,789,714     75.86%\nlayer4.0.downsample.0          Conv2d          131,072    39,670        30.27%\nlayer4.1.conv1                 Conv2d          2,359,296  1,789,491     75.85%\nlayer4.1.conv2                 Conv2d          2,359,296  1,789,027     75.83%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 7,816,837     70.00%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'weight', 'global', small_final)\nsparsifier.sparsify_model(70)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      9,407         99.99%\nlayer1.0.conv1                 Conv2d          36,864     456            1.24%\nlayer1.0.conv2                 Conv2d          36,864     327            0.89%\nlayer1.1.conv1                 Conv2d          36,864     435            1.18%\nlayer1.1.conv2                 Conv2d          36,864     905            2.45%\nlayer2.0.conv1                 Conv2d          73,728     4,653          6.31%\nlayer2.0.conv2                 Conv2d          147,456    6,854          4.65%\nlayer2.0.downsample.0          Conv2d          8,192      8              0.10%\nlayer2.1.conv1                 Conv2d          147,456    6,538          4.43%\nlayer2.1.conv2                 Conv2d          147,456    9,241          6.27%\nlayer3.0.conv1                 Conv2d          294,912    83,006        28.15%\nlayer3.0.conv2                 Conv2d          589,824    22,507         3.82%\nlayer3.0.downsample.0          Conv2d          32,768     11             0.03%\nlayer3.1.conv1                 Conv2d          589,824    47,880         8.12%\nlayer3.1.conv2                 Conv2d          589,824    105,624       17.91%\nlayer4.0.conv1                 Conv2d          1,179,648  1,094,504     92.78%\nlayer4.0.conv2                 Conv2d          2,359,296  2,143,119     90.84%\nlayer4.0.downsample.0          Conv2d          131,072    378            0.29%\nlayer4.1.conv1                 Conv2d          2,359,296  1,921,688     81.45%\nlayer4.1.conv2                 Conv2d          2,359,296  2,359,296    100.00%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 7,816,837     70.00%\n\n\nFor more information and examples about the pruning criteria, I suggest you to take a look at the corresponding section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#remark",
    "href": "tutorials/sparse/sparsifier.html#remark",
    "title": "Sparsifier",
    "section": "Remark",
    "text": "Remark\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of 8, this can be done by passing the round_to parameter.\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'filter', 'local', large_final)\nsparsifier.sparsify_model(70, round_to=8)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      5,880         62.50%\nlayer1.0.conv1                 Conv2d          36,864     23,040        62.50%\nlayer1.0.conv2                 Conv2d          36,864     23,040        62.50%\nlayer1.1.conv1                 Conv2d          36,864     23,040        62.50%\nlayer1.1.conv2                 Conv2d          36,864     23,040        62.50%\nlayer2.0.conv1                 Conv2d          73,728     50,688        68.75%\nlayer2.0.conv2                 Conv2d          147,456    101,376       68.75%\nlayer2.0.downsample.0          Conv2d          8,192      5,632         68.75%\nlayer2.1.conv1                 Conv2d          147,456    101,376       68.75%\nlayer2.1.conv2                 Conv2d          147,456    101,376       68.75%\nlayer3.0.conv1                 Conv2d          294,912    202,754       68.75%\nlayer3.0.conv2                 Conv2d          589,824    405,504       68.75%\nlayer3.0.downsample.0          Conv2d          32,768     22,528        68.75%\nlayer3.1.conv1                 Conv2d          589,824    405,504       68.75%\nlayer3.1.conv2                 Conv2d          589,824    405,504       68.75%\nlayer4.0.conv1                 Conv2d          1,179,648  811,008       68.75%\nlayer4.0.conv2                 Conv2d          2,359,296  1,622,016     68.75%\nlayer4.0.downsample.0          Conv2d          131,072    90,112        68.75%\nlayer4.1.conv1                 Conv2d          2,359,296  1,622,016     68.75%\nlayer4.1.conv2                 Conv2d          2,359,296  1,622,016     68.75%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 7,667,450     68.66%\n\n\n\nmodel = resnet18()\nsparsifier = Sparsifier(model, 'filter', 'global', large_final)\nsparsifier.sparsify_model(70, round_to=8)\n\n\nsparsifier.print_sparsity()\n\n\nSparsity Report:\n--------------------------------------------------------------------------------\nLayer                          Type            Params     Zeros      Sparsity  \n--------------------------------------------------------------------------------\nconv1                          Conv2d          9,408      8,232         87.50%\nlayer1.0.conv1                 Conv2d          36,864     0              0.00%\nlayer1.0.conv2                 Conv2d          36,864     0              0.00%\nlayer1.1.conv1                 Conv2d          36,864     0              0.00%\nlayer1.1.conv2                 Conv2d          36,864     0              0.00%\nlayer2.0.conv1                 Conv2d          73,728     69,120        93.75%\nlayer2.0.conv2                 Conv2d          147,456    138,240       93.75%\nlayer2.0.downsample.0          Conv2d          8,192      0              0.00%\nlayer2.1.conv1                 Conv2d          147,456    138,240       93.75%\nlayer2.1.conv2                 Conv2d          147,456    138,240       93.75%\nlayer3.0.conv1                 Conv2d          294,912    285,696       96.88%\nlayer3.0.conv2                 Conv2d          589,824    571,392       96.88%\nlayer3.0.downsample.0          Conv2d          32,768     0              0.00%\nlayer3.1.conv1                 Conv2d          589,824    571,392       96.88%\nlayer3.1.conv2                 Conv2d          589,824    571,392       96.88%\nlayer4.0.conv1                 Conv2d          1,179,648  1,161,216     98.44%\nlayer4.0.conv2                 Conv2d          2,359,296  2,322,432     98.44%\nlayer4.0.downsample.0          Conv2d          131,072    0              0.00%\nlayer4.1.conv1                 Conv2d          2,359,296  2,322,432     98.44%\nlayer4.1.conv2                 Conv2d          2,359,296  2,285,568     96.88%\n--------------------------------------------------------------------------------\nOverall                        all             11,166,912 10,583,592    94.78%\n\n\nFor more information about granularities at which you can operate, please check the related page.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#summary",
    "href": "tutorials/sparse/sparsifier.html#summary",
    "title": "Sparsifier",
    "section": "Summary",
    "text": "Summary\n\n\n\nTool\nPurpose\n\n\n\n\nSparsifier\nCore class for zeroing out weights\n\n\nsparsify_model()\nApply sparsification to all matching layers\n\n\nsparsify_layer()\nApply sparsification to a single layer\n\n\nprint_sparsity()\nReport per-layer sparsity statistics\n\n\nlarge_final\nCriteria: keep weights with largest magnitude\n\n\nGranularity options\nweight, vector, kernel, filter\n\n\nContext options\nlocal (per-layer) vs global (network-wide)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/sparsifier.html#see-also",
    "href": "tutorials/sparse/sparsifier.html#see-also",
    "title": "Sparsifier",
    "section": "See Also",
    "text": "See Also\n\nSparsifier API - Full Sparsifier API reference\nSparsifyCallback - Integrate sparsification into training\nSchedules - Control sparsification progression\nCriteria - All importance scoring methods\nGranularity - What gets sparsified at each level",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Sparsifier"
    ]
  },
  {
    "objectID": "tutorials/sparse/transformers.html",
    "href": "tutorials/sparse/transformers.html",
    "title": "Prune Transformers",
    "section": "",
    "text": "Note\n\n\n\nThis example code is taken from the fastai docs\npretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)\npath = untar_data(URLs.WIKITEXT_TINY)\nLetâ€™s create our fastai Learner.\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())\nAnd letâ€™s try to extend a given prompt with the pretrained model.\nprompt = \"\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn\"\npreds = learn.model.generate(inp, max_length=40, num_beams=5, temperature=1.5)\ntokenizer.decode(preds[0].cpu().numpy())\nlearn.validate()\nlearn.fit_one_cycle(1, 1e-4)\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None]\n\npreds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n\ntokenizer.decode(preds[0].cpu().numpy())",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "tutorials/sparse/transformers.html#make-it-sparse",
    "href": "tutorials/sparse/transformers.html#make-it-sparse",
    "title": "Prune Transformers",
    "section": "Make it sparse !",
    "text": "Make it sparse !\nLetâ€™s see now if we retrain our model, this time introducing sparsity\n\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())\n\nUnfortunately, the transformer model uses a custom layer: Conv1D, which is not a part of PyTorch. To overcome this problem, we have to add this layer to our Granularities class, so that it knows what to sparsify.\nHere, the Conv1D behaves like a Linear layer, i.e.Â the weights are defined by a matrix of dimension (nf,nx)\n\ndoc(Conv1D)\n\nWe can thus add the Conv1D granularity by using the add_granularity method, indicating the target module and the corresponding granularities that it can handle (the same as Linear so we can reuse it)\n\nGranularities.add_granularity(Conv1D, Granularities._granularities_Linear)\n\nLetâ€™s now define our SparsifyCallback. Letâ€™s say we want to make our model 30% sparse, by removing the highest-norm weight in each attention head.\n\nsp_cb = SparsifyCallback(sparsity=30, granularity='weight', context='local', criteria=large_final, schedule=one_cycle, layer_type=Conv1D)\n\nWe now only have to pass our callback to fastai\n\nlearn.fit_one_cycle(1, 1e-4, cbs=sp_cb)\n\nAnd we can check the predicion to the same prompt as before\n\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None]\n\npreds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n\ntokenizer.decode(preds[0].cpu().numpy())\n\nThatâ€™s it ! You now have a sparse Transformer as performant as the whole model. However, this model is currently not more efficient speed and storage wise. To have such a speed-up, I suggest you to look at the granularity section.",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "tutorials/sparse/transformers.html#summary",
    "href": "tutorials/sparse/transformers.html#summary",
    "title": "Prune Transformers",
    "section": "Summary",
    "text": "Summary\n\n\n\nTool\nPurpose\n\n\n\n\nSparsifyCallback\nSparsify transformer weights during training\n\n\nSparsifier\nCore sparsification class for applying masks\n\n\nlarge_final\nCriteria keeping weights with largest magnitude\n\n\none_shot / cos\nScheduling: one-shot vs gradual sparsification\n\n\nprint_sparsity()\nReport per-layer sparsity statistics",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "tutorials/sparse/transformers.html#see-also",
    "href": "tutorials/sparse/transformers.html#see-also",
    "title": "Prune Transformers",
    "section": "See Also",
    "text": "See Also\n\nSparsifier - Core sparsification API\nSparsifyCallback - Callback for training integration\nSchedules - All available pruning schedules\nSparsifier Tutorial - Basic sparsification walkthrough",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Sparse",
      "Prune Transformers"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html",
    "href": "tutorials/misc/fc_decomposer.html",
    "title": "Fully-Connected layers decomposition",
    "section": "",
    "text": "FC Layer Decomposition uses Singular Value Decomposition (SVD) to factorize large fully-connected layers into smaller, more efficient layers. This is particularly effective for models with large FC layers like VGG.\n\n\nA weight matrix \\(W \\in \\mathbb{R}^{m \\times n}\\) is decomposed as: \\[W \\approx U \\cdot S \\cdot V^T\\]\nBy keeping only the top-\\(k\\) singular values, we replace one large layer with two smaller layers: - Original: Linear(n â†’ m) with \\(m \\times n\\) parameters - Decomposed: Linear(n â†’ k) + Linear(k â†’ m) with \\(k \\times (m + n)\\) parameters\nWhen \\(k &lt;&lt; \\min(m, n)\\), this significantly reduces parameters.\n\n\n\n\n\n\nModel Type\nFC Layer Size\nRecommendation\n\n\n\n\nVGG-style\nVery large (4096Ã—4096)\nâœ… Highly effective\n\n\nResNet-style\nSmall (512Ã—classes)\nâŒ Not needed\n\n\nTransformers\nMedium (hiddenÃ—4Ã—hidden)\nâš ï¸ May help\n\n\n\nBest for: Models where FC layers dominate the parameter count (e.g., VGG has ~90% of parameters in FC layers).",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#overview",
    "href": "tutorials/misc/fc_decomposer.html#overview",
    "title": "Fully-Connected layers decomposition",
    "section": "",
    "text": "FC Layer Decomposition uses Singular Value Decomposition (SVD) to factorize large fully-connected layers into smaller, more efficient layers. This is particularly effective for models with large FC layers like VGG.\n\n\nA weight matrix \\(W \\in \\mathbb{R}^{m \\times n}\\) is decomposed as: \\[W \\approx U \\cdot S \\cdot V^T\\]\nBy keeping only the top-\\(k\\) singular values, we replace one large layer with two smaller layers: - Original: Linear(n â†’ m) with \\(m \\times n\\) parameters - Decomposed: Linear(n â†’ k) + Linear(k â†’ m) with \\(k \\times (m + n)\\) parameters\nWhen \\(k &lt;&lt; \\min(m, n)\\), this significantly reduces parameters.\n\n\n\n\n\n\nModel Type\nFC Layer Size\nRecommendation\n\n\n\n\nVGG-style\nVery large (4096Ã—4096)\nâœ… Highly effective\n\n\nResNet-style\nSmall (512Ã—classes)\nâŒ Not needed\n\n\nTransformers\nMedium (hiddenÃ—4Ã—hidden)\nâš ï¸ May help\n\n\n\nBest for: Models where FC layers dominate the parameter count (e.g., VGG has ~90% of parameters in FC layers).",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#setup-and-data",
    "href": "tutorials/misc/fc_decomposer.html#setup-and-data",
    "title": "Fully-Connected layers decomposition",
    "section": "1. Setup and Data",
    "text": "1. Setup and Data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#train-the-model",
    "href": "tutorials/misc/fc_decomposer.html#train-the-model",
    "title": "Fully-Connected layers decomposition",
    "section": "2. Train the Model",
    "text": "2. Train the Model\nWe use VGG16 with batch normalization - a model with very large FC layers:\n\nlearn = Learner(dls, vgg16_bn(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(5, 1e-5)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.696439\n0.601888\n0.686739\n00:03\n\n\n1\n0.654363\n0.559391\n0.701624\n00:03\n\n\n2\n0.624809\n0.566203\n0.697564\n00:03\n\n\n3\n0.592064\n0.534797\n0.730717\n00:03\n\n\n4\n0.591283\n0.531738\n0.736130\n00:03",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#apply-fc-decomposition",
    "href": "tutorials/misc/fc_decomposer.html#apply-fc-decomposition",
    "title": "Fully-Connected layers decomposition",
    "section": "3. Apply FC Decomposition",
    "text": "3. Apply FC Decomposition\nUse FC_Decomposer to factorize the fully-connected layers:\n\nfc = FC_Decomposer()\nnew_model = fc.decompose(learn.model)\n\nNotice how each FC layer is now replaced by a Sequential of two smaller layers. For example: - Original: Linear(25088 â†’ 4096) = 102M parameters - Decomposed: Linear(25088 â†’ 2048) + Linear(2048 â†’ 4096) = 59M parameters\n\nnew_model\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=25088, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Sequential(\n      (0): Linear(in_features=4096, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Sequential(\n      (0): Linear(in_features=4096, out_features=1, bias=False)\n      (1): Linear(in_features=1, out_features=2, bias=True)\n    )\n  )\n)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#compare-results",
    "href": "tutorials/misc/fc_decomposer.html#compare-results",
    "title": "Fully-Connected layers decomposition",
    "section": "4. Compare Results",
    "text": "4. Compare Results\n\nParameter Reduction\n\ncount_parameters(learn.model)\n\n134277186\n\n\n\ncount_parameters(new_model)\n\n91281476\n\n\nA reduction of ~43 million parameters (~32% smaller)!\n\n\nAccuracy Trade-off\nSVD decomposition is an approximation, so some accuracy loss is expected. The accuracy depends on how many singular values are retained:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\nnew_learn.validate()\n\n\n\n\n\n\n\n\n[0.5516967177391052, 0.7050067782402039]\n\n\nThe accuracy drop from ~90% to ~68% is significant. To recover accuracy, fine-tune the decomposed model:\nnew_learn = Learner(dls, new_model, metrics=accuracy)\nnew_learn.fit_one_cycle(5, 1e-4)  # Fine-tune with small learning rate",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#parameter-reference",
    "href": "tutorials/misc/fc_decomposer.html#parameter-reference",
    "title": "Fully-Connected layers decomposition",
    "section": "5. Parameter Reference",
    "text": "5. Parameter Reference\n\nFC_Decomposer Parameters\n\n\n\n\n\n\n\n\nParameter\nDefault\nDescription\n\n\n\n\nrank_ratio\n0.5\nFraction of singular values to keep (0-1). Lower = more compression, more accuracy loss\n\n\n\n\n\nChoosing rank_ratio\n\n\n\nrank_ratio\nCompression\nAccuracy Impact\n\n\n\n\n0.8\nLow\nMinimal\n\n\n0.5\nMedium\nModerate\n\n\n0.25\nHigh\nSignificant (requires fine-tuning)",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#summary",
    "href": "tutorials/misc/fc_decomposer.html#summary",
    "title": "Fully-Connected layers decomposition",
    "section": "Summary",
    "text": "Summary\n\n\n\nMetric\nOriginal VGG16\nDecomposed\nChange\n\n\n\n\nParameters\n134M\n91M\n-32%\n\n\nFC Layer Params\n~120M\n~77M\n-36%\n\n\nAccuracy (before fine-tune)\n90%\n68%\nNeeds fine-tuning\n\n\n\n\nRecommended Workflow\nfrom fasterai.misc.all import *\n\n# 1. Train model\nlearn.fit_one_cycle(5)\n\n# 2. Decompose FC layers\nfc = FC_Decomposer(rank_ratio=0.5)\nnew_model = fc.decompose(learn.model)\n\n# 3. Fine-tune to recover accuracy\nnew_learn = Learner(dls, new_model, metrics=accuracy)\nnew_learn.fit_one_cycle(3, 1e-4)\n\n# 4. (Optional) Apply other compressions\n# - Pruning, sparsification, quantization",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/misc/fc_decomposer.html#see-also",
    "href": "tutorials/misc/fc_decomposer.html#see-also",
    "title": "Fully-Connected layers decomposition",
    "section": "See Also",
    "text": "See Also\n\nBN Folding - Combine with BN folding for more optimization\nPruner - Apply structured pruning after decomposition\nONNX Export - Export optimized model for deployment\nSparsifier - Add sparsity for further compression",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Misc",
      "Fully-Connected layers decomposition"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html",
    "href": "tutorials/distill/distill_callback.html",
    "title": "KnowledgeDistillation Callback",
    "section": "",
    "text": "Knowledge Distillation transfers knowledge from a large, accurate â€œteacherâ€ model to a smaller, faster â€œstudentâ€ model. The student learns not just from ground truth labels, but also from the teacherâ€™s soft predictionsâ€”capturing the teacherâ€™s learned relationships between classes.\n\n\n\n\n\nApproach\nModel Size\nTraining Data\nAccuracy\n\n\n\n\nTrain small model alone\nSmall\nLabels only\nLower\n\n\nDistillation\nSmall\nLabels + Teacher knowledge\nHigher\n\n\n\n\n\n\n\nSmaller deployment models - Student can be much smaller than teacher\nBetter than training from scratch - Teacher provides richer supervision\nNo additional labeled data needed - Uses existing training set\nFlexible loss functions - Soft targets, attention transfer, feature matching\n\nIn this tutorial, weâ€™ll distill a ResNet-34 (teacher) into a ResNet-18 (student).",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#overview",
    "href": "tutorials/distill/distill_callback.html#overview",
    "title": "KnowledgeDistillation Callback",
    "section": "",
    "text": "Knowledge Distillation transfers knowledge from a large, accurate â€œteacherâ€ model to a smaller, faster â€œstudentâ€ model. The student learns not just from ground truth labels, but also from the teacherâ€™s soft predictionsâ€”capturing the teacherâ€™s learned relationships between classes.\n\n\n\n\n\nApproach\nModel Size\nTraining Data\nAccuracy\n\n\n\n\nTrain small model alone\nSmall\nLabels only\nLower\n\n\nDistillation\nSmall\nLabels + Teacher knowledge\nHigher\n\n\n\n\n\n\n\nSmaller deployment models - Student can be much smaller than teacher\nBetter than training from scratch - Teacher provides richer supervision\nNo additional labeled data needed - Uses existing training set\nFlexible loss functions - Soft targets, attention transfer, feature matching\n\nIn this tutorial, weâ€™ll distill a ResNet-34 (teacher) into a ResNet-18 (student).",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#setup-and-data",
    "href": "tutorials/distill/distill_callback.html#setup-and-data",
    "title": "KnowledgeDistillation Callback",
    "section": "1. Setup and Data",
    "text": "1. Setup and Data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#train-the-teacher-model",
    "href": "tutorials/distill/distill_callback.html#train-the-teacher-model",
    "title": "KnowledgeDistillation Callback",
    "section": "2. Train the Teacher Model",
    "text": "2. Train the Teacher Model\nFirst, we train the larger teacher model (ResNet-34) to achieve good accuracy on our dataset:\n\nteacher = vision_learner(dls, resnet34, metrics=accuracy)\nteacher.unfreeze()\nteacher.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.663302\n0.382650\n0.881597\n00:02\n\n\n1\n0.444977\n1.731543\n0.723951\n00:02\n\n\n2\n0.456336\n0.390448\n0.847091\n00:02\n\n\n3\n0.463871\n0.314980\n0.864005\n00:02\n\n\n4\n0.399526\n0.548000\n0.845061\n00:03\n\n\n5\n0.267582\n0.222926\n0.903248\n00:02\n\n\n6\n0.177511\n0.180466\n0.933694\n00:02\n\n\n7\n0.121694\n0.195583\n0.927605\n00:02\n\n\n8\n0.077676\n0.192459\n0.936401\n00:02\n\n\n9\n0.047532\n0.180056\n0.936401\n00:02",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#baseline-student-without-distillation",
    "href": "tutorials/distill/distill_callback.html#baseline-student-without-distillation",
    "title": "KnowledgeDistillation Callback",
    "section": "3. Baseline: Student Without Distillation",
    "text": "3. Baseline: Student Without Distillation\nLetâ€™s train a ResNet-18 student model without distillation to establish a baseline:\nTraining from scratch with only ground truth labels:\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nstudent.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.611359\n0.660552\n0.676590\n00:02\n\n\n1\n0.565523\n0.669257\n0.704330\n00:02\n\n\n2\n0.537007\n0.567621\n0.728011\n00:02\n\n\n3\n0.498747\n0.541553\n0.741543\n00:02\n\n\n4\n0.449077\n0.455508\n0.783491\n00:02\n\n\n5\n0.399169\n0.393245\n0.828823\n00:02\n\n\n6\n0.342478\n0.369859\n0.834912\n00:02\n\n\n7\n0.272756\n0.334547\n0.853857\n00:02\n\n\n8\n0.187447\n0.346933\n0.859269\n00:02\n\n\n9\n0.147805\n0.358428\n0.859946\n00:02",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#student-with-knowledge-distillation",
    "href": "tutorials/distill/distill_callback.html#student-with-knowledge-distillation",
    "title": "KnowledgeDistillation Callback",
    "section": "4. Student With Knowledge Distillation",
    "text": "4. Student With Knowledge Distillation\nNow letâ€™s train the same architecture with help from the teacher using SoftTarget loss:\nThe SoftTarget loss combines: - Classification loss (Cross-Entropy with ground truth) - Distillation loss (KL divergence between student and teacher soft predictions)\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, SoftTarget, schedule=cos)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.622423\n0.658045\n0.692828\n00:03\n\n\n1\n0.654330\n1.211342\n0.677267\n00:02\n\n\n2\n0.736943\n0.757770\n0.736807\n00:03\n\n\n3\n0.830559\n0.949577\n0.698241\n00:02\n\n\n4\n0.882739\n0.915873\n0.793640\n00:03\n\n\n5\n0.890884\n0.799081\n0.824763\n00:02\n\n\n6\n0.817516\n1.475584\n0.737483\n00:02\n\n\n7\n0.687356\n0.730070\n0.866035\n00:02\n\n\n8\n0.523237\n0.718984\n0.866035\n00:03\n\n\n9\n0.452811\n0.703519\n0.870771\n00:03\n\n\n\n\n\nWith teacher guidance, the student achieves better accuracy!",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#advanced-attention-transfer",
    "href": "tutorials/distill/distill_callback.html#advanced-attention-transfer",
    "title": "KnowledgeDistillation Callback",
    "section": "5. Advanced: Attention Transfer",
    "text": "5. Advanced: Attention Transfer\nBeyond soft targets, fasterai supports more sophisticated distillation losses like Attention Transfer from â€œPaying Attention to Attentionâ€. Here, the student learns to replicate the teacherâ€™s attention maps at intermediate layers.\nTo use intermediate layer losses, specify which layers to match using their string names. Use get_model_layers to discover available layers.\nHere we match attention maps after each residual block:\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, Attention, ['layer1', 'layer2', 'layer3', 'layer4'], ['0.4', '0.5', '0.6', '0.7'], weight=0.9)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.092506\n0.091555\n0.678620\n00:03\n\n\n1\n0.083053\n0.084819\n0.648173\n00:03\n\n\n2\n0.071733\n0.073612\n0.705007\n00:02\n\n\n3\n0.062212\n0.059138\n0.815291\n00:03\n\n\n4\n0.055396\n0.053225\n0.827470\n00:03\n\n\n5\n0.047694\n0.052672\n0.821380\n00:03\n\n\n6\n0.041354\n0.048255\n0.860622\n00:03\n\n\n7\n0.031322\n0.042128\n0.874831\n00:03\n\n\n8\n0.024217\n0.042546\n0.879567\n00:03\n\n\n9\n0.019581\n0.042967\n0.886333\n00:03",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#parameter-guide",
    "href": "tutorials/distill/distill_callback.html#parameter-guide",
    "title": "KnowledgeDistillation Callback",
    "section": "6. Parameter Guide",
    "text": "6. Parameter Guide\n\nKnowledgeDistillationCallback Parameters\n\n\n\nParameter\nDescription\n\n\n\n\nteacher\nThe trained teacher model\n\n\nloss\nDistillation loss function (SoftTarget, Attention, FitNet, etc.)\n\n\nstudent_layers\n(For intermediate losses) Layers in student to extract features from\n\n\nteacher_layers\n(For intermediate losses) Corresponding layers in teacher\n\n\nweight\nWeight of distillation loss vs classification loss\n\n\n\n\n\nAvailable Loss Functions\n\n\n\nLoss\nType\nDescription\n\n\n\n\nSoftTarget\nOutput\nMatch teacherâ€™s softened predictions\n\n\nAttention\nIntermediate\nMatch attention maps (spatial activation patterns)\n\n\nFitNet\nIntermediate\nDirectly match feature maps (requires same dimensions)\n\n\nRKD\nRelational\nMatch distance/angle relationships between samples\n\n\nPKT\nProbabilistic\nMatch probability distributions in feature space",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#summary",
    "href": "tutorials/distill/distill_callback.html#summary",
    "title": "KnowledgeDistillation Callback",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nKnowledge Distillation\nTraining a small student to mimic a large teacher\n\n\nKnowledgeDistillationCallback\nfastai callback for distillation during training\n\n\nSoftTarget\nBasic distillation using teacherâ€™s soft predictions\n\n\nAttention Transfer\nAdvanced distillation using intermediate attention maps\n\n\nTypical Benefit\n1-3% accuracy improvement over training student alone",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/distill/distill_callback.html#see-also",
    "href": "tutorials/distill/distill_callback.html#see-also",
    "title": "KnowledgeDistillation Callback",
    "section": "See Also",
    "text": "See Also\n\nDistillation Losses - All available distillation loss functions\nPruner - Combine distillation with pruning for even smaller models\nSparsifier - Add sparsity to distilled models",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Distill",
      "KnowledgeDistillation Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/prune_callback.html",
    "href": "tutorials/prune/prune_callback.html",
    "title": "Prune Callback",
    "section": "",
    "text": "Structured Pruning removes entire filters, channels, or layers from neural networks, resulting in genuinely smaller and faster models. Unlike sparsification (which zeros individual weights), pruning physically removes parameters.\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nWhatâ€™s Removed\nModel Size\nSpeed Benefit\nHardware\n\n\n\n\nSparsification\nIndividual weights\nSame\nRequires sparse support\nSpecialized\n\n\nStructured Pruning\nEntire filters\nSmaller\nImmediate\nStandard\n\n\n\n\n\n\n\nReal speedup - Fewer parameters = faster inference on any hardware\nSmaller models - Reduced memory footprint for deployment\nGradual pruning - Remove filters progressively during training\nFlexible targeting - Global or local pruning strategies",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/prune_callback.html#overview",
    "href": "tutorials/prune/prune_callback.html#overview",
    "title": "Prune Callback",
    "section": "",
    "text": "Structured Pruning removes entire filters, channels, or layers from neural networks, resulting in genuinely smaller and faster models. Unlike sparsification (which zeros individual weights), pruning physically removes parameters.\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nWhatâ€™s Removed\nModel Size\nSpeed Benefit\nHardware\n\n\n\n\nSparsification\nIndividual weights\nSame\nRequires sparse support\nSpecialized\n\n\nStructured Pruning\nEntire filters\nSmaller\nImmediate\nStandard\n\n\n\n\n\n\n\nReal speedup - Fewer parameters = faster inference on any hardware\nSmaller models - Reduced memory footprint for deployment\nGradual pruning - Remove filters progressively during training\nFlexible targeting - Global or local pruning strategies",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/prune_callback.html#setup-and-baseline",
    "href": "tutorials/prune/prune_callback.html#setup-and-baseline",
    "title": "Prune Callback",
    "section": "1. Setup and Baseline",
    "text": "1. Setup and Baseline\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nFirst, train a baseline ResNet-18 to establish expected performance:\n\nlearn = vision_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\nlearn.fit_one_cycle(1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.573910\n0.346901\n0.848444\n00:02\n\n\n\n\n\n\nbase_macs, base_params = tp.utils.count_ops_and_params(learn.model, torch.randn(1,3,224,224).to(default_device()))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/prune_callback.html#training-with-prunecallback",
    "href": "tutorials/prune/prune_callback.html#training-with-prunecallback",
    "title": "Prune Callback",
    "section": "2. Training with PruneCallback",
    "text": "2. Training with PruneCallback\nNow letâ€™s train with gradual filter pruning. Weâ€™ll remove 40% of filters using a one-cycle schedule:\nConfiguration: - pruning_ratio=40 - Remove 40% of filters - context='global' - Remove least important filters from anywhere in the network - criteria=large_final - Keep filters with largest final weights - schedule=one_cycle - Gradually increase pruning following one-cycle pattern\n\npr_cb = PruneCallback(pruning_ratio=0.4, context='global', criteria=large_final, schedule=one_cycle)\nlearn.fit_one_cycle(10, cbs=pr_cb)\n\nIgnoring output layer: 1.8\nTotal ignored layers: 1\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.350107\n0.277946\n0.875507\n00:02\n\n\n1\n0.270526\n0.309414\n0.881597\n00:03\n\n\n2\n0.247778\n0.240875\n0.903924\n00:03\n\n\n3\n0.224332\n0.608088\n0.708390\n00:03\n\n\n4\n0.193209\n0.221060\n0.897835\n00:03\n\n\n5\n0.249345\n0.259771\n0.895805\n00:04\n\n\n6\n0.266264\n0.265805\n0.890392\n00:04\n\n\n7\n0.234256\n0.263015\n0.888363\n00:02\n\n\n8\n0.224429\n0.255041\n0.890392\n00:02\n\n\n9\n0.196133\n0.255395\n0.892422\n00:03\n\n\n\n\n\nSparsity at the end of epoch 0: 0.39%\nSparsity at the end of epoch 1: 1.54%\nSparsity at the end of epoch 2: 5.60%\nSparsity at the end of epoch 3: 15.91%\nSparsity at the end of epoch 4: 29.13%\nSparsity at the end of epoch 5: 36.64%\nSparsity at the end of epoch 6: 39.12%\nSparsity at the end of epoch 7: 39.79%\nSparsity at the end of epoch 8: 39.96%\nSparsity at the end of epoch 9: 40.00%\n\n\n\npruned_macs, pruned_params = tp.utils.count_ops_and_params(learn.model, torch.randn(1,3,224,224).to(default_device()))",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/prune_callback.html#measuring-compression",
    "href": "tutorials/prune/prune_callback.html#measuring-compression",
    "title": "Prune Callback",
    "section": "3. Measuring Compression",
    "text": "3. Measuring Compression\nThe pruned model has fewer parameters and requires less compute:\n\nprint(f'The pruned model has {pruned_macs/base_macs:.2f} the compute of original model')\n\nThe pruned model has 0.63 the compute of original model\n\n\n\nprint(f'The pruned model has {pruned_params/base_params:.2f} the parameters of original model')\n\nThe pruned model has 0.18 the parameters of original model",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/prune_callback.html#summary",
    "href": "tutorials/prune/prune_callback.html#summary",
    "title": "Prune Callback",
    "section": "Summary",
    "text": "Summary\n\n\n\nMetric\nOriginal\nPruned (40%)\nImprovement\n\n\n\n\nParameters\n100%\n~18%\n5.5x smaller\n\n\nCompute (MACs)\n100%\n~63%\n1.6x fewer ops\n\n\nAccuracy\nBaseline\n~1% drop\nMinimal impact\n\n\n\n\nParameter Reference\n\n\n\n\n\n\n\n\nParameter\nDescription\nExample\n\n\n\n\npruning_ratio\nPercentage of filters to remove\n40\n\n\ncontext\nPruning scope\n'global' (whole model) or 'local' (per-layer)\n\n\ncriteria\nImportance measure\nlarge_final, magnitude, taylor\n\n\nschedule\nHow pruning increases over training\none_cycle, cos, linear",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "tutorials/prune/prune_callback.html#see-also",
    "href": "tutorials/prune/prune_callback.html#see-also",
    "title": "Prune Callback",
    "section": "See Also",
    "text": "See Also\n\nPruner - Lower-level API for one-shot pruning\nSparsifier - For unstructured sparsification\nSchedules - Available pruning schedules\nCriteria - Filter importance measures\nYOLO Pruning Tutorial - Pruning detection models",
    "crumbs": [
      "Contact Me",
      "Tutorials",
      "Prune",
      "Prune Callback"
    ]
  },
  {
    "objectID": "prune/pruner.html",
    "href": "prune/pruner.html",
    "title": "Pruner",
    "section": "",
    "text": "The Pruner class provides structured pruning capabilities using the torch-pruning library. Unlike unstructured pruning (which zeros individual weights), structured pruning removes entire filters/channels, resulting in a genuinely smaller and faster model.\nKey Features: - Automatic dependency handling across layers - Support for both local (per-layer) and global (cross-layer) pruning - Automatic detection and handling of attention layers in transformers - Compatible with various importance criteria from fasterai.core.criteria\n\n\n\n\n\n\n\n\n\n\nAspect\nSparsifier\nPruner\n\n\n\n\nWhat it removes\nIndividual weights (unstructured)\nEntire filters/channels (structured)\n\n\nModel size\nSame architecture, sparse weights\nSmaller architecture\n\n\nSpeedup\nRequires sparse hardware/libraries\nImmediate speedup on any hardware\n\n\nAccuracy impact\nGenerally lower at same sparsity\nMay need fine-tuning\n\n\nBest for\nResearch, sparse-aware inference\nProduction deployment\n\n\n\n\nsource\n\n\n\n\ndef prune_model(\n    \n):\n\nExecute one pruning step and restore attention layer configurations\n\nsource\n\n\n\n\ndef group_importance(\n    group\n):\n\nCompute importance scores for a dependency group\n\nsource\n\n\n\n\ndef print_sparsity(\n    \n)-&gt;None:\n\nPrint pruning report showing channel counts and parameter reduction",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Pruner"
    ]
  },
  {
    "objectID": "prune/pruner.html#overview",
    "href": "prune/pruner.html#overview",
    "title": "Pruner",
    "section": "",
    "text": "The Pruner class provides structured pruning capabilities using the torch-pruning library. Unlike unstructured pruning (which zeros individual weights), structured pruning removes entire filters/channels, resulting in a genuinely smaller and faster model.\nKey Features: - Automatic dependency handling across layers - Support for both local (per-layer) and global (cross-layer) pruning - Automatic detection and handling of attention layers in transformers - Compatible with various importance criteria from fasterai.core.criteria\n\n\n\n\n\n\n\n\n\n\nAspect\nSparsifier\nPruner\n\n\n\n\nWhat it removes\nIndividual weights (unstructured)\nEntire filters/channels (structured)\n\n\nModel size\nSame architecture, sparse weights\nSmaller architecture\n\n\nSpeedup\nRequires sparse hardware/libraries\nImmediate speedup on any hardware\n\n\nAccuracy impact\nGenerally lower at same sparsity\nMay need fine-tuning\n\n\nBest for\nResearch, sparse-aware inference\nProduction deployment\n\n\n\n\nsource\n\n\n\n\ndef prune_model(\n    \n):\n\nExecute one pruning step and restore attention layer configurations\n\nsource\n\n\n\n\ndef group_importance(\n    group\n):\n\nCompute importance scores for a dependency group\n\nsource\n\n\n\n\ndef print_sparsity(\n    \n)-&gt;None:\n\nPrint pruning report showing channel counts and parameter reduction",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Pruner"
    ]
  },
  {
    "objectID": "prune/pruner.html#usage-examples",
    "href": "prune/pruner.html#usage-examples",
    "title": "Pruner",
    "section": "Usage Examples",
    "text": "Usage Examples\nLetâ€™s try the Pruner with a VGG16 model\nmodel = resnet18()\npruner = Pruner(model, 30, 'local', large_final)\npruner.prune_model()",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Pruner"
    ]
  },
  {
    "objectID": "prune/pruner.html#see-also",
    "href": "prune/pruner.html#see-also",
    "title": "Pruner",
    "section": "See Also",
    "text": "See Also\n\nPruneCallback - Apply structured pruning during fastai training\nCriteria - Different importance measures for selecting what to prune\nSchedules - Control pruning progression during training\nSparsifier - Unstructured pruning (zeroing weights without removing them)\ntorch-pruning documentation - The underlying library used by Pruner",
    "crumbs": [
      "Contact Me",
      "Prune",
      "Pruner"
    ]
  }
]