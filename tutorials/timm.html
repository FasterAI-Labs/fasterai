<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="TIMM Pruning">

<title>TIMM Pruning – fasterai</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-11812d3bc142aca4717562015a1909d7.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-3c8ca0cc7bbe25650b9aaaacd4d5c9ab.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-424WWZFZ5F"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-424WWZFZ5F', { 'anonymize_ip': true});
</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="TIMM Pruning – fasterai">
<meta property="og:description" content="TIMM Pruning">
<meta property="og:site_name" content="fasterai">
<meta name="twitter:title" content="TIMM Pruning – fasterai">
<meta name="twitter:description" content="TIMM Pruning">
<meta name="twitter:creator" content="@nathanhubens">
<meta name="twitter:site" content="@fasterai">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed quarto-dark"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = true;
    const darkModeDefault = authorPrefersDark;
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">fasterai</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="mailto:nathan.hubens@gmail.com?subject=Hello"> <i class="bi bi-chat-right-text" role="img">
</i> 
<span class="menu-text">Contact Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/nathanhubens/fasterai/issues"> <i class="bi bi-bug" role="img">
</i> 
<span class="menu-text">Report an Issue</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://discord.gg/32BwhJSB9u"> <i class="bi bi-discord" role="img">
</i> 
<span class="menu-text">Join the Community</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/nathanhubens/fasterai"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/fasterai"> <i class="bi bi-twitter" role="img" aria-label="FasterAI Twitter">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">TIMM Pruning</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../quickstart.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quick Start</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Tutorials</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/walkthrough.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Walkthrough</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Sparse</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/sparse/schedules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedules</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/sparse/sparsifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/sparse/sparsify_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsify Callback</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/sparse/lottery_ticket.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Lottery Ticket Hypothesis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/sparse/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prune Transformers</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Prune</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/prune/prune_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prune Callback</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/prune/yolov8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">YOLOV8</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Distill</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/distill/distill_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KnowledgeDistillation Callback</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Regularize</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/regularize/regularize_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regularize Callback</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/misc/bn_folding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">BatchNorm Folding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/misc/fc_decomposer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fully-Connected layers decomposition</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Core</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../core/granularity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Granularity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../core/criteria.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Criteria</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../core/schedules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedules</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Sparse</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sparse/sparsifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sparse/sparsify_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sparsify Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Prune</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../prune/pruner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Pruner</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../prune/prune_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prune Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Distill</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../distill/distillation_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Knowledge Distillation</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Quantize</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../quantize/quantizer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantizer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../quantize/quantize_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quantize Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Regularize</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regularize/regularize_callback.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Regularize Callback</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../misc/bn_folding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Batch Norm Folding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../misc/fc_decomposer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Fully-Connected Layers Decomposer</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">TIMM Pruning</h1>
</div>

<div>
  <div class="description">
    TIMM Pruning
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div id="0835765a-6022-4896-b04a-86993a3c628c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.callback.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fasterai.core.criteria <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch_pruning <span class="im">as</span> tp</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch_pruning.pruner <span class="im">import</span> function</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch_pruning <span class="im">as</span> tp</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5d3b587a-93c6-47cd-8ca7-0c73245c1a40" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> onecycle_scheduler(pruning_ratio_dict, steps, start<span class="op">=</span><span class="dv">0</span>, end<span class="op">=</span><span class="dv">1</span>, α<span class="op">=</span><span class="dv">14</span>, β<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        sched_onecycle(start, end, i <span class="op">/</span> <span class="bu">float</span>(steps), α, β) <span class="op">*</span> pruning_ratio_dict</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(steps <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sched_onecycle(start, end, pos, α<span class="op">=</span><span class="dv">14</span>, β<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>α <span class="op">+</span> β)) <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp((<span class="op">-</span>α <span class="op">*</span> pos) <span class="op">+</span> β))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> start <span class="op">+</span> (end <span class="op">-</span> start) <span class="op">*</span> out</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c1725a08-aecf-4253-a0bc-f1d754bbf3ba" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_dls(size, bs):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    path <span class="op">=</span> URLs.IMAGENETTE_160</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    source <span class="op">=</span> untar_data(path)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    blocks<span class="op">=</span>(ImageBlock, CategoryBlock)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    tfms <span class="op">=</span> [RandomResizedCrop(size, min_scale<span class="op">=</span><span class="fl">0.35</span>), FlipItem(<span class="fl">0.5</span>)]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    batch_tfms <span class="op">=</span> [Normalize.from_stats(<span class="op">*</span>imagenet_stats)]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    csv_file <span class="op">=</span> <span class="st">'noisy_imagenette.csv'</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    inp <span class="op">=</span> pd.read_csv(source<span class="op">/</span>csv_file)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    dblock <span class="op">=</span> DataBlock(blocks<span class="op">=</span>blocks,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>               splitter<span class="op">=</span>ColSplitter(),</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>               get_x<span class="op">=</span>ColReader(<span class="st">'path'</span>, pref<span class="op">=</span>source),</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>               get_y<span class="op">=</span>ColReader(<span class="ss">f'noisy_labels_0'</span>),</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>               item_tfms<span class="op">=</span>tfms,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>               batch_tfms<span class="op">=</span>batch_tfms)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dblock.dataloaders(inp, path<span class="op">=</span>source, bs<span class="op">=</span>bs)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="0aea31be-e73f-43d9-b3bb-c0e8b986e20d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(<span class="st">'resnet18'</span>, pretrained<span class="op">=</span><span class="va">False</span>, no_jit<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="6b51141b-56d1-4df7-b287-1ae464647804" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> get_dls(model.default_cfg[<span class="st">'input_size'</span>][<span class="dv">2</span>], <span class="dv">16</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="65f8edb7-d2de-4bef-9a00-5db38ac60ef1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#learn = vision_learner(dls, 'bat_resnext26ts', metrics = [accuracy])</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">#learn.unfreeze()</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(<span class="st">'beit_base_patch16_224'</span>, pretrained<span class="op">=</span><span class="va">False</span>, no_jit<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ignored_layers <span class="op">=</span> []</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> {}</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>pruning_ratio_dict <span class="op">=</span> {}</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">#ratios = [0.265625,0.234375,0.265625,0.265625,0.93359375,0.328125,0.2265625,0.58984375,0.54296875,0.701171875,0.919921875,0.04296875,0.796875,0.240966796875,0.07763671875]</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">#k = 0</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> model.modules():</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">#if hasattr(m, 'head'): #isinstance(m, nn.Linear) and m.out_features == model.num_classes:</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Linear) <span class="kw">and</span> m.out_features <span class="op">==</span> model.num_classes:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        ignored_layers.append(m)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Ignore classifier layer: "</span>, m)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Attention layers</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'num_heads'</span>):</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'qkv'</span>):</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>            num_heads[m.qkv] <span class="op">=</span> m.num_heads</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Attention layer: "</span>, m.qkv, m.num_heads)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">hasattr</span>(m, <span class="st">'qkv_proj'</span>):</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>            num_heads[m.qkv_proj] <span class="op">=</span> m.num_heads</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">#elif isinstance(m, nn.Conv2d):</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    pruning_ratio_dict[m] = ratios[k]</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    print(k)</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    k+=1</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, metrics <span class="op">=</span> [accuracy])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Attention layer:  Linear(in_features=768, out_features=2304, bias=False) 12
Ignore classifier layer:  Linear(in_features=768, out_features=1000, bias=True)</code></pre>
</div>
</div>
<div id="45775057-7ee9-4c39-9ee6-b756865aa349" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>xb, _ <span class="op">=</span> dls.one_batch()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[5], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> xb, _ <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">dls</span><span style="color:rgb(98,98,98)">.</span>one_batch()

<span class="ansi-red-fg">NameError</span>: name 'dls' is not defined</pre>
</div>
</div>
</div>
<div id="f4f8324b-22c9-4045-904d-6fa0207f6e1b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">3</span>, <span class="fl">1e-3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.302582</td>
<td>1.146651</td>
<td>0.622166</td>
<td>00:44</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.953016</td>
<td>0.809723</td>
<td>0.741147</td>
<td>00:45</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.828404</td>
<td>0.692629</td>
<td>0.775796</td>
<td>00:44</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="0d8d3226-cb67-4ca0-916d-109c2cbbe235" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>pruner <span class="op">=</span> tp.pruner.MetaPruner(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                        model, </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>                        xb.to(<span class="st">'cpu'</span>), </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                        global_pruning<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                        importance<span class="op">=</span>tp.importance.GroupNormImportance(), </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>                        iterative_steps<span class="op">=</span><span class="dv">10000</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>                        pruning_ratio<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>                        <span class="co">#pruning_ratio_dict=pruning_ratio_dict,</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>                        num_heads<span class="op">=</span>num_heads,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>                        ignored_layers<span class="op">=</span>ignored_layers,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">#for g in pruner.step(interactive=True):</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">#    g.prune()</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>pruner.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/HubensN/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch_pruning/dependency.py:697: UserWarning: Unwrapped parameters detected: ['cls_token', 'blocks.1.attn.v_bias', 'blocks.6.attn.relative_position_bias_table', 'blocks.0.attn.v_bias', 'blocks.3.gamma_2', 'blocks.3.attn.q_bias', 'blocks.4.attn.relative_position_bias_table', 'blocks.5.attn.relative_position_bias_table', 'blocks.9.gamma_2', 'blocks.9.attn.q_bias', 'blocks.11.attn.relative_position_bias_table', 'blocks.2.attn.relative_position_bias_table', 'blocks.4.gamma_2', 'blocks.6.attn.q_bias', 'blocks.7.attn.q_bias', 'blocks.7.attn.relative_position_bias_table', 'blocks.9.attn.v_bias', 'blocks.10.gamma_2', 'blocks.10.attn.relative_position_bias_table', 'blocks.0.attn.q_bias', 'blocks.0.attn.relative_position_bias_table', 'blocks.1.gamma_2', 'blocks.4.attn.q_bias', 'blocks.8.attn.q_bias', 'blocks.8.attn.v_bias', 'blocks.10.attn.q_bias', 'blocks.11.gamma_1', 'blocks.11.gamma_2', 'blocks.0.gamma_2', 'blocks.5.gamma_1', 'blocks.6.gamma_1', 'blocks.6.attn.v_bias', 'blocks.8.gamma_2', 'blocks.8.attn.relative_position_bias_table', 'blocks.3.gamma_1', 'blocks.0.gamma_1', 'blocks.1.attn.relative_position_bias_table', 'blocks.2.gamma_1', 'blocks.3.attn.relative_position_bias_table', 'blocks.9.attn.relative_position_bias_table', 'blocks.11.attn.v_bias', 'blocks.1.gamma_1', 'blocks.2.attn.v_bias', 'blocks.3.attn.v_bias', 'blocks.4.attn.v_bias', 'blocks.5.gamma_2', 'blocks.5.attn.v_bias', 'blocks.7.attn.v_bias', 'blocks.10.gamma_1', 'blocks.10.attn.v_bias', 'blocks.11.attn.q_bias', 'blocks.1.attn.q_bias', 'blocks.2.gamma_2', 'blocks.2.attn.q_bias', 'blocks.4.gamma_1', 'blocks.5.attn.q_bias', 'blocks.6.gamma_2', 'blocks.7.gamma_1', 'blocks.7.gamma_2', 'blocks.8.gamma_1', 'blocks.9.gamma_1'].
 Torch-Pruning will prune the last non-singleton dimension of these parameters. If you wish to change this behavior, please provide an unwrapped_parameters argument.
  warnings.warn(warning_str)</code></pre>
</div>
</div>
<div id="44332be1-c236-4da3-9d31-b07d8d52a55c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> model.modules():</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Attention layers</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'num_heads'</span>):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'qkv'</span>):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>            m.num_heads <span class="op">=</span> num_heads[m.qkv]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>            m.head_dim <span class="op">=</span> m.qkv.out_features <span class="op">//</span> (<span class="dv">3</span> <span class="op">*</span> m.num_heads)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">hasattr</span>(m, <span class="st">'qkv_proj'</span>):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>            m.num_heads <span class="op">=</span> num_heads[m.qqkv_projkv]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>            m.head_dim <span class="op">=</span> m.qkv_proj.out_features <span class="op">//</span> (<span class="dv">3</span> <span class="op">*</span> m.num_heads)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="3ef09402-e838-4358-9f1c-079512227a57" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model_old <span class="op">=</span> timm.create_model(<span class="st">'convnext_xxlarge'</span>, pretrained<span class="op">=</span><span class="va">False</span>, no_jit<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>base_macs, base_params <span class="op">=</span> tp.utils.count_ops_and_params(model_old, xb.to(<span class="st">'cpu'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="54fbdc44-abfb-4cdb-bef1-61b5053cfccb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pruned_macs, pruned_params <span class="op">=</span> tp.utils.count_ops_and_params(model, xb.to(<span class="st">'cpu'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">RuntimeError</span>                              Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[137], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> pruned_macs, pruned_params <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">tp</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">utils</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">count_ops_and_params</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">model</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">xb</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">to</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">cpu</span><span style="color:rgb(175,0,0)" class="ansi-yellow-bg">'</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/utils/_contextlib.py:116</span>, in <span class="ansi-cyan-fg">context_decorator.&lt;locals&gt;.decorate_context</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">    113</span> <span style="color:rgb(175,0,255)">@functools</span><span style="color:rgb(98,98,98)">.</span>wraps(func)
<span class="ansi-green-fg ansi-bold">    114</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">decorate_context</span>(<span style="color:rgb(98,98,98)">*</span>args, <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>kwargs):
<span class="ansi-green-fg ansi-bold">    115</span>     <span style="font-weight:bold;color:rgb(0,135,0)">with</span> ctx_factory():
<span class="ansi-green-fg">--&gt; 116</span>         <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch_pruning/utils/op_counter.py:35</span>, in <span class="ansi-cyan-fg">count_ops_and_params</span><span class="ansi-blue-fg">(model, example_inputs, layer_wise)</span>
<span class="ansi-green-fg ansi-bold">     33</span>     _ <span style="color:rgb(98,98,98)">=</span> flops_model(<span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>example_inputs)
<span class="ansi-green-fg ansi-bold">     34</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">---&gt; 35</span>     _ <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">flops_model</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">example_inputs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">     36</span> flops_count, params_count, _layer_flops, _layer_params <span style="color:rgb(98,98,98)">=</span> flops_model<span style="color:rgb(98,98,98)">.</span>compute_average_flops_cost()
<span class="ansi-green-fg ansi-bold">     37</span> layer_flops <span style="color:rgb(98,98,98)">=</span> {}

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1553</span>, in <span class="ansi-cyan-fg">Module._wrapped_call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1551</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_compiled_call_impl(<span style="color:rgb(98,98,98)">*</span>args, <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>kwargs)  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[misc]</span>
<span class="ansi-green-fg ansi-bold">   1552</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 1553</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_call_impl</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1603</span>, in <span class="ansi-cyan-fg">Module._call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1600</span>     bw_hook <span style="color:rgb(98,98,98)">=</span> hooks<span style="color:rgb(98,98,98)">.</span>BackwardHook(<span style="color:rgb(0,135,0)">self</span>, full_backward_hooks, backward_pre_hooks)
<span class="ansi-green-fg ansi-bold">   1601</span>     args <span style="color:rgb(98,98,98)">=</span> bw_hook<span style="color:rgb(98,98,98)">.</span>setup_input_hook(args)
<span class="ansi-green-fg">-&gt; 1603</span> result <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">forward_call</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1604</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> _global_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_forward_hooks:
<span class="ansi-green-fg ansi-bold">   1605</span>     <span style="font-weight:bold;color:rgb(0,135,0)">for</span> hook_id, hook <span style="font-weight:bold;color:rgb(175,0,255)">in</span> (
<span class="ansi-green-fg ansi-bold">   1606</span>         <span style="color:rgb(98,98,98)">*</span>_global_forward_hooks<span style="color:rgb(98,98,98)">.</span>items(),
<span class="ansi-green-fg ansi-bold">   1607</span>         <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_forward_hooks<span style="color:rgb(98,98,98)">.</span>items(),
<span class="ansi-green-fg ansi-bold">   1608</span>     ):
<span class="ansi-green-fg ansi-bold">   1609</span>         <span style="font-style:italic;color:rgb(95,135,135)"># mark that always called hook is run</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:521</span>, in <span class="ansi-cyan-fg">Beit.forward</span><span class="ansi-blue-fg">(self, x)</span>
<span class="ansi-green-fg ansi-bold">    520</span> <span style="font-weight:bold;color:rgb(0,135,0)">def</span> <span style="color:rgb(0,0,255)">forward</span>(<span style="color:rgb(0,135,0)">self</span>, x):
<span class="ansi-green-fg">--&gt; 521</span>     x <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">forward_features</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    522</span>     x <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>forward_head(x)
<span class="ansi-green-fg ansi-bold">    523</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> x

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:509</span>, in <span class="ansi-cyan-fg">Beit.forward_features</span><span class="ansi-blue-fg">(self, x)</span>
<span class="ansi-green-fg ansi-bold">    507</span>         x <span style="color:rgb(98,98,98)">=</span> checkpoint(blk, x, shared_rel_pos_bias<span style="color:rgb(98,98,98)">=</span>rel_pos_bias)
<span class="ansi-green-fg ansi-bold">    508</span>     <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 509</span>         x <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">blk</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">shared_rel_pos_bias</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">rel_pos_bias</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    510</span> x <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>norm(x)
<span class="ansi-green-fg ansi-bold">    511</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> x

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1553</span>, in <span class="ansi-cyan-fg">Module._wrapped_call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1551</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_compiled_call_impl(<span style="color:rgb(98,98,98)">*</span>args, <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>kwargs)  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[misc]</span>
<span class="ansi-green-fg ansi-bold">   1552</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 1553</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_call_impl</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1562</span>, in <span class="ansi-cyan-fg">Module._call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1557</span> <span style="font-style:italic;color:rgb(95,135,135)"># If we don't have any hooks, we want to skip the rest of the logic in</span>
<span class="ansi-green-fg ansi-bold">   1558</span> <span style="font-style:italic;color:rgb(95,135,135)"># this function, and just call forward.</span>
<span class="ansi-green-fg ansi-bold">   1559</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> (<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_backward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_forward_pre_hooks
<span class="ansi-green-fg ansi-bold">   1560</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1561</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1562</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">forward_call</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1564</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg ansi-bold">   1565</span>     result <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:248</span>, in <span class="ansi-cyan-fg">Block.forward</span><span class="ansi-blue-fg">(self, x, shared_rel_pos_bias)</span>
<span class="ansi-green-fg ansi-bold">    246</span>     x <span style="color:rgb(98,98,98)">=</span> x <span style="color:rgb(98,98,98)">+</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>drop_path2(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>mlp(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>norm2(x)))
<span class="ansi-green-fg ansi-bold">    247</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 248</span>     x <span style="color:rgb(98,98,98)">=</span> x <span style="color:rgb(98,98,98)">+</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>drop_path1(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>gamma_1 <span style="color:rgb(98,98,98)">*</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">attn</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">norm1</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">shared_rel_pos_bias</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">shared_rel_pos_bias</span><span class="ansi-yellow-bg">)</span>)
<span class="ansi-green-fg ansi-bold">    249</span>     x <span style="color:rgb(98,98,98)">=</span> x <span style="color:rgb(98,98,98)">+</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>drop_path2(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>gamma_2 <span style="color:rgb(98,98,98)">*</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>mlp(<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>norm2(x)))
<span class="ansi-green-fg ansi-bold">    250</span> <span style="font-weight:bold;color:rgb(0,135,0)">return</span> x

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1553</span>, in <span class="ansi-cyan-fg">Module._wrapped_call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1551</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_compiled_call_impl(<span style="color:rgb(98,98,98)">*</span>args, <span style="color:rgb(98,98,98)">*</span><span style="color:rgb(98,98,98)">*</span>kwargs)  <span style="font-style:italic;color:rgb(95,135,135)"># type: ignore[misc]</span>
<span class="ansi-green-fg ansi-bold">   1552</span> <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 1553</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_call_impl</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/nn/modules/module.py:1562</span>, in <span class="ansi-cyan-fg">Module._call_impl</span><span class="ansi-blue-fg">(self, *args, **kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1557</span> <span style="font-style:italic;color:rgb(95,135,135)"># If we don't have any hooks, we want to skip the rest of the logic in</span>
<span class="ansi-green-fg ansi-bold">   1558</span> <span style="font-style:italic;color:rgb(95,135,135)"># this function, and just call forward.</span>
<span class="ansi-green-fg ansi-bold">   1559</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> (<span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_backward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>_forward_pre_hooks
<span class="ansi-green-fg ansi-bold">   1560</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_pre_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_backward_hooks
<span class="ansi-green-fg ansi-bold">   1561</span>         <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_hooks <span style="font-weight:bold;color:rgb(175,0,255)">or</span> _global_forward_pre_hooks):
<span class="ansi-green-fg">-&gt; 1562</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span class="ansi-yellow-bg">forward_call</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1564</span> <span style="font-weight:bold;color:rgb(0,135,0)">try</span>:
<span class="ansi-green-fg ansi-bold">   1565</span>     result <span style="color:rgb(98,98,98)">=</span> <span style="font-weight:bold;color:rgb(0,135,0)">None</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/timm/models/beit.py:149</span>, in <span class="ansi-cyan-fg">Attention.forward</span><span class="ansi-blue-fg">(self, x, shared_rel_pos_bias)</span>
<span class="ansi-green-fg ansi-bold">    147</span>         qkv <span style="color:rgb(98,98,98)">+</span><span style="color:rgb(98,98,98)">=</span> qkv_bias
<span class="ansi-green-fg ansi-bold">    148</span>     <span style="font-weight:bold;color:rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 149</span>         qkv <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">F</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">linear</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">x</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">weight</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span style="color:rgb(0,135,0)" class="ansi-yellow-bg">self</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">qkv</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">weight</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">bias</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">=</span><span class="ansi-yellow-bg">qkv_bias</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    150</span> qkv <span style="color:rgb(98,98,98)">=</span> qkv<span style="color:rgb(98,98,98)">.</span>reshape(B, N, <span style="color:rgb(98,98,98)">3</span>, <span style="color:rgb(0,135,0)">self</span><span style="color:rgb(98,98,98)">.</span>num_heads, <span style="color:rgb(98,98,98)">-</span><span style="color:rgb(98,98,98)">1</span>)<span style="color:rgb(98,98,98)">.</span>permute(<span style="color:rgb(98,98,98)">2</span>, <span style="color:rgb(98,98,98)">0</span>, <span style="color:rgb(98,98,98)">3</span>, <span style="color:rgb(98,98,98)">1</span>, <span style="color:rgb(98,98,98)">4</span>)
<span class="ansi-green-fg ansi-bold">    151</span> q, k, v <span style="color:rgb(98,98,98)">=</span> qkv<span style="color:rgb(98,98,98)">.</span>unbind(<span style="color:rgb(98,98,98)">0</span>)  <span style="font-style:italic;color:rgb(95,135,135)"># B, num_heads, N, head_dim</span>

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/fastai/torch_core.py:382</span>, in <span class="ansi-cyan-fg">TensorBase.__torch_function__</span><span class="ansi-blue-fg">(cls, func, types, args, kwargs)</span>
<span class="ansi-green-fg ansi-bold">    380</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">cls</span><span style="color:rgb(98,98,98)">.</span>debug <span style="font-weight:bold;color:rgb(175,0,255)">and</span> func<span style="color:rgb(98,98,98)">.</span><span style="color:rgb(0,0,135)">__name__</span> <span style="font-weight:bold;color:rgb(175,0,255)">not</span> <span style="font-weight:bold;color:rgb(175,0,255)">in</span> (<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">__str__</span><span style="color:rgb(175,0,0)">'</span>,<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">__repr__</span><span style="color:rgb(175,0,0)">'</span>): <span style="color:rgb(0,135,0)">print</span>(func, types, args, kwargs)
<span class="ansi-green-fg ansi-bold">    381</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> _torch_handled(args, <span style="color:rgb(0,135,0)">cls</span><span style="color:rgb(98,98,98)">.</span>_opt, func): types <span style="color:rgb(98,98,98)">=</span> (torch<span style="color:rgb(98,98,98)">.</span>Tensor,)
<span class="ansi-green-fg">--&gt; 382</span> res <span style="color:rgb(98,98,98)">=</span> <span style="color:rgb(0,135,0)" class="ansi-yellow-bg">super</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">)</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">__torch_function__</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">types</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">ifnone</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span class="ansi-yellow-bg">{</span><span class="ansi-yellow-bg">}</span><span class="ansi-yellow-bg">)</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">    383</span> dict_objs <span style="color:rgb(98,98,98)">=</span> _find_args(args) <span style="font-weight:bold;color:rgb(0,135,0)">if</span> args <span style="font-weight:bold;color:rgb(0,135,0)">else</span> _find_args(<span style="color:rgb(0,135,0)">list</span>(kwargs<span style="color:rgb(98,98,98)">.</span>values()))
<span class="ansi-green-fg ansi-bold">    384</span> <span style="font-weight:bold;color:rgb(0,135,0)">if</span> <span style="color:rgb(0,135,0)">issubclass</span>(<span style="color:rgb(0,135,0)">type</span>(res),TensorBase) <span style="font-weight:bold;color:rgb(175,0,255)">and</span> dict_objs: res<span style="color:rgb(98,98,98)">.</span>set_meta(dict_objs[<span style="color:rgb(98,98,98)">0</span>],as_copy<span style="color:rgb(98,98,98)">=</span><span style="font-weight:bold;color:rgb(0,135,0)">True</span>)

File <span class="ansi-green-fg">~/miniconda3/envs/fasterai/lib/python3.9/site-packages/torch/_tensor.py:1437</span>, in <span class="ansi-cyan-fg">Tensor.__torch_function__</span><span class="ansi-blue-fg">(cls, func, types, args, kwargs)</span>
<span class="ansi-green-fg ansi-bold">   1434</span>     <span style="font-weight:bold;color:rgb(0,135,0)">return</span> <span style="color:rgb(0,135,0)">NotImplemented</span>
<span class="ansi-green-fg ansi-bold">   1436</span> <span style="font-weight:bold;color:rgb(0,135,0)">with</span> _C<span style="color:rgb(98,98,98)">.</span>DisableTorchFunctionSubclass():
<span class="ansi-green-fg">-&gt; 1437</span>     ret <span style="color:rgb(98,98,98)">=</span> <span class="ansi-yellow-bg">func</span><span class="ansi-yellow-bg">(</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">args</span><span class="ansi-yellow-bg">,</span><span class="ansi-yellow-bg"> </span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span style="color:rgb(98,98,98)" class="ansi-yellow-bg">*</span><span class="ansi-yellow-bg">kwargs</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg ansi-bold">   1438</span>     <span style="font-weight:bold;color:rgb(0,135,0)">if</span> func <span style="font-weight:bold;color:rgb(175,0,255)">in</span> get_default_nowrap_functions():
<span class="ansi-green-fg ansi-bold">   1439</span>         <span style="font-weight:bold;color:rgb(0,135,0)">return</span> ret

<span class="ansi-red-fg">RuntimeError</span>: mat1 and mat2 shapes cannot be multiplied (3152x767 and 768x2304)</pre>
</div>
</div>
</div>
<div id="fb10d6d0-638c-4a41-a1ca-35e2111389aa" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MACs: </span><span class="sc">%.4f</span><span class="st"> G =&gt; </span><span class="sc">%.4f</span><span class="st"> G"</span><span class="op">%</span>(base_macs<span class="op">/</span><span class="fl">1e9</span>, pruned_macs<span class="op">/</span><span class="fl">1e9</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Params: </span><span class="sc">%.4f</span><span class="st"> M =&gt; </span><span class="sc">%.4f</span><span class="st"> M"</span><span class="op">%</span>(base_params<span class="op">/</span><span class="fl">1e6</span>, pruned_params<span class="op">/</span><span class="fl">1e6</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>MACs: 151.5881 G =&gt; 7.0919 G
Params: 846.4710 M =&gt; 58.2010 M</code></pre>
</div>
</div>
<div id="a7255784-1329-4691-890e-566adc45ce43" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PruneCallback(Callback):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, pruning_ratio, schedule, criteria, ignored_layers, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        store_attr()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sparsity_levels <span class="op">=</span> []</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.extra_args <span class="op">=</span> args</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.extra_kwargs <span class="op">=</span> kwargs</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_fit(<span class="va">self</span>):</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        n_batches_per_epoch <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.learn.dls.train)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        total_training_steps <span class="op">=</span> n_batches_per_epoch <span class="op">*</span> <span class="va">self</span>.learn.n_epoch</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.total_training_steps <span class="op">=</span> total_training_steps </span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="va">self</span>.total_training_steps)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.example_inputs, _ <span class="op">=</span> <span class="va">self</span>.learn.dls.one_batch()</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sparsity_levels <span class="op">=</span> <span class="va">self</span>.schedule(<span class="va">self</span>.pruning_ratio, total_training_steps)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pruner <span class="op">=</span> tp.pruner.MetaPruner(</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learn.model,</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        example_inputs<span class="op">=</span> torch.randn(<span class="va">self</span>.example_inputs.shape).to(<span class="st">'cuda:0'</span>),</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        importance<span class="op">=</span><span class="va">self</span>.criteria,</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        pruning_ratio<span class="op">=</span><span class="va">self</span>.pruning_ratio, </span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>        ignored_layers<span class="op">=</span><span class="va">self</span>.ignored_layers,</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        iterative_steps<span class="op">=</span> <span class="va">self</span>.total_training_steps, </span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">#iterative_steps= 1, </span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#iterative_pruning_ratio_scheduler=self.schedule,</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">#global_pruning=self.context, </span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span><span class="va">self</span>.extra_args, </span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span><span class="va">self</span>.extra_kwargs</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> before_step(<span class="va">self</span>):</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training: </span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>           <span class="co">#self.pruner.step()</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> g <span class="kw">in</span> <span class="va">self</span>.pruner.step(interactive<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>                g.prune()</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#for m in self.pruner.model.modules():</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    # Attention layers</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#    if hasattr(m, 'num_heads'):</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#        if hasattr(m, 'qkv'):</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">#            m.num_heads = num_heads[m.qkv]</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">#            m.head_dim = m.qkv.out_features // (3 * m.num_heads)</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>        <span class="co">#        elif hasattr(m, 'qkv_proj'):</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">#            m.num_heads = num_heads[m.qqkv_projkv]</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        <span class="co">#            m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> after_epoch(<span class="va">self</span>):</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        completed_steps <span class="op">=</span> (<span class="va">self</span>.epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> <span class="bu">len</span>(<span class="va">self</span>.learn.dls.train)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        current_sparsity <span class="op">=</span> <span class="va">self</span>.sparsity_levels[completed_steps <span class="op">-</span> <span class="dv">1</span>]</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Sparsity at the end of epoch </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>epoch<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>current_sparsity<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="f177c932-caff-4671-aac4-5aeef5e509ff" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>timm.list_models()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="2c2695c9-8d0e-4a10-898c-8be84317f693" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(<span class="st">'resnet18'</span>, pretrained<span class="op">=</span><span class="va">True</span>, no_jit<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>ignored_layers <span class="op">=</span> []</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> {}</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">#k = 0</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> model.modules():</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Linear) <span class="kw">and</span> m.out_features <span class="op">==</span> model.num_classes:</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        ignored_layers.append(m)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Ignore classifier layer: "</span>, m)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Attention layers</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'num_heads'</span>):</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(m, <span class="st">'qkv'</span>):</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            num_heads[m.qkv] <span class="op">=</span> m.num_heads</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Attention layer: "</span>, m.qkv, m.num_heads)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">hasattr</span>(m, <span class="st">'qkv_proj'</span>):</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            num_heads[m.qkv_proj] <span class="op">=</span> m.num_heads</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> Learner(dls, model, metrics <span class="op">=</span> [accuracy])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ignore classifier layer:  Linear(in_features=512, out_features=1000, bias=True)</code></pre>
</div>
</div>
<div id="49c13b65-ebad-4468-b79b-d898fd76573b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">5</span>, <span class="fl">1e-3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.410828</td>
<td>0.291440</td>
<td>0.907771</td>
<td>00:12</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.378970</td>
<td>0.237411</td>
<td>0.920764</td>
<td>00:12</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.254743</td>
<td>0.161710</td>
<td>0.946752</td>
<td>00:12</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.195507</td>
<td>0.142778</td>
<td>0.954395</td>
<td>00:12</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.124310</td>
<td>0.112485</td>
<td>0.965350</td>
<td>00:12</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="3bb5235e-e3a1-4173-b476-c5f9eeb9cd26" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>pr_cb <span class="op">=</span> PruneCallback(pruning_ratio<span class="op">=</span><span class="fl">0.25</span>, schedule<span class="op">=</span>onecycle_scheduler, global_pruning<span class="op">=</span><span class="va">True</span>, criteria<span class="op">=</span>tp.importance.GroupNormImportance(normalizer<span class="op">=</span><span class="va">None</span>, target_types<span class="op">=</span>[nn.modules.conv._ConvNd, nn.Linear]), num_heads<span class="op">=</span>num_heads, ignored_layers<span class="op">=</span>ignored_layers)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">10</span>, <span class="fl">1e-4</span>, cbs<span class="op">=</span>pr_cb)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>5910</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<div>
      <progress value="4" class="" max="10" style="width:300px; height:20px; vertical-align: middle;"></progress>
      40.00% [4/10 00:58&lt;01:27]
    </div>
    

<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.190778</td>
<td>0.223286</td>
<td>0.929172</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.906825</td>
<td>0.996437</td>
<td>0.685605</td>
<td>00:14</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2.154549</td>
<td>2.038115</td>
<td>0.295796</td>
<td>00:14</td>
</tr>
<tr class="even">
<td>3</td>
<td>2.019212</td>
<td>1.988954</td>
<td>0.316688</td>
<td>00:13</td>
</tr>
</tbody>
</table>
<p>

    </p><div>
      <progress value="165" class="" max="591" style="width:300px; height:20px; vertical-align: middle;"></progress>
      27.92% [165/591 00:03&lt;00:09 2.0123]
    </div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Sparsity at the end of epoch 0: 0.25%
Sparsity at the end of epoch 1: 0.98%
Sparsity at the end of epoch 2: 3.54%
Sparsity at the end of epoch 3: 10.02%</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
KeyboardInterrupt
</code></pre>
</div>
</div>
<div id="0f96bd5f-c2d9-408d-8daf-f2dace590cf8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>pr_cb <span class="op">=</span> PruneCallback(pruning_ratio<span class="op">=</span><span class="fl">0.25</span>, schedule<span class="op">=</span>onecycle_scheduler, global_pruning<span class="op">=</span><span class="va">True</span>, criteria<span class="op">=</span>GroupNormImportance(normalizer<span class="op">=</span><span class="va">None</span>, target_types<span class="op">=</span>[nn.modules.conv._ConvNd, nn.Linear]), num_heads<span class="op">=</span>num_heads, ignored_layers<span class="op">=</span>ignored_layers)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>learn.fit_one_cycle(<span class="dv">15</span>, <span class="fl">1e-4</span>, cbs<span class="op">=</span>pr_cb)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>8865</code></pre>
</div>
<div class="cell-output cell-output-display">

<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div>
<div class="cell-output cell-output-display">
<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">accuracy</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.118503</td>
<td>0.118367</td>
<td>0.963057</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.128137</td>
<td>0.115662</td>
<td>0.962038</td>
<td>00:16</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.122357</td>
<td>0.126387</td>
<td>0.960255</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>3</td>
<td>0.142834</td>
<td>0.112623</td>
<td>0.966115</td>
<td>00:16</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.124087</td>
<td>0.115061</td>
<td>0.964076</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>5</td>
<td>0.134429</td>
<td>0.125728</td>
<td>0.960510</td>
<td>00:16</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.128918</td>
<td>0.143516</td>
<td>0.954650</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>7</td>
<td>0.131373</td>
<td>0.176553</td>
<td>0.958217</td>
<td>00:16</td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.135654</td>
<td>0.177501</td>
<td>0.954395</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>9</td>
<td>0.180335</td>
<td>0.197476</td>
<td>0.946497</td>
<td>00:16</td>
</tr>
<tr class="odd">
<td>10</td>
<td>0.207450</td>
<td>0.194991</td>
<td>0.935796</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>11</td>
<td>0.233158</td>
<td>0.203113</td>
<td>0.937325</td>
<td>00:16</td>
</tr>
<tr class="odd">
<td>12</td>
<td>0.231163</td>
<td>0.219680</td>
<td>0.932484</td>
<td>00:16</td>
</tr>
<tr class="even">
<td>13</td>
<td>0.314865</td>
<td>0.245912</td>
<td>0.926369</td>
<td>00:15</td>
</tr>
<tr class="odd">
<td>14</td>
<td>0.311438</td>
<td>0.276028</td>
<td>0.922038</td>
<td>00:15</td>
</tr>
</tbody>
</table>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Sparsity at the end of epoch 0: 0.16%
Sparsity at the end of epoch 1: 0.39%
Sparsity at the end of epoch 2: 0.98%
Sparsity at the end of epoch 3: 2.35%
Sparsity at the end of epoch 4: 5.21%
Sparsity at the end of epoch 5: 10.03%
Sparsity at the end of epoch 6: 15.75%
Sparsity at the end of epoch 7: 20.31%
Sparsity at the end of epoch 8: 22.93%
Sparsity at the end of epoch 9: 24.15%
Sparsity at the end of epoch 10: 24.66%
Sparsity at the end of epoch 11: 24.87%
Sparsity at the end of epoch 12: 24.95%
Sparsity at the end of epoch 13: 24.99%
Sparsity at the end of epoch 14: 25.00%</code></pre>
</div>
</div>
<div id="56ab7cdc-c14e-48a0-be7c-629f3bdf1faa" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(<span class="st">'tf_efficientnet_b3'</span>, pretrained<span class="op">=</span><span class="va">False</span>, no_jit<span class="op">=</span><span class="va">True</span>).<span class="bu">eval</span>()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>base_macs, base_params <span class="op">=</span> tp.utils.count_ops_and_params(model, xb.to(<span class="st">'cpu'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cff1b898-a769-40e7-8d9f-d6fdbadb8e88" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>pruned_macs, pruned_params <span class="op">=</span> tp.utils.count_ops_and_params(learn.model, xb.to(<span class="st">'cuda:0'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="73f0e372-302a-4287-a7b9-80d19171dc97" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MACs: </span><span class="sc">%.4f</span><span class="st"> G =&gt; </span><span class="sc">%.4f</span><span class="st"> G"</span><span class="op">%</span>(base_macs<span class="op">/</span><span class="fl">1e9</span>, pruned_macs<span class="op">/</span><span class="fl">1e9</span>))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Params: </span><span class="sc">%.4f</span><span class="st"> M =&gt; </span><span class="sc">%.4f</span><span class="st"> M"</span><span class="op">%</span>(base_params<span class="op">/</span><span class="fl">1e6</span>, pruned_params<span class="op">/</span><span class="fl">1e6</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>MACs: 0.9399 G =&gt; 0.5407 G
Params: 12.2332 M =&gt; 7.2501 M</code></pre>
</div>
</div>
<div id="7299a495-675d-4bde-85ce-f922acf4a369" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> abc</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> typing</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch_pruning <span class="im">import</span> function</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch_pruning.dependency <span class="im">import</span> Group</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Importance(abc.ABC):</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Estimate the importance of a tp.Dependency.Group, and return an 1-D per-channel importance score.</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co">        It should accept a group as inputs, and return a 1-D tensor with the same length as the number of channels.</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="co">        All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups.</span></span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Example:</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="co">            ```python</span></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co">            DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224)) </span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="co">            group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )    </span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="co">            scorer = MagnitudeImportance()    </span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="co">            imp_score = scorer(group)    </span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="co">            #imp_score is a 1-D tensor with length 3 for channels [2, 6, 9]  </span></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a><span class="co">            min_score = imp_score.min() </span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a><span class="co">            ``` </span></span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">@abc.abstractclassmethod</span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, group: Group) <span class="op">-&gt;</span> torch.Tensor: </span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GroupNormImportance(Importance):</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>                 p: <span class="bu">int</span><span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>                 group_reduction: <span class="bu">str</span><span class="op">=</span><span class="st">"mean"</span>, </span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>                 normalizer: <span class="bu">str</span><span class="op">=</span><span class="st">'mean'</span>, </span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>                 bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>                 target_types:<span class="bu">list</span><span class="op">=</span>[nn.modules.conv._ConvNd, nn.Linear, nn.modules.batchnorm._BatchNorm, nn.LayerNorm]):</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.group_reduction <span class="op">=</span> group_reduction</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.normalizer <span class="op">=</span> normalizer</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_types <span class="op">=</span> target_types</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> bias</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _lamp(<span class="va">self</span>, scores): <span class="co"># Layer-adaptive Sparsity for the Magnitude-based Pruning</span></span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a><span class="co">        Normalizing scheme for LAMP.</span></span>
<span id="cb33-48"><a href="#cb33-48" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb33-49"><a href="#cb33-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sort scores in an ascending order</span></span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a>        sorted_scores,sorted_idx <span class="op">=</span> scores.view(<span class="op">-</span><span class="dv">1</span>).sort(descending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute cumulative sum</span></span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>        scores_cumsum_temp <span class="op">=</span> sorted_scores.cumsum(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>        scores_cumsum <span class="op">=</span> torch.zeros(scores_cumsum_temp.shape,device<span class="op">=</span>scores.device)</span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>        scores_cumsum[<span class="dv">1</span>:] <span class="op">=</span> scores_cumsum_temp[:<span class="bu">len</span>(scores_cumsum_temp)<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># normalize by cumulative sum</span></span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>        sorted_scores <span class="op">/=</span> (scores.<span class="bu">sum</span>() <span class="op">-</span> scores_cumsum)</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tidy up and output</span></span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>        new_scores <span class="op">=</span> torch.zeros(scores_cumsum.shape,device<span class="op">=</span>scores.device)</span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>        new_scores[sorted_idx] <span class="op">=</span> sorted_scores</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> new_scores.view(scores.shape)</span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _normalize(<span class="va">self</span>, group_importance, normalizer):</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> normalizer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(normalizer, typing.Callable):</span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> normalizer(group_importance)</span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"sum"</span>:</span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> group_importance.<span class="bu">sum</span>()</span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"standarization"</span>:</span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (group_importance <span class="op">-</span> group_importance.<span class="bu">min</span>()) <span class="op">/</span> (group_importance.<span class="bu">max</span>() <span class="op">-</span> group_importance.<span class="bu">min</span>()<span class="op">+</span><span class="fl">1e-8</span>)</span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"mean"</span>:</span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> group_importance.mean()</span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"max"</span>:</span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> group_importance.<span class="bu">max</span>()</span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">'gaussian'</span>:</span>
<span id="cb33-77"><a href="#cb33-77" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (group_importance <span class="op">-</span> group_importance.mean()) <span class="op">/</span> (group_importance.std()<span class="op">+</span><span class="fl">1e-8</span>)</span>
<span id="cb33-78"><a href="#cb33-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer.startswith(<span class="st">'sentinel'</span>): <span class="co"># normalize the score with the k-th smallest element. e.g. sentinel_0.5 means median normalization</span></span>
<span id="cb33-79"><a href="#cb33-79" aria-hidden="true" tabindex="-1"></a>            sentinel <span class="op">=</span> <span class="bu">float</span>(normalizer.split(<span class="st">'_'</span>)[<span class="dv">1</span>]) <span class="op">*</span> <span class="bu">len</span>(group_importance)</span>
<span id="cb33-80"><a href="#cb33-80" aria-hidden="true" tabindex="-1"></a>            sentinel <span class="op">=</span> torch.argsort(group_importance, dim<span class="op">=</span><span class="dv">0</span>, descending<span class="op">=</span><span class="va">False</span>)[<span class="bu">int</span>(sentinel)]</span>
<span id="cb33-81"><a href="#cb33-81" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> (group_importance[sentinel]<span class="op">+</span><span class="fl">1e-8</span>)</span>
<span id="cb33-82"><a href="#cb33-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer<span class="op">==</span><span class="st">'lamp'</span>:</span>
<span id="cb33-83"><a href="#cb33-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>._lamp(group_importance)</span>
<span id="cb33-84"><a href="#cb33-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb33-85"><a href="#cb33-85" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb33-86"><a href="#cb33-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-87"><a href="#cb33-87" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _reduce(<span class="va">self</span>, group_imp: typing.List[torch.Tensor], group_idxs: typing.List[typing.List[<span class="bu">int</span>]]):</span>
<span id="cb33-88"><a href="#cb33-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(group_imp) <span class="op">==</span> <span class="dv">0</span>: <span class="cf">return</span> group_imp</span>
<span id="cb33-89"><a href="#cb33-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'prod'</span>:</span>
<span id="cb33-90"><a href="#cb33-90" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">=</span> torch.ones_like(group_imp[<span class="dv">0</span>])</span>
<span id="cb33-91"><a href="#cb33-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'max'</span>:</span>
<span id="cb33-92"><a href="#cb33-92" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">=</span> torch.ones_like(group_imp[<span class="dv">0</span>]) <span class="op">*</span> <span class="op">-</span><span class="dv">99999</span></span>
<span id="cb33-93"><a href="#cb33-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb33-94"><a href="#cb33-94" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">=</span> torch.zeros_like(group_imp[<span class="dv">0</span>])</span>
<span id="cb33-95"><a href="#cb33-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-96"><a href="#cb33-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (imp, root_idxs) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(group_imp, group_idxs)):</span>
<span id="cb33-97"><a href="#cb33-97" aria-hidden="true" tabindex="-1"></a>            imp <span class="op">=</span> imp.to(reduced_imp.device)</span>
<span id="cb33-98"><a href="#cb33-98" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"sum"</span> <span class="kw">or</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"mean"</span>:</span>
<span id="cb33-99"><a href="#cb33-99" aria-hidden="true" tabindex="-1"></a>                reduced_imp.scatter_add_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), imp) <span class="co"># accumulated importance</span></span>
<span id="cb33-100"><a href="#cb33-100" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"max"</span>: <span class="co"># keep the max importance</span></span>
<span id="cb33-101"><a href="#cb33-101" aria-hidden="true" tabindex="-1"></a>                selected_imp <span class="op">=</span> torch.index_select(reduced_imp, <span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device))</span>
<span id="cb33-102"><a href="#cb33-102" aria-hidden="true" tabindex="-1"></a>                selected_imp <span class="op">=</span> torch.maximum(<span class="bu">input</span><span class="op">=</span>selected_imp, other<span class="op">=</span>imp)</span>
<span id="cb33-103"><a href="#cb33-103" aria-hidden="true" tabindex="-1"></a>                reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), selected_imp)</span>
<span id="cb33-104"><a href="#cb33-104" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"prod"</span>: <span class="co"># product of importance</span></span>
<span id="cb33-105"><a href="#cb33-105" aria-hidden="true" tabindex="-1"></a>                selected_imp <span class="op">=</span> torch.index_select(reduced_imp, <span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device))</span>
<span id="cb33-106"><a href="#cb33-106" aria-hidden="true" tabindex="-1"></a>                torch.mul(selected_imp, imp, out<span class="op">=</span>selected_imp)</span>
<span id="cb33-107"><a href="#cb33-107" aria-hidden="true" tabindex="-1"></a>                reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), selected_imp)</span>
<span id="cb33-108"><a href="#cb33-108" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'first'</span>:</span>
<span id="cb33-109"><a href="#cb33-109" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-110"><a href="#cb33-110" aria-hidden="true" tabindex="-1"></a>                    reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), imp)</span>
<span id="cb33-111"><a href="#cb33-111" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'gate'</span>:</span>
<span id="cb33-112"><a href="#cb33-112" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">==</span> <span class="bu">len</span>(group_imp)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb33-113"><a href="#cb33-113" aria-hidden="true" tabindex="-1"></a>                    reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), imp)</span>
<span id="cb33-114"><a href="#cb33-114" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb33-115"><a href="#cb33-115" aria-hidden="true" tabindex="-1"></a>                reduced_imp <span class="op">=</span> torch.stack(group_imp, dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># no reduction</span></span>
<span id="cb33-116"><a href="#cb33-116" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb33-117"><a href="#cb33-117" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb33-118"><a href="#cb33-118" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-119"><a href="#cb33-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"mean"</span>:</span>
<span id="cb33-120"><a href="#cb33-120" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">/=</span> <span class="bu">len</span>(group_imp)</span>
<span id="cb33-121"><a href="#cb33-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> reduced_imp</span>
<span id="cb33-122"><a href="#cb33-122" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-123"><a href="#cb33-123" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb33-124"><a href="#cb33-124" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, group: Group):</span>
<span id="cb33-125"><a href="#cb33-125" aria-hidden="true" tabindex="-1"></a>        group_imp <span class="op">=</span> []</span>
<span id="cb33-126"><a href="#cb33-126" aria-hidden="true" tabindex="-1"></a>        group_idxs <span class="op">=</span> []</span>
<span id="cb33-127"><a href="#cb33-127" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Iterate over all groups and estimate group importance</span></span>
<span id="cb33-128"><a href="#cb33-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (dep, idxs) <span class="kw">in</span> <span class="bu">enumerate</span>(group):</span>
<span id="cb33-129"><a href="#cb33-129" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> dep.layer</span>
<span id="cb33-130"><a href="#cb33-130" aria-hidden="true" tabindex="-1"></a>            prune_fn <span class="op">=</span> dep.pruning_fn</span>
<span id="cb33-131"><a href="#cb33-131" aria-hidden="true" tabindex="-1"></a>            root_idxs <span class="op">=</span> group[i].root_idxs</span>
<span id="cb33-132"><a href="#cb33-132" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(layer, <span class="bu">tuple</span>(<span class="va">self</span>.target_types)):</span>
<span id="cb33-133"><a href="#cb33-133" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb33-134"><a href="#cb33-134" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-135"><a href="#cb33-135" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Conv/Linear Output</span></span>
<span id="cb33-136"><a href="#cb33-136" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-137"><a href="#cb33-137" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> prune_fn <span class="kw">in</span> [</span>
<span id="cb33-138"><a href="#cb33-138" aria-hidden="true" tabindex="-1"></a>                function.prune_conv_out_channels,</span>
<span id="cb33-139"><a href="#cb33-139" aria-hidden="true" tabindex="-1"></a>                function.prune_linear_out_channels,</span>
<span id="cb33-140"><a href="#cb33-140" aria-hidden="true" tabindex="-1"></a>            ]:</span>
<span id="cb33-141"><a href="#cb33-141" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(layer, <span class="st">"transposed"</span>) <span class="kw">and</span> layer.transposed:</span>
<span id="cb33-142"><a href="#cb33-142" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data.transpose(<span class="dv">1</span>, <span class="dv">0</span>)[idxs].flatten(<span class="dv">1</span>)</span>
<span id="cb33-143"><a href="#cb33-143" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb33-144"><a href="#cb33-144" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data[idxs].flatten(<span class="dv">1</span>)</span>
<span id="cb33-145"><a href="#cb33-145" aria-hidden="true" tabindex="-1"></a>                <span class="co">#local_imp = w.abs().pow(self.p).sum(1)</span></span>
<span id="cb33-146"><a href="#cb33-146" aria-hidden="true" tabindex="-1"></a>                local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p).mean(<span class="dv">1</span>)</span>
<span id="cb33-147"><a href="#cb33-147" aria-hidden="true" tabindex="-1"></a>                group_imp.append(local_imp)</span>
<span id="cb33-148"><a href="#cb33-148" aria-hidden="true" tabindex="-1"></a>                group_idxs.append(root_idxs)</span>
<span id="cb33-149"><a href="#cb33-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-150"><a href="#cb33-150" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">and</span> layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb33-151"><a href="#cb33-151" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> layer.bias.data[idxs].<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb33-152"><a href="#cb33-152" aria-hidden="true" tabindex="-1"></a>                    group_imp.append(local_imp)</span>
<span id="cb33-153"><a href="#cb33-153" aria-hidden="true" tabindex="-1"></a>                    group_idxs.append(root_idxs)</span>
<span id="cb33-154"><a href="#cb33-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-155"><a href="#cb33-155" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-156"><a href="#cb33-156" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Conv/Linear Input</span></span>
<span id="cb33-157"><a href="#cb33-157" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-158"><a href="#cb33-158" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> prune_fn <span class="kw">in</span> [</span>
<span id="cb33-159"><a href="#cb33-159" aria-hidden="true" tabindex="-1"></a>                function.prune_conv_in_channels,</span>
<span id="cb33-160"><a href="#cb33-160" aria-hidden="true" tabindex="-1"></a>                function.prune_linear_in_channels,</span>
<span id="cb33-161"><a href="#cb33-161" aria-hidden="true" tabindex="-1"></a>            ]:</span>
<span id="cb33-162"><a href="#cb33-162" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(layer, <span class="st">"transposed"</span>) <span class="kw">and</span> layer.transposed:</span>
<span id="cb33-163"><a href="#cb33-163" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> (layer.weight.data).flatten(<span class="dv">1</span>)</span>
<span id="cb33-164"><a href="#cb33-164" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb33-165"><a href="#cb33-165" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> (layer.weight.data).transpose(<span class="dv">0</span>, <span class="dv">1</span>).flatten(<span class="dv">1</span>)</span>
<span id="cb33-166"><a href="#cb33-166" aria-hidden="true" tabindex="-1"></a>                <span class="co">#local_imp = w.abs().pow(self.p).sum(1)</span></span>
<span id="cb33-167"><a href="#cb33-167" aria-hidden="true" tabindex="-1"></a>                local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p).mean(<span class="dv">1</span>)</span>
<span id="cb33-168"><a href="#cb33-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-169"><a href="#cb33-169" aria-hidden="true" tabindex="-1"></a>                <span class="co"># repeat importance for group convolutions</span></span>
<span id="cb33-170"><a href="#cb33-170" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> prune_fn <span class="op">==</span> function.prune_conv_in_channels <span class="kw">and</span> layer.groups <span class="op">!=</span> layer.in_channels <span class="kw">and</span> layer.groups <span class="op">!=</span> <span class="dv">1</span>:</span>
<span id="cb33-171"><a href="#cb33-171" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> local_imp.repeat(layer.groups)</span>
<span id="cb33-172"><a href="#cb33-172" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb33-173"><a href="#cb33-173" aria-hidden="true" tabindex="-1"></a>                local_imp <span class="op">=</span> local_imp[idxs]</span>
<span id="cb33-174"><a href="#cb33-174" aria-hidden="true" tabindex="-1"></a>                group_imp.append(local_imp)</span>
<span id="cb33-175"><a href="#cb33-175" aria-hidden="true" tabindex="-1"></a>                group_idxs.append(root_idxs)</span>
<span id="cb33-176"><a href="#cb33-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-177"><a href="#cb33-177" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-178"><a href="#cb33-178" aria-hidden="true" tabindex="-1"></a>            <span class="co"># BatchNorm</span></span>
<span id="cb33-179"><a href="#cb33-179" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-180"><a href="#cb33-180" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> prune_fn <span class="op">==</span> function.prune_batchnorm_out_channels:</span>
<span id="cb33-181"><a href="#cb33-181" aria-hidden="true" tabindex="-1"></a>                <span class="co"># regularize BN</span></span>
<span id="cb33-182"><a href="#cb33-182" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> layer.affine:</span>
<span id="cb33-183"><a href="#cb33-183" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data[idxs]</span>
<span id="cb33-184"><a href="#cb33-184" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb33-185"><a href="#cb33-185" aria-hidden="true" tabindex="-1"></a>                    group_imp.append(local_imp)</span>
<span id="cb33-186"><a href="#cb33-186" aria-hidden="true" tabindex="-1"></a>                    group_idxs.append(root_idxs)</span>
<span id="cb33-187"><a href="#cb33-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-188"><a href="#cb33-188" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">and</span> layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb33-189"><a href="#cb33-189" aria-hidden="true" tabindex="-1"></a>                        local_imp <span class="op">=</span> layer.bias.data[idxs].<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb33-190"><a href="#cb33-190" aria-hidden="true" tabindex="-1"></a>                        group_imp.append(local_imp)</span>
<span id="cb33-191"><a href="#cb33-191" aria-hidden="true" tabindex="-1"></a>                        group_idxs.append(root_idxs)</span>
<span id="cb33-192"><a href="#cb33-192" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-193"><a href="#cb33-193" aria-hidden="true" tabindex="-1"></a>            <span class="co"># LayerNorm</span></span>
<span id="cb33-194"><a href="#cb33-194" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb33-195"><a href="#cb33-195" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> prune_fn <span class="op">==</span> function.prune_layernorm_out_channels:</span>
<span id="cb33-196"><a href="#cb33-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-197"><a href="#cb33-197" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> layer.elementwise_affine:</span>
<span id="cb33-198"><a href="#cb33-198" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data[idxs]</span>
<span id="cb33-199"><a href="#cb33-199" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb33-200"><a href="#cb33-200" aria-hidden="true" tabindex="-1"></a>                    group_imp.append(local_imp)</span>
<span id="cb33-201"><a href="#cb33-201" aria-hidden="true" tabindex="-1"></a>                    group_idxs.append(root_idxs)</span>
<span id="cb33-202"><a href="#cb33-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-203"><a href="#cb33-203" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">and</span> layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb33-204"><a href="#cb33-204" aria-hidden="true" tabindex="-1"></a>                        local_imp <span class="op">=</span> layer.bias.data[idxs].<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb33-205"><a href="#cb33-205" aria-hidden="true" tabindex="-1"></a>                        group_imp.append(local_imp)</span>
<span id="cb33-206"><a href="#cb33-206" aria-hidden="true" tabindex="-1"></a>                        group_idxs.append(root_idxs)</span>
<span id="cb33-207"><a href="#cb33-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-208"><a href="#cb33-208" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(group_imp) <span class="op">==</span> <span class="dv">0</span>: <span class="co"># skip groups without parameterized layers</span></span>
<span id="cb33-209"><a href="#cb33-209" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb33-210"><a href="#cb33-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-211"><a href="#cb33-211" aria-hidden="true" tabindex="-1"></a>        group_imp <span class="op">=</span> <span class="va">self</span>._reduce(group_imp, group_idxs)</span>
<span id="cb33-212"><a href="#cb33-212" aria-hidden="true" tabindex="-1"></a>        group_imp <span class="op">=</span> <span class="va">self</span>._normalize(group_imp, <span class="va">self</span>.normalizer)</span>
<span id="cb33-213"><a href="#cb33-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> group_imp</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="a8ff06e9-35bf-4623-97ba-3326c7a29b4b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>timm_models <span class="op">=</span> timm.list_models(module<span class="op">=</span><span class="st">'resnet'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="1c292803-0387-4de3-bcb6-f0960e7fd958" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>timm.list_models(exclude_filters<span class="op">=</span><span class="st">'*vit*'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>['bat_resnext26ts',
 'beit_base_patch16_224',
 'beit_base_patch16_384',
 'beit_large_patch16_224',
 'beit_large_patch16_384',
 'beit_large_patch16_512',
 'beitv2_base_patch16_224',
 'beitv2_large_patch16_224',
 'botnet26t_256',
 'botnet50ts_256',
 'caformer_b36',
 'caformer_m36',
 'caformer_s18',
 'caformer_s36',
 'cait_m36_384',
 'cait_m48_448',
 'cait_s24_224',
 'cait_s24_384',
 'cait_s36_384',
 'cait_xs24_384',
 'cait_xxs24_224',
 'cait_xxs24_384',
 'cait_xxs36_224',
 'cait_xxs36_384',
 'coat_lite_medium',
 'coat_lite_medium_384',
 'coat_lite_mini',
 'coat_lite_small',
 'coat_lite_tiny',
 'coat_mini',
 'coat_small',
 'coat_tiny',
 'coatnet_0_224',
 'coatnet_0_rw_224',
 'coatnet_1_224',
 'coatnet_1_rw_224',
 'coatnet_2_224',
 'coatnet_2_rw_224',
 'coatnet_3_224',
 'coatnet_3_rw_224',
 'coatnet_4_224',
 'coatnet_5_224',
 'coatnet_bn_0_rw_224',
 'coatnet_nano_cc_224',
 'coatnet_nano_rw_224',
 'coatnet_pico_rw_224',
 'coatnet_rmlp_0_rw_224',
 'coatnet_rmlp_1_rw2_224',
 'coatnet_rmlp_1_rw_224',
 'coatnet_rmlp_2_rw_224',
 'coatnet_rmlp_2_rw_384',
 'coatnet_rmlp_3_rw_224',
 'coatnet_rmlp_nano_rw_224',
 'coatnext_nano_rw_224',
 'convformer_b36',
 'convformer_m36',
 'convformer_s18',
 'convformer_s36',
 'convmixer_768_32',
 'convmixer_1024_20_ks9_p14',
 'convmixer_1536_20',
 'convnext_atto',
 'convnext_atto_ols',
 'convnext_base',
 'convnext_femto',
 'convnext_femto_ols',
 'convnext_large',
 'convnext_large_mlp',
 'convnext_nano',
 'convnext_nano_ols',
 'convnext_pico',
 'convnext_pico_ols',
 'convnext_small',
 'convnext_tiny',
 'convnext_tiny_hnf',
 'convnext_xlarge',
 'convnext_xxlarge',
 'convnextv2_atto',
 'convnextv2_base',
 'convnextv2_femto',
 'convnextv2_huge',
 'convnextv2_large',
 'convnextv2_nano',
 'convnextv2_pico',
 'convnextv2_small',
 'convnextv2_tiny',
 'cs3darknet_focus_l',
 'cs3darknet_focus_m',
 'cs3darknet_focus_s',
 'cs3darknet_focus_x',
 'cs3darknet_l',
 'cs3darknet_m',
 'cs3darknet_s',
 'cs3darknet_x',
 'cs3edgenet_x',
 'cs3se_edgenet_x',
 'cs3sedarknet_l',
 'cs3sedarknet_x',
 'cs3sedarknet_xdw',
 'cspdarknet53',
 'cspresnet50',
 'cspresnet50d',
 'cspresnet50w',
 'cspresnext50',
 'darknet17',
 'darknet21',
 'darknet53',
 'darknetaa53',
 'deit3_base_patch16_224',
 'deit3_base_patch16_384',
 'deit3_huge_patch14_224',
 'deit3_large_patch16_224',
 'deit3_large_patch16_384',
 'deit3_medium_patch16_224',
 'deit3_small_patch16_224',
 'deit3_small_patch16_384',
 'deit_base_distilled_patch16_224',
 'deit_base_distilled_patch16_384',
 'deit_base_patch16_224',
 'deit_base_patch16_384',
 'deit_small_distilled_patch16_224',
 'deit_small_patch16_224',
 'deit_tiny_distilled_patch16_224',
 'deit_tiny_patch16_224',
 'densenet121',
 'densenet161',
 'densenet169',
 'densenet201',
 'densenet264d',
 'densenetblur121d',
 'dla34',
 'dla46_c',
 'dla46x_c',
 'dla60',
 'dla60_res2net',
 'dla60_res2next',
 'dla60x',
 'dla60x_c',
 'dla102',
 'dla102x',
 'dla102x2',
 'dla169',
 'dm_nfnet_f0',
 'dm_nfnet_f1',
 'dm_nfnet_f2',
 'dm_nfnet_f3',
 'dm_nfnet_f4',
 'dm_nfnet_f5',
 'dm_nfnet_f6',
 'dpn48b',
 'dpn68',
 'dpn68b',
 'dpn92',
 'dpn98',
 'dpn107',
 'dpn131',
 'eca_botnext26ts_256',
 'eca_halonext26ts',
 'eca_nfnet_l0',
 'eca_nfnet_l1',
 'eca_nfnet_l2',
 'eca_nfnet_l3',
 'eca_resnet33ts',
 'eca_resnext26ts',
 'eca_vovnet39b',
 'ecaresnet26t',
 'ecaresnet50d',
 'ecaresnet50d_pruned',
 'ecaresnet50t',
 'ecaresnet101d',
 'ecaresnet101d_pruned',
 'ecaresnet200d',
 'ecaresnet269d',
 'ecaresnetlight',
 'ecaresnext26t_32x4d',
 'ecaresnext50t_32x4d',
 'edgenext_base',
 'edgenext_small',
 'edgenext_small_rw',
 'edgenext_x_small',
 'edgenext_xx_small',
 'efficientformer_l1',
 'efficientformer_l3',
 'efficientformer_l7',
 'efficientformerv2_l',
 'efficientformerv2_s0',
 'efficientformerv2_s1',
 'efficientformerv2_s2',
 'efficientnet_b0',
 'efficientnet_b0_g8_gn',
 'efficientnet_b0_g16_evos',
 'efficientnet_b0_gn',
 'efficientnet_b1',
 'efficientnet_b1_pruned',
 'efficientnet_b2',
 'efficientnet_b2_pruned',
 'efficientnet_b3',
 'efficientnet_b3_g8_gn',
 'efficientnet_b3_gn',
 'efficientnet_b3_pruned',
 'efficientnet_b4',
 'efficientnet_b5',
 'efficientnet_b6',
 'efficientnet_b7',
 'efficientnet_b8',
 'efficientnet_cc_b0_4e',
 'efficientnet_cc_b0_8e',
 'efficientnet_cc_b1_8e',
 'efficientnet_el',
 'efficientnet_el_pruned',
 'efficientnet_em',
 'efficientnet_es',
 'efficientnet_es_pruned',
 'efficientnet_l2',
 'efficientnet_lite0',
 'efficientnet_lite1',
 'efficientnet_lite2',
 'efficientnet_lite3',
 'efficientnet_lite4',
 'efficientnetv2_l',
 'efficientnetv2_m',
 'efficientnetv2_rw_m',
 'efficientnetv2_rw_s',
 'efficientnetv2_rw_t',
 'efficientnetv2_s',
 'efficientnetv2_xl',
 'ese_vovnet19b_dw',
 'ese_vovnet19b_slim',
 'ese_vovnet19b_slim_dw',
 'ese_vovnet39b',
 'ese_vovnet39b_evos',
 'ese_vovnet57b',
 'ese_vovnet99b',
 'eva02_base_patch14_224',
 'eva02_base_patch14_448',
 'eva02_base_patch16_clip_224',
 'eva02_enormous_patch14_clip_224',
 'eva02_large_patch14_224',
 'eva02_large_patch14_448',
 'eva02_large_patch14_clip_224',
 'eva02_large_patch14_clip_336',
 'eva02_small_patch14_224',
 'eva02_small_patch14_336',
 'eva02_tiny_patch14_224',
 'eva02_tiny_patch14_336',
 'eva_giant_patch14_224',
 'eva_giant_patch14_336',
 'eva_giant_patch14_560',
 'eva_giant_patch14_clip_224',
 'eva_large_patch14_196',
 'eva_large_patch14_336',
 'fbnetc_100',
 'fbnetv3_b',
 'fbnetv3_d',
 'fbnetv3_g',
 'focalnet_base_lrf',
 'focalnet_base_srf',
 'focalnet_huge_fl3',
 'focalnet_huge_fl4',
 'focalnet_large_fl3',
 'focalnet_large_fl4',
 'focalnet_small_lrf',
 'focalnet_small_srf',
 'focalnet_tiny_lrf',
 'focalnet_tiny_srf',
 'focalnet_xlarge_fl3',
 'focalnet_xlarge_fl4',
 'gc_efficientnetv2_rw_t',
 'gcresnet33ts',
 'gcresnet50t',
 'gcresnext26ts',
 'gcresnext50ts',
 'gernet_l',
 'gernet_m',
 'gernet_s',
 'ghostnet_050',
 'ghostnet_100',
 'ghostnet_130',
 'ghostnetv2_100',
 'ghostnetv2_130',
 'ghostnetv2_160',
 'gmixer_12_224',
 'gmixer_24_224',
 'gmlp_b16_224',
 'gmlp_s16_224',
 'gmlp_ti16_224',
 'halo2botnet50ts_256',
 'halonet26t',
 'halonet50ts',
 'halonet_h1',
 'haloregnetz_b',
 'hardcorenas_a',
 'hardcorenas_b',
 'hardcorenas_c',
 'hardcorenas_d',
 'hardcorenas_e',
 'hardcorenas_f',
 'hgnet_base',
 'hgnet_small',
 'hgnet_tiny',
 'hgnetv2_b0',
 'hgnetv2_b1',
 'hgnetv2_b2',
 'hgnetv2_b3',
 'hgnetv2_b4',
 'hgnetv2_b5',
 'hgnetv2_b6',
 'hrnet_w18',
 'hrnet_w18_small',
 'hrnet_w18_small_v2',
 'hrnet_w18_ssld',
 'hrnet_w30',
 'hrnet_w32',
 'hrnet_w40',
 'hrnet_w44',
 'hrnet_w48',
 'hrnet_w48_ssld',
 'hrnet_w64',
 'inception_next_base',
 'inception_next_small',
 'inception_next_tiny',
 'inception_resnet_v2',
 'inception_v3',
 'inception_v4',
 'lambda_resnet26rpt_256',
 'lambda_resnet26t',
 'lambda_resnet50ts',
 'lamhalobotnet50ts_256',
 'lcnet_035',
 'lcnet_050',
 'lcnet_075',
 'lcnet_100',
 'lcnet_150',
 'legacy_senet154',
 'legacy_seresnet18',
 'legacy_seresnet34',
 'legacy_seresnet50',
 'legacy_seresnet101',
 'legacy_seresnet152',
 'legacy_seresnext26_32x4d',
 'legacy_seresnext50_32x4d',
 'legacy_seresnext101_32x4d',
 'legacy_xception',
 'mixer_b16_224',
 'mixer_b32_224',
 'mixer_l16_224',
 'mixer_l32_224',
 'mixer_s16_224',
 'mixer_s32_224',
 'mixnet_l',
 'mixnet_m',
 'mixnet_s',
 'mixnet_xl',
 'mixnet_xxl',
 'mnasnet_050',
 'mnasnet_075',
 'mnasnet_100',
 'mnasnet_140',
 'mnasnet_small',
 'mobilenetv2_035',
 'mobilenetv2_050',
 'mobilenetv2_075',
 'mobilenetv2_100',
 'mobilenetv2_110d',
 'mobilenetv2_120d',
 'mobilenetv2_140',
 'mobilenetv3_large_075',
 'mobilenetv3_large_100',
 'mobilenetv3_rw',
 'mobilenetv3_small_050',
 'mobilenetv3_small_075',
 'mobilenetv3_small_100',
 'mobileone_s0',
 'mobileone_s1',
 'mobileone_s2',
 'mobileone_s3',
 'mobileone_s4',
 'nasnetalarge',
 'nest_base',
 'nest_base_jx',
 'nest_small',
 'nest_small_jx',
 'nest_tiny',
 'nest_tiny_jx',
 'nf_ecaresnet26',
 'nf_ecaresnet50',
 'nf_ecaresnet101',
 'nf_regnet_b0',
 'nf_regnet_b1',
 'nf_regnet_b2',
 'nf_regnet_b3',
 'nf_regnet_b4',
 'nf_regnet_b5',
 'nf_resnet26',
 'nf_resnet50',
 'nf_resnet101',
 'nf_seresnet26',
 'nf_seresnet50',
 'nf_seresnet101',
 'nfnet_f0',
 'nfnet_f1',
 'nfnet_f2',
 'nfnet_f3',
 'nfnet_f4',
 'nfnet_f5',
 'nfnet_f6',
 'nfnet_f7',
 'nfnet_l0',
 'pit_b_224',
 'pit_b_distilled_224',
 'pit_s_224',
 'pit_s_distilled_224',
 'pit_ti_224',
 'pit_ti_distilled_224',
 'pit_xs_224',
 'pit_xs_distilled_224',
 'pnasnet5large',
 'poolformer_m36',
 'poolformer_m48',
 'poolformer_s12',
 'poolformer_s24',
 'poolformer_s36',
 'poolformerv2_m36',
 'poolformerv2_m48',
 'poolformerv2_s12',
 'poolformerv2_s24',
 'poolformerv2_s36',
 'pvt_v2_b0',
 'pvt_v2_b1',
 'pvt_v2_b2',
 'pvt_v2_b2_li',
 'pvt_v2_b3',
 'pvt_v2_b4',
 'pvt_v2_b5',
 'regnetv_040',
 'regnetv_064',
 'regnetx_002',
 'regnetx_004',
 'regnetx_004_tv',
 'regnetx_006',
 'regnetx_008',
 'regnetx_016',
 'regnetx_032',
 'regnetx_040',
 'regnetx_064',
 'regnetx_080',
 'regnetx_120',
 'regnetx_160',
 'regnetx_320',
 'regnety_002',
 'regnety_004',
 'regnety_006',
 'regnety_008',
 'regnety_008_tv',
 'regnety_016',
 'regnety_032',
 'regnety_040',
 'regnety_040_sgn',
 'regnety_064',
 'regnety_080',
 'regnety_080_tv',
 'regnety_120',
 'regnety_160',
 'regnety_320',
 'regnety_640',
 'regnety_1280',
 'regnety_2560',
 'regnetz_005',
 'regnetz_040',
 'regnetz_040_h',
 'regnetz_b16',
 'regnetz_b16_evos',
 'regnetz_c16',
 'regnetz_c16_evos',
 'regnetz_d8',
 'regnetz_d8_evos',
 'regnetz_d32',
 'regnetz_e8',
 'repghostnet_050',
 'repghostnet_058',
 'repghostnet_080',
 'repghostnet_100',
 'repghostnet_111',
 'repghostnet_130',
 'repghostnet_150',
 'repghostnet_200',
 'repvgg_a0',
 'repvgg_a1',
 'repvgg_a2',
 'repvgg_b0',
 'repvgg_b1',
 'repvgg_b1g4',
 'repvgg_b2',
 'repvgg_b2g4',
 'repvgg_b3',
 'repvgg_b3g4',
 'repvgg_d2se',
 'res2net50_14w_8s',
 'res2net50_26w_4s',
 'res2net50_26w_6s',
 'res2net50_26w_8s',
 'res2net50_48w_2s',
 'res2net50d',
 'res2net101_26w_4s',
 'res2net101d',
 'res2next50',
 'resmlp_12_224',
 'resmlp_24_224',
 'resmlp_36_224',
 'resmlp_big_24_224',
 'resnest14d',
 'resnest26d',
 'resnest50d',
 'resnest50d_1s4x24d',
 'resnest50d_4s2x40d',
 'resnest101e',
 'resnest200e',
 'resnest269e',
 'resnet10t',
 'resnet14t',
 'resnet18',
 'resnet18d',
 'resnet26',
 'resnet26d',
 'resnet26t',
 'resnet32ts',
 'resnet33ts',
 'resnet34',
 'resnet34d',
 'resnet50',
 'resnet50_gn',
 'resnet50c',
 'resnet50d',
 'resnet50s',
 'resnet50t',
 'resnet51q',
 'resnet61q',
 'resnet101',
 'resnet101c',
 'resnet101d',
 'resnet101s',
 'resnet152',
 'resnet152c',
 'resnet152d',
 'resnet152s',
 'resnet200',
 'resnet200d',
 'resnetaa34d',
 'resnetaa50',
 'resnetaa50d',
 'resnetaa101d',
 'resnetblur18',
 'resnetblur50',
 'resnetblur50d',
 'resnetblur101d',
 'resnetrs50',
 'resnetrs101',
 'resnetrs152',
 'resnetrs200',
 'resnetrs270',
 'resnetrs350',
 'resnetrs420',
 'resnetv2_50',
 'resnetv2_50d',
 'resnetv2_50d_evos',
 'resnetv2_50d_frn',
 'resnetv2_50d_gn',
 'resnetv2_50t',
 'resnetv2_50x1_bit',
 'resnetv2_50x3_bit',
 'resnetv2_101',
 'resnetv2_101d',
 'resnetv2_101x1_bit',
 'resnetv2_101x3_bit',
 'resnetv2_152',
 'resnetv2_152d',
 'resnetv2_152x2_bit',
 'resnetv2_152x4_bit',
 'resnext26ts',
 'resnext50_32x4d',
 'resnext50d_32x4d',
 'resnext101_32x4d',
 'resnext101_32x8d',
 'resnext101_32x16d',
 'resnext101_32x32d',
 'resnext101_64x4d',
 'rexnet_100',
 'rexnet_130',
 'rexnet_150',
 'rexnet_200',
 'rexnet_300',
 'rexnetr_100',
 'rexnetr_130',
 'rexnetr_150',
 'rexnetr_200',
 'rexnetr_300',
 'sebotnet33ts_256',
 'sedarknet21',
 'sehalonet33ts',
 'selecsls42',
 'selecsls42b',
 'selecsls60',
 'selecsls60b',
 'selecsls84',
 'semnasnet_050',
 'semnasnet_075',
 'semnasnet_100',
 'semnasnet_140',
 'senet154',
 'sequencer2d_l',
 'sequencer2d_m',
 'sequencer2d_s',
 'seresnet18',
 'seresnet33ts',
 'seresnet34',
 'seresnet50',
 'seresnet50t',
 'seresnet101',
 'seresnet152',
 'seresnet152d',
 'seresnet200d',
 'seresnet269d',
 'seresnetaa50d',
 'seresnext26d_32x4d',
 'seresnext26t_32x4d',
 'seresnext26ts',
 'seresnext50_32x4d',
 'seresnext101_32x4d',
 'seresnext101_32x8d',
 'seresnext101_64x4d',
 'seresnext101d_32x8d',
 'seresnextaa101d_32x8d',
 'seresnextaa201d_32x8d',
 'skresnet18',
 'skresnet34',
 'skresnet50',
 'skresnet50d',
 'skresnext50_32x4d',
 'spnasnet_100',
 'swin_base_patch4_window7_224',
 'swin_base_patch4_window12_384',
 'swin_large_patch4_window7_224',
 'swin_large_patch4_window12_384',
 'swin_s3_base_224',
 'swin_s3_small_224',
 'swin_s3_tiny_224',
 'swin_small_patch4_window7_224',
 'swin_tiny_patch4_window7_224',
 'swinv2_base_window8_256',
 'swinv2_base_window12_192',
 'swinv2_base_window12to16_192to256',
 'swinv2_base_window12to24_192to384',
 'swinv2_base_window16_256',
 'swinv2_cr_base_224',
 'swinv2_cr_base_384',
 'swinv2_cr_base_ns_224',
 'swinv2_cr_giant_224',
 'swinv2_cr_giant_384',
 'swinv2_cr_huge_224',
 'swinv2_cr_huge_384',
 'swinv2_cr_large_224',
 'swinv2_cr_large_384',
 'swinv2_cr_small_224',
 'swinv2_cr_small_384',
 'swinv2_cr_small_ns_224',
 'swinv2_cr_small_ns_256',
 'swinv2_cr_tiny_224',
 'swinv2_cr_tiny_384',
 'swinv2_cr_tiny_ns_224',
 'swinv2_large_window12_192',
 'swinv2_large_window12to16_192to256',
 'swinv2_large_window12to24_192to384',
 'swinv2_small_window8_256',
 'swinv2_small_window16_256',
 'swinv2_tiny_window8_256',
 'swinv2_tiny_window16_256',
 'tf_efficientnet_b0',
 'tf_efficientnet_b1',
 'tf_efficientnet_b2',
 'tf_efficientnet_b3',
 'tf_efficientnet_b4',
 'tf_efficientnet_b5',
 'tf_efficientnet_b6',
 'tf_efficientnet_b7',
 'tf_efficientnet_b8',
 'tf_efficientnet_cc_b0_4e',
 'tf_efficientnet_cc_b0_8e',
 'tf_efficientnet_cc_b1_8e',
 'tf_efficientnet_el',
 'tf_efficientnet_em',
 'tf_efficientnet_es',
 'tf_efficientnet_l2',
 'tf_efficientnet_lite0',
 'tf_efficientnet_lite1',
 'tf_efficientnet_lite2',
 'tf_efficientnet_lite3',
 'tf_efficientnet_lite4',
 'tf_efficientnetv2_b0',
 'tf_efficientnetv2_b1',
 'tf_efficientnetv2_b2',
 'tf_efficientnetv2_b3',
 'tf_efficientnetv2_l',
 'tf_efficientnetv2_m',
 'tf_efficientnetv2_s',
 'tf_efficientnetv2_xl',
 'tf_mixnet_l',
 'tf_mixnet_m',
 'tf_mixnet_s',
 'tf_mobilenetv3_large_075',
 'tf_mobilenetv3_large_100',
 'tf_mobilenetv3_large_minimal_100',
 'tf_mobilenetv3_small_075',
 'tf_mobilenetv3_small_100',
 'tf_mobilenetv3_small_minimal_100',
 'tinynet_a',
 'tinynet_b',
 'tinynet_c',
 'tinynet_d',
 'tinynet_e',
 'tnt_b_patch16_224',
 'tnt_s_patch16_224',
 'tresnet_l',
 'tresnet_m',
 'tresnet_v2_l',
 'tresnet_xl',
 'twins_pcpvt_base',
 'twins_pcpvt_large',
 'twins_pcpvt_small',
 'twins_svt_base',
 'twins_svt_large',
 'twins_svt_small',
 'vgg11',
 'vgg11_bn',
 'vgg13',
 'vgg13_bn',
 'vgg16',
 'vgg16_bn',
 'vgg19',
 'vgg19_bn',
 'visformer_small',
 'visformer_tiny',
 'volo_d1_224',
 'volo_d1_384',
 'volo_d2_224',
 'volo_d2_384',
 'volo_d3_224',
 'volo_d3_448',
 'volo_d4_224',
 'volo_d4_448',
 'volo_d5_224',
 'volo_d5_448',
 'volo_d5_512',
 'vovnet39a',
 'vovnet57a',
 'wide_resnet50_2',
 'wide_resnet101_2',
 'xception41',
 'xception41p',
 'xception65',
 'xception65p',
 'xception71',
 'xcit_large_24_p8_224',
 'xcit_large_24_p8_384',
 'xcit_large_24_p16_224',
 'xcit_large_24_p16_384',
 'xcit_medium_24_p8_224',
 'xcit_medium_24_p8_384',
 'xcit_medium_24_p16_224',
 'xcit_medium_24_p16_384',
 'xcit_nano_12_p8_224',
 'xcit_nano_12_p8_384',
 'xcit_nano_12_p16_224',
 'xcit_nano_12_p16_384',
 'xcit_small_12_p8_224',
 'xcit_small_12_p8_384',
 'xcit_small_12_p16_224',
 'xcit_small_12_p16_384',
 'xcit_small_24_p8_224',
 'xcit_small_24_p8_384',
 'xcit_small_24_p16_224',
 'xcit_small_24_p16_384',
 'xcit_tiny_12_p8_224',
 'xcit_tiny_12_p8_384',
 'xcit_tiny_12_p16_224',
 'xcit_tiny_12_p16_384',
 'xcit_tiny_24_p8_224',
 'xcit_tiny_24_p8_384',
 'xcit_tiny_24_p16_224',
 'xcit_tiny_24_p16_384']</code></pre>
</div>
</div>
<div id="e79e1992-1f22-4e90-9383-99e362d258b4" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> timm.create_model(<span class="st">'seresnet18'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="991ffa7c-15a4-4f24-a6ae-ad9f6ab3c702" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<pre><code>ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (drop_block): Identity()
      (act1): ReLU(inplace=True)
      (aa): Identity()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (se): SEModule(
        (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
        (bn): Identity()
        (act): ReLU(inplace=True)
        (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
      (act2): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)</code></pre>
</div>
</div>
<div id="2e805623-0226-4efc-97bf-38c61cff6ac7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fasterai.prune.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="4d105013-4076-4a3f-9d8e-c62274e57a07" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>dummy_input <span class="op">=</span> torch.randn(<span class="dv">16</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="8f228819-fbae-47ad-8562-f2a144460223" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> timm.create_model(<span class="st">'seresnet18'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c27e7daa-a62d-4526-9389-e83851178310" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>benchmark(m, dummy_input)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Inference time CPU (ms/image):6.694 ms +/- 0.092 ms
FPS CPU: 149.3904126039974
Inference time GPU (ms/image): 0.267 ms +/- 0.003 ms
FPS GPU: 3743.6464639069623
Nombre de paramètres: 11.780 M
Taille du modèle: 44.936 MiB
Nombre de MACs: 1817.400 M</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>(107.10191986960126, 1.479711068861181, 4.27390784740448, 0.05194234267945458)</code></pre>
</div>
</div>
<div id="3fa95998-1a99-498d-a58a-60b80975c205" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>pr <span class="op">=</span> Pruner(m, <span class="st">'local'</span>, large_final, layer_type<span class="op">=</span>[nn.Conv2d])</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>pr.prune_model(<span class="dv">30</span>, round_to<span class="op">=</span><span class="dv">8</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="df100666-0a6e-4e54-87fa-fc26a3eb52e5" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>benchmark(m, dummy_input)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Inference time CPU (ms/image):3.316 ms +/- 0.040 ms
FPS CPU: 301.55608187204695
Inference time GPU (ms/image): 0.183 ms +/- 0.003 ms
FPS GPU: 5469.600455484674
Nombre de paramètres: 5.657 M
Taille du modèle: 21.580 MiB
Nombre de MACs: 836.963 M</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>(53.058124050003244,
 0.6349589479868257,
 2.9252593731880188,
 0.04034922064140752)</code></pre>
</div>
</div>
<div id="5bb931e0-6956-41c9-b46e-f6e634bc446d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> abc</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> typing</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch_pruning <span class="im">import</span> function</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch_pruning.dependency <span class="im">import</span> Group</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Importance(abc.ABC):</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Estimate the importance of a tp.Dependency.Group, and return an 1-D per-channel importance score.</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="co">        It should accept a group as inputs, and return a 1-D tensor with the same length as the number of channels.</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="co">        All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups.</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Example:</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a><span class="co">            ```python</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a><span class="co">            DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224)) </span></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="co">            group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )    </span></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a><span class="co">            scorer = MagnitudeImportance()    </span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="co">            imp_score = scorer(group)    </span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="co">            #imp_score is a 1-D tensor with length 3 for channels [2, 6, 9]  </span></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a><span class="co">            min_score = imp_score.min() </span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a><span class="co">            ``` </span></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">@abc.abstractclassmethod</span></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, group: Group) <span class="op">-&gt;</span> torch.Tensor: </span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GroupNormImportance(Importance):</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, </span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>                 p: <span class="bu">int</span><span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>                 group_reduction: <span class="bu">str</span><span class="op">=</span><span class="st">"mean"</span>, </span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>                 normalizer: <span class="bu">str</span><span class="op">=</span><span class="st">'mean'</span>, </span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>                 bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>                 target_types:<span class="bu">list</span><span class="op">=</span>[nn.modules.conv._ConvNd, nn.Linear, nn.modules.batchnorm._BatchNorm, nn.LayerNorm]):</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.group_reduction <span class="op">=</span> group_reduction</span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.normalizer <span class="op">=</span> normalizer</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_types <span class="op">=</span> target_types</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> bias</span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _lamp(<span class="va">self</span>, scores): <span class="co"># Layer-adaptive Sparsity for the Magnitude-based Pruning</span></span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a><span class="co">        Normalizing scheme for LAMP.</span></span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sort scores in an ascending order</span></span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>        sorted_scores,sorted_idx <span class="op">=</span> scores.view(<span class="op">-</span><span class="dv">1</span>).sort(descending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute cumulative sum</span></span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>        scores_cumsum_temp <span class="op">=</span> sorted_scores.cumsum(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>        scores_cumsum <span class="op">=</span> torch.zeros(scores_cumsum_temp.shape,device<span class="op">=</span>scores.device)</span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a>        scores_cumsum[<span class="dv">1</span>:] <span class="op">=</span> scores_cumsum_temp[:<span class="bu">len</span>(scores_cumsum_temp)<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># normalize by cumulative sum</span></span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a>        sorted_scores <span class="op">/=</span> (scores.<span class="bu">sum</span>() <span class="op">-</span> scores_cumsum)</span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># tidy up and output</span></span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a>        new_scores <span class="op">=</span> torch.zeros(scores_cumsum.shape,device<span class="op">=</span>scores.device)</span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>        new_scores[sorted_idx] <span class="op">=</span> sorted_scores</span>
<span id="cb50-60"><a href="#cb50-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-61"><a href="#cb50-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> new_scores.view(scores.shape)</span>
<span id="cb50-62"><a href="#cb50-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-63"><a href="#cb50-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _normalize(<span class="va">self</span>, group_importance, normalizer):</span>
<span id="cb50-64"><a href="#cb50-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> normalizer <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb50-65"><a href="#cb50-65" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance</span>
<span id="cb50-66"><a href="#cb50-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(normalizer, typing.Callable):</span>
<span id="cb50-67"><a href="#cb50-67" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> normalizer(group_importance)</span>
<span id="cb50-68"><a href="#cb50-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"sum"</span>:</span>
<span id="cb50-69"><a href="#cb50-69" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> group_importance.<span class="bu">sum</span>()</span>
<span id="cb50-70"><a href="#cb50-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"standarization"</span>:</span>
<span id="cb50-71"><a href="#cb50-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (group_importance <span class="op">-</span> group_importance.<span class="bu">min</span>()) <span class="op">/</span> (group_importance.<span class="bu">max</span>() <span class="op">-</span> group_importance.<span class="bu">min</span>()<span class="op">+</span><span class="fl">1e-8</span>)</span>
<span id="cb50-72"><a href="#cb50-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"mean"</span>:</span>
<span id="cb50-73"><a href="#cb50-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> group_importance.mean()</span>
<span id="cb50-74"><a href="#cb50-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">"max"</span>:</span>
<span id="cb50-75"><a href="#cb50-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> group_importance.<span class="bu">max</span>()</span>
<span id="cb50-76"><a href="#cb50-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer <span class="op">==</span> <span class="st">'gaussian'</span>:</span>
<span id="cb50-77"><a href="#cb50-77" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> (group_importance <span class="op">-</span> group_importance.mean()) <span class="op">/</span> (group_importance.std()<span class="op">+</span><span class="fl">1e-8</span>)</span>
<span id="cb50-78"><a href="#cb50-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer.startswith(<span class="st">'sentinel'</span>): <span class="co"># normalize the score with the k-th smallest element. e.g. sentinel_0.5 means median normalization</span></span>
<span id="cb50-79"><a href="#cb50-79" aria-hidden="true" tabindex="-1"></a>            sentinel <span class="op">=</span> <span class="bu">float</span>(normalizer.split(<span class="st">'_'</span>)[<span class="dv">1</span>]) <span class="op">*</span> <span class="bu">len</span>(group_importance)</span>
<span id="cb50-80"><a href="#cb50-80" aria-hidden="true" tabindex="-1"></a>            sentinel <span class="op">=</span> torch.argsort(group_importance, dim<span class="op">=</span><span class="dv">0</span>, descending<span class="op">=</span><span class="va">False</span>)[<span class="bu">int</span>(sentinel)]</span>
<span id="cb50-81"><a href="#cb50-81" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> group_importance <span class="op">/</span> (group_importance[sentinel]<span class="op">+</span><span class="fl">1e-8</span>)</span>
<span id="cb50-82"><a href="#cb50-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> normalizer<span class="op">==</span><span class="st">'lamp'</span>:</span>
<span id="cb50-83"><a href="#cb50-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>._lamp(group_importance)</span>
<span id="cb50-84"><a href="#cb50-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb50-85"><a href="#cb50-85" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb50-86"><a href="#cb50-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-87"><a href="#cb50-87" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _reduce(<span class="va">self</span>, group_imp: typing.List[torch.Tensor], group_idxs: typing.List[typing.List[<span class="bu">int</span>]]):</span>
<span id="cb50-88"><a href="#cb50-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(group_imp) <span class="op">==</span> <span class="dv">0</span>: <span class="cf">return</span> group_imp</span>
<span id="cb50-89"><a href="#cb50-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'prod'</span>:</span>
<span id="cb50-90"><a href="#cb50-90" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">=</span> torch.ones_like(group_imp[<span class="dv">0</span>])</span>
<span id="cb50-91"><a href="#cb50-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'max'</span>:</span>
<span id="cb50-92"><a href="#cb50-92" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">=</span> torch.ones_like(group_imp[<span class="dv">0</span>]) <span class="op">*</span> <span class="op">-</span><span class="dv">99999</span></span>
<span id="cb50-93"><a href="#cb50-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb50-94"><a href="#cb50-94" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">=</span> torch.zeros_like(group_imp[<span class="dv">0</span>])</span>
<span id="cb50-95"><a href="#cb50-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-96"><a href="#cb50-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (imp, root_idxs) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(group_imp, group_idxs)):</span>
<span id="cb50-97"><a href="#cb50-97" aria-hidden="true" tabindex="-1"></a>            imp <span class="op">=</span> imp.to(reduced_imp.device)</span>
<span id="cb50-98"><a href="#cb50-98" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"sum"</span> <span class="kw">or</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"mean"</span>:</span>
<span id="cb50-99"><a href="#cb50-99" aria-hidden="true" tabindex="-1"></a>                reduced_imp.scatter_add_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), imp) <span class="co"># accumulated importance</span></span>
<span id="cb50-100"><a href="#cb50-100" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"max"</span>: <span class="co"># keep the max importance</span></span>
<span id="cb50-101"><a href="#cb50-101" aria-hidden="true" tabindex="-1"></a>                selected_imp <span class="op">=</span> torch.index_select(reduced_imp, <span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device))</span>
<span id="cb50-102"><a href="#cb50-102" aria-hidden="true" tabindex="-1"></a>                selected_imp <span class="op">=</span> torch.maximum(<span class="bu">input</span><span class="op">=</span>selected_imp, other<span class="op">=</span>imp)</span>
<span id="cb50-103"><a href="#cb50-103" aria-hidden="true" tabindex="-1"></a>                reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), selected_imp)</span>
<span id="cb50-104"><a href="#cb50-104" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"prod"</span>: <span class="co"># product of importance</span></span>
<span id="cb50-105"><a href="#cb50-105" aria-hidden="true" tabindex="-1"></a>                selected_imp <span class="op">=</span> torch.index_select(reduced_imp, <span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device))</span>
<span id="cb50-106"><a href="#cb50-106" aria-hidden="true" tabindex="-1"></a>                torch.mul(selected_imp, imp, out<span class="op">=</span>selected_imp)</span>
<span id="cb50-107"><a href="#cb50-107" aria-hidden="true" tabindex="-1"></a>                reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), selected_imp)</span>
<span id="cb50-108"><a href="#cb50-108" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'first'</span>:</span>
<span id="cb50-109"><a href="#cb50-109" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb50-110"><a href="#cb50-110" aria-hidden="true" tabindex="-1"></a>                    reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), imp)</span>
<span id="cb50-111"><a href="#cb50-111" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">'gate'</span>:</span>
<span id="cb50-112"><a href="#cb50-112" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> i <span class="op">==</span> <span class="bu">len</span>(group_imp)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb50-113"><a href="#cb50-113" aria-hidden="true" tabindex="-1"></a>                    reduced_imp.scatter_(<span class="dv">0</span>, torch.tensor(root_idxs, device<span class="op">=</span>imp.device), imp)</span>
<span id="cb50-114"><a href="#cb50-114" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> <span class="va">self</span>.group_reduction <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb50-115"><a href="#cb50-115" aria-hidden="true" tabindex="-1"></a>                reduced_imp <span class="op">=</span> torch.stack(group_imp, dim<span class="op">=</span><span class="dv">0</span>) <span class="co"># no reduction</span></span>
<span id="cb50-116"><a href="#cb50-116" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb50-117"><a href="#cb50-117" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb50-118"><a href="#cb50-118" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-119"><a href="#cb50-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.group_reduction <span class="op">==</span> <span class="st">"mean"</span>:</span>
<span id="cb50-120"><a href="#cb50-120" aria-hidden="true" tabindex="-1"></a>            reduced_imp <span class="op">/=</span> <span class="bu">len</span>(group_imp)</span>
<span id="cb50-121"><a href="#cb50-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> reduced_imp</span>
<span id="cb50-122"><a href="#cb50-122" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-123"><a href="#cb50-123" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb50-124"><a href="#cb50-124" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, group: Group):</span>
<span id="cb50-125"><a href="#cb50-125" aria-hidden="true" tabindex="-1"></a>        group_imp <span class="op">=</span> []</span>
<span id="cb50-126"><a href="#cb50-126" aria-hidden="true" tabindex="-1"></a>        group_idxs <span class="op">=</span> []</span>
<span id="cb50-127"><a href="#cb50-127" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Iterate over all groups and estimate group importance</span></span>
<span id="cb50-128"><a href="#cb50-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (dep, idxs) <span class="kw">in</span> <span class="bu">enumerate</span>(group):</span>
<span id="cb50-129"><a href="#cb50-129" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> dep.layer</span>
<span id="cb50-130"><a href="#cb50-130" aria-hidden="true" tabindex="-1"></a>            prune_fn <span class="op">=</span> dep.pruning_fn</span>
<span id="cb50-131"><a href="#cb50-131" aria-hidden="true" tabindex="-1"></a>            root_idxs <span class="op">=</span> group[i].root_idxs</span>
<span id="cb50-132"><a href="#cb50-132" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(layer, <span class="bu">tuple</span>(<span class="va">self</span>.target_types)):</span>
<span id="cb50-133"><a href="#cb50-133" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb50-134"><a href="#cb50-134" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-135"><a href="#cb50-135" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Conv/Linear Output</span></span>
<span id="cb50-136"><a href="#cb50-136" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-137"><a href="#cb50-137" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> prune_fn <span class="kw">in</span> [</span>
<span id="cb50-138"><a href="#cb50-138" aria-hidden="true" tabindex="-1"></a>                function.prune_conv_out_channels,</span>
<span id="cb50-139"><a href="#cb50-139" aria-hidden="true" tabindex="-1"></a>                function.prune_linear_out_channels,</span>
<span id="cb50-140"><a href="#cb50-140" aria-hidden="true" tabindex="-1"></a>            ]:</span>
<span id="cb50-141"><a href="#cb50-141" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(layer, <span class="st">"transposed"</span>) <span class="kw">and</span> layer.transposed:</span>
<span id="cb50-142"><a href="#cb50-142" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data.transpose(<span class="dv">1</span>, <span class="dv">0</span>)[idxs].flatten(<span class="dv">1</span>)</span>
<span id="cb50-143"><a href="#cb50-143" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb50-144"><a href="#cb50-144" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data[idxs].flatten(<span class="dv">1</span>)</span>
<span id="cb50-145"><a href="#cb50-145" aria-hidden="true" tabindex="-1"></a>                <span class="co">#local_imp = w.abs().pow(self.p).sum(1)</span></span>
<span id="cb50-146"><a href="#cb50-146" aria-hidden="true" tabindex="-1"></a>                local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p).mean(<span class="dv">1</span>)</span>
<span id="cb50-147"><a href="#cb50-147" aria-hidden="true" tabindex="-1"></a>                group_imp.append(local_imp)</span>
<span id="cb50-148"><a href="#cb50-148" aria-hidden="true" tabindex="-1"></a>                group_idxs.append(root_idxs)</span>
<span id="cb50-149"><a href="#cb50-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-150"><a href="#cb50-150" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">and</span> layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb50-151"><a href="#cb50-151" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> layer.bias.data[idxs].<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb50-152"><a href="#cb50-152" aria-hidden="true" tabindex="-1"></a>                    group_imp.append(local_imp)</span>
<span id="cb50-153"><a href="#cb50-153" aria-hidden="true" tabindex="-1"></a>                    group_idxs.append(root_idxs)</span>
<span id="cb50-154"><a href="#cb50-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-155"><a href="#cb50-155" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-156"><a href="#cb50-156" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Conv/Linear Input</span></span>
<span id="cb50-157"><a href="#cb50-157" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-158"><a href="#cb50-158" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> prune_fn <span class="kw">in</span> [</span>
<span id="cb50-159"><a href="#cb50-159" aria-hidden="true" tabindex="-1"></a>                function.prune_conv_in_channels,</span>
<span id="cb50-160"><a href="#cb50-160" aria-hidden="true" tabindex="-1"></a>                function.prune_linear_in_channels,</span>
<span id="cb50-161"><a href="#cb50-161" aria-hidden="true" tabindex="-1"></a>            ]:</span>
<span id="cb50-162"><a href="#cb50-162" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(layer, <span class="st">"transposed"</span>) <span class="kw">and</span> layer.transposed:</span>
<span id="cb50-163"><a href="#cb50-163" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> (layer.weight.data).flatten(<span class="dv">1</span>)</span>
<span id="cb50-164"><a href="#cb50-164" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb50-165"><a href="#cb50-165" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> (layer.weight.data).transpose(<span class="dv">0</span>, <span class="dv">1</span>).flatten(<span class="dv">1</span>)</span>
<span id="cb50-166"><a href="#cb50-166" aria-hidden="true" tabindex="-1"></a>                <span class="co">#local_imp = w.abs().pow(self.p).sum(1)</span></span>
<span id="cb50-167"><a href="#cb50-167" aria-hidden="true" tabindex="-1"></a>                local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p).mean(<span class="dv">1</span>)</span>
<span id="cb50-168"><a href="#cb50-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-169"><a href="#cb50-169" aria-hidden="true" tabindex="-1"></a>                <span class="co"># repeat importance for group convolutions</span></span>
<span id="cb50-170"><a href="#cb50-170" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> prune_fn <span class="op">==</span> function.prune_conv_in_channels <span class="kw">and</span> layer.groups <span class="op">!=</span> layer.in_channels <span class="kw">and</span> layer.groups <span class="op">!=</span> <span class="dv">1</span>:</span>
<span id="cb50-171"><a href="#cb50-171" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> local_imp.repeat(layer.groups)</span>
<span id="cb50-172"><a href="#cb50-172" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb50-173"><a href="#cb50-173" aria-hidden="true" tabindex="-1"></a>                local_imp <span class="op">=</span> local_imp[idxs]</span>
<span id="cb50-174"><a href="#cb50-174" aria-hidden="true" tabindex="-1"></a>                group_imp.append(local_imp)</span>
<span id="cb50-175"><a href="#cb50-175" aria-hidden="true" tabindex="-1"></a>                group_idxs.append(root_idxs)</span>
<span id="cb50-176"><a href="#cb50-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-177"><a href="#cb50-177" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-178"><a href="#cb50-178" aria-hidden="true" tabindex="-1"></a>            <span class="co"># BatchNorm</span></span>
<span id="cb50-179"><a href="#cb50-179" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-180"><a href="#cb50-180" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> prune_fn <span class="op">==</span> function.prune_batchnorm_out_channels:</span>
<span id="cb50-181"><a href="#cb50-181" aria-hidden="true" tabindex="-1"></a>                <span class="co"># regularize BN</span></span>
<span id="cb50-182"><a href="#cb50-182" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> layer.affine:</span>
<span id="cb50-183"><a href="#cb50-183" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data[idxs]</span>
<span id="cb50-184"><a href="#cb50-184" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb50-185"><a href="#cb50-185" aria-hidden="true" tabindex="-1"></a>                    group_imp.append(local_imp)</span>
<span id="cb50-186"><a href="#cb50-186" aria-hidden="true" tabindex="-1"></a>                    group_idxs.append(root_idxs)</span>
<span id="cb50-187"><a href="#cb50-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-188"><a href="#cb50-188" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">and</span> layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb50-189"><a href="#cb50-189" aria-hidden="true" tabindex="-1"></a>                        local_imp <span class="op">=</span> layer.bias.data[idxs].<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb50-190"><a href="#cb50-190" aria-hidden="true" tabindex="-1"></a>                        group_imp.append(local_imp)</span>
<span id="cb50-191"><a href="#cb50-191" aria-hidden="true" tabindex="-1"></a>                        group_idxs.append(root_idxs)</span>
<span id="cb50-192"><a href="#cb50-192" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-193"><a href="#cb50-193" aria-hidden="true" tabindex="-1"></a>            <span class="co"># LayerNorm</span></span>
<span id="cb50-194"><a href="#cb50-194" aria-hidden="true" tabindex="-1"></a>            <span class="co">####################</span></span>
<span id="cb50-195"><a href="#cb50-195" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> prune_fn <span class="op">==</span> function.prune_layernorm_out_channels:</span>
<span id="cb50-196"><a href="#cb50-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-197"><a href="#cb50-197" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> layer.elementwise_affine:</span>
<span id="cb50-198"><a href="#cb50-198" aria-hidden="true" tabindex="-1"></a>                    w <span class="op">=</span> layer.weight.data[idxs]</span>
<span id="cb50-199"><a href="#cb50-199" aria-hidden="true" tabindex="-1"></a>                    local_imp <span class="op">=</span> w.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb50-200"><a href="#cb50-200" aria-hidden="true" tabindex="-1"></a>                    group_imp.append(local_imp)</span>
<span id="cb50-201"><a href="#cb50-201" aria-hidden="true" tabindex="-1"></a>                    group_idxs.append(root_idxs)</span>
<span id="cb50-202"><a href="#cb50-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-203"><a href="#cb50-203" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">and</span> layer.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb50-204"><a href="#cb50-204" aria-hidden="true" tabindex="-1"></a>                        local_imp <span class="op">=</span> layer.bias.data[idxs].<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="va">self</span>.p)</span>
<span id="cb50-205"><a href="#cb50-205" aria-hidden="true" tabindex="-1"></a>                        group_imp.append(local_imp)</span>
<span id="cb50-206"><a href="#cb50-206" aria-hidden="true" tabindex="-1"></a>                        group_idxs.append(root_idxs)</span>
<span id="cb50-207"><a href="#cb50-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-208"><a href="#cb50-208" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(group_imp) <span class="op">==</span> <span class="dv">0</span>: <span class="co"># skip groups without parameterized layers</span></span>
<span id="cb50-209"><a href="#cb50-209" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb50-210"><a href="#cb50-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-211"><a href="#cb50-211" aria-hidden="true" tabindex="-1"></a>        group_imp <span class="op">=</span> <span class="va">self</span>._reduce(group_imp, group_idxs)</span>
<span id="cb50-212"><a href="#cb50-212" aria-hidden="true" tabindex="-1"></a>        group_imp <span class="op">=</span> <span class="va">self</span>._normalize(group_imp, <span class="va">self</span>.normalizer)</span>
<span id="cb50-213"><a href="#cb50-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> group_imp</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/FasterAI-Labs\.github\.io\/fasterai\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© By Nathan Hubens</p>
<div class="toc-actions"><ul><li><a href="https://github.com/FasterAI-Labs/fasterai/tree/master/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>