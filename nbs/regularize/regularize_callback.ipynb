{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3fef4a5c",
   "metadata": {},
   "source": [
    "---\n",
    "description: Perform Group Regularization in fastai Callback system\n",
    "output-file: regularizer.html\n",
    "title: Regularize Callback\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp regularize.regularize_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9314896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastai.callback.all import *\n",
    "from fastcore.basics import store_attr, listify\n",
    "from fasterai.core.criteria import *\n",
    "from fasterai.core.granularity import *\n",
    "from fasterai.core.schedule import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Union, Optional, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularize-overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `RegularizeCallback` applies structured regularization during training to encourage weight sparsity at various granularities. This is useful as a pre-pruning step: by regularizing groups of weights toward zero during training, subsequent pruning can remove more parameters with less accuracy loss.\n",
    "\n",
    "**Key Features:**\n",
    "- Supports multiple granularity levels (`'weight'`, `'vector'`, `'kernel'`, `'filter'`)\n",
    "- Compatible with any criteria from `fasterai.core.criteria`\n",
    "- Optional scheduling to vary regularization strength over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RegularizeCallback(Callback):\n",
    "    def __init__(self, \n",
    "                 criteria: Union[Criteria, list[Criteria]],            # Criteria(s) to use for regularization\n",
    "                 granularity: Union[str, list[str]],                   # Granularity level(s) for grouping\n",
    "                 weight: float = 0.01,                                 # Regularization weight\n",
    "                 layer_types: Union[Type, list[Type]] = nn.Conv2d,     # Layer types to apply regularization to\n",
    "                 schedule: Optional[Schedule] = None,                  # Optional schedule for regularization weight\n",
    "                 per_layer_weights: Optional[dict[str, float]] = None, # Optional per-layer weights\n",
    "                 verbose: bool = False                                 # Whether to report regularization weight\n",
    "    ):\n",
    "        \"Callback to apply regularization using criteria during training\"\n",
    "        store_attr()\n",
    "        self.criteria = listify(criteria)\n",
    "        self.granularity = listify(granularity)\n",
    "        self.layer_types = listify(layer_types)\n",
    "        self.per_layer_weights = per_layer_weights or {}\n",
    "        self.current_weight = weight\n",
    "        \n",
    "    def before_batch(self) -> None:\n",
    "        \"Update regularization weight if scheduled\"\n",
    "        if self.schedule is not None:\n",
    "            self.current_weight = self.schedule([self.weight], self.pct_train)[0]\n",
    "        \n",
    "    def after_loss(self) -> None:\n",
    "        \"Apply regularization after computing the main loss\"\n",
    "        reg = self.get_norm()\n",
    "        self.learn.loss_grad += reg\n",
    "        self.learn.loss = self.learn.loss_grad.clone()\n",
    "    \n",
    "    def _iter_layers(self):\n",
    "        \"Iterate over matching layers with weights\"\n",
    "        for m in self.learn.model.modules():\n",
    "            if any(isinstance(m, lt) for lt in self.layer_types) and hasattr(m, 'weight'):\n",
    "                yield m\n",
    "            \n",
    "    def get_norm(self) -> torch.Tensor:\n",
    "        \"Compute regularization using the specified criteria and granularities\"\n",
    "        # Pre-filter modules once\n",
    "        layers = list(self._iter_layers())\n",
    "        \n",
    "        layer_regs = []\n",
    "        for crit in self.criteria:\n",
    "            for g in self.granularity:\n",
    "                for m in layers:\n",
    "                    try:\n",
    "                        scores = crit.f(m.weight)[None].abs().sum(Granularities.get_dim(m, g))\n",
    "                        layer_regs.append(self.current_weight * scores.sum())\n",
    "                    except (KeyError, ValueError) as e:\n",
    "                        import warnings\n",
    "                        warnings.warn(f\"Skipping regularization for {type(m).__name__}: {e}\")\n",
    "                    except RuntimeError as e:\n",
    "                        import warnings\n",
    "                        warnings.warn(f\"Runtime error in regularization for {type(m).__name__}: {e}\")\n",
    "        \n",
    "        return torch.stack(layer_regs).sum() if layer_regs else torch.tensor(0.0)\n",
    "    \n",
    "    def after_epoch(self) -> None:\n",
    "        \"Report current regularization weight if verbose\"\n",
    "        if self.verbose:\n",
    "            print(f\"Current regularization weight: {self.current_weight:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd300f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RegularizeCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularize-params",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "- `criteria`: Importance criteria to use for computing regularization (e.g., `large_final`)\n",
    "- `granularity`: Level at which to group weights (`'weight'`, `'vector'`, `'kernel'`, `'filter'`)\n",
    "- `weight`: Regularization coefficient (higher = stronger regularization)\n",
    "- `layer_types`: Module types to regularize (default: `nn.Conv2d`)\n",
    "- `schedule`: Optional schedule to vary regularization strength over training\n",
    "- `verbose`: Print regularization weight after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separator-1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-header",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "\n",
    "Apply filter-level L1 regularization to encourage entire filters to become unimportant (making them easier to prune later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usage-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fasterai.regularize.regularize_callback import RegularizeCallback\n",
    "# from fasterai.core.criteria import large_final\n",
    "\n",
    "# # Apply L1 regularization at filter granularity\n",
    "# cb = RegularizeCallback(\n",
    "#     criteria=large_final,\n",
    "#     granularity='filter',\n",
    "#     weight=0.01,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# learn.fit(10, cbs=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-explain",
   "metadata": {},
   "source": [
    "**Typical Workflow:**\n",
    "1. Train with `RegularizeCallback` to push unimportant filter groups toward zero\n",
    "2. After training, use `PruneCallback` or `Pruner` to remove the zeroed-out structures\n",
    "3. Fine-tune the pruned model to recover any lost accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64avuvp2127",
   "metadata": {},
   "source": "---\n\n## See Also\n\n- [Sparsifier](../sparse/sparsifier.html) - Apply sparsification after regularization pushes weights to zero\n- [Criteria](../core/criteria.html) - Importance measures that can leverage regularized weights\n- [SparsifyCallback](../sparse/sparsify_callback.html) - Combine with sparsification for gradual pruning"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
