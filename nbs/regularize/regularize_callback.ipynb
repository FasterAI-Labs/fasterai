{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3fef4a5c",
   "metadata": {},
   "source": [
    "---\n",
    "description: Perform Group Regularization in fastai Callback system\n",
    "output-file: regularizer.html\n",
    "title: Regularize Callback\n",
    "skip_showdoc: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp regularize.regularize_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9314896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import warnings\n",
    "\n",
    "from fastai.callback.all import *\n",
    "from fastcore.basics import store_attr, listify\n",
    "from fasterai.core.criteria import *\n",
    "from fasterai.core.granularity import *\n",
    "from fasterai.core.schedule import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularize-overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `RegularizeCallback` applies structured regularization during training to encourage weight sparsity at various granularities. This is useful as a pre-pruning step: by regularizing groups of weights toward zero during training, subsequent pruning can remove more parameters with less accuracy loss.\n",
    "\n",
    "**Key Features:**\n",
    "- Supports multiple granularity levels (`'weight'`, `'vector'`, `'kernel'`, `'filter'`)\n",
    "- Compatible with any criteria from `fasterai.core.criteria`\n",
    "- Optional scheduling to vary regularization strength over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RegularizeCallback(Callback):\n",
    "    def __init__(self, \n",
    "                 criteria: Criteria | list[Criteria],            # Criteria(s) to use for regularization\n",
    "                 granularity: str | list[str],                   # Granularity level(s) for grouping\n",
    "                 weight: float = 0.01,                                 # Regularization weight\n",
    "                 layer_types: Type | list[Type] = nn.Conv2d,     # Layer types to apply regularization to\n",
    "                 schedule: Schedule | None = None,                  # Optional schedule for regularization weight\n",
    "                 verbose: bool = False                                 # Whether to report regularization weight\n",
    "    ):\n",
    "        \"Callback to apply regularization using criteria during training\"\n",
    "        store_attr()\n",
    "        self.criteria = listify(criteria)\n",
    "        self.granularity = listify(granularity)\n",
    "        self.layer_types = listify(layer_types)\n",
    "        self.current_weight = weight\n",
    "        \n",
    "    def before_batch(self) -> None:\n",
    "        \"Update regularization weight if scheduled\"\n",
    "        if self.schedule is not None:\n",
    "            progress = self.schedule.progress(self.pct_train)\n",
    "            self.current_weight = self.weight * progress\n",
    "        \n",
    "    def after_loss(self) -> None:\n",
    "        \"Apply regularization after computing the main loss\"\n",
    "        reg = self.get_norm()\n",
    "        self.learn.loss_grad += reg\n",
    "        self.learn.loss = self.learn.loss_grad.clone()\n",
    "    \n",
    "    def _iter_layers(self):\n",
    "        \"Iterate over matching layers with weights\"\n",
    "        for m in self.learn.model.modules():\n",
    "            if any(isinstance(m, lt) for lt in self.layer_types) and hasattr(m, 'weight'):\n",
    "                yield m\n",
    "            \n",
    "    def get_norm(self) -> torch.Tensor:\n",
    "        \"Compute regularization using the specified criteria and granularities\"\n",
    "        # Pre-filter modules once\n",
    "        layers = list(self._iter_layers())\n",
    "        \n",
    "        layer_regs = []\n",
    "        for crit in self.criteria:\n",
    "            for g in self.granularity:\n",
    "                for m in layers:\n",
    "                    try:\n",
    "                        scores = crit.f(m.weight)[None].abs().sum(Granularities.get_dim(m, g))\n",
    "                        layer_regs.append(self.current_weight * scores.sum())\n",
    "                    except (KeyError, ValueError) as e:\n",
    "                        warnings.warn(f\"Skipping regularization for {type(m).__name__}: {e}\")\n",
    "                    except RuntimeError as e:\n",
    "                        warnings.warn(f\"Runtime error in regularization for {type(m).__name__}: {e}\")\n",
    "        \n",
    "        return torch.stack(layer_regs).sum() if layer_regs else torch.tensor(0.0)\n",
    "    \n",
    "    def after_epoch(self) -> None:\n",
    "        \"Report current regularization weight if verbose\"\n",
    "        if self.verbose:\n",
    "            print(f\"Current regularization weight: {self.current_weight:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd300f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/regularize/regularize_callback.py#L21){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RegularizeCallback\n",
       "\n",
       "```python\n",
       "\n",
       "def RegularizeCallback(\n",
       "    criteria:Criteria | list[Criteria], # Criteria(s) to use for regularization\n",
       "    granularity:str | list[str], # Granularity level(s) for grouping\n",
       "    weight:float=0.01, # Regularization weight\n",
       "    layer_types:Type | list[Type]=<class 'torch.nn.modules.conv.Conv2d'>, # Layer types to apply regularization to\n",
       "    schedule:Schedule | None=None, # Optional schedule for regularization weight\n",
       "    verbose:bool=False, # Whether to report regularization weight\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def RegularizeCallback(\n",
       "    criteria:Criteria | list[Criteria], # Criteria(s) to use for regularization\n",
       "    granularity:str | list[str], # Granularity level(s) for grouping\n",
       "    weight:float=0.01, # Regularization weight\n",
       "    layer_types:Type | list[Type]=<class 'torch.nn.modules.conv.Conv2d'>, # Layer types to apply regularization to\n",
       "    schedule:Schedule | None=None, # Optional schedule for regularization weight\n",
       "    verbose:bool=False, # Whether to report regularization weight\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RegularizeCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regularize-params",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "- `criteria`: Importance criteria to use for computing regularization (e.g., `large_final`)\n",
    "- `granularity`: Level at which to group weights (`'weight'`, `'vector'`, `'kernel'`, `'filter'`)\n",
    "- `weight`: Regularization coefficient (higher = stronger regularization)\n",
    "- `layer_types`: Module types to regularize (default: `nn.Conv2d`)\n",
    "- `schedule`: Optional schedule to vary regularization strength over training\n",
    "- `verbose`: Print regularization weight after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separator-1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-header",
   "metadata": {},
   "source": [
    "## Usage Example\n",
    "\n",
    "Apply filter-level L1 regularization to encourage entire filters to become unimportant (making them easier to prune later):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31b921-c288-4010-9cb0-674220175efa",
   "metadata": {},
   "source": [
    "```python\n",
    "from fasterai.regularize.regularize_callback import RegularizeCallback\n",
    "from fasterai.core.criteria import large_final\n",
    "\n",
    "# Apply L1 regularization at filter granularity\n",
    "cb = RegularizeCallback(\n",
    "    criteria=large_final,\n",
    "    granularity='filter',\n",
    "    weight=0.01,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "learn.fit(10, cbs=[cb])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-explain",
   "metadata": {},
   "source": [
    "**Typical Workflow:**\n",
    "1. Train with `RegularizeCallback` to push unimportant filter groups toward zero\n",
    "2. After training, use `PruneCallback` or `Pruner` to remove the zeroed-out structures\n",
    "3. Fine-tune the pruned model to recover any lost accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6dawsdmiog",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# Single criteria + granularity\n",
    "cb = RegularizeCallback(criteria=large_final, granularity='filter', weight=1e-4)\n",
    "test_eq(cb.weight, 1e-4)\n",
    "test_eq(cb.current_weight, 1e-4)\n",
    "test_eq(len(cb.criteria), 1)\n",
    "test_eq(len(cb.granularity), 1)\n",
    "\n",
    "# List of criteria/granularities\n",
    "cb_m = RegularizeCallback(\n",
    "    criteria=[large_final, large_final],\n",
    "    granularity=['filter', 'weight']\n",
    ")\n",
    "test_eq(len(cb_m.criteria), 2)\n",
    "test_eq(len(cb_m.granularity), 2)\n",
    "\n",
    "# Default layer_types is Conv2d (listified)\n",
    "test_eq(len(cb.layer_types), 1)\n",
    "assert nn.Conv2d in cb.layer_types\n",
    "\n",
    "# Schedule is None by default\n",
    "test_eq(cb.schedule, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ay5isrq2we",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.301912</td>\n",
       "      <td>2.272374</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.299030</td>\n",
       "      <td>2.271772</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "#| slow\n",
    "# Training with RegularizeCallback â€” verify it runs without error\n",
    "from torch.utils.data import TensorDataset\n",
    "from fastai.data.core import DataLoaders\n",
    "from fastai.learner import Learner\n",
    "\n",
    "_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(16, 10)\n",
    ")\n",
    "\n",
    "_X = torch.randn(64, 3, 8, 8)\n",
    "_y = torch.randint(0, 10, (64,))\n",
    "_dls = DataLoaders.from_dsets(\n",
    "    TensorDataset(_X[:48], _y[:48]),\n",
    "    TensorDataset(_X[48:], _y[48:]),\n",
    "    bs=16, device='cpu'\n",
    ")\n",
    "\n",
    "_cb = RegularizeCallback(criteria=large_final, granularity='filter', weight=1e-4)\n",
    "_learn = Learner(_dls, _model, loss_func=nn.CrossEntropyLoss(), cbs=[_cb])\n",
    "_learn.fit(2)  # verify it runs end-to-end without error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64avuvp2127",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Sparsifier](../sparse/sparsifier.html) - Apply sparsification after regularization pushes weights to zero\n",
    "- [Criteria](../core/criteria.html) - Importance measures that can leverage regularized weights\n",
    "- [SparsifyCallback](../sparse/sparsify_callback.html) - Combine with sparsification for gradual pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
