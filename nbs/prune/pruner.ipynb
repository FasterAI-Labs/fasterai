{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a33dc9c",
   "metadata": {},
   "source": [
    "---\n",
    "description: Remove useless filters to recreate a dense network\n",
    "output-file: pruner.html\n",
    "title: Pruner\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prune.pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_pruning as tp\n",
    "from torch_pruning.pruner import function\n",
    "\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "from fastcore.basics import store_attr, listify, true\n",
    "from fasterai.core.criteria import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "from torch_pruning.pruner.algorithms.scheduler import linear_scheduler\n",
    "from torch.fx import symbolic_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqm0as2fn3k",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `Pruner` class provides structured pruning capabilities using the [torch-pruning](https://github.com/VainF/Torch-Pruning) library. Unlike unstructured pruning (which zeros individual weights), structured pruning removes entire filters/channels, resulting in a genuinely smaller and faster model.\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic dependency handling across layers\n",
    "- Support for both local (per-layer) and global (cross-layer) pruning\n",
    "- Automatic detection and handling of attention layers in transformers\n",
    "- Compatible with various importance criteria from `fasterai.core.criteria`\n",
    "\n",
    "### Sparsifier vs Pruner: When to Use Which?\n",
    "\n",
    "| Aspect | Sparsifier | Pruner |\n",
    "|--------|------------|--------|\n",
    "| **What it removes** | Individual weights (unstructured) | Entire filters/channels (structured) |\n",
    "| **Model size** | Same architecture, sparse weights | Smaller architecture |\n",
    "| **Speedup** | Requires sparse hardware/libraries | Immediate speedup on any hardware |\n",
    "| **Accuracy impact** | Generally lower at same sparsity | May need fine-tuning |\n",
    "| **Best for** | Research, sparse-aware inference | Production deployment |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63acddeb-f30e-448b-a397-d4cac2adba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fasterai.core.schedule import Schedule\n",
    "\n",
    "class Pruner():\n",
    "    \"Structured pruning for neural networks using torch_pruning\"\n",
    "    def __init__(self, model, pruning_ratio, context, criteria, schedule=linear_scheduler, ignored_layers=None, example_inputs=torch.randn(1, 3, 224, 224), *args, **kwargs):\n",
    "        store_attr()\n",
    "        self.num_heads = {}\n",
    "        self._original_params = sum(p.numel() for p in model.parameters())\n",
    "        if not self.ignored_layers: self.get_ignored_layers(self.model)\n",
    "\n",
    "        # Handle pruning_ratio as float or dict\n",
    "        self.pruning_ratio_dict = None\n",
    "        if isinstance(self.pruning_ratio, dict):\n",
    "            # Convert name-based dict to module-based dict for torch-pruning\n",
    "            self.pruning_ratio_dict = self._resolve_pruning_ratio_dict(self.pruning_ratio)\n",
    "            self.default_pruning_ratio = kwargs.pop('default_pruning_ratio', 0.0)\n",
    "            print(f\"Using per-layer pruning with {len(self.pruning_ratio_dict)} layer-specific ratios\")\n",
    "        else:\n",
    "            if self.pruning_ratio > 1: self.pruning_ratio = self.pruning_ratio / 100\n",
    "            if not (0 < self.pruning_ratio <= 1):\n",
    "                raise ValueError(f\"pruning_ratio must be in range (0, 1], got {self.pruning_ratio}\")\n",
    "            self.default_pruning_ratio = self.pruning_ratio\n",
    "\n",
    "        # Convert Schedule object to torch-pruning compatible function\n",
    "        tp_schedule = self._to_tp_scheduler(self.schedule)\n",
    "\n",
    "        self.pruner = tp.pruner.MetaPruner(\n",
    "            self.model,\n",
    "            example_inputs=self.example_inputs.to(next(self.model.parameters()).device),\n",
    "            importance=self.group_importance,\n",
    "            pruning_ratio=self.default_pruning_ratio,\n",
    "            pruning_ratio_dict=self.pruning_ratio_dict,\n",
    "            ignored_layers=self.ignored_layers,\n",
    "            global_pruning=True if self.context=='global' else False,\n",
    "            num_heads=self.num_heads,\n",
    "            iterative_pruning_ratio_scheduler=tp_schedule,\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def _build_pruning_schedule(self, sched_func):\n",
    "        \"Create a schedule function compatible with torch-pruning's Pruner\"\n",
    "        def scheduler(pruning_ratio, steps, start=0, end=1):\n",
    "            return [\n",
    "                sched_func(start, end, i / float(steps)) * pruning_ratio\n",
    "                for i in range(steps + 1)\n",
    "            ]\n",
    "        return scheduler\n",
    "\n",
    "    def _to_tp_scheduler(self, schedule):\n",
    "        \"Convert Schedule object or callable to torch-pruning compatible scheduler\"\n",
    "        # If it's a Schedule object, extract sched_func and build compatible function\n",
    "        if isinstance(schedule, Schedule):\n",
    "            return self._build_pruning_schedule(schedule.sched_func)\n",
    "        # Otherwise assume it's already a compatible callable (like linear_scheduler)\n",
    "        return schedule\n",
    "\n",
    "    def _resolve_pruning_ratio_dict(self, ratio_dict):\n",
    "        \"Convert layer name strings to module references for torch-pruning\"\n",
    "        name_to_module = dict(self.model.named_modules())\n",
    "        resolved = {}\n",
    "        for key, ratio in ratio_dict.items():\n",
    "            if isinstance(key, str):\n",
    "                if key in name_to_module:\n",
    "                    module = name_to_module[key]\n",
    "                    # Normalize ratio to 0-1 range\n",
    "                    resolved[module] = ratio / 100 if ratio > 1 else ratio\n",
    "                else:\n",
    "                    print(f\"Warning: Layer '{key}' not found in model, skipping\")\n",
    "            elif isinstance(key, nn.Module):\n",
    "                resolved[key] = ratio / 100 if ratio > 1 else ratio\n",
    "        return resolved\n",
    "          \n",
    "    def prune_model(self):\n",
    "        \"Execute one pruning step and restore attention layer configurations\"\n",
    "        self.pruner.step()\n",
    "        self.restore_attention_layers()\n",
    "\n",
    "\n",
    "    def get_linear_layers_to_ignore(self, \n",
    "                                    model: nn.Module  # The model to analyze\n",
    "    ):\n",
    "        \"Find and ignore output Linear layers to preserve model output dimensions\"\n",
    "        try:\n",
    "            traced = symbolic_trace(model)\n",
    "            for node in traced.graph.nodes:\n",
    "                if node.op == \"output\":  # Identify the output\n",
    "                    for input_node in node.all_input_nodes:\n",
    "                        if input_node.target:  # Find the corresponding layer\n",
    "                            module = dict(model.named_modules()).get(input_node.target)\n",
    "                            if isinstance(module, torch.nn.Linear):\n",
    "                                self.ignored_layers.append(module)\n",
    "                                print(f\"Ignoring output layer: {input_node.target}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not trace model for output layer detection: {e}\")\n",
    "\n",
    "\n",
    "    def get_attention_layers_to_ignore(self, \n",
    "                                       model: nn.Module  # The model to analyze\n",
    "    ):\n",
    "        \"Find and ignore attention layers (qkv projections) to preserve attention structure\"\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'num_heads'):\n",
    "                if hasattr(module, 'qkv'):\n",
    "                    self.ignored_layers.append(module.qkv)\n",
    "                    self.num_heads[module.qkv] = module.num_heads\n",
    "                    print(f\"Attention layer ignored: {module.qkv}, num_heads={module.num_heads}\")\n",
    "                elif hasattr(module, 'qkv_proj'):\n",
    "                    self.ignored_layers.append(module.qkv_proj)\n",
    "                    self.num_heads[module.qkv_proj] = module.num_heads\n",
    "                    print(f\"Attention layer ignored: {module.qkv_proj}, num_heads={module.num_heads}\")\n",
    "\n",
    "    \n",
    "    def get_ignored_layers(self, \n",
    "                           model: nn.Module  # The model to analyze\n",
    "    ):\n",
    "        \"Build list of layers to ignore during pruning\"\n",
    "        self.ignored_layers = []\n",
    "        self.get_linear_layers_to_ignore(model)\n",
    "        self.get_attention_layers_to_ignore(model)\n",
    "        print(f\"Total ignored layers: {len(self.ignored_layers)}\")\n",
    "    \n",
    "                \n",
    "    def restore_attention_layers(self):\n",
    "        \"Restore num_heads and head_dim attributes after pruning attention layers\"\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'num_heads'):\n",
    "                if hasattr(m, 'qkv'):\n",
    "                    m.num_heads = self.num_heads[m.qkv]\n",
    "                    m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "                elif hasattr(m, 'qkv_proj'):\n",
    "                    m.num_heads = self.num_heads[m.qkv_proj]\n",
    "                    m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)\n",
    "\n",
    "\n",
    "    def group_importance(self, group):\n",
    "        \"Compute importance scores for a dependency group\"\n",
    "        handler_map = {\n",
    "            function.prune_conv_out_channels: 'filter',\n",
    "            function.prune_linear_out_channels: 'row',\n",
    "            function.prune_linear_in_channels: 'column',\n",
    "            function.prune_conv_in_channels: 'shared_kernel',\n",
    "        }\n",
    "    \n",
    "        group_imp = []\n",
    "        group_idxs = []\n",
    "    \n",
    "        for i, (dep, idxs) in enumerate(group):\n",
    "            if dep.handler in handler_map:\n",
    "                impo = self.criteria(dep.target.module, handler_map.get(dep.handler), squeeze=True)\n",
    "                group_imp.append(impo)\n",
    "                group_idxs.append(group[i].root_idxs)\n",
    "    \n",
    "        if len(group_imp) == 0:\n",
    "            return torch.tensor([])\n",
    "            \n",
    "        reduced_imp = torch.zeros_like(group_imp[0])\n",
    "    \n",
    "        for i, (imp, root_idxs) in enumerate(zip(group_imp, group_idxs)):\n",
    "            imp = imp.to('cpu')\n",
    "            reduced_imp = reduced_imp.to('cpu')\n",
    "            reduced_imp.scatter_add_(0, torch.tensor(root_idxs, device=imp.device), imp)\n",
    "    \n",
    "        reduced_imp /= len(group_imp)\n",
    "    \n",
    "        return reduced_imp.to(default_device())\n",
    "\n",
    "    def print_sparsity(self) -> None:\n",
    "        \"Print pruning report showing channel counts and parameter reduction\"\n",
    "        total_params = 0\n",
    "        \n",
    "        print(\"\\nPruning Report:\")\n",
    "        print(\"-\" * 85)\n",
    "        print(f\"{'Layer':<35} {'Type':<12} {'In Ch':<8} {'Out Ch':<8} {'Params':<12}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for name, m in self.model.named_modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                params = sum(p.numel() for p in m.parameters())\n",
    "                total_params += params\n",
    "                \n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    in_ch, out_ch = m.in_channels, m.out_channels\n",
    "                    layer_type = \"Conv2d\"\n",
    "                else:\n",
    "                    in_ch, out_ch = m.in_features, m.out_features\n",
    "                    layer_type = \"Linear\"\n",
    "                \n",
    "                print(f\"{name:<35} {layer_type:<12} {in_ch:<8} {out_ch:<8} {params:<12,}\")\n",
    "        \n",
    "        print(\"-\" * 85)\n",
    "        reduction = 100 * (1 - total_params / self._original_params) if self._original_params > 0 else 0\n",
    "        print(f\"{'Total':<35} {'':<12} {'':<8} {'':<8} {total_params:<12,}\")\n",
    "        print(f\"{'Original':<35} {'':<12} {'':<8} {'':<8} {self._original_params:<12,}\")\n",
    "        print(f\"{'Reduction':<35} {'':<12} {'':<8} {'':<8} {reduction:>10.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf56b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/prune/pruner.py#L97){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Pruner.prune_model\n",
       "\n",
       "```python\n",
       "\n",
       "def prune_model(\n",
       "    \n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Execute one pruning step and restore attention layer configurations*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def prune_model(\n",
       "    \n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Execute one pruning step and restore attention layer configurations*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Pruner.prune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ikic0p5dwq",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/prune/pruner.py#L159){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Pruner.group_importance\n",
       "\n",
       "```python\n",
       "\n",
       "def group_importance(\n",
       "    group\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Compute importance scores for a dependency group*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def group_importance(\n",
       "    group\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Compute importance scores for a dependency group*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Pruner.group_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jz2cfdsyqp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/prune/pruner.py#L191){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Pruner.print_sparsity\n",
       "\n",
       "```python\n",
       "\n",
       "def print_sparsity(\n",
       "    \n",
       ")->None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Print pruning report showing channel counts and parameter reduction*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def print_sparsity(\n",
       "    \n",
       ")->None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Print pruning report showing channel counts and parameter reduction*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Pruner.print_sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49hkgcb6xu3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a113a5a",
   "metadata": {},
   "source": [
    "Let's try the `Pruner` with a VGG16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60753d13-3dab-4f86-9d5d-a2bf0fb7dd48",
   "metadata": {},
   "source": [
    "```python\n",
    "model = resnet18()\n",
    "pruner = Pruner(model, 30, 'local', large_final)\n",
    "pruner.prune_model()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x0g8lno4gfp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: 8\n",
      "Total ignored layers: 1\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "def _test_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 3, padding=1),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "# Pruner construction\n",
    "model = _test_model()\n",
    "x = torch.randn(1, 3, 8, 8)\n",
    "pruner = Pruner(model, 30, 'local', large_final, example_inputs=x)\n",
    "assert pruner is not None\n",
    "\n",
    "# Prune model reduces parameter count\n",
    "params_before = sum(p.numel() for p in model.parameters())\n",
    "pruner.prune_model()\n",
    "params_after = sum(p.numel() for p in model.parameters())\n",
    "assert params_after < params_before\n",
    "\n",
    "# Model still produces valid output after pruning\n",
    "out = model(x)\n",
    "test_eq(out.shape[0], 1)  # batch dim preserved\n",
    "test_eq(out.shape[1], 10)  # output classes preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x4a3c375y4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [PruneCallback](prune_callback.html) - Apply structured pruning during fastai training\n",
    "- [Criteria](../core/criteria.html) - Different importance measures for selecting what to prune\n",
    "- [Schedules](../core/schedules.html) - Control pruning progression during training\n",
    "- [Sparsifier](../sparse/sparsifier.html) - Unstructured pruning (zeroing weights without removing them)\n",
    "- [torch-pruning documentation](https://github.com/VainF/Torch-Pruning) - The underlying library used by Pruner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
