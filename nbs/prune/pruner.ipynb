{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a33dc9c",
   "metadata": {},
   "source": [
    "---\n",
    "description: Remove useless filters to recreate a dense network\n",
    "output-file: pruner.html\n",
    "title: Pruner\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prune.pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_pruning as tp\n",
    "from torch_pruning.pruner import function\n",
    "\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "from fastcore.basics import store_attr, listify, true\n",
    "from fasterai.core.criteria import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "\n",
    "from torch_pruning.pruner.algorithms.scheduler import linear_scheduler\n",
    "from torch.fx import symbolic_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63acddeb-f30e-448b-a397-d4cac2adba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pruner():\n",
    "    \"Structured pruning for neural networks using torch_pruning\"\n",
    "    def __init__(self, model, pruning_ratio, context, criteria, schedule=linear_scheduler, ignored_layers=None, example_inputs=torch.randn(1, 3, 224, 224), *args, **kwargs):\n",
    "        store_attr()\n",
    "        self.num_heads = {}\n",
    "        if not self.ignored_layers: self.get_ignored_layers(self.model)\n",
    "        if self.pruning_ratio>1: self.pruning_ratio = self.pruning_ratio/100\n",
    "        if not (0 < self.pruning_ratio <= 1):\n",
    "            raise ValueError(f\"pruning_ratio must be in range (0, 1], got {self.pruning_ratio}\")\n",
    "        self.pruner = tp.pruner.MetaPruner(\n",
    "        self.model,\n",
    "        example_inputs=self.example_inputs.to(next(self.model.parameters()).device),\n",
    "        importance=self.group_importance,\n",
    "        pruning_ratio=self.pruning_ratio, \n",
    "        ignored_layers=self.ignored_layers,\n",
    "        global_pruning=True if self.context=='global' else False,\n",
    "        num_heads = self.num_heads,\n",
    "        iterative_pruning_ratio_scheduler=self.schedule,\n",
    "        *args, \n",
    "        **kwargs\n",
    "        )\n",
    "          \n",
    "    def prune_model(self):\n",
    "        \"Execute one pruning step and restore attention layer configurations\"\n",
    "        self.pruner.step()\n",
    "        self.restore_attention_layers()\n",
    "\n",
    "\n",
    "    def get_linear_layers_to_ignore(self, \n",
    "                                    model: nn.Module  # The model to analyze\n",
    "    ):\n",
    "        \"Find and ignore output Linear layers to preserve model output dimensions\"\n",
    "        traced = symbolic_trace(model)\n",
    "        for node in traced.graph.nodes:\n",
    "            if node.op == \"output\":  # Identify the output\n",
    "                for input_node in node.all_input_nodes:\n",
    "                    if input_node.target:  # Find the corresponding layer\n",
    "                        module = dict(model.named_modules()).get(input_node.target)\n",
    "                        if isinstance(module, torch.nn.Linear):\n",
    "                            self.ignored_layers.append(module)\n",
    "                            print(f\"Ignoring output layer: {module}\")\n",
    "\n",
    "\n",
    "    def get_attention_layers_to_ignore(self, \n",
    "                                       model: nn.Module  # The model to analyze\n",
    "    ):\n",
    "        \"Find and ignore attention layers (qkv projections) to preserve attention structure\"\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'num_heads'):\n",
    "                if hasattr(module, 'qkv'):\n",
    "                    self.ignored_layers.append(module.qkv)\n",
    "                    self.num_heads[module.qkv] = module.num_heads\n",
    "                    print(f\"Attention layer ignored: {module.qkv}, num_heads={module.num_heads}\")\n",
    "                elif hasattr(module, 'qkv_proj'):\n",
    "                    self.ignored_layers.append(module.qkv_proj)\n",
    "                    self.num_heads[module.qkv_proj] = module.num_heads\n",
    "                    print(f\"Attention layer ignored: {module.qkv_proj}, num_heads={module.num_heads}\")\n",
    "\n",
    "    \n",
    "    def get_ignored_layers(self, \n",
    "                           model: nn.Module  # The model to analyze\n",
    "    ):\n",
    "        \"Build list of layers to ignore during pruning\"\n",
    "        self.ignored_layers = []\n",
    "        self.get_linear_layers_to_ignore(model)\n",
    "        self.get_attention_layers_to_ignore(model)\n",
    "        print(f\"Total ignored layers: {len(self.ignored_layers)}\")\n",
    "    \n",
    "                \n",
    "    def restore_attention_layers(self):\n",
    "        \"Restore num_heads and head_dim attributes after pruning attention layers\"\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'num_heads'):\n",
    "                if hasattr(m, 'qkv'):\n",
    "                    m.num_heads = self.num_heads[m.qkv]\n",
    "                    m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "                elif hasattr(m, 'qkv_proj'):\n",
    "                    m.num_heads = self.num_heads[m.qkv_proj]\n",
    "                    m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)\n",
    "\n",
    "\n",
    "    def group_importance(self, group):\n",
    "        \"Compute importance scores for a dependency group\"\n",
    "        handler_map = {\n",
    "            function.prune_conv_out_channels: 'filter',\n",
    "            function.prune_linear_out_channels: 'row',\n",
    "            function.prune_linear_in_channels: 'column',\n",
    "            function.prune_conv_in_channels: 'shared_kernel',\n",
    "        }\n",
    "    \n",
    "        group_imp = []\n",
    "        group_idxs = []\n",
    "    \n",
    "        for i, (dep, idxs) in enumerate(group):\n",
    "            if dep.handler in handler_map:\n",
    "                impo = self.criteria(dep.target.module, handler_map.get(dep.handler), squeeze=True)\n",
    "                group_imp.append(impo)\n",
    "                group_idxs.append(group[i].root_idxs)\n",
    "    \n",
    "        if len(group_imp) == 0:\n",
    "            return torch.tensor([])\n",
    "            \n",
    "        reduced_imp = torch.zeros_like(group_imp[0])\n",
    "    \n",
    "        for i, (imp, root_idxs) in enumerate(zip(group_imp, group_idxs)):\n",
    "            imp = imp.to('cpu')\n",
    "            reduced_imp = reduced_imp.to('cpu')\n",
    "            reduced_imp.scatter_add_(0, torch.tensor(root_idxs, device=imp.device), imp)\n",
    "    \n",
    "        reduced_imp /= len(group_imp)\n",
    "    \n",
    "        return reduced_imp.to(default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Pruner.prune_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a113a5a",
   "metadata": {},
   "source": [
    "Let's try the `Pruner` with a VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(); model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8ac36",
   "metadata": {},
   "source": [
    "The `Pruner`can either remove filters based on `local` criteria (i.e. each layer will be trimmed of the same % of filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = Pruner(model, 30, 'local', large_final)\n",
    "pruner.prune_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7476da",
   "metadata": {},
   "source": [
    "The `Pruner`can also remove filters based on `global` criteria (i.e. each layer will be trimmed of a different % of filters, but we specify the sparsity of the whole network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "pruner = Pruner(model, 50, 'global', large_final)\n",
    "pruner.prune_model()\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
