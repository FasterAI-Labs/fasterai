{
 "cells": [
  {
   "cell_type": "raw",
   "id": "08159415",
   "metadata": {},
   "source": [
    "---\n",
    "description: Use the pruner in fastai Callback system\n",
    "output-file: prune_callback.html\n",
    "title: Prune Callback\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5148f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prune.prune_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce26620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fasterai.prune.pruner import *\n",
    "from fasterai.core.criteria import *\n",
    "from fasterai.core.schedule import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fi8yjhfmyg",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `PruneCallback` integrates structured pruning into the fastai training loop. Unlike sparsification (which zeros weights), pruning physically removes network structures (filters, channels) to reduce model size and computation.\n",
    "\n",
    "**Key Differences from SparsifyCallback:**\n",
    "- Removes structures entirely (not just zeros)\n",
    "- Uses `torch-pruning` library for dependency handling\n",
    "- Supports various pruning criteria and schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50598138-7d55-4774-b711-114c1c42dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PruneCallback(Callback):\n",
    "    def __init__(self, pruning_ratio, schedule, context, criteria, *args, **kwargs):\n",
    "        store_attr()\n",
    "        self.sparsity_levels = []\n",
    "        self.extra_args = args\n",
    "        self.extra_kwargs = kwargs\n",
    "\n",
    "    def _build_pruning_schedule(self, sched_func):\n",
    "        \"Create a schedule function compatible with torch-pruning's Pruner\"\n",
    "        start_val, end_val = self.schedule.start_val, self.schedule.end_val\n",
    "        def scheduler(pruning_ratio, steps, start=start_val, end=end_val):\n",
    "            return [\n",
    "                sched_func(start, end, i / float(steps)) * pruning_ratio\n",
    "                for i in range(steps + 1)\n",
    "            ]\n",
    "        return scheduler\n",
    "\n",
    "    def before_fit(self) -> None:\n",
    "        \"Setup pruner before training\"\n",
    "        n_batches_per_epoch = len(self.learn.dls.train)\n",
    "        total_training_steps = n_batches_per_epoch * self.learn.n_epoch\n",
    "        self.pruning_ratio = self.pruning_ratio/100 if self.pruning_ratio>1 else self.pruning_ratio\n",
    "        \n",
    "        # Validate pruning_ratio is in valid range\n",
    "        if not (0 < self.pruning_ratio <= 1):\n",
    "            raise ValueError(f\"pruning_ratio must be in range (0, 1], got {self.pruning_ratio}\")\n",
    "\n",
    "        self.example_inputs, _ = self.learn.dls.one_batch()\n",
    "        \n",
    "        # Build schedule function for torch-pruning compatibility\n",
    "        pruning_schedule = self._build_pruning_schedule(self.schedule.sched_func)\n",
    "        self.sparsity_levels = pruning_schedule(self.pruning_ratio, total_training_steps)\n",
    "        \n",
    "        self.pruner = Pruner(\n",
    "            self.learn.model,\n",
    "            criteria=self.criteria,\n",
    "            pruning_ratio=self.pruning_ratio, \n",
    "            context=self.context,\n",
    "            iterative_steps=total_training_steps, \n",
    "            schedule=pruning_schedule,\n",
    "            *self.extra_args, \n",
    "            **self.extra_kwargs\n",
    "        )\n",
    "        \n",
    "    def before_step(self) -> None:\n",
    "        \"Apply pruning before optimizer step\"\n",
    "        if self.training: \n",
    "            self.pruner.prune_model()\n",
    "\n",
    "    def after_epoch(self) -> None:\n",
    "        \"Log sparsity after each epoch\"\n",
    "        completed_steps = (self.epoch + 1) * len(self.learn.dls.train)\n",
    "        # Bounds check for sparsity_levels access\n",
    "        if completed_steps > 0 and completed_steps <= len(self.sparsity_levels):\n",
    "            current_sparsity = self.sparsity_levels[completed_steps - 1]\n",
    "            print(f'Sparsity at the end of epoch {self.epoch}: {current_sparsity*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1921c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/prune/prune_callback.py#L18){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PruneCallback\n",
       "\n",
       "```python\n",
       "\n",
       "def PruneCallback(\n",
       "    pruning_ratio, schedule, context, criteria, args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def PruneCallback(\n",
       "    pruning_ratio, schedule, context, criteria, args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PruneCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jmkcd80xs5",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "\n",
    "- `pruning_ratio`: Target ratio of parameters to remove (0-1 or 0-100). Values >1 are treated as percentages.\n",
    "- `schedule`: When to prune (from `fasterai.core.schedule`). Controls how pruning progresses over training.\n",
    "- `context`: `'local'` (per-layer pruning) or `'global'` (across entire model).\n",
    "- `criteria`: How to select what to prune (from `fasterai.core.criteria`).\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "from fasterai.prune.prune_callback import PruneCallback\n",
    "from fasterai.core.schedule import agp\n",
    "from fasterai.core.criteria import large_final\n",
    "\n",
    "# Prune 30% of parameters using automated gradual pruning schedule\n",
    "cb = PruneCallback(\n",
    "    pruning_ratio=30,        # Remove 30% of parameters\n",
    "    schedule=agp,            # Gradual pruning (cubic decay)\n",
    "    context='global',        # Prune globally across all layers\n",
    "    criteria=large_final     # Keep weights with largest magnitude\n",
    ")\n",
    "\n",
    "learn.fit(10, cbs=[cb])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f3567-c81d-4936-b4be-9c7df0055e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kbf84hizzcg",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# Construction with valid params\n",
    "cb = PruneCallback(\n",
    "    pruning_ratio=30,\n",
    "    schedule=agp,\n",
    "    context='global',\n",
    "    criteria=large_final\n",
    ")\n",
    "test_eq(cb.pruning_ratio, 30)\n",
    "test_eq(cb.context, 'global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egrbro1y05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: 8\n",
      "Total ignored layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.315615</td>\n",
       "      <td>2.329493</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.315022</td>\n",
       "      <td>2.330559</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.314490</td>\n",
       "      <td>2.331829</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 30.00%\n",
      "Sparsity at the end of epoch 1: 30.00%\n",
      "Sparsity at the end of epoch 2: 30.00%\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| slow\n",
    "# Full training with PruneCallback — verify parameter reduction\n",
    "from torch.utils.data import TensorDataset\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(32, 10)\n",
    ")\n",
    "_params_before = sum(p.numel() for p in _model.parameters())\n",
    "\n",
    "_X = torch.randn(64, 3, 8, 8)\n",
    "_y = torch.randint(0, 10, (64,))\n",
    "_dls = DataLoaders.from_dsets(\n",
    "    TensorDataset(_X[:48], _y[:48]),\n",
    "    TensorDataset(_X[48:], _y[48:]),\n",
    "    bs=16, device='cpu'\n",
    ")\n",
    "\n",
    "_cb = PruneCallback(pruning_ratio=30, schedule=one_shot, context='local', criteria=large_final)\n",
    "_learn = Learner(_dls, _model, loss_func=nn.CrossEntropyLoss(), cbs=[_cb])\n",
    "_learn.fit(3)\n",
    "\n",
    "_params_after = sum(p.numel() for p in _model.parameters())\n",
    "assert _params_after < _params_before, f\"Expected params to decrease: {_params_before} → {_params_after}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8hx4b80psx",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Pruner](pruner.html) - Core structured pruning class used by this callback\n",
    "- [Schedules](../core/schedules.html) - Control pruning progression during training\n",
    "- [Criteria](../core/criteria.html) - Importance measures for selecting filters to prune\n",
    "- [SparsifyCallback](../sparse/sparsify_callback.html) - Unstructured pruning alternative"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
