{
 "cells": [
  {
   "cell_type": "raw",
   "id": "08159415",
   "metadata": {},
   "source": [
    "---\n",
    "description: Use the pruner in fastai Callback system\n",
    "output-file: prune_callback.html\n",
    "title: Prune Callback\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5148f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prune.prune_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce26620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fasterai.prune.pruner import *\n",
    "from fasterai.core.criteria import *\n",
    "from fasterai.core.schedule import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fi8yjhfmyg",
   "metadata": {},
   "source": "## Overview\n\nThe `PruneCallback` integrates structured pruning into the fastai training loop. Unlike sparsification (which zeros weights), pruning physically removes network structures (filters, channels) to reduce model size and computation.\n\n**Key Differences from SparsifyCallback:**\n- Removes structures entirely (not just zeros)\n- Uses `torch-pruning` library for dependency handling\n- Supports various pruning criteria and schedules"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50598138-7d55-4774-b711-114c1c42dce8",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nclass PruneCallback(Callback):\n    def __init__(self, pruning_ratio, schedule, context, criteria, *args, **kwargs):\n        store_attr()\n        self.sparsity_levels = []\n        self.extra_args = args\n        self.extra_kwargs = kwargs\n\n    def before_fit(self) -> None:\n        \"Setup pruner before training\"\n        n_batches_per_epoch = len(self.learn.dls.train)\n        total_training_steps = n_batches_per_epoch * self.learn.n_epoch\n        self.pruning_ratio = self.pruning_ratio/100 if self.pruning_ratio>1 else self.pruning_ratio\n        \n        # Validate pruning_ratio is in valid range\n        if not (0 < self.pruning_ratio <= 1):\n            raise ValueError(f\"pruning_ratio must be in range (0, 1], got {self.pruning_ratio}\")\n\n        self.example_inputs, _ = self.learn.dls.one_batch()\n        self.sparsity_levels = self.schedule._scheduler(self.pruning_ratio, total_training_steps)\n        \n        self.pruner = Pruner(\n        self.learn.model,\n        criteria=self.criteria,\n        pruning_ratio=self.pruning_ratio, \n        context=self.context,\n        iterative_steps= total_training_steps, \n        schedule=self.schedule._scheduler,\n        *self.extra_args, \n        **self.extra_kwargs\n        )\n        \n    def before_step(self) -> None:\n        \"Apply pruning before optimizer step\"\n        if self.training: \n            self.pruner.prune_model()\n\n    def after_epoch(self) -> None:\n        \"Log sparsity after each epoch\"\n        completed_steps = (self.epoch + 1) * len(self.learn.dls.train)\n        # Bounds check for sparsity_levels access\n        if completed_steps > 0 and completed_steps <= len(self.sparsity_levels):\n            current_sparsity = self.sparsity_levels[completed_steps - 1]\n            print(f'Sparsity at the end of epoch {self.epoch}: {current_sparsity*100:.2f}%')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1921c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PruneCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jmkcd80xs5",
   "metadata": {},
   "source": "**Parameters:**\n\n- `pruning_ratio`: Target ratio of parameters to remove (0-1 or 0-100). Values >1 are treated as percentages.\n- `schedule`: When to prune (from `fasterai.core.schedule`). Controls how pruning progresses over training.\n- `context`: `'local'` (per-layer pruning) or `'global'` (across entire model).\n- `criteria`: How to select what to prune (from `fasterai.core.criteria`).\n\n---\n\n## Usage Example\n\n```python\nfrom fasterai.prune.prune_callback import PruneCallback\nfrom fasterai.core.schedule import agp\nfrom fasterai.core.criteria import large_final\n\n# Prune 30% of parameters using automated gradual pruning schedule\ncb = PruneCallback(\n    pruning_ratio=30,        # Remove 30% of parameters\n    schedule=agp,            # Gradual pruning (cubic decay)\n    context='global',        # Prune globally across all layers\n    criteria=large_final     # Keep weights with largest magnitude\n)\n\nlearn.fit(10, cbs=[cb])\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f3567-c81d-4936-b4be-9c7df0055e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8hx4b80psx",
   "metadata": {},
   "source": "---\n\n## See Also\n\n- [Pruner](pruner.html) - Core structured pruning class used by this callback\n- [Schedules](../core/schedules.html) - Control pruning progression during training\n- [Criteria](../core/criteria.html) - Importance measures for selecting filters to prune\n- [SparsifyCallback](../sparse/sparsify_callback.html) - Unstructured pruning alternative"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
