{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp sparse.sparsify_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-script",
   "metadata": {},
   "source": [
    "# SparsifyCallback\n",
    "\n",
    "> Use the sparsifier in fastai Callback system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fasterai.sparse.sparsifier import *\n",
    "from fasterai.sparse.criteria import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SparsifyCallback(Callback):\n",
    "        \n",
    "    def __init__(self, end_sparsity, granularity, method, criteria, sched_func, start_sparsity=0, start_epoch=0, end_epoch=None, lth=False, rewind_epoch=0, reset_end=False, model=None):\n",
    "        store_attr()\n",
    "        self.current_sparsity, self.previous_sparsity = 0, 0\n",
    "        \n",
    "        assert self.start_epoch>=self.rewind_epoch, 'You must rewind to an epoch before the start of the pruning process'\n",
    "    \n",
    "    def before_fit(self):\n",
    "        print(f'Pruning of {self.granularity} until a sparsity of {self.end_sparsity}%')\n",
    "        self.end_epoch = self.n_epoch if self.end_epoch is None else self.end_epoch\n",
    "        assert self.end_epoch <= self.n_epoch, 'Your end_epoch must be smaller than total number of epoch'\n",
    "        \n",
    "        model = self.learn.model if self.model is None else self.model # Pass a model if you don't want the whole model to be pruned\n",
    "        self.sparsifier = Sparsifier(model, self.granularity, self.method, self.criteria)\n",
    "        self.n_batches = math.floor(len(self.learn.dls.dataset)/self.learn.dls.bs)\n",
    "        self.total_iters = self.end_epoch * self.n_batches\n",
    "        self.start_iter = self.start_epoch * self.n_batches\n",
    "    \n",
    "    def before_epoch(self):\n",
    "        if self.epoch == self.rewind_epoch:\n",
    "            print(f'Saving Weights at epoch {self.epoch}')\n",
    "            self.sparsifier._save_weights()\n",
    "        \n",
    "    def before_batch(self):\n",
    "        if self.epoch>=self.start_epoch:\n",
    "            if self.epoch < self.end_epoch: self._set_sparsity()\n",
    "            self.sparsifier.prune_model(self.current_sparsity)\n",
    "\n",
    "            if self.lth and self.current_sparsity!=self.previous_sparsity: # If sparsity has changed, the network has been pruned\n",
    "                    print(f'Resetting Weights to their epoch {self.rewind_epoch} values')\n",
    "                    self.sparsifier._reset_weights()\n",
    "\n",
    "            self.previous_sparsity = self.current_sparsity\n",
    "        \n",
    "    def before_step(self):\n",
    "        if self.epoch>=self.start_epoch:\n",
    "            self.sparsifier._mask_grad()\n",
    "            \n",
    "    def after_epoch(self):\n",
    "        print(f'Sparsity at the end of epoch {self.epoch}: {self.current_sparsity:.2f}%')\n",
    "        \n",
    "    def after_fit(self):\n",
    "        print(f'Final Sparsity: {self.current_sparsity:.2f}')\n",
    "        if self.reset_end:\n",
    "            self.sparsifier._reset_weights()\n",
    "        self.sparsifier._clean_buffers() # Remove buffers at the end of training\n",
    "        \n",
    "    def _set_sparsity(self):\n",
    "        self.current_sparsity = self.sched_func(start=self.start_sparsity, end=self.end_sparsity, pos=(self.train_iter-self.start_iter)/(self.total_iters-self.start_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-activation",
   "metadata": {},
   "source": [
    "The most important part of our `Callback` happens in `before_batch`. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.622748</td>\n",
       "      <td>0.415529</td>\n",
       "      <td>0.835589</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.328581</td>\n",
       "      <td>0.299027</td>\n",
       "      <td>0.891746</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.173837</td>\n",
       "      <td>0.219676</td>\n",
       "      <td>0.903924</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c25750",
   "metadata": {},
   "source": [
    "Let's now try adding some sparsity in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-praise",
   "metadata": {},
   "source": [
    "You can use any scheduling function already [available](https://docs.fast.ai/callback.schedule.html#Annealing) in fastai or come up with your own ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(end_sparsity=50, granularity='weight', method='local', criteria=large_final, sched_func=sched_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.629220</td>\n",
       "      <td>0.381072</td>\n",
       "      <td>0.845061</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.374266</td>\n",
       "      <td>0.255751</td>\n",
       "      <td>0.895805</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.191138</td>\n",
       "      <td>0.224797</td>\n",
       "      <td>0.909337</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 12.50%\n",
      "Sparsity at the end of epoch 1: 37.50%\n",
      "Sparsity at the end of epoch 2: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-stationery",
   "metadata": {},
   "source": [
    "Surprisinlgy, our network that is composed of $50 \\%$ of zeroes performs reasonnably well when compared to our plain and dense network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097284d",
   "metadata": {},
   "source": [
    "For more information about the pruning schedules, take a look at the [Schedules section](https://nathanhubens.github.io/fasterai/schedules.html) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766490de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902dff8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06c60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f9866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a87c643f",
   "metadata": {},
   "source": [
    "Here is for example how to implement the [Automated Gradual Pruning](https://arxiv.org/pdf/1710.01878.pdf) schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012503a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sched_agp(start, end, pos): return end + start - end * (1 - pos)**3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-outside",
   "metadata": {},
   "source": [
    "When pruning a neural network, there are 3 main schedules that are used:\n",
    "\n",
    "- **One-Shot Pruning:** starting from a trained network, prune the network to desired sparsity and fine-tune it.\n",
    "- **Iterative Pruning:** starting from a trained network, alternate pruning and fine-tuning steps until desired sparsity.\n",
    "- **Gradual Pruning:** inject the pruning operation directly in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-cookbook",
   "metadata": {},
   "source": [
    "While those methods have for a long time thought to be very different, all of them can be grouped an applied in the same way... with a fastai Callback ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sched_it(start, end, pos, n_steps=3):\n",
    "    \"Perform iterative pruning, and pruning in `n_steps` steps\"\n",
    "    return start + ((end-start)/n_steps)*(np.ceil((pos)*n_steps))\n",
    "    \n",
    "def sched_os(start, end, pos)->float: return end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-purpose",
   "metadata": {},
   "source": [
    "As One-Shot and Iterative Pruning usually start from an already trained network, we need to take that possibility into account. This can be done by using the `start_epoch` argument, which sets the `epoch` at which we start to prune the network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
