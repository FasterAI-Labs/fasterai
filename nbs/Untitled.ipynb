{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0528c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from fastcore.basics import store_attr\n",
    "from fasterai.sparse.all import *\n",
    "from fasterai.prune.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e404e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.ConvTranspose2d(3,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b28957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeTrunk(nn.Module):\n",
    "    def __init__(self, n_feat=64, n_block=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filter = nn.ModuleList(\n",
    "            [nn.Conv2d(n_feat, n_feat, 3, 1, 1, bias=False) for _ in range(n_block)]\n",
    "        )\n",
    "\n",
    "        self.upscale = nn.ConvTranspose2d(n_feat, n_feat, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for filt in self.filter:\n",
    "            x = F.relu(filt(x), inplace=False)\n",
    "        x = self.upscale(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SRNet(nn.Module):\n",
    "    def __init__(self, n_channel=1, n_feat=64, n_block=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_in = nn.Conv2d(n_channel, n_feat, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.edge_x2 = EdgeTrunk(n_feat, n_block)\n",
    "        self.tail_x2 = nn.Conv2d(n_feat, n_channel, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.edge_x4 = EdgeTrunk(n_feat, n_block)\n",
    "        #self.tail_x4 = nn.Conv2d(n_feat, n_channel, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.body_x2 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)\n",
    "        #self.body_x4 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, din):\n",
    "\n",
    "        print(f\"Input shape to SRNet: {din.shape}\")  # Print the shape of input\n",
    "        inp_ref = self.head_in(din)\n",
    "        print(f\"Output shape after head_in: {inp_ref.shape}\")\n",
    "\n",
    "        edge_x2 = self.edge_x2(inp_ref)\n",
    "        print(f\"Output shape after edge_x2: {edge_x2.shape}\")\n",
    "        edge_x4 = self.edge_x4(edge_x2)\n",
    "\n",
    "        body_x2 = self.body_x2(din)\n",
    "        eout_x2 = self.tail_x2(edge_x2)\n",
    "        dout_x2 = body_x2 + eout_x2\n",
    "\n",
    "        #body_x4 = self.body_x4(dout_x2)\n",
    "        #eout_x4 = self.tail_x4(edge_x4)\n",
    "        #dout_x4 = body_x4 + eout_x4\n",
    "        #print(f\"Final output shape from SRNet: {dout_x4.shape}\")\n",
    "        return eout_x2, edge_x4, dout_x2#, edge_x2 , dout_x2, eout_x4, eout_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20042e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeTrunk(nn.Module):\n",
    "    def __init__(self, n_feat=64, n_block=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filter = nn.ModuleList(\n",
    "            [nn.Conv2d(n_feat, n_feat, 3, 1, 1, bias=False) for _ in range(n_block)]\n",
    "        )\n",
    "\n",
    "        self.upscale = nn.ConvTranspose2d(n_feat, n_feat, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for filt in self.filter:\n",
    "            x = F.relu(filt(x), inplace=False)\n",
    "        x = self.upscale(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SRNet(nn.Module):\n",
    "    def __init__(self, n_channel=1, n_feat=64, n_block=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_in = nn.Conv2d(n_channel, n_feat, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.edge_x2 = EdgeTrunk(n_feat, n_block)\n",
    "        self.tail_x2 = nn.Conv2d(n_feat, n_channel, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.edge_x4 = EdgeTrunk(n_feat, n_block)\n",
    "        self.tail_x4 = nn.Conv2d(n_feat, n_channel, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.body_x2 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)\n",
    "        self.body_x4 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, din):\n",
    "\n",
    "        print(f\"Input shape to SRNet: {din.shape}\")  # Print the shape of input\n",
    "        inp_ref = self.head_in(din)\n",
    "        print(f\"Output shape after head_in: {inp_ref.shape}\")\n",
    "\n",
    "        edge_x2 = self.edge_x2(inp_ref)\n",
    "        print(f\"Output shape after edge_x2: {edge_x2.shape}\")\n",
    "        edge_x4 = self.edge_x4(edge_x2)\n",
    "\n",
    "        body_x2 = self.body_x2(din)\n",
    "        eout_x2 = self.tail_x2(edge_x2)\n",
    "        dout_x2 = body_x2 + eout_x2\n",
    "\n",
    "        body_x4 = self.body_x4(dout_x2)\n",
    "        eout_x4 = self.tail_x4(edge_x4)\n",
    "        dout_x4 = body_x4 + eout_x4\n",
    "        print(f\"Final output shape from SRNet: {dout_x4.shape}\")\n",
    "        #return dout_x4, eout_x4#, edge_x2 , dout_x2, eout_x4, eout_x2\n",
    "        return dout_x4 , dout_x2, eout_x4, eout_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeTrunk(nn.Module):\n",
    "    def __init__(self, n_feat=64, n_block=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filter = nn.ModuleList(\n",
    "            [nn.Conv2d(n_feat, n_feat, 3, 1, 1, bias=False) for _ in range(n_block)]\n",
    "        )\n",
    "\n",
    "        self.upscale = nn.ConvTranspose2d(n_feat, n_feat, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for filt in self.filter:\n",
    "            x = F.relu(filt(x), inplace=False)\n",
    "        x = self.upscale(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SRNet(nn.Module):\n",
    "    def __init__(self, n_channel=1, n_feat=64, n_block=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_in = nn.Conv2d(n_channel, n_feat, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.edge_x2 = EdgeTrunk(n_feat, n_block)\n",
    "        self.tail_x2 = nn.Conv2d(n_feat, n_channel, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.edge_x4 = EdgeTrunk(n_feat, n_block)\n",
    "        self.tail_x4 = nn.Conv2d(n_feat, n_channel, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.body_x2 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)\n",
    "        self.body_x4 = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, din):\n",
    "\n",
    "        print(f\"Input shape to SRNet: {din.shape}\")  # Print the shape of input\n",
    "        inp_ref = self.head_in(din)\n",
    "        print(f\"Output shape after head_in: {inp_ref.shape}\")\n",
    "\n",
    "        edge_x2 = self.edge_x2(inp_ref)\n",
    "        print(f\"Output shape after edge_x2: {edge_x2.shape}\")\n",
    "        edge_x4 = self.edge_x4(edge_x2)\n",
    "\n",
    "        body_x2 = self.body_x2(din)\n",
    "        eout_x2 = self.tail_x2(edge_x2)\n",
    "        dout_x2 = body_x2 + eout_x2\n",
    "\n",
    "        body_x4 = self.body_x4(dout_x2)\n",
    "        eout_x4 = self.tail_x4(edge_x4)\n",
    "        dout_x4 = body_x4 + eout_x4\n",
    "        print(f\"Final output shape from SRNet: {dout_x4.shape}\")\n",
    "        return dout_x4 , dout_x2, eout_x4, eout_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d095a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85af116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to SRNet: torch.Size([1, 32, 32])\n",
      "Output shape after head_in: torch.Size([64, 32, 32])\n",
      "Output shape after edge_x2: torch.Size([64, 64, 64])\n",
      "Final output shape from SRNet: torch.Size([1, 128, 128])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported module type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pruner \u001b[38;5;241m=\u001b[39m Pruner(model\u001b[38;5;241m=\u001b[39mmodel, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m, criteria\u001b[38;5;241m=\u001b[39msmall_final, example_inputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m), layer_type\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpruner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[246], line 30\u001b[0m, in \u001b[0;36mPruner.prune_model\u001b[0;34m(self, sparsity, round_to)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprune_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, sparsity, round_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_threshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparsity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ix, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDG\u001b[38;5;241m.\u001b[39mget_all_groups(root_module_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_type])):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_group(group, ix, sparsity, round_to)\n",
      "Cell \u001b[0;32mIn[246], line 13\u001b[0m, in \u001b[0;36mPruner.compute_threshold\u001b[0;34m(self, sparsity)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_importance \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, grp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDG\u001b[38;5;241m.\u001b[39mget_all_groups(root_module_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_type])):\n\u001b[0;32m---> 13\u001b[0m     imp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_importance[ix] \u001b[38;5;241m=\u001b[39m imp\n\u001b[1;32m     16\u001b[0m global_imp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_importance\u001b[38;5;241m.\u001b[39mvalues()), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[246], line 91\u001b[0m, in \u001b[0;36mPruner.group_importance\u001b[0;34m(self, group)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dep\u001b[38;5;241m.\u001b[39mhandler \u001b[38;5;129;01min\u001b[39;00m handler_map:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m granularity, squeeze_dims \u001b[38;5;129;01min\u001b[39;00m [handler_map\u001b[38;5;241m.\u001b[39mget(dep\u001b[38;5;241m.\u001b[39mhandler)]:\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;66;03m#if isinstance(dep.target.module, nn.Conv2d): \u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m                 group_importance\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlarge_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgranularity\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(squeeze_dims))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(group_importance)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/prune/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fasterai/nbs/fasterai/core/criteria.py:27\u001b[0m, in \u001b[0;36mCriteria.__call__\u001b[0;34m(self, m, g)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, m, g):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         dim \u001b[38;5;241m=\u001b[39m \u001b[43mGranularities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid granularity\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/fasterai/nbs/fasterai/core/granularity.py:28\u001b[0m, in \u001b[0;36mGranularities.get_dim\u001b[0;34m(cls, m, g)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, k):\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_granularities[k][g]\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported module type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsupported module type"
     ]
    }
   ],
   "source": [
    "pruner = Pruner(model=model, context=\"global\", criteria=small_final, example_inputs=torch.randn(1, 1, 32, 32), layer_type=nn.Conv2d)\n",
    "\n",
    "pruner.prune_model(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573809f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4682360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee24399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c93d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ad2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/HubensN/fasterai/nbs/fasterai/core/granularity.py\u001b[0m(28)\u001b[0;36mget_dim\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     26 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m                \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_granularities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 28 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported module type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> m\n",
      "ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "ipdb> quit()\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_pruning as tp\n",
    "from torch_pruning.pruner import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1645b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333725fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ff09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to SRNet: torch.Size([1, 32, 32])\n",
      "Output shape after head_in: torch.Size([64, 32, 32])\n",
      "Output shape after edge_x2: torch.Size([64, 64, 64])\n",
      "Final output shape from SRNet: torch.Size([1, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch_pruning.dependency.DependencyGraph>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DG = tp.DependencyGraph()\n",
    "DG.build_dependency(model, example_inputs=torch.randn(1, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa67043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn\u001b[38;5;241m.\u001b[39mConv2d):\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;66;03m#print(m)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[43mDG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_out_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#        setattr(m, '_init_out_channels', self.DG.get_out_channels(m))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/prune/lib/python3.9/site-packages/torch_pruning/dependency.py:533\u001b[0m, in \u001b[0;36mDependencyGraph.get_out_channels\u001b[0;34m(self, module_or_node)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_or_node\n\u001b[0;32m--> 533\u001b[0m     pruning_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule2node\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpruning_dim\n\u001b[1;32m    534\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pruner_of_module(module)\n\u001b[1;32m    535\u001b[0m p\u001b[38;5;241m.\u001b[39mpruning_dim \u001b[38;5;241m=\u001b[39m pruning_dim\n",
      "\u001b[0;31mKeyError\u001b[0m: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
     ]
    }
   ],
   "source": [
    "for m in model.modules():\n",
    "    if hasattr(m, 'weight'):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            #print(m)\n",
    "            print(DG.get_out_channels(m))\n",
    "#        setattr(m, '_init_out_channels', self.DG.get_out_channels(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861e2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/HubensN/miniconda3/envs/prune/lib/python3.9/site-packages/torch_pruning/dependency.py\u001b[0m(533)\u001b[0;36mget_out_channels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    531 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    532 \u001b[0;31m            \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_or_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 533 \u001b[0;31m            \u001b[0mpruning_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule2node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruning_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    534 \u001b[0;31m        \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pruner_of_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    535 \u001b[0;31m        \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruning_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruning_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> module\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "ipdb> self.module2node\n",
      "{_ElementWiseOp_0(AddBackward0): <Node: (_ElementWiseOp_0(AddBackward0))>, ConvTranspose2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False): <Node: (body_x4 (ConvTranspose2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)))>, Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): <Node: (tail_x4 (Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)))>, _ElementWiseOp_1(ConvolutionBackward0): <Node: (_ElementWiseOp_1(ConvolutionBackward0))>, _ElementWiseOp_2(UnsqueezeBackward0): <Node: (_ElementWiseOp_2(UnsqueezeBackward0))>, ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False): <Node: (edge_x4.upscale (ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)))>, _ElementWiseOp_3(ConvolutionBackward0): <Node: (_ElementWiseOp_3(ConvolutionBackward0))>, _ElementWiseOp_4(UnsqueezeBackward0): <Node: (_ElementWiseOp_4(UnsqueezeBackward0))>, _ElementWiseOp_5(AsStridedBackward0): <Node: (_ElementWiseOp_5(AsStridedBackward0))>, _ElementWiseOp_6(torch::autograd::CopySlices): <Node: (_ElementWiseOp_6(torch::autograd::CopySlices))>, _ElementWiseOp_7(ConvolutionBackward0): <Node: (_ElementWiseOp_7(ConvolutionBackward0))>, _ElementWiseOp_8(UnsqueezeBackward0): <Node: (_ElementWiseOp_8(UnsqueezeBackward0))>, _ElementWiseOp_9(AsStridedBackward0): <Node: (_ElementWiseOp_9(AsStridedBackward0))>, _ElementWiseOp_10(torch::autograd::CopySlices): <Node: (_ElementWiseOp_10(torch::autograd::CopySlices))>, _ElementWiseOp_11(ConvolutionBackward0): <Node: (_ElementWiseOp_11(ConvolutionBackward0))>, _ElementWiseOp_12(UnsqueezeBackward0): <Node: (_ElementWiseOp_12(UnsqueezeBackward0))>, _ElementWiseOp_13(AsStridedBackward0): <Node: (_ElementWiseOp_13(AsStridedBackward0))>, _ElementWiseOp_14(torch::autograd::CopySlices): <Node: (_ElementWiseOp_14(torch::autograd::CopySlices))>, _ElementWiseOp_15(ConvolutionBackward0): <Node: (_ElementWiseOp_15(ConvolutionBackward0))>, _ElementWiseOp_16(UnsqueezeBackward0): <Node: (_ElementWiseOp_16(UnsqueezeBackward0))>, _ElementWiseOp_17(AsStridedBackward0): <Node: (_ElementWiseOp_17(AsStridedBackward0))>, _ElementWiseOp_18(torch::autograd::CopySlices): <Node: (_ElementWiseOp_18(torch::autograd::CopySlices))>, _ElementWiseOp_19(ConvolutionBackward0): <Node: (_ElementWiseOp_19(ConvolutionBackward0))>, _ElementWiseOp_20(UnsqueezeBackward0): <Node: (_ElementWiseOp_20(UnsqueezeBackward0))>, _ElementWiseOp_21(AsStridedBackward0): <Node: (_ElementWiseOp_21(AsStridedBackward0))>, _ElementWiseOp_22(torch::autograd::CopySlices): <Node: (_ElementWiseOp_22(torch::autograd::CopySlices))>, _ElementWiseOp_23(ConvolutionBackward0): <Node: (_ElementWiseOp_23(ConvolutionBackward0))>, _ElementWiseOp_24(UnsqueezeBackward0): <Node: (_ElementWiseOp_24(UnsqueezeBackward0))>, ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False): <Node: (edge_x2.upscale (ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)))>, _ElementWiseOp_25(ConvolutionBackward0): <Node: (_ElementWiseOp_25(ConvolutionBackward0))>, _ElementWiseOp_26(UnsqueezeBackward0): <Node: (_ElementWiseOp_26(UnsqueezeBackward0))>, _ElementWiseOp_27(AsStridedBackward0): <Node: (_ElementWiseOp_27(AsStridedBackward0))>, _ElementWiseOp_28(torch::autograd::CopySlices): <Node: (_ElementWiseOp_28(torch::autograd::CopySlices))>, _ElementWiseOp_29(ConvolutionBackward0): <Node: (_ElementWiseOp_29(ConvolutionBackward0))>, _ElementWiseOp_30(UnsqueezeBackward0): <Node: (_ElementWiseOp_30(UnsqueezeBackward0))>, _ElementWiseOp_31(AsStridedBackward0): <Node: (_ElementWiseOp_31(AsStridedBackward0))>, _ElementWiseOp_32(torch::autograd::CopySlices): <Node: (_ElementWiseOp_32(torch::autograd::CopySlices))>, _ElementWiseOp_33(ConvolutionBackward0): <Node: (_ElementWiseOp_33(ConvolutionBackward0))>, _ElementWiseOp_34(UnsqueezeBackward0): <Node: (_ElementWiseOp_34(UnsqueezeBackward0))>, _ElementWiseOp_35(AsStridedBackward0): <Node: (_ElementWiseOp_35(AsStridedBackward0))>, _ElementWiseOp_36(torch::autograd::CopySlices): <Node: (_ElementWiseOp_36(torch::autograd::CopySlices))>, _ElementWiseOp_37(ConvolutionBackward0): <Node: (_ElementWiseOp_37(ConvolutionBackward0))>, _ElementWiseOp_38(UnsqueezeBackward0): <Node: (_ElementWiseOp_38(UnsqueezeBackward0))>, _ElementWiseOp_39(AsStridedBackward0): <Node: (_ElementWiseOp_39(AsStridedBackward0))>, _ElementWiseOp_40(torch::autograd::CopySlices): <Node: (_ElementWiseOp_40(torch::autograd::CopySlices))>, _ElementWiseOp_41(ConvolutionBackward0): <Node: (_ElementWiseOp_41(ConvolutionBackward0))>, _ElementWiseOp_42(UnsqueezeBackward0): <Node: (_ElementWiseOp_42(UnsqueezeBackward0))>, _ElementWiseOp_43(AsStridedBackward0): <Node: (_ElementWiseOp_43(AsStridedBackward0))>, _ElementWiseOp_44(torch::autograd::CopySlices): <Node: (_ElementWiseOp_44(torch::autograd::CopySlices))>, _ElementWiseOp_45(ConvolutionBackward0): <Node: (_ElementWiseOp_45(ConvolutionBackward0))>, _ElementWiseOp_46(UnsqueezeBackward0): <Node: (_ElementWiseOp_46(UnsqueezeBackward0))>, Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): <Node: (head_in (Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)))>, _ElementWiseOp_47(ConvolutionBackward0): <Node: (_ElementWiseOp_47(ConvolutionBackward0))>, _ElementWiseOp_48(ConvolutionBackward0): <Node: (_ElementWiseOp_48(ConvolutionBackward0))>, _ElementWiseOp_49(UnsqueezeBackward0): <Node: (_ElementWiseOp_49(UnsqueezeBackward0))>, _ElementWiseOp_50(AddBackward0): <Node: (_ElementWiseOp_50(AddBackward0))>, ConvTranspose2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False): <Node: (body_x2 (ConvTranspose2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)))>, Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): <Node: (tail_x2 (Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)))>, _ElementWiseOp_51(ConvolutionBackward0): <Node: (_ElementWiseOp_51(ConvolutionBackward0))>, _ElementWiseOp_52(UnsqueezeBackward0): <Node: (_ElementWiseOp_52(UnsqueezeBackward0))>, _ElementWiseOp_53(ConvolutionBackward0): <Node: (_ElementWiseOp_53(ConvolutionBackward0))>}\n",
      "ipdb> module\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "ipdb> module_or_node\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9593756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_importance(group):\n",
    "        handler_map = {\n",
    "            function.prune_conv_out_channels: ('filter', (1, 2, 3)),\n",
    "            function.prune_linear_out_channels: ('row', None),\n",
    "            function.prune_conv_in_channels: ('shared_kernel', (0, 2, 3)),\n",
    "            function.prune_linear_out_channels: ('column', None)\n",
    "        }\n",
    "        \n",
    "        group_importance = []\n",
    "        for dep,_ in group:\n",
    "            if dep.handler in handler_map:\n",
    "                for granularity, squeeze_dims in [handler_map.get(dep.handler)]:\n",
    "                    if isinstance(dep.target.module, nn.Conv2d): \n",
    "                        group_importance.append(large_final(dep.target.module, granularity).squeeze(squeeze_dims))\n",
    "\n",
    "        return torch.stack(group_importance).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4773f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0215])\n",
      "tensor([0.1577, 0.1589, 0.1851, 0.1699, 0.1711, 0.1946, 0.1950, 0.1528, 0.2012,\n",
      "        0.0936, 0.1671, 0.1755, 0.1809, 0.1209, 0.2134, 0.1907, 0.1419, 0.1998,\n",
      "        0.1224, 0.2104, 0.2102, 0.1903, 0.1464, 0.1237, 0.1904, 0.1596, 0.1174,\n",
      "        0.2071, 0.1591, 0.1585, 0.1297, 0.1945, 0.1433, 0.1829, 0.1825, 0.1288,\n",
      "        0.1049, 0.1670, 0.1339, 0.1168, 0.1890, 0.2042, 0.1353, 0.1756, 0.1621,\n",
      "        0.1986, 0.1145, 0.1274, 0.1926, 0.1321, 0.1376, 0.1953, 0.1881, 0.1340,\n",
      "        0.1294, 0.1273, 0.1645, 0.1397, 0.2407, 0.2426, 0.1304, 0.2365, 0.1710,\n",
      "        0.1865])\n",
      "tensor([0.0219])\n"
     ]
    }
   ],
   "source": [
    "for ix, grp in enumerate(DG.get_all_groups(root_module_types=[nn.Conv2d])):\n",
    "    #print(grp)\n",
    "    print(group_importance(grp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pruner():\n",
    "    def __init__(self, model, context, criteria, layer_type=nn.Conv2d, example_inputs=torch.randn(1,3,224,224)):\n",
    "        store_attr()\n",
    "        self.DG = tp.DependencyGraph()\n",
    "        self.DG.build_dependency(self.model, example_inputs=example_inputs.to(next(model.parameters()).device))\n",
    "        self._save_init_state()\n",
    "        self._reset_threshold()\n",
    "        self.init_num_groups = None\n",
    "\n",
    "    def compute_threshold(self, sparsity):\n",
    "        self.global_importance = {}\n",
    "        for ix, grp in enumerate(self.DG.get_all_groups(root_module_types=[self.layer_type])):\n",
    "            imp = self.group_importance(grp)\n",
    "            self.global_importance[ix] = imp\n",
    "\n",
    "        global_imp = torch.cat(list(self.global_importance.values()), dim=0)\n",
    "\n",
    "        self.init_num_groups = self.init_num_groups or len(global_imp)\n",
    "        n_pruned = np.clip(int((1-sparsity/100)*self.init_num_groups), 1, len(global_imp))\n",
    "        self.global_threshold = torch.topk(global_imp, n_pruned)[0].min()\n",
    "\n",
    "    def prune_group(self, group, ix, sparsity, round_to):\n",
    "        module = group[0][0].target.module\n",
    "        pruning_fn = group[0][0].handler\n",
    "        pruning_idxs = self.prune_method(group, ix, sparsity, round_to)\n",
    "        group = self.DG.get_pruning_group(module, pruning_fn, pruning_idxs.tolist())\n",
    "        group.prune()\n",
    "    \n",
    "    def prune_model(self, sparsity, round_to=None):\n",
    "        if self.context=='global': self.compute_threshold(sparsity)\n",
    "\n",
    "        for ix, group in enumerate(self.DG.get_all_groups(root_module_types=[self.layer_type])):\n",
    "            self.prune_group(group, ix, sparsity, round_to)\n",
    "\n",
    "    def prune_method(self, group, ix, sparsity, round_to):\n",
    "        if self.context=='global':\n",
    "            imp = self.global_importance[ix]\n",
    "            n_pruned = max(1, int(imp.ge(self.global_threshold).sum()))\n",
    "        else:\n",
    "            imp = self.group_importance(group)\n",
    "            n_pruned = max(1, int((1-sparsity/100)*group[0].dep.target.module._init_out_channels))\n",
    " \n",
    "        threshold = torch.topk(imp, int(self._rounded_sparsity(torch.tensor(n_pruned), round_to)))[0].min() if round_to else torch.topk(imp, n_pruned)[0].min()\n",
    "        return imp.lt(threshold).nonzero().view(-1)\n",
    "                \n",
    "    def updated_sparsity(self, m, sparsity):\n",
    "        init_channels = m._init_out_channels\n",
    "        return sparsity\n",
    "                \n",
    "    def _save_init_state(self):\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                setattr(m, '_init_out_channels', self.DG.get_out_channels(m))\n",
    "\n",
    "    def _rounded_sparsity(self, n_to_prune, round_to):\n",
    "        return max(round_to*torch.floor(n_to_prune/round_to), round_to)\n",
    "    \n",
    "    def _reset_threshold(self):\n",
    "        self.global_threshold=None\n",
    "    \n",
    "    def group_importance1(self, group):\n",
    "        handler_map = {\n",
    "            function.prune_conv_out_channels: ('filter', (1, 2, 3)),\n",
    "            function.prune_linear_out_channels: ('row', None),\n",
    "            function.prune_conv_in_channels: ('shared_kernel', (0, 2, 3)),\n",
    "            function.prune_linear_out_channels: ('column', None)\n",
    "        }\n",
    "\n",
    "        group_importance = [\n",
    "            self.criteria(dep.target.module, granularity).squeeze(squeeze_dims)\n",
    "            for dep, _ in group\n",
    "            if dep.handler in handler_map\n",
    "            for granularity, squeeze_dims in [handler_map.get(dep.handler)]\n",
    "        ]\n",
    "\n",
    "        return torch.stack(group_importance).mean(0)\n",
    "    \n",
    "    def group_importance(self, group):\n",
    "            handler_map = {\n",
    "                function.prune_conv_out_channels: ('filter', (1, 2, 3)),\n",
    "                function.prune_linear_out_channels: ('row', None),\n",
    "                function.prune_conv_in_channels: ('shared_kernel', (0, 2, 3)),\n",
    "                function.prune_linear_out_channels: ('column', None)\n",
    "            }\n",
    "\n",
    "            group_importance = []\n",
    "            for dep,_ in group:\n",
    "                if dep.handler in handler_map:\n",
    "                    for granularity, squeeze_dims in [handler_map.get(dep.handler)]:\n",
    "                        #if isinstance(dep.target.module, nn.Conv2d): \n",
    "                            group_importance.append(large_final(dep.target.module, granularity).squeeze(squeeze_dims))\n",
    "\n",
    "            return torch.stack(group_importance).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55c2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
