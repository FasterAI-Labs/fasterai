{
 "cells": [
  {
   "cell_type": "raw",
   "id": "08159415",
   "metadata": {},
   "source": [
    "---\n",
    "description: Use the sparsifier in fastai Callback system\n",
    "output-file: sparsify_callback.html\n",
    "title: Sparsify Callback\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5148f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sparse.sparsify_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce26620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fasterai.sparse.sparsifier import *\n",
    "from fasterai.core.criteria import *\n",
    "from fasterai.core.schedule import *\n",
    "from typing import Callable, Type\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khlqrkci5ec",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `SparsifyCallback` integrates weight sparsification into the fastai training loop. Unlike pruning (which removes structures), sparsification zeros out individual weights while maintaining the original network shape.\n",
    "\n",
    "**Key Features:**\n",
    "- Gradual sparsification according to a schedule\n",
    "- Support for Lottery Ticket Hypothesis (LTH) training\n",
    "- Multiple granularity levels (weight, vector, kernel, filter)\n",
    "- Global or local sparsification context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b720750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SparsifyCallback(Callback):\n",
    "    def __init__(self, \n",
    "                 sparsity: float | dict[str, float],        # Target sparsity (float) or per-layer dict\n",
    "                 granularity: str,                           # Type of pruning granularity (e.g., 'weight', 'filter')\n",
    "                 context: str,                               # Pruning context ('global' or 'local')\n",
    "                 criteria: Criteria,                         # Criteria for determining weights to keep\n",
    "                 schedule: Schedule,                         # Pruning schedule to use\n",
    "                 lth: bool = False,                          # Whether to use Lottery Ticket Hypothesis approach\n",
    "                 rewind_epoch: int = 0,                      # Epoch to rewind weights to for LTH\n",
    "                 reset_end: bool = False,                    # Whether to reset weights after pruning\n",
    "                 save_tickets: bool = False,                 # Whether to save pruned models as \"winning tickets\"\n",
    "                 model: nn.Module | None = None,             # Model to sparsify (if None, uses learn.model)\n",
    "                 round_to: int | None = None,                # Round pruning to multiple of this value\n",
    "                 nm: bool = False,                           # Whether to use N:M structured sparsity\n",
    "                 layer_type: Type[nn.Module] = nn.Conv2d     # Layer type to apply pruning to\n",
    "    ):\n",
    "        \"Callback to sparsify model during training according to a schedule\"\n",
    "        store_attr()\n",
    "        self.current_sparsity = 0.0\n",
    "\n",
    "    def _sparsity_value(self) -> float:\n",
    "        \"Extract a single sparsity value for logging/saving (first value if dict)\"\n",
    "        if isinstance(self.current_sparsity, dict):\n",
    "            return next(iter(self.current_sparsity.values()))\n",
    "        return self.current_sparsity\n",
    "\n",
    "    def before_fit(self) -> None:\n",
    "        \"Setup sparsifier before training\"\n",
    "        print(f'Pruning of {self.granularity} until a sparsity of {self.sparsity}%')\n",
    "        assert self.schedule.start_pct*self.n_epoch>=self.rewind_epoch, 'You must rewind to an epoch before the start of the pruning process'\n",
    "        model = self.model or self.learn.model\n",
    "        self.sparsifier = Sparsifier(model, self.granularity, self.context, self.criteria, self.nm, self.layer_type)\n",
    "\n",
    "    def before_epoch(self) -> None:\n",
    "        \"Save weights at rewind epoch if using LTH\"\n",
    "        if self.epoch == self.rewind_epoch:\n",
    "            print(f'Saving Weights at epoch {self.epoch}')\n",
    "            self.sparsifier._save_weights()\n",
    "\n",
    "    def before_batch(self) -> None:\n",
    "        \"Update sparsity level and potentially apply pruning\"\n",
    "        progress = self.schedule.progress(round(self.pct_train, 3))\n",
    "        \n",
    "        # Compute current sparsity: float * progress or {layer: sp * progress}\n",
    "        if isinstance(self.sparsity, dict):\n",
    "            self.current_sparsity = {k: v * progress for k, v in self.sparsity.items()}\n",
    "        else:\n",
    "            self.current_sparsity = self.sparsity * progress\n",
    "        \n",
    "        if self.schedule.changed and self.training:\n",
    "            if self.lth and self.save_tickets:\n",
    "                print('Saving Intermediate Ticket')\n",
    "                self.sparsifier.save_model(f'winning_ticket_{self._sparsity_value():.2f}.pth', self.learn.model)\n",
    "            self.sparsifier.sparsify_model(self.current_sparsity, self.round_to)\n",
    "\n",
    "    def after_step(self) -> None:\n",
    "        \"Handle post-pruning steps\"\n",
    "        if self.lth and self.schedule.changed:\n",
    "            print(f'Resetting Weights to their epoch {self.rewind_epoch} values')\n",
    "            self.sparsifier._reset_weights(self.learn.model)\n",
    "        self.schedule.after_step()\n",
    "        self.sparsifier._apply_masks()\n",
    "\n",
    "    def after_epoch(self) -> None:\n",
    "        \"Log sparsity after each epoch\"\n",
    "        if isinstance(self.current_sparsity, dict):\n",
    "            avg_sparsity = sum(self.current_sparsity.values()) / len(self.current_sparsity)\n",
    "            print(f'Sparsity at the end of epoch {self.epoch}: avg={avg_sparsity:.2f}%')\n",
    "        else:\n",
    "            print(f'Sparsity at the end of epoch {self.epoch}: {self.current_sparsity:.2f}%')\n",
    "\n",
    "    def after_fit(self) -> None:\n",
    "        \"Clean up after training\"\n",
    "        if self.save_tickets:\n",
    "            print('Saving Final Ticket')\n",
    "            self.sparsifier.save_model(f'winning_ticket_{self._sparsity_value():.2f}.pth', self.learn.model)\n",
    "        \n",
    "        if isinstance(self.current_sparsity, dict):\n",
    "            print(f'Final Sparsity: {self.current_sparsity}')\n",
    "        else:\n",
    "            print(f'Final Sparsity: {self.current_sparsity:.2f}%')\n",
    "        \n",
    "        if self.reset_end: self.sparsifier._reset_weights()\n",
    "        self.sparsifier._clean_buffers()\n",
    "        self.schedule.reset()\n",
    "        self.sparsifier.print_sparsity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1921c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsify_callback.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SparsifyCallback\n",
       "\n",
       "```python\n",
       "\n",
       "def SparsifyCallback(\n",
       "    sparsity:float | dict[str, float], # Target sparsity (float) or per-layer dict\n",
       "    granularity:str, # Type of pruning granularity (e.g., 'weight', 'filter')\n",
       "    context:str, # Pruning context ('global' or 'local')\n",
       "    criteria:Criteria, # Criteria for determining weights to keep\n",
       "    schedule:Schedule, # Pruning schedule to use\n",
       "    lth:bool=False, # Whether to use Lottery Ticket Hypothesis approach\n",
       "    rewind_epoch:int=0, # Epoch to rewind weights to for LTH\n",
       "    reset_end:bool=False, # Whether to reset weights after pruning\n",
       "    save_tickets:bool=False, # Whether to save pruned models as \"winning tickets\"\n",
       "    model:nn.Module | None=None, # Model to sparsify (if None, uses learn.model)\n",
       "    round_to:int | None=None, # Round pruning to multiple of this value\n",
       "    nm:bool=False, # Whether to use N:M structured sparsity\n",
       "    layer_type:Type[nn.Module]=<class 'torch.nn.modules.conv.Conv2d'>, # Layer type to apply pruning to\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def SparsifyCallback(\n",
       "    sparsity:float | dict[str, float], # Target sparsity (float) or per-layer dict\n",
       "    granularity:str, # Type of pruning granularity (e.g., 'weight', 'filter')\n",
       "    context:str, # Pruning context ('global' or 'local')\n",
       "    criteria:Criteria, # Criteria for determining weights to keep\n",
       "    schedule:Schedule, # Pruning schedule to use\n",
       "    lth:bool=False, # Whether to use Lottery Ticket Hypothesis approach\n",
       "    rewind_epoch:int=0, # Epoch to rewind weights to for LTH\n",
       "    reset_end:bool=False, # Whether to reset weights after pruning\n",
       "    save_tickets:bool=False, # Whether to save pruned models as \"winning tickets\"\n",
       "    model:nn.Module | None=None, # Model to sparsify (if None, uses learn.model)\n",
       "    round_to:int | None=None, # Round pruning to multiple of this value\n",
       "    nm:bool=False, # Whether to use N:M structured sparsity\n",
       "    layer_type:Type[nn.Module]=<class 'torch.nn.modules.conv.Conv2d'>, # Layer type to apply pruning to\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SparsifyCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c13f6",
   "metadata": {},
   "source": [
    "The most important part of our `Callback` happens in `before_batch`. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f373da5",
   "metadata": {},
   "source": [
    "The `SparsifyCallback` requires a new argument compared to the `Sparsifier`. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d931837",
   "metadata": {},
   "source": [
    "You can use any scheduling function already [available](https://docs.fast.ai/callback.schedule.html#Annealing) in fastai or come up with your own ! For more information about the pruning schedules, take a look at the [Schedules section](https://nathanhubens.github.io/fasterai/schedules.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46323843",
   "metadata": {},
   "source": [
    "On top of that, the `SparsifyCallback`can also take many optionnal arguments: \n",
    "\n",
    "- `lth`: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\n",
    "- `rewind_epoch`: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\n",
    "- `reset_end`: whether you want to reset the weights to their original values after training (pruning masks are still applied)\n",
    "- `save_tickets`: whether to save intermediate winning tickets.\n",
    "- `model`: pass a model or a part of the model if you don't want to apply pruning on the whole model trained.\n",
    "- `round_to`: if specified, the weights will be pruned to the closest multiple value of `round_to`.\n",
    "- `layer_type`: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sg8iicc39x8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "from fasterai.sparse.sparsify_callback import SparsifyCallback\n",
    "from fasterai.core.schedule import cos\n",
    "from fasterai.core.criteria import large_final\n",
    "\n",
    "# Gradually sparsify to 50% using cosine schedule\n",
    "cb = SparsifyCallback(\n",
    "    sparsity=50,\n",
    "    granularity='weight',\n",
    "    context='global',\n",
    "    criteria=large_final,\n",
    "    schedule=cos\n",
    ")\n",
    "\n",
    "learn.fit(10, cbs=[cb])\n",
    "```\n",
    "\n",
    "### Per-Layer Sparsity with Dict\n",
    "\n",
    "```python\n",
    "# Different sparsity targets for different layers\n",
    "cb = SparsifyCallback(\n",
    "    sparsity={'conv1': 30, 'layer1': 50, 'layer2': 70},\n",
    "    granularity='weight',\n",
    "    context='local',\n",
    "    criteria=large_final,\n",
    "    schedule=cos\n",
    ")\n",
    "```\n",
    "\n",
    "### With Lottery Ticket Hypothesis\n",
    "\n",
    "```python\n",
    "# Train with LTH - rewind weights to epoch 2 values after each pruning step\n",
    "cb = SparsifyCallback(\n",
    "    sparsity=90,\n",
    "    granularity='weight',\n",
    "    context='global', \n",
    "    criteria=large_final,\n",
    "    schedule=one_cycle,\n",
    "    lth=True,\n",
    "    rewind_epoch=2\n",
    ")\n",
    "\n",
    "learn.fit(20, cbs=[cb])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cjclqrgs92u",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# Construction with valid params (objects, not strings)\n",
    "cb = SparsifyCallback(\n",
    "    sparsity=50, granularity='weight', context='local',\n",
    "    criteria=large_final, schedule=one_shot\n",
    ")\n",
    "test_eq(cb.sparsity, 50)\n",
    "test_eq(cb.granularity, 'weight')\n",
    "test_eq(cb.context, 'local')\n",
    "test_eq(cb.current_sparsity, 0.0)\n",
    "\n",
    "# Dict-based sparsity\n",
    "cb_dict = SparsifyCallback(\n",
    "    sparsity={'conv1': 30, 'conv2': 60}, granularity='weight',\n",
    "    context='local', criteria=large_final, schedule=lin\n",
    ")\n",
    "assert isinstance(cb_dict.sparsity, dict)\n",
    "test_eq(cb_dict.sparsity['conv1'], 30)\n",
    "test_eq(cb_dict.sparsity['conv2'], 60)\n",
    "\n",
    "# _sparsity_value helper with float\n",
    "cb.current_sparsity = 42.0\n",
    "test_eq(cb._sparsity_value(), 42.0)\n",
    "\n",
    "# _sparsity_value helper with dict\n",
    "cb_dict.current_sparsity = {'a': 10.0, 'b': 20.0}\n",
    "test_eq(cb_dict._sparsity_value(), 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddlfs73mvbt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.315830</td>\n",
       "      <td>2.356278</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.313395</td>\n",
       "      <td>2.356196</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.310454</td>\n",
       "      <td>2.356880</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Sparsity at the end of epoch 1: 50.00%\n",
      "Sparsity at the end of epoch 2: 50.00%\n",
      "Final Sparsity: 50.00%\n",
      "\n",
      "Sparsity Report:\n",
      "--------------------------------------------------------------------------------\n",
      "Layer                          Type            Params     Zeros      Sparsity  \n",
      "--------------------------------------------------------------------------------\n",
      "0                              Conv2d          432        216           50.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Overall                        all             432        216           50.00%\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| slow\n",
    "# Full training loop with SparsifyCallback â€” verify sparsity is applied\n",
    "from torch.utils.data import TensorDataset\n",
    "from fastai.data.core import DataLoaders\n",
    "\n",
    "_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(16, 10)\n",
    ")\n",
    "_X = torch.randn(64, 3, 8, 8)\n",
    "_y = torch.randint(0, 10, (64,))\n",
    "_dls = DataLoaders.from_dsets(\n",
    "    TensorDataset(_X[:48], _y[:48]),\n",
    "    TensorDataset(_X[48:], _y[48:]),\n",
    "    bs=16, device='cpu'\n",
    ")\n",
    "\n",
    "_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local',\n",
    "                       criteria=large_final, schedule=one_shot)\n",
    "_learn = Learner(_dls, _model, loss_func=nn.CrossEntropyLoss(), cbs=[_cb])\n",
    "_learn.fit(3)\n",
    "\n",
    "# Verify sparsification was applied to conv layers\n",
    "for m in _model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        _sp = (m.weight == 0).float().mean().item() * 100\n",
    "        test_close(_sp, 50.0, eps=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fj8dpsjb4w6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Sparsifier](sparsifier.html) - Core sparsification class used by this callback\n",
    "- [Schedules](../core/schedules.html) - Control sparsification progression (one_shot, agp, etc.)\n",
    "- [Criteria](../core/criteria.html) - Importance measures (large_final, movement, etc.)\n",
    "- [Lottery Ticket Tutorial](../tutorials/sparse/lottery_ticket.html) - Finding winning tickets with sparsification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
