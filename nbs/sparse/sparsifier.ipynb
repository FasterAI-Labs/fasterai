{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Make your neural network sparse\n",
    "output-file: sparsifier.html\n",
    "title: Sparsifier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sparse.sparsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "from fastcore.basics import store_attr, listify, true\n",
    "from typing import Callable, Optional, Union, List, Tuple, Type\n",
    "from fasterai.core.criteria import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network's weight are mostly zeroes.\n",
    "\n",
    "With fasterai, you can do that thanks to the `Sparsifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sparsifier():\n",
    "    \"Class providing sparsifying capabilities\"\n",
    "    def __init__(self, \n",
    "                 model: nn.Module,                        # The model to sparsify\n",
    "                 granularity: str,                        # Granularity of sparsification (e.g., 'weight', 'filter')\n",
    "                 context: str,                            # Context for sparsification ('global' or 'local')\n",
    "                 criteria: Criteria,                      # Criteria to determine which weights to keep\n",
    "                 nm: bool = False,                        # Whether to use N:M sparsity pattern (forces 2:4 sparsity)\n",
    "                 layer_type: Type[nn.Module] = nn.Conv2d  # Type of layers to apply sparsification to\n",
    "    ):\n",
    "        if nm == True: print('Sparsity automatically set to 50% with 2:4 pattern')\n",
    "        store_attr()\n",
    "        self._save_weights()\n",
    "        self._reset_threshold()\n",
    "\n",
    "    def sparsify_layer(self, \n",
    "                       m: nn.Module,                   # The layer to sparsify\n",
    "                       sparsity: float,                # Target sparsity level (percentage)\n",
    "                       round_to: Optional[int] = None  # Round to a multiple of this value\n",
    "    ) -> None:\n",
    "        \"Apply sparsification to a single layer\"\n",
    "        scores    = self._compute_scores(m, sparsity)\n",
    "        threshold = self._compute_threshold(scores, sparsity, round_to)\n",
    "        mask      = self._compute_mask(scores, threshold)\n",
    "        m.register_buffer('_mask', mask)\n",
    "        self._apply(m)\n",
    "        self.criteria.update_weights(m)\n",
    "\n",
    "    def sparsify_model(self, \n",
    "                       sparsity: Union[float, List[float]],  # Target sparsity level(s)\n",
    "                       round_to: Optional[int] = None        # Round to a multiple of this value\n",
    "    ) -> None:\n",
    "        \"Apply sparsification to all matching layers in the model\"\n",
    "        self._reset_threshold()\n",
    "        sparsity_list = listify(sparsity)\n",
    "        if len(sparsity_list) > 1 and self.context != 'local': raise ValueError(f\"A list of sparsities can only be used with 'local' context, not {self.context}\")\n",
    "        sparsities = cycle(sparsity_list) if len(sparsity_list)==1 else iter(sparsity_list)\n",
    "        mods = list(self.model.modules())\n",
    "        for k,m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, self.layer_type): \n",
    "                sp = next(sparsities)\n",
    "                self.sparsify_layer(m, sp, round_to)\n",
    "                if k+1 < len(mods) and isinstance(mods[k+1], nn.modules.batchnorm._BatchNorm): self.sparsify_batchnorm(m, mods[k+1])\n",
    "                \n",
    "    def sparsify_batchnorm(self, \n",
    "                          m: nn.Module,       # The layer before batch norm\n",
    "                          bn: nn.Module       # The batch norm layer\n",
    "    ) -> None:\n",
    "        \"Apply filter pruning to batch norm parameters if appropriate\"\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if self.granularity == 'filter' and true(mask):\n",
    "            bn.weight.data.mul_(mask.squeeze())\n",
    "            bn.bias.data.mul_(mask.squeeze())\n",
    "            \n",
    "    def _apply_masks(self) -> None:\n",
    "        \"Apply all stored masks to model weights\"\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, self.layer_type):\n",
    "                self._apply(m)\n",
    "        \n",
    "    def _apply(self, \n",
    "              m: nn.Module  # Module to apply mask to\n",
    "    ) -> None:\n",
    "        \"Apply mask to a module's weights\"\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if true(mask): m.weight.data.mul_(mask)\n",
    "        if self.granularity == 'filter' and true(m.bias):\n",
    "            if true(mask): m.bias.data.mul_(mask.squeeze())\n",
    "    \n",
    "    def _reset_weights(self, \n",
    "                      model: Optional[nn.Module] = None  # Model to reset (default: self.model)\n",
    "    ) -> None:\n",
    "        \"Reset weights to their initial values\"\n",
    "        model = model or self.model\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                init_weights = getattr(m, \"_init_weights\", m.weight)\n",
    "                init_biases = getattr(m, \"_init_biases\", m.bias)\n",
    "                with torch.no_grad():\n",
    "                    if true(m.weight): m.weight.copy_(init_weights)\n",
    "                    if true(m.bias): m.bias.copy_(init_biases)\n",
    "                self._apply(m)\n",
    "            if isinstance(m, nn.modules.batchnorm._BatchNorm): m.reset_parameters()\n",
    "                \n",
    "    def _save_weights(self) -> None:\n",
    "        \"Save initial weights of the model\"\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'weight'):              \n",
    "                m.register_buffer(\"_init_weights\", m.weight.clone())\n",
    "                bias = getattr(m, 'bias', None)\n",
    "                if true(bias): m.register_buffer(\"_init_biases\", bias.clone())\n",
    "                    \n",
    "    def save_model(self, \n",
    "                  path: str,             # Path to save the model\n",
    "                  model: Optional[nn.Module] = None  # Model to save (default: self.model)\n",
    "    ) -> None:\n",
    "        \"Save model without sparsification buffers\"\n",
    "        model = model or self.model\n",
    "        tmp_model = copy.deepcopy(model)\n",
    "        self._reset_weights(tmp_model)\n",
    "        self._clean_buffers(tmp_model)\n",
    "        torch.save(tmp_model, path)\n",
    "\n",
    "    def _clean_buffers(self, \n",
    "                      model: Optional[nn.Module] = None  # Model to clean (default: self.model)\n",
    "    ) -> None:\n",
    "        \"Remove internal buffers used for sparsification\"\n",
    "        model = model or self.model\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                if hasattr(m, '_mask'): del m._buffers[\"_mask\"]\n",
    "                if hasattr(m, '_init_weights'): del m._buffers[\"_init_weights\"]\n",
    "                if hasattr(m, '_init_biases'): del m._buffers[\"_init_biases\"]\n",
    "                    \n",
    "    def _reset_threshold(self) -> None:\n",
    "        \"Reset the threshold used for global pruning\"\n",
    "        self.threshold=None\n",
    "            \n",
    "    def _rounded_sparsity(self, \n",
    "                         n_to_prune: int,  # Number of elements to prune\n",
    "                         round_to: int     # Rounding value\n",
    "    ) -> int:\n",
    "        \"Round the number of elements to keep to a multiple of round_to\"\n",
    "        return max(round_to*torch.ceil(n_to_prune/round_to), round_to)\n",
    "    \n",
    "    def _compute_scores(self, \n",
    "                       m: nn.Module,  # Module to compute scores for\n",
    "                       sparsity: float # Target sparsity level\n",
    "    ) -> torch.Tensor:\n",
    "        \"Compute importance scores for weights based on criteria\"\n",
    "        return self.criteria(m, self.granularity)\n",
    "                \n",
    "    def _compute_threshold(self, \n",
    "                          scores: torch.Tensor,  # Importance scores\n",
    "                          sparsity: float,       # Target sparsity level\n",
    "                          round_to: Optional[int] # Rounding value\n",
    "    ) -> torch.Tensor:\n",
    "        \"Compute threshold for pruning, with optional rounding\"\n",
    "        if self.context == 'global':\n",
    "            if self.threshold is None: \n",
    "                global_scores  = torch.cat([self.criteria(m, self.granularity).view(-1) for m in self.model.modules() if isinstance(m, self.layer_type)])\n",
    "                self.threshold = torch.quantile(global_scores.view(-1), sparsity/100)   \n",
    "        elif self.context == 'local': \n",
    "            self.threshold = torch.quantile(scores.view(-1), sparsity/100)\n",
    "        else: raise ValueError(f'Invalid context: {self.context}. Must be \"global\" or \"local\"')\n",
    "            \n",
    "        if round_to:\n",
    "            n_to_keep = sum(scores.ge(self.threshold)).squeeze()\n",
    "            self.threshold = torch.topk(scores.squeeze(), int(self._rounded_sparsity(n_to_keep, round_to)))[0].min()\n",
    "        return self.threshold\n",
    "    \n",
    "    def _compute_mask(self, \n",
    "                     scores: torch.Tensor,  # Importance scores\n",
    "                     threshold: torch.Tensor # Threshold for pruning\n",
    "    ) -> torch.Tensor:\n",
    "        \"Compute binary mask for weights based on scores and threshold\"\n",
    "        if self.nm == True: return self._apply_nm_sparsity(scores)\n",
    "        if threshold > scores.max(): threshold = scores.max() # Make sure we don't remove every weight of a given layer\n",
    "        return scores.ge(threshold).to(dtype=scores.dtype)\n",
    "\n",
    "    def _apply_nm_sparsity(self, \n",
    "                          scores: torch.Tensor  # Importance scores\n",
    "    ) -> torch.Tensor:\n",
    "        \"Apply 2:4 structured sparsity pattern (N:M sparsity where N=2, M=4)\"\n",
    "        out_channels, in_channels, kernel_height, kernel_width = scores.shape\n",
    "    \n",
    "        if in_channels % 4 != 0 or in_channels * kernel_height * kernel_width % 16 != 0:\n",
    "            print(f\"Skipping 2:4 sparsity, Cin * Kh * Kw is not a multiple of 16\")\n",
    "            return torch.ones_like(scores)\n",
    "    \n",
    "        blocked_scores = rearrange(scores, 'o (b c) h w -> h w o b c', c=4)\n",
    "        threshold = blocked_scores.topk(k=2, dim=-1).values[..., -1:]\n",
    "        mask = (blocked_scores >= threshold).float()\n",
    "        return rearrange(mask, 'h w o b c -> o (b c) h w')\n",
    "\n",
    "    def print_sparsity(self) -> None:\n",
    "        total_params = 0\n",
    "        total_zeros = 0\n",
    "        \n",
    "        print(\"\\nSparsity Report:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Layer':<20} {'Type':<15} {'Params':<10} {'Zeros':<10} {'Sparsity':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for k, m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, self.layer_type):\n",
    "                zeros = torch.sum(m.weight == 0).item()\n",
    "                total = m.weight.nelement()\n",
    "                sparsity_pct = 100.0 * zeros / total if total > 0 else 0\n",
    "                \n",
    "                print(f\"{f'Layer {k}':<20} {m.__class__.__name__:<15} \"\n",
    "                      f\"{total:<10,d} {zeros:<10,d} {sparsity_pct:>8.2f}%\")\n",
    "                \n",
    "                total_params += total\n",
    "                total_zeros += zeros\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        overall_sparsity = 100.0 * total_zeros / total_params if total_params > 0 else 0\n",
    "        print(f\"{'Overall':<20} {'all':<15} {total_params:<10,d} \"\n",
    "              f\"{total_zeros:<10,d} {overall_sparsity:>8.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier\n",
       "\n",
       ">      Sparsifier (model:torch.nn.modules.module.Module, granularity:str,\n",
       ">                  context:str, criteria:fasterai.core.criteria.Criteria,\n",
       ">                  nm:bool=False,\n",
       ">                  layer_type:Type[torch.nn.modules.module.Module]=<class\n",
       ">                  'torch.nn.modules.conv.Conv2d'>)\n",
       "\n",
       "*Class providing sparsifying capabilities*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model | Module |  | The model to sparsify |\n",
       "| granularity | str |  | Granularity of sparsification (e.g., 'weight', 'filter') |\n",
       "| context | str |  | Context for sparsification ('global' or 'local') |\n",
       "| criteria | Criteria |  | Criteria to determine which weights to keep |\n",
       "| nm | bool | False | Whether to use N:M sparsity pattern (forces 2:4 sparsity) |\n",
       "| layer_type | Type | Conv2d | Type of layers to apply sparsification to |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier\n",
       "\n",
       ">      Sparsifier (model:torch.nn.modules.module.Module, granularity:str,\n",
       ">                  context:str, criteria:fasterai.core.criteria.Criteria,\n",
       ">                  nm:bool=False,\n",
       ">                  layer_type:Type[torch.nn.modules.module.Module]=<class\n",
       ">                  'torch.nn.modules.conv.Conv2d'>)\n",
       "\n",
       "*Class providing sparsifying capabilities*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| model | Module |  | The model to sparsify |\n",
       "| granularity | str |  | Granularity of sparsification (e.g., 'weight', 'filter') |\n",
       "| context | str |  | Context for sparsification ('global' or 'local') |\n",
       "| criteria | Criteria |  | Criteria to determine which weights to keep |\n",
       "| nm | bool | False | Whether to use N:M sparsity pattern (forces 2:4 sparsity) |\n",
       "| layer_type | Type | Conv2d | Type of layers to apply sparsification to |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sparsifier` class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n",
    "\n",
    "- The `granularity`, i.e. the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\n",
    "- The `context`, i.e. if you want to consider each layer independently (`local`), or compare the parameters to remove across the whole network (`global`).\n",
    "- The `criteria`, i.e. the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can pass a single layer to prune by using the  `Sparsifier.sparsify_layer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier.sparsify_layer\n",
       "\n",
       ">      Sparsifier.sparsify_layer (m:torch.nn.modules.module.Module,\n",
       ">                                 sparsity:float, round_to:Optional[int]=None)\n",
       "\n",
       "*Apply sparsification to a single layer*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| m | Module |  | The layer to sparsify |\n",
       "| sparsity | float |  | Target sparsity level (percentage) |\n",
       "| round_to | Optional | None | Round to a multiple of this value |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier.sparsify_layer\n",
       "\n",
       ">      Sparsifier.sparsify_layer (m:torch.nn.modules.module.Module,\n",
       ">                                 sparsity:float, round_to:Optional[int]=None)\n",
       "\n",
       "*Apply sparsification to a single layer*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| m | Module |  | The layer to sparsify |\n",
       "| sparsity | float |  | Target sparsity level (percentage) |\n",
       "| round_to | Optional | None | Round to a multiple of this value |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.sparsify_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we may want to prune the whole model at once, using the `Sparsifier.prune_model` method, indicating the percentage of sparsity to you want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L45){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier.sparsify_model\n",
       "\n",
       ">      Sparsifier.sparsify_model (sparsity:Union[float,List[float]],\n",
       ">                                 round_to:Optional[int]=None)\n",
       "\n",
       "*Apply sparsification to all matching layers in the model*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sparsity | Union |  | Target sparsity level(s) |\n",
       "| round_to | Optional | None | Round to a multiple of this value |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L45){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier.sparsify_model\n",
       "\n",
       ">      Sparsifier.sparsify_model (sparsity:Union[float,List[float]],\n",
       ">                                 round_to:Optional[int]=None)\n",
       "\n",
       "*Apply sparsification to all matching layers in the model*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| sparsity | Union |  | Target sparsity level(s) |\n",
       "| round_to | Optional | None | Round to a multiple of this value |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.sparsify_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some case, you may want to impose the remaining amount of parameters to be a multiple of a given number (e.g. 8), this can be done by passing the `round_to` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, instead of passing a single value of sparsity, a list of sparsities can also be provided. In that case, each value in the list is the sparsity that will be applied to all layers.\n",
    "\n",
    "**Example**: I have a 4-layer network and want to remove half of the parameters from the layers 2 and 3, I can provide the list: `sparsity = [0, 50, 50, 0]`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
