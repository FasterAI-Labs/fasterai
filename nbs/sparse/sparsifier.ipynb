{
 "cells": [
  {
   "cell_type": "raw",
   "id": "71a46a42",
   "metadata": {},
   "source": [
    "---\n",
    "description: Make your neural network sparse\n",
    "output-file: sparsifier.html\n",
    "title: Sparsifier\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sparse.sparsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from fastcore.basics import store_attr, true\n",
    "from typing import Callable, Type\n",
    "from fasterai.core.criteria import *\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sparsifier-overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network's weights are mostly zeroes.\n",
    "\n",
    "With fasterai, you can do that thanks to the `Sparsifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sparsifier():\n",
    "    \"Class providing sparsifying capabilities\"\n",
    "    def __init__(self, \n",
    "                 model: nn.Module,                        # The model to sparsify\n",
    "                 granularity: str,                        # Granularity of sparsification (e.g., 'weight', 'filter')\n",
    "                 context: str,                            # Context for sparsification ('global' or 'local')\n",
    "                 criteria: Criteria,                      # Criteria to determine which weights to keep\n",
    "                 nm: bool = False,                        # Whether to use N:M sparsity pattern (forces 2:4 sparsity)\n",
    "                 layer_type: Type[nn.Module] = nn.Conv2d  # Type of layers to apply sparsification to\n",
    "    ):\n",
    "        if nm: print('Sparsity automatically set to 50% with 2:4 pattern')\n",
    "        store_attr()\n",
    "        self._save_weights()\n",
    "        self._reset_threshold()\n",
    "\n",
    "    def _iter_layers(self, \n",
    "                     filter_type: str = 'layer_type',       # Filter: 'layer_type' or 'has_weight'\n",
    "                     model: nn.Module | None = None         # Model to iterate (default: self.model)\n",
    "    ):\n",
    "        \"Iterate over model modules with filtering\"\n",
    "        model = model or self.model\n",
    "        for m in model.modules():\n",
    "            if filter_type == 'layer_type' and isinstance(m, self.layer_type):\n",
    "                yield m\n",
    "            elif filter_type == 'has_weight' and hasattr(m, 'weight'):\n",
    "                yield m\n",
    "\n",
    "    def _iter_named_layers(self):\n",
    "        \"Iterate over matching layers with their names\"\n",
    "        for name, m in self.model.named_modules():\n",
    "            if isinstance(m, self.layer_type):\n",
    "                yield name, m\n",
    "\n",
    "    def _to_sparsity_dict(self, \n",
    "                          sparsity: float | dict  # Sparsity value or per-layer dict\n",
    "    ) -> dict:\n",
    "        \"Convert any sparsity input to a {module: sparsity} dict\"\n",
    "        name_to_module = dict(self.model.named_modules())\n",
    "        \n",
    "        # Float: apply same sparsity to all layers\n",
    "        if isinstance(sparsity, (int, float)):\n",
    "            if not (0 <= sparsity <= 100):\n",
    "                raise ValueError(f\"sparsity must be in range [0, 100], got {sparsity}\")\n",
    "            return {m: sparsity for m in self._iter_layers()}\n",
    "        \n",
    "        # Dict: resolve names to modules\n",
    "        if isinstance(sparsity, dict):\n",
    "            resolved = {}\n",
    "            for key, sp in sparsity.items():\n",
    "                if not (0 <= sp <= 100):\n",
    "                    raise ValueError(f\"sparsity must be in range [0, 100], got {sp}\")\n",
    "                if isinstance(key, str):\n",
    "                    if key in name_to_module:\n",
    "                        resolved[name_to_module[key]] = sp\n",
    "                    else:\n",
    "                        print(f\"Warning: Layer '{key}' not found in model, skipping\")\n",
    "                elif isinstance(key, nn.Module):\n",
    "                    resolved[key] = sp\n",
    "            return resolved\n",
    "        \n",
    "        raise TypeError(f\"sparsity must be float or dict, got {type(sparsity)}\")\n",
    "\n",
    "    def sparsify_layer(self, \n",
    "                       m: nn.Module,              # The layer to sparsify\n",
    "                       sparsity: float,           # Target sparsity level (percentage)\n",
    "                       round_to: int | None = None  # Round to a multiple of this value\n",
    "    ) -> None:\n",
    "        \"Apply sparsification to a single layer\"\n",
    "        if not (0 <= sparsity <= 100):\n",
    "            raise ValueError(f\"sparsity must be in range [0, 100], got {sparsity}\")\n",
    "        scores    = self._compute_scores(m, sparsity)\n",
    "        threshold = self._compute_threshold(scores, sparsity, round_to)\n",
    "        mask      = self._compute_mask(scores, threshold)\n",
    "        m.register_buffer('_mask', mask)\n",
    "        self._apply(m)\n",
    "        self.criteria.update_weights(m)\n",
    "\n",
    "    def sparsify_model(self, \n",
    "                       sparsity: float | dict,        # Target sparsity level or per-layer dict\n",
    "                       round_to: int | None = None    # Round to a multiple of this value\n",
    "    ) -> None:\n",
    "        \"Apply sparsification to all matching layers in the model\"\n",
    "        self._reset_threshold()\n",
    "        \n",
    "        # Validate context for non-uniform sparsity\n",
    "        if isinstance(sparsity, dict) and self.context == 'global':\n",
    "            raise ValueError(\"Dict-based sparsity requires 'local' context\")\n",
    "        \n",
    "        # Convert to unified dict format\n",
    "        sparsity_map = self._to_sparsity_dict(sparsity)\n",
    "        \n",
    "        # Single iteration loop for all cases\n",
    "        mods = list(self.model.modules())\n",
    "        for name, m in self._iter_named_layers():\n",
    "            if m not in sparsity_map:\n",
    "                continue\n",
    "            sp = sparsity_map[m]\n",
    "            self.sparsify_layer(m, sp, round_to)\n",
    "            # Handle batch norm if present\n",
    "            mod_idx = mods.index(m)\n",
    "            if mod_idx + 1 < len(mods) and isinstance(mods[mod_idx + 1], nn.modules.batchnorm._BatchNorm):\n",
    "                self.sparsify_batchnorm(m, mods[mod_idx + 1])\n",
    "                \n",
    "    def sparsify_batchnorm(self, \n",
    "                          m: nn.Module,       # The layer before batch norm\n",
    "                          bn: nn.Module       # The batch norm layer\n",
    "    ) -> None:\n",
    "        \"Apply filter pruning to batch norm parameters if appropriate\"\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if self.granularity == 'filter' and true(mask):\n",
    "            bn.weight.data.mul_(mask.squeeze())\n",
    "            bn.bias.data.mul_(mask.squeeze())\n",
    "            \n",
    "    def _apply_masks(self) -> None:\n",
    "        \"Apply all stored masks to model weights\"\n",
    "        for m in self._iter_layers():\n",
    "            self._apply(m)\n",
    "        \n",
    "    def _apply(self, \n",
    "              m: nn.Module  # Module to apply mask to\n",
    "    ) -> None:\n",
    "        \"Apply mask to a module's weights\"\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if true(mask): m.weight.data.mul_(mask)\n",
    "        if self.granularity == 'filter' and true(m.bias):\n",
    "            if true(mask): m.bias.data.mul_(mask.squeeze())\n",
    "    \n",
    "    def _reset_weights(self, \n",
    "                      model: nn.Module | None = None  # Model to reset (default: self.model)\n",
    "    ) -> None:\n",
    "        \"Reset weights to their initial values\"\n",
    "        model = model or self.model\n",
    "        for m in self._iter_layers('has_weight', model):\n",
    "            init_weights = getattr(m, \"_init_weights\", m.weight)\n",
    "            init_biases = getattr(m, \"_init_biases\", m.bias)\n",
    "            with torch.no_grad():\n",
    "                if true(m.weight): m.weight.copy_(init_weights)\n",
    "                if true(m.bias): m.bias.copy_(init_biases)\n",
    "            self._apply(m)\n",
    "            if isinstance(m, nn.modules.batchnorm._BatchNorm): m.reset_parameters()\n",
    "                \n",
    "    def _save_weights(self) -> None:\n",
    "        \"Save initial weights of the model\"\n",
    "        for m in self._iter_layers('has_weight'):\n",
    "            m.register_buffer(\"_init_weights\", m.weight.clone())\n",
    "            bias = getattr(m, 'bias', None)\n",
    "            if true(bias): m.register_buffer(\"_init_biases\", bias.clone())\n",
    "                    \n",
    "    def save_model(self, \n",
    "                  path: str,                            # Path to save the model\n",
    "                  model: nn.Module | None = None        # Model to save (default: self.model)\n",
    "    ) -> None:\n",
    "        \"Save model without sparsification buffers\"\n",
    "        model = model or self.model\n",
    "        tmp_model = copy.deepcopy(model)\n",
    "        self._reset_weights(tmp_model)\n",
    "        self._clean_buffers(tmp_model)\n",
    "        torch.save(tmp_model, path)\n",
    "\n",
    "    def _clean_buffers(self, \n",
    "                      model: nn.Module | None = None  # Model to clean (default: self.model)\n",
    "    ) -> None:\n",
    "        \"Remove internal buffers used for sparsification\"\n",
    "        model = model or self.model\n",
    "        for m in self._iter_layers('has_weight', model):\n",
    "            if hasattr(m, '_mask'): del m._buffers[\"_mask\"]\n",
    "            if hasattr(m, '_init_weights'): del m._buffers[\"_init_weights\"]\n",
    "            if hasattr(m, '_init_biases'): del m._buffers[\"_init_biases\"]\n",
    "                    \n",
    "    def _reset_threshold(self) -> None:\n",
    "        \"Reset the threshold used for global pruning\"\n",
    "        self.threshold = None\n",
    "            \n",
    "    def _rounded_sparsity(self, \n",
    "                         n_to_prune: int,  # Number of elements to prune\n",
    "                         round_to: int     # Rounding value\n",
    "    ) -> int:\n",
    "        \"Round the number of elements to keep to a multiple of round_to\"\n",
    "        if round_to == 0:\n",
    "            raise ValueError(\"round_to must be non-zero\")\n",
    "        return max(round_to * torch.ceil(n_to_prune / round_to), round_to)\n",
    "    \n",
    "    def _compute_scores(self, \n",
    "                       m: nn.Module,   # Module to compute scores for\n",
    "                       sparsity: float # Target sparsity level\n",
    "    ) -> torch.Tensor:\n",
    "        \"Compute importance scores for weights based on criteria\"\n",
    "        return self.criteria(m, self.granularity)\n",
    "                \n",
    "    def _compute_threshold(self, \n",
    "                          scores: torch.Tensor,  # Importance scores\n",
    "                          sparsity: float,       # Target sparsity level\n",
    "                          round_to: int | None   # Rounding value\n",
    "    ) -> torch.Tensor:\n",
    "        \"Compute threshold for pruning, with optional rounding\"\n",
    "        if self.context == 'global':\n",
    "            if self.threshold is None: \n",
    "                global_scores = torch.cat([self.criteria(m, self.granularity).view(-1) for m in self._iter_layers()])\n",
    "                self.threshold = torch.quantile(global_scores.view(-1), sparsity / 100)   \n",
    "        elif self.context == 'local': \n",
    "            self.threshold = torch.quantile(scores.view(-1), sparsity / 100)\n",
    "        else: \n",
    "            raise ValueError(f'Invalid context: {self.context}. Must be \"global\" or \"local\"')\n",
    "            \n",
    "        if round_to:\n",
    "            n_to_keep = sum(scores.ge(self.threshold)).squeeze()\n",
    "            self.threshold = torch.topk(scores.squeeze(), int(self._rounded_sparsity(n_to_keep, round_to)))[0].min()\n",
    "        return self.threshold\n",
    "    \n",
    "    def _compute_mask(self, \n",
    "                     scores: torch.Tensor,   # Importance scores\n",
    "                     threshold: torch.Tensor # Threshold for pruning\n",
    "    ) -> torch.Tensor:\n",
    "        \"Compute binary mask for weights based on scores and threshold\"\n",
    "        if self.nm: return self._apply_nm_sparsity(scores)\n",
    "        if threshold > scores.max(): threshold = scores.max()\n",
    "        return scores.ge(threshold).to(dtype=scores.dtype)\n",
    "\n",
    "    def _apply_nm_sparsity(self, \n",
    "                          scores: torch.Tensor  # Importance scores\n",
    "    ) -> torch.Tensor:\n",
    "        \"Apply 2:4 structured sparsity pattern (N:M sparsity where N=2, M=4)\"\n",
    "        out_channels, in_channels, kernel_height, kernel_width = scores.shape\n",
    "    \n",
    "        if in_channels % 4 != 0 or in_channels * kernel_height * kernel_width % 16 != 0:\n",
    "            print(f\"Skipping 2:4 sparsity, Cin * Kh * Kw is not a multiple of 16\")\n",
    "            return torch.ones_like(scores)\n",
    "    \n",
    "        blocked_scores = rearrange(scores, 'o (b c) h w -> h w o b c', c=4)\n",
    "        threshold = blocked_scores.topk(k=2, dim=-1).values[..., -1:]\n",
    "        mask = (blocked_scores >= threshold).float()\n",
    "        return rearrange(mask, 'h w o b c -> o (b c) h w')\n",
    "\n",
    "    def print_sparsity(self) -> None:\n",
    "        \"Print sparsity report for all layers\"\n",
    "        total_params = 0\n",
    "        total_zeros = 0\n",
    "        \n",
    "        print(\"\\nSparsity Report:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Layer':<30} {'Type':<15} {'Params':<10} {'Zeros':<10} {'Sparsity':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for name, m in self._iter_named_layers():\n",
    "            zeros = torch.sum(m.weight == 0).item()\n",
    "            total = m.weight.nelement()\n",
    "            sparsity_pct = 100.0 * zeros / total if total > 0 else 0\n",
    "            \n",
    "            print(f\"{name:<30} {m.__class__.__name__:<15} \"\n",
    "                  f\"{total:<10,d} {zeros:<10,d} {sparsity_pct:>8.2f}%\")\n",
    "            \n",
    "            total_params += total\n",
    "            total_zeros += zeros\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        overall_sparsity = 100.0 * total_zeros / total_params if total_params > 0 else 0\n",
    "        print(f\"{'Overall':<30} {'all':<15} {total_params:<10,d} \"\n",
    "              f\"{total_zeros:<10,d} {overall_sparsity:>8.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier\n",
       "\n",
       "```python\n",
       "\n",
       "def Sparsifier(\n",
       "    model:nn.Module, # The model to sparsify\n",
       "    granularity:str, # Granularity of sparsification (e.g., 'weight', 'filter')\n",
       "    context:str, # Context for sparsification ('global' or 'local')\n",
       "    criteria:Criteria, # Criteria to determine which weights to keep\n",
       "    nm:bool=False, # Whether to use N:M sparsity pattern (forces 2:4 sparsity)\n",
       "    layer_type:Type[nn.Module]=<class 'torch.nn.modules.conv.Conv2d'>, # Type of layers to apply sparsification to\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Class providing sparsifying capabilities*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def Sparsifier(\n",
       "    model:nn.Module, # The model to sparsify\n",
       "    granularity:str, # Granularity of sparsification (e.g., 'weight', 'filter')\n",
       "    context:str, # Context for sparsification ('global' or 'local')\n",
       "    criteria:Criteria, # Criteria to determine which weights to keep\n",
       "    nm:bool=False, # Whether to use N:M sparsity pattern (forces 2:4 sparsity)\n",
       "    layer_type:Type[nn.Module]=<class 'torch.nn.modules.conv.Conv2d'>, # Type of layers to apply sparsification to\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Class providing sparsifying capabilities*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sparsifier-params",
   "metadata": {},
   "source": [
    "The `Sparsifier` class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n",
    "\n",
    "- The `granularity`, i.e. the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\n",
    "- The `context`, i.e. if you want to consider each layer independently (`local`), or compare the parameters to remove across the whole network (`global`).\n",
    "- The `criteria`, i.e. the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separator-1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-header",
   "metadata": {},
   "source": [
    "## Key Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "layer-desc",
   "metadata": {},
   "source": [
    "User can pass a single layer to sparsify by using the `Sparsifier.sparsify_layer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31eed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier.sparsify_layer\n",
       "\n",
       "```python\n",
       "\n",
       "def sparsify_layer(\n",
       "    m:nn.Module, # The layer to sparsify\n",
       "    sparsity:float, # Target sparsity level (percentage)\n",
       "    round_to:int | None=None, # Round to a multiple of this value\n",
       ")->None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Apply sparsification to a single layer*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def sparsify_layer(\n",
       "    m:nn.Module, # The layer to sparsify\n",
       "    sparsity:float, # Target sparsity level (percentage)\n",
       "    round_to:int | None=None, # Round to a multiple of this value\n",
       ")->None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Apply sparsification to a single layer*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.sparsify_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separator-2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-desc",
   "metadata": {},
   "source": [
    "Most of the time, we may want to sparsify the whole model at once, using the `Sparsifier.sparsify_model` method, indicating the percentage of sparsity you want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/sparse/sparsifier.py#L94){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Sparsifier.sparsify_model\n",
       "\n",
       "```python\n",
       "\n",
       "def sparsify_model(\n",
       "    sparsity:float | dict, # Target sparsity level or per-layer dict\n",
       "    round_to:int | None=None, # Round to a multiple of this value\n",
       ")->None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Apply sparsification to all matching layers in the model*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def sparsify_model(\n",
       "    sparsity:float | dict, # Target sparsity level or per-layer dict\n",
       "    round_to:int | None=None, # Round to a multiple of this value\n",
       ")->None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Apply sparsification to all matching layers in the model*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.sparsify_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separator-3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-header",
   "metadata": {},
   "source": [
    "## Advanced Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-to-desc",
   "metadata": {},
   "source": [
    "In some case, you may want to impose the remaining amount of parameters to be a multiple of a given number (e.g. 8), this can be done by passing the `round_to` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "list-sparsity-desc",
   "metadata": {},
   "source": [
    "Instead of passing a single value of sparsity, a dictionary of per-layer sparsities can be provided. This allows fine-grained control over which layers get sparsified and by how much.\n",
    "\n",
    "**Example**: Apply different sparsity levels to specific layers:\n",
    "\n",
    "```python\n",
    "sparsity_levels = {\n",
    "    'conv1': 30,           # 30% sparsity on first conv\n",
    "    'layer1.0.conv1': 50,  # 50% sparsity\n",
    "    'layer2.0.conv1': 70,  # 70% sparsity (more aggressive)\n",
    "}\n",
    "sparsifier.sparsify_model(sparsity=sparsity_levels)\n",
    "```\n",
    "\n",
    "This works seamlessly with `SensitivityResult.to_layer_targets()` to apply sensitivity-aware compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g6p5iiif5mr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparsity Report:\n",
      "--------------------------------------------------------------------------------\n",
      "Layer                          Type            Params     Zeros      Sparsity  \n",
      "--------------------------------------------------------------------------------\n",
      "0                              Conv2d          432        216           50.00%\n",
      "3                              Conv2d          4,608      2,304         50.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Overall                        all             5,040      2,520         50.00%\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "def _test_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 3, padding=1),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "# Sparsify single layer at 50%\n",
    "conv = nn.Conv2d(3, 16, 3)\n",
    "sp = Sparsifier(nn.Sequential(conv), 'weight', 'local', large_final, layer_type=nn.Conv2d)\n",
    "sp.sparsify_layer(conv, 50)\n",
    "actual = (conv.weight == 0).float().mean().item() * 100\n",
    "test_close(actual, 50.0, eps=5.0)\n",
    "\n",
    "# Buffers created\n",
    "assert hasattr(conv, '_mask')\n",
    "assert hasattr(conv, '_init_weights')\n",
    "\n",
    "# Clean buffers\n",
    "sp._clean_buffers()\n",
    "assert not hasattr(conv, '_mask')\n",
    "\n",
    "# sparsify_model with float\n",
    "model = _test_model()\n",
    "sp2 = Sparsifier(model, 'weight', 'local', large_final)\n",
    "sp2.sparsify_model(30)\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        assert (m.weight == 0).any()  # some sparsification applied\n",
    "\n",
    "# Dict + context='global' raises ValueError\n",
    "model2 = _test_model()\n",
    "sp_g = Sparsifier(model2, 'weight', 'global', large_final)\n",
    "with ExceptionExpected(ValueError):\n",
    "    sp_g.sparsify_model({'0': 30, '3': 60})\n",
    "\n",
    "# Invalid sparsity range\n",
    "model3 = _test_model()\n",
    "sp3 = Sparsifier(model3, 'weight', 'local', large_final)\n",
    "conv3 = nn.Conv2d(3, 16, 3)\n",
    "with ExceptionExpected(ValueError): sp3.sparsify_layer(conv3, 150)\n",
    "with ExceptionExpected(ValueError): sp3.sparsify_layer(conv3, -10)\n",
    "\n",
    "# print_sparsity runs without error\n",
    "model4 = _test_model()\n",
    "sp4 = Sparsifier(model4, 'weight', 'local', large_final)\n",
    "sp4.sparsify_model(50)\n",
    "sp4.print_sparsity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ric9vypda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| slow\n",
    "from torchvision.models import resnet18\n",
    "model_lg = resnet18(weights=None)\n",
    "sp_lg = Sparsifier(model_lg, 'weight', 'local', large_final)\n",
    "sp_lg.sparsify_model(60)\n",
    "total_zeros = sum((m.weight==0).sum().item() for m in model_lg.modules() if isinstance(m, nn.Conv2d))\n",
    "total_params = sum(m.weight.numel() for m in model_lg.modules() if isinstance(m, nn.Conv2d))\n",
    "test_close(100*total_zeros/total_params, 60.0, eps=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tqfe1jmxxya",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [SparsifyCallback](sparsify_callback.html) - Apply sparsification during fastai training\n",
    "- [Criteria](../core/criteria.html) - Different importance measures for selecting what to sparsify\n",
    "- [Granularity](../core/granularity.html) - Control what gets sparsified (weights, filters, etc.)\n",
    "- [Schedules](../core/schedules.html) - Control sparsification progression during training\n",
    "- [Pruner](../prune/pruner.html) - Structured pruning that removes filters entirely"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
