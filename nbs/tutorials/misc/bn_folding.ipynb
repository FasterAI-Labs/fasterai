{
 "cells": [
  {
   "cell_type": "raw",
   "id": "72e09f0d",
   "metadata": {},
   "source": [
    "---\n",
    "description: Fold your BatchNorm layers\n",
    "output-file: tutorial.bn_folding.html\n",
    "title: BatchNorm Folding\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045575cd",
   "metadata": {},
   "source": "## Overview\n\n**BatchNorm Folding** is an optimization technique that merges batch normalization layers into preceding convolutional layers. This eliminates the batch norm computation entirely while maintaining mathematically equivalent results.\n\n### Why Fold BatchNorm?\n\n| Aspect | Before Folding | After Folding |\n|--------|----------------|---------------|\n| Layers | Conv → BN → ReLU | Conv → ReLU |\n| Parameters | Conv weights + BN params | Modified Conv weights only |\n| Inference Speed | Slower (extra ops) | Faster (no BN overhead) |\n| Accuracy | Baseline | **Identical** (mathematically equivalent) |\n\n### How It Works\n\nDuring inference, batch normalization applies a linear transformation:\n$$y = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n\nSince convolution is also linear, we can fold BN parameters into the conv weights:\n$$W_{new} = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot W_{old}$$\n$$b_{new} = \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot (b_{old} - \\mu) + \\beta$$\n\nThe result is **identical outputs** with **fewer operations**."
  },
  {
   "cell_type": "markdown",
   "id": "b6739d7d",
   "metadata": {},
   "source": "## 1. Setup and Training\n\nFirst, let's train a model with batch normalization layers."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a447d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from fastai.vision.all import *\n",
    "from fasterai.misc.all import *\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c101a",
   "metadata": {},
   "source": "### Load Data"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outside-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef89f1",
   "metadata": {},
   "source": "### Train the Model"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "received-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.604740</td>\n",
       "      <td>0.685939</td>\n",
       "      <td>0.685386</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.565022</td>\n",
       "      <td>0.724329</td>\n",
       "      <td>0.694858</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.512418</td>\n",
       "      <td>0.516759</td>\n",
       "      <td>0.736807</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.445161</td>\n",
       "      <td>0.466733</td>\n",
       "      <td>0.763193</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.362070</td>\n",
       "      <td>0.433802</td>\n",
       "      <td>0.792963</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea3885",
   "metadata": {},
   "source": "## 2. Fold BatchNorm Layers\n\nUse `BN_Folder` to fold all batch normalization layers into their preceding convolutions:"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c977d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f5dac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BN_Folder()\n",
    "new_model = bn.fold(learn.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00056d",
   "metadata": {},
   "source": "The batch norm layers have been replaced by `Identity` layers, and the convolution weights have been modified to incorporate the batch norm transformation."
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162c8310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn1): Identity()\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): Identity()\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ef578",
   "metadata": {},
   "source": "## 3. Comparing Results\n\n### Parameter Count\n\nThe folded model has fewer parameters (BN parameters are absorbed into conv weights):"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d14980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11177538"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "168b97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11172738"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920c6d7",
   "metadata": {},
   "source": "### Inference Speed\n\nThe folded model is faster because batch norm operations are eliminated:"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87db3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2efb5cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19 ms ± 4.31 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "learn.model(x[0][None].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e669c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 μs ± 1.79 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "new_model(x[0][None].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c188966",
   "metadata": {},
   "source": "### Accuracy Verification\n\nMost importantly, the folded model produces **identical results** to the original:"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd9be029",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_learn = Learner(dls, new_model, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ecfa79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.4338044822216034, 0.792963445186615]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dv3i436ka38",
   "source": "## Summary\n\n| Metric | Original | Folded | Improvement |\n|--------|----------|--------|-------------|\n| Parameters | 11,177,538 | 11,172,738 | ~5K fewer |\n| Inference (single image) | 1.19 ms | 0.77 ms | **~35% faster** |\n| Accuracy | Baseline | **Identical** | No change |\n\n### When to Use BN Folding\n\n| Scenario | Recommendation |\n|----------|----------------|\n| **Inference/deployment** | ✅ Always fold - free speedup |\n| **Before quantization** | ✅ Fold first - cleaner quantization |\n| **During training** | ❌ Don't fold - BN helps training |\n| **Models without BN** | N/A - Nothing to fold |\n\n---\n\n## See Also\n\n- [Quantize Callback](../quantize/quantize_callback.html) - Apply quantization after folding for maximum compression\n- [ONNX Exporter](../../export/onnx_exporter.html) - Export folded models to ONNX for deployment\n- [Pruner](../../prune/pruner.html) - Combine with pruning for smaller, faster models",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}