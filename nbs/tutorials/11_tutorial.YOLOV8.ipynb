{
 "cells": [
  {
   "cell_type": "raw",
   "id": "10c41965-a076-4a77-a864-5230d7b79d70",
   "metadata": {},
   "source": [
    "---\n",
    "description: YOLOV8\n",
    "output-file: tutorial.YOLOV8.html\n",
    "title: YOLOV8\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4991349-58bc-46bb-9322-793e9d3d927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO, __version__\n",
    "from ultralytics.nn.modules import Detect, C2f, Conv, Bottleneck\n",
    "from ultralytics.nn.tasks import attempt_load_one_weight\n",
    "from ultralytics.yolo.engine.model import TASK_MAP\n",
    "from ultralytics.yolo.engine.trainer import BaseTrainer\n",
    "from ultralytics.yolo.utils import yaml_load, LOGGER, RANK, DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS\n",
    "from ultralytics.yolo.utils.checks import check_yaml\n",
    "from ultralytics.yolo.utils.torch_utils import initialize_weights, de_parallel\n",
    "\n",
    "import torch_pruning as tp\n",
    "from fasterai.prune.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastcore.basics import store_attr, listify, true\n",
    "from torch_pruning.pruner import function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fea81-6877-43a7-a383-e96a566f6364",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb742075-3158-48ba-836a-8bfe51388b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "def infer_shortcut(bottleneck):\n",
    "    c1 = bottleneck.cv1.conv.in_channels\n",
    "    c2 = bottleneck.cv2.conv.out_channels\n",
    "    return c1 == c2 and hasattr(bottleneck, 'add') and bottleneck.add\n",
    "\n",
    "\n",
    "class C2f_v2(nn.Module):\n",
    "    # CSP Bottleneck with 2 convolutions\n",
    "    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
    "        super().__init__()\n",
    "        self.c = int(c2 * e)  # hidden channels\n",
    "        self.cv0 = Conv(c1, self.c, 1, 1)\n",
    "        self.cv1 = Conv(c1, self.c, 1, 1)\n",
    "        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)\n",
    "        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # y = list(self.cv1(x).chunk(2, 1))\n",
    "        y = [self.cv0(x), self.cv1(x)]\n",
    "        y.extend(m(y[-1]) for m in self.m)\n",
    "        return self.cv2(torch.cat(y, 1))\n",
    "\n",
    "\n",
    "def transfer_weights(c2f, c2f_v2):\n",
    "    c2f_v2.cv2 = c2f.cv2\n",
    "    c2f_v2.m = c2f.m\n",
    "\n",
    "    state_dict = c2f.state_dict()\n",
    "    state_dict_v2 = c2f_v2.state_dict()\n",
    "\n",
    "    # Transfer cv1 weights from C2f to cv0 and cv1 in C2f_v2\n",
    "    old_weight = state_dict['cv1.conv.weight']\n",
    "    half_channels = old_weight.shape[0] // 2\n",
    "    state_dict_v2['cv0.conv.weight'] = old_weight[:half_channels]\n",
    "    state_dict_v2['cv1.conv.weight'] = old_weight[half_channels:]\n",
    "\n",
    "    # Transfer cv1 batchnorm weights and buffers from C2f to cv0 and cv1 in C2f_v2\n",
    "    for bn_key in ['weight', 'bias', 'running_mean', 'running_var']:\n",
    "        old_bn = state_dict[f'cv1.bn.{bn_key}']\n",
    "        state_dict_v2[f'cv0.bn.{bn_key}'] = old_bn[:half_channels]\n",
    "        state_dict_v2[f'cv1.bn.{bn_key}'] = old_bn[half_channels:]\n",
    "\n",
    "    # Transfer remaining weights and buffers\n",
    "    for key in state_dict:\n",
    "        if not key.startswith('cv1.'):\n",
    "            state_dict_v2[key] = state_dict[key]\n",
    "\n",
    "    # Transfer all non-method attributes\n",
    "    for attr_name in dir(c2f):\n",
    "        attr_value = getattr(c2f, attr_name)\n",
    "        if not callable(attr_value) and '_' not in attr_name:\n",
    "            setattr(c2f_v2, attr_name, attr_value)\n",
    "\n",
    "    c2f_v2.load_state_dict(state_dict_v2)\n",
    "\n",
    "\n",
    "def replace_c2f_with_c2f_v2(module):\n",
    "    for name, child_module in module.named_children():\n",
    "        if isinstance(child_module, C2f):\n",
    "            # Replace C2f with C2f_v2 while preserving its parameters\n",
    "            shortcut = infer_shortcut(child_module.m[0])\n",
    "            c2f_v2 = C2f_v2(child_module.cv1.conv.in_channels, child_module.cv2.conv.out_channels,\n",
    "                            n=len(child_module.m), shortcut=shortcut,\n",
    "                            g=child_module.m[0].cv2.conv.groups,\n",
    "                            e=child_module.c / child_module.cv2.conv.out_channels)\n",
    "            transfer_weights(child_module, c2f_v2)\n",
    "            setattr(module, name, c2f_v2)\n",
    "        else:\n",
    "            replace_c2f_with_c2f_v2(child_module)\n",
    "\n",
    "\n",
    "def save_model_v2(self: BaseTrainer):\n",
    "    \"\"\"\n",
    "    Disabled half precision saving. originated from ultralytics/yolo/engine/trainer.py\n",
    "    \"\"\"\n",
    "    ckpt = {\n",
    "        'epoch': self.epoch,\n",
    "        'best_fitness': self.best_fitness,\n",
    "        'model': deepcopy(de_parallel(self.model)),\n",
    "        'ema': deepcopy(self.ema.ema),\n",
    "        'updates': self.ema.updates,\n",
    "        'optimizer': self.optimizer.state_dict(),\n",
    "        'train_args': vars(self.args),  # save as dict\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'version': __version__}\n",
    "\n",
    "    # Save last, best and delete\n",
    "    torch.save(ckpt, self.last)\n",
    "    if self.best_fitness == self.fitness:\n",
    "        torch.save(ckpt, self.best)\n",
    "    if (self.epoch > 0) and (self.save_period > 0) and (self.epoch % self.save_period == 0):\n",
    "        torch.save(ckpt, self.wdir / f'epoch{self.epoch}.pt')\n",
    "    del ckpt\n",
    "\n",
    "def final_eval_v2(self: BaseTrainer):\n",
    "    \"\"\"\n",
    "    originated from ultralytics/yolo/engine/trainer.py\n",
    "    \"\"\"\n",
    "    for f in self.last, self.best:\n",
    "        if f.exists():\n",
    "            strip_optimizer_v2(f)  # strip optimizers\n",
    "            if f is self.best:\n",
    "                LOGGER.info(f'\\nValidating {f}...')\n",
    "                self.metrics = self.validator(model=f)\n",
    "                self.metrics.pop('fitness', None)\n",
    "                self.run_callbacks('on_fit_epoch_end')\n",
    "\n",
    "def strip_optimizer_v2(f: Union[str, Path] = 'best.pt', s: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Disabled half precision saving. originated from ultralytics/yolo/utils/torch_utils.py\n",
    "    \"\"\"\n",
    "    x = torch.load(f, map_location=torch.device('cpu'))\n",
    "    args = {**DEFAULT_CFG_DICT, **x['train_args']}  # combine model args with default args, preferring model args\n",
    "    if x.get('ema'):\n",
    "        x['model'] = x['ema']  # replace model with ema\n",
    "    for k in 'optimizer', 'ema', 'updates':  # keys\n",
    "        x[k] = None\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    x['train_args'] = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # strip non-default keys\n",
    "    # x['model'].args = x['train_args']\n",
    "    torch.save(x, s or f)\n",
    "    mb = os.path.getsize(s or f) / 1E6  # filesize\n",
    "    LOGGER.info(f\"Optimizer stripped from {f},{f' saved as {s},' if s else ''} {mb:.1f}MB\")\n",
    "\n",
    "\n",
    "def train_v2(self: YOLO, pruning=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Disabled loading new model when pruning flag is set. originated from ultralytics/yolo/engine/model.py\n",
    "    \"\"\"\n",
    "\n",
    "    self._check_is_pytorch_model()\n",
    "    if self.session:  # Ultralytics HUB session\n",
    "        if any(kwargs):\n",
    "            LOGGER.warning('WARNING ⚠️ using HUB training arguments, ignoring local training arguments.')\n",
    "        kwargs = self.session.train_args\n",
    "    overrides = self.overrides.copy()\n",
    "    overrides.update(kwargs)\n",
    "    if kwargs.get('cfg'):\n",
    "        LOGGER.info(f\"cfg file passed. Overriding default params with {kwargs['cfg']}.\")\n",
    "        overrides = yaml_load(check_yaml(kwargs['cfg']))\n",
    "    overrides['mode'] = 'train'\n",
    "    if not overrides.get('data'):\n",
    "        raise AttributeError(\"Dataset required but missing, i.e. pass 'data=coco128.yaml'\")\n",
    "    if overrides.get('resume'):\n",
    "        overrides['resume'] = self.ckpt_path\n",
    "\n",
    "    self.task = overrides.get('task') or self.task\n",
    "    self.trainer = TASK_MAP[self.task][1](overrides=overrides, _callbacks=self.callbacks)\n",
    "\n",
    "    if not pruning:\n",
    "        if not overrides.get('resume'):  # manually set model only if not resuming\n",
    "            self.trainer.model = self.trainer.get_model(weights=self.model if self.ckpt else None, cfg=self.model.yaml)\n",
    "            self.model = self.trainer.model\n",
    "\n",
    "    else:\n",
    "        # pruning mode\n",
    "        self.trainer.pruning = True\n",
    "        self.trainer.model = self.model\n",
    "\n",
    "        # replace some functions to disable half precision saving\n",
    "        self.trainer.save_model = save_model_v2.__get__(self.trainer)\n",
    "        self.trainer.final_eval = final_eval_v2.__get__(self.trainer)\n",
    "\n",
    "    self.trainer.hub_session = self.session  # attach optional HUB session\n",
    "    self.trainer.train()\n",
    "    # Update model and cfg after training\n",
    "    if RANK in (-1, 0):\n",
    "        self.model, _ = attempt_load_one_weight(str(self.trainer.best))\n",
    "        self.overrides = self.model.args\n",
    "        self.metrics = getattr(self.trainer.validator, 'metrics', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03d638-01ae-4d35-88c1-6215e5211cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "def prune(args):\n",
    "    # load trained yolov8 model\n",
    "    model = YOLO(args.model)\n",
    "    model.__setattr__(\"train_v2\", train_v2.__get__(model))\n",
    "    pruning_cfg = yaml_load(check_yaml(args.cfg))\n",
    "    batch_size = pruning_cfg['batch']\n",
    "    \n",
    "    pruning_cfg['data'] = \"coco128.yaml\"\n",
    "    pruning_cfg['epochs'] = 10\n",
    "    pruning_cfg['verbose'] = False\n",
    "    \n",
    "    model.model.train()\n",
    "    replace_c2f_with_c2f_v2(model.model)\n",
    "    initialize_weights(model.model)\n",
    "    \n",
    "    validation_model = deepcopy(model)\n",
    "    metric = validation_model.val(**pruning_cfg)\n",
    "    init_map = metric.box.map\n",
    "    example_inputs = torch.randn(1, 3, pruning_cfg[\"imgsz\"], pruning_cfg[\"imgsz\"]).to(model.device)\n",
    "    \n",
    "    base_macs, base_nparams = tp.utils.count_ops_and_params(model.model, example_inputs)\n",
    "    print(f\"Before Pruning: MACs={base_macs / 1e9: .5f} G, #Params={base_nparams / 1e6: .5f} M, mAP={init_map: .5f}\")\n",
    "    \n",
    "    for name, param in model.model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    model.train_v2(pruning=True, **pruning_cfg)\n",
    "\n",
    "    pruning_cfg['epochs'] = 10\n",
    "    \n",
    "    macs_list, nparams_list, map_list, pruned_map_list = [], [], [], []\n",
    "    base_macs, base_nparams = tp.utils.count_ops_and_params(model.model, example_inputs)\n",
    "    \n",
    "    pruning_cfg['name'] = f\"baseline_val\"\n",
    "    pruning_cfg['batch'] = 1\n",
    "    \n",
    "    \n",
    "    validation_model.model.model = deepcopy(model.model.model)\n",
    "    metric = validation_model.val(**pruning_cfg)\n",
    "    init_map = metric.box.map\n",
    "    macs_list.append(base_macs)\n",
    "    nparams_list.append(100)\n",
    "    map_list.append(init_map)\n",
    "    pruned_map_list.append(init_map)\n",
    "    print(f\"Before Pruning: MACs={base_macs / 1e9: .5f} G, #Params={base_nparams / 1e6: .5f} M, mAP={init_map: .5f}\")\n",
    "    \n",
    "    for name, param in model.model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "        ignored_layers = []\n",
    "        unwrapped_parameters = []\n",
    "        for m in model.model.modules():\n",
    "            if isinstance(m, (Detect,)):\n",
    "                ignored_layers.append(m)\n",
    "    \n",
    "    pruner = Pruner(model.model, 'local', large_final, ignored_layers=ignored_layers)\n",
    "    print(model.model.model[0].conv)\n",
    "\n",
    "    for i in range(args.iterative_steps):\n",
    "\n",
    "        pruning_ratio = args.sched(args.target_prune_rate*100, i/args.iterative_steps)\n",
    "\n",
    "        pruner.prune_model(pruning_ratio[0])\n",
    "        print(pruning_ratio[0])\n",
    "\n",
    "        print('After Pruning')\n",
    "        print('Model', model.model.model[0].conv)\n",
    "        print('Pruner', pruner.model.model[0].conv)\n",
    "\n",
    "        pruning_cfg['name'] = f\"step_{i}_pre_val\"\n",
    "        pruning_cfg['batch'] = 1\n",
    "        validation_model.model.model = deepcopy(pruner.model.model)\n",
    "        metric = validation_model.val(**pruning_cfg)\n",
    "        pruned_map = metric.box.map\n",
    "        pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(pruner.model.to(default_device()), example_inputs.to(default_device()))\n",
    "        \n",
    "        print('After post-pruning Validation')\n",
    "        print('Model', model.model.model[0].conv)\n",
    "        print('Pruner', pruner.model.model[0].conv)\n",
    "        \n",
    "        \n",
    "        current_speed_up = float(macs_list[0]) / pruned_macs\n",
    "        print(f\"After pruning iter {i + 1}: MACs={pruned_macs / 1e9} G, #Params={pruned_nparams / 1e6} M, \"\n",
    "              f\"mAP={pruned_map}, speed up={current_speed_up}\")\n",
    "\n",
    "        \n",
    "        # fine-tuning\n",
    "        for name, param in model.model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "        pruning_cfg['name'] = f\"step_{i}_finetune\"\n",
    "        pruning_cfg['batch'] = batch_size  # restore batch size\n",
    "        model.model = pruner.model\n",
    "        model.train_v2(pruning=True, **pruning_cfg)\n",
    "\n",
    "        print('After fine-tuning')\n",
    "        print('Model', model.model.model[0].conv)\n",
    "        print('Pruner', pruner.model.model[0].conv)\n",
    "        \n",
    "        \n",
    "        # post fine-tuning validation\n",
    "        pruning_cfg['name'] = f\"step_{i}_post_val\"\n",
    "        pruning_cfg['batch'] = 1\n",
    "        validation_model = YOLO(model.trainer.best)\n",
    "        validation_model.model = deepcopy(model.model)\n",
    "        metric = validation_model.val( **pruning_cfg)\n",
    "        current_map = metric.box.map\n",
    "        print(f\"After fine tuning mAP={current_map}\")\n",
    "\n",
    "        print('After post fine-tuning validation')\n",
    "        print('Model', model.model.model[0].conv)\n",
    "        print('Pruner', pruner.model.model[0].conv)\n",
    "    \n",
    "\n",
    "        macs_list.append(pruned_macs)\n",
    "        nparams_list.append(pruned_nparams / base_nparams * 100)\n",
    "        pruned_map_list.append(pruned_map)\n",
    "        map_list.append(current_map)\n",
    "\n",
    "        if init_map - current_map > args.max_map_drop:\n",
    "            print(\"Pruning early stop\")\n",
    "            break\n",
    "\n",
    "\n",
    "    model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098fc1b4-57d5-4a31-a557-920688c094a7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b08319-6d07-4e42-9528-3e09a0558e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.731      0.768      0.828      0.659\n",
      "Speed: 0.1ms preprocess, 7.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val59\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train49\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.65869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/train49/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train49\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      14.4G     0.8537     0.7447      1.082        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.776      0.741      0.832      0.667\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      12.8G     0.8612     0.7059      1.079        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.859       0.75      0.861      0.697\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      12.7G     0.8249     0.6306      1.054        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.882      0.753      0.862      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      12.8G     0.7998     0.5746      1.047         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.882      0.799       0.87      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      13.1G     0.8028     0.5566      1.034         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.88      0.804      0.876      0.728\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      12.8G     0.8042     0.5415      1.047        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.896      0.833      0.901      0.741\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      12.8G     0.7493     0.5095      1.003         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.906      0.827      0.902      0.746\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      12.8G     0.7589     0.5373      1.012        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.91      0.828      0.903      0.749\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      12.8G     0.7234     0.4783     0.9947        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.912      0.832      0.906      0.754\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      12.7G     0.7445     0.4764     0.9944        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.905      0.839      0.905      0.754\n",
      "\n",
      "10 epochs completed in 0.027 hours.\n",
      "Optimizer stripped from runs/detect/train49/weights/last.pt, 175.3MB\n",
      "Optimizer stripped from runs/detect/train49/weights/best.pt, 175.3MB\n",
      "\n",
      "Validating runs/detect/train49/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.904      0.841      0.905      0.755\n",
      "Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train49\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.917      0.823      0.901      0.754\n",
      "Speed: 0.2ms preprocess, 10.8ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/baseline_val184\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Pruning: MACs= 82.72641 G, #Params= 43.69152 M, mAP= 0.75438\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27046189978777607\n",
      "After Pruning\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 43325836 parameters, 74176 gradients, 163.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.883      0.849      0.903      0.743\n",
      "Speed: 0.2ms preprocess, 12.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_0_pre_val131\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_0_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_0_finetune103\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 1: MACs=81.8125668 G, #Params=43.348966 M, mAP=0.7428735001565969, speed up=1.0111699172357467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_0_finetune103/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_0_finetune103\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      13.6G     0.7161     0.4777     0.9953        122        640: 100%|██████████| 8/8 [00:03\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.92      0.841      0.907       0.75\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      13.3G     0.6503     0.4152     0.9541        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.904      0.851       0.91      0.765\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      13.2G     0.6809      0.434     0.9746        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.902      0.854      0.907      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      13.2G     0.6464     0.4095     0.9681         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.919      0.853      0.913      0.773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      13.2G     0.6799     0.4357     0.9637         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.916      0.856      0.918      0.779\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      13.2G     0.6787     0.4261     0.9708        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.924      0.851      0.923       0.78\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      13.2G     0.6758     0.4286      0.957         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.91      0.852       0.92      0.785\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      13.2G     0.6818     0.4492     0.9705        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.899      0.856      0.921      0.786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      13.2G     0.6594      0.429     0.9635        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.894      0.871      0.925      0.794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      13.2G     0.6705     0.4277     0.9541        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.899       0.87      0.927      0.793\n",
      "\n",
      "10 epochs completed in 0.038 hours.\n",
      "Optimizer stripped from runs/detect/step_0_finetune103/weights/last.pt, 173.9MB\n",
      "Optimizer stripped from runs/detect/step_0_finetune103/weights/best.pt, 173.9MB\n",
      "\n",
      "Validating runs/detect/step_0_finetune103/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 43325836 parameters, 0 gradients, 163.3 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.895       0.87      0.925      0.793\n",
      "Speed: 0.1ms preprocess, 5.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_0_finetune103\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 43325836 parameters, 0 gradients, 163.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.897      0.863      0.922      0.787\n",
      "Speed: 0.2ms preprocess, 12.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_0_post_val75\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.7869655326484724\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "0.5179586515491672\n",
      "After Pruning\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 43081939 parameters, 74176 gradients, 162.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.913      0.872      0.929      0.788\n",
      "Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_1_pre_val66\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_1_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_1_finetune62\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 2: MACs=81.5020432 G, #Params=43.105009 M, mAP=0.7879549975477981, speed up=1.0150224847369225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_1_finetune62/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_1_finetune62\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      13.3G     0.5906     0.3832     0.9224        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.903      0.868      0.925      0.797\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      13.3G     0.5313     0.3408     0.9001        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.908       0.87      0.925      0.796\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      13.2G     0.5791     0.3608     0.9246        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.902      0.871      0.927      0.799\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      13.3G     0.5612     0.3602     0.9298         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.914      0.867      0.929      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      13.2G     0.5719     0.3787     0.9132         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.906      0.875       0.93      0.792\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      13.2G     0.5878     0.3844     0.9333        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.935      0.859       0.93      0.796\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      13.3G     0.5939     0.3776     0.9134         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.928       0.86      0.928      0.799\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      13.2G     0.6093     0.3903     0.9311        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.862      0.933      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      13.3G     0.6003      0.386     0.9318        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.92      0.877      0.935      0.804\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      13.2G     0.6393      0.408     0.9322        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.911      0.888      0.937      0.806\n",
      "\n",
      "10 epochs completed in 0.025 hours.\n",
      "Optimizer stripped from runs/detect/step_1_finetune62/weights/last.pt, 173.0MB\n",
      "Optimizer stripped from runs/detect/step_1_finetune62/weights/best.pt, 173.0MB\n",
      "\n",
      "Validating runs/detect/step_1_finetune62/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.911      0.888      0.937      0.806\n",
      "Speed: 0.1ms preprocess, 5.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_1_finetune62\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 43081939 parameters, 0 gradients, 162.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.907      0.888      0.937      0.804\n",
      "Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_1_post_val48\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.804165683147925\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "0.9769531739708688\n",
      "After Pruning\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 42712366 parameters, 74176 gradients, 161.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.923      0.871      0.933      0.794\n",
      "Speed: 0.2ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_2_pre_val50\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_2_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_2_finetune48\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 3: MACs=80.7933916 G, #Params=42.735334 M, mAP=0.7940590327289188, speed up=1.0239254072854147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_2_finetune48/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_2_finetune48\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      13.4G      0.548     0.3528     0.9023        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.923      0.871      0.935      0.801\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      13.3G     0.4746     0.3015     0.8763        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.93      0.877       0.94      0.806\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      13.2G     0.5379     0.3445     0.9065        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.871      0.942      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      13.3G     0.5157     0.3339     0.9019         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.93      0.877      0.938      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      13.3G     0.5169     0.3404     0.8804         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.921      0.883      0.939       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      13.3G     0.5339     0.3559     0.9031        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.929      0.878       0.94      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      13.3G     0.5597     0.3561     0.8945         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.939      0.876      0.941      0.813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      13.3G     0.5733     0.3857     0.9149        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.936      0.879      0.941      0.818\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      13.3G     0.5769     0.3717     0.9222        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.941      0.882      0.941      0.821\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      13.2G     0.6167     0.3871     0.9151        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.939      0.882      0.941      0.821\n",
      "\n",
      "10 epochs completed in 0.024 hours.\n",
      "Optimizer stripped from runs/detect/step_2_finetune48/weights/last.pt, 171.5MB\n",
      "Optimizer stripped from runs/detect/step_2_finetune48/weights/best.pt, 171.5MB\n",
      "\n",
      "Validating runs/detect/step_2_finetune48/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.882      0.942      0.822\n",
      "Speed: 0.1ms preprocess, 4.9ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_2_finetune48\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 42712366 parameters, 0 gradients, 161.3 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.92      0.888      0.943      0.813\n",
      "Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_2_post_val42\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8133375576554807\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "1.7924759478681729\n",
      "After Pruning\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 42094706 parameters, 74176 gradients, 158.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.864      0.936      0.804\n",
      "Speed: 0.1ms preprocess, 12.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_3_pre_val36\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_3_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_3_finetune36\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 63, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 4: MACs=79.5541908 G, #Params=42.117503 M, mAP=0.8043294271876973, speed up=1.0398749024796818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_3_finetune36/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_3_finetune36\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      13.9G     0.5395     0.3534      0.897        122        640: 100%|██████████| 8/8 [00:42\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.875      0.937      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      13.2G     0.4523     0.2943     0.8601        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.875      0.942      0.816\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      13.2G     0.5011     0.3242      0.885        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.879       0.94      0.816\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      13.2G     0.4896     0.3239      0.881         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.945      0.873      0.941      0.818\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      13.2G     0.4877     0.3266     0.8653         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.924      0.883      0.939      0.819\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      13.2G     0.5175      0.341     0.8913        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.879      0.943      0.824\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      13.2G     0.5484     0.3518     0.8896         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.882      0.944      0.825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      13.2G     0.5657     0.3636      0.901        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.947      0.877      0.944      0.827\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      13.2G     0.5557     0.3553      0.908        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.888      0.945      0.827\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      13.2G     0.6072      0.381     0.9066        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.938      0.885      0.945      0.827\n",
      "\n",
      "10 epochs completed in 0.037 hours.\n",
      "Optimizer stripped from runs/detect/step_3_finetune36/weights/last.pt, 169.0MB\n",
      "Optimizer stripped from runs/detect/step_3_finetune36/weights/best.pt, 169.0MB\n",
      "\n",
      "Validating runs/detect/step_3_finetune36/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.947      0.877      0.944      0.827\n",
      "Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_3_finetune36\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 42094706 parameters, 0 gradients, 158.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.945      0.875      0.943      0.824\n",
      "Speed: 0.2ms preprocess, 12.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_3_post_val34\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8242263974664863\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "3.1368842425083825\n",
      "After Pruning\n",
      "Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 40919781 parameters, 74176 gradients, 154.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.913      0.876      0.935      0.792\n",
      "Speed: 0.2ms preprocess, 12.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_4_pre_val32\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_4_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_4_finetune32\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 5: MACs=77.3600192 G, #Params=40.942254 M, mAP=0.7920074671210469, speed up=1.0693690003634333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_4_finetune32/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_4_finetune32\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      13.8G      0.573     0.3665     0.9011        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.926      0.881       0.94      0.803\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      13.4G     0.4596     0.2974     0.8554        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.879      0.943      0.815\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      13.2G     0.5037     0.3324     0.8775        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.875       0.94      0.815\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      13.3G     0.4863      0.313     0.8774         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.954      0.872      0.943      0.813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      13.2G     0.4905     0.3254     0.8586         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942       0.87      0.939      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      13.2G     0.5016     0.3312     0.8863        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.914      0.888      0.939      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      13.2G     0.5446     0.3534     0.8808         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.937      0.874      0.942      0.818\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      13.2G      0.554     0.3697     0.8957        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.927      0.885      0.942      0.821\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      13.2G     0.5756      0.359     0.9062        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.889      0.945      0.825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      13.2G     0.6089     0.3807     0.9001        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.884      0.945      0.826\n",
      "\n",
      "10 epochs completed in 0.035 hours.\n",
      "Optimizer stripped from runs/detect/step_4_finetune32/weights/last.pt, 164.3MB\n",
      "Optimizer stripped from runs/detect/step_4_finetune32/weights/best.pt, 164.3MB\n",
      "\n",
      "Validating runs/detect/step_4_finetune32/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.884      0.945      0.827\n",
      "Speed: 0.1ms preprocess, 4.9ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_4_finetune32\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 40919781 parameters, 0 gradients, 154.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.929      0.891      0.944       0.82\n",
      "Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_4_post_val31\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.820206153122929\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "5.101267981852869\n",
      "After Pruning\n",
      "Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 39455305 parameters, 74176 gradients, 149.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.916      0.864      0.929      0.789\n",
      "Speed: 0.2ms preprocess, 13.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_5_pre_val31\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_5_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_5_finetune31\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 6: MACs=74.8418608 G, #Params=39.477376 M, mAP=0.7891253582912163, speed up=1.1053494062777232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_5_finetune31/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_5_finetune31\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      13.5G     0.5773     0.3687     0.8973        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.904      0.881      0.935      0.801\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10        13G     0.4697     0.3058     0.8551        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.906      0.893      0.941      0.807\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      12.9G     0.5138     0.3263     0.8829        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.92      0.889      0.942      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10        13G     0.4938     0.3293     0.8823         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.928      0.883      0.944      0.813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      12.9G     0.5047     0.3398     0.8637         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.934       0.88      0.943      0.819\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10        13G     0.5144     0.3483      0.886        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.869       0.94      0.815\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10        13G      0.539     0.3521      0.879         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.943      0.865      0.941      0.814\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10        13G     0.5493     0.3683      0.895        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.934      0.875      0.939      0.817\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10        13G      0.581     0.3607     0.9096        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.939      0.876       0.94       0.82\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      12.9G     0.6293     0.3874     0.9156        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.938      0.886      0.946      0.822\n",
      "\n",
      "10 epochs completed in 0.035 hours.\n",
      "Optimizer stripped from runs/detect/step_5_finetune31/weights/last.pt, 158.5MB\n",
      "Optimizer stripped from runs/detect/step_5_finetune31/weights/best.pt, 158.5MB\n",
      "\n",
      "Validating runs/detect/step_5_finetune31/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.938      0.886      0.946      0.822\n",
      "Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_5_finetune31\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 39455305 parameters, 0 gradients, 149.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.928      0.891      0.943       0.82\n",
      "Speed: 0.2ms preprocess, 12.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_5_post_val31\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8197008467567249\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "7.518590641324997\n",
      "After Pruning\n",
      "Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 37708749 parameters, 74176 gradients, 143.2 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.904      0.848      0.923       0.76\n",
      "Speed: 0.2ms preprocess, 10.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_6_pre_val28\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_6_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_6_finetune28\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 7: MACs=71.732976 G, #Params=37.730325 M, mAP=0.7604747253578685, speed up=1.1532549046898597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_6_finetune28/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_6_finetune28\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      13.3G     0.6267     0.3973     0.9214        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.909      0.859      0.932      0.782\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      12.8G     0.5114     0.3263     0.8662        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.874      0.939      0.803\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      12.7G     0.5466     0.3434     0.8876        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.931      0.875      0.939      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      12.7G     0.5238     0.3378     0.8878         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.939      0.874      0.939       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      12.7G     0.5198     0.3522     0.8708         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.947       0.87       0.94      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      12.8G     0.5276      0.352     0.8876        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.933      0.879      0.939       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      12.8G     0.5516     0.3594     0.8792         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.931      0.882      0.943       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      12.8G     0.5795     0.3736     0.9109        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.933      0.893      0.949      0.817\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      12.8G      0.597     0.3801     0.9136        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932       0.89      0.948      0.815\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      12.7G     0.6406     0.3889     0.9059        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.928      0.899      0.948      0.816\n",
      "\n",
      "10 epochs completed in 0.034 hours.\n",
      "Optimizer stripped from runs/detect/step_6_finetune28/weights/last.pt, 151.5MB\n",
      "Optimizer stripped from runs/detect/step_6_finetune28/weights/best.pt, 151.5MB\n",
      "\n",
      "Validating runs/detect/step_6_finetune28/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.933      0.889      0.948      0.817\n",
      "Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_6_finetune28\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 37708749 parameters, 0 gradients, 143.2 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.937      0.878      0.946      0.808\n",
      "Speed: 0.1ms preprocess, 10.8ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_6_post_val27\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8082043641470185\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "9.935913300797125\n",
      "After Pruning\n",
      "Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 35995675 parameters, 74176 gradients, 136.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.838      0.847      0.905      0.744\n",
      "Speed: 0.2ms preprocess, 12.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_7_pre_val25\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_7_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_7_finetune25\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 59, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 8: MACs=68.4860368 G, #Params=36.016747 M, mAP=0.7439133908787243, speed up=1.207930992438447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_7_finetune25/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_7_finetune25\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10        13G     0.6576     0.4219     0.9433        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.884      0.852      0.921      0.764\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      13.2G     0.5285     0.3538     0.8714        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.921      0.864      0.937      0.782\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      12.4G     0.5672     0.3781     0.8972        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.921       0.87       0.94      0.791\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      12.4G     0.5324     0.3593     0.8898         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.921      0.869      0.937      0.796\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      12.1G     0.5564      0.395     0.8841         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.926      0.888      0.941      0.799\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      12.3G     0.5555     0.3674     0.9059        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.92      0.891      0.942      0.797\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      12.3G     0.5972     0.3946     0.9014         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.909      0.897      0.942      0.797\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      12.3G     0.6033     0.4048     0.9106        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.919      0.892      0.943      0.805\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      12.3G     0.6098     0.3878     0.9253        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.933      0.884      0.944      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      12.3G     0.6518     0.4124     0.9181        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.934      0.887      0.945      0.811\n",
      "\n",
      "10 epochs completed in 0.036 hours.\n",
      "Optimizer stripped from runs/detect/step_7_finetune25/weights/last.pt, 144.6MB\n",
      "Optimizer stripped from runs/detect/step_7_finetune25/weights/best.pt, 144.6MB\n",
      "\n",
      "Validating runs/detect/step_7_finetune25/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.934      0.887      0.945       0.81\n",
      "Speed: 0.1ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_7_finetune25\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 35995675 parameters, 0 gradients, 136.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.934      0.877      0.942      0.805\n",
      "Speed: 0.1ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_7_post_val25\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8047311680941978\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "11.900297040141613\n",
      "After Pruning\n",
      "Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 34583399 parameters, 74176 gradients, 131.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.861      0.846      0.915      0.747\n",
      "Speed: 0.2ms preprocess, 12.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_8_pre_val24\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_8_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_8_finetune24\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 9: MACs=65.8289424 G, #Params=34.604045 M, mAP=0.746892685800743, speed up=1.2566874597092115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_8_finetune24/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_8_finetune24\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      12.7G     0.6527     0.4186     0.9399        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.878       0.86      0.925      0.769\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      12.9G     0.5123     0.3376     0.8642        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.902      0.884      0.932       0.78\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      12.2G     0.5575     0.3672     0.8903        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.918      0.887      0.935      0.784\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      12.2G     0.5313     0.3422     0.8975         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.913      0.897      0.936      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      12.2G      0.543     0.3699      0.874         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.922      0.891      0.939      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      12.2G     0.5544     0.3693        0.9        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.936      0.885      0.938      0.798\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      12.2G     0.5915     0.3854     0.8924         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.883      0.939      0.801\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10        12G     0.6192     0.4081     0.9123        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.882       0.94      0.803\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      12.2G     0.6259     0.4123     0.9284        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.947       0.88      0.941      0.806\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      12.1G     0.6654     0.4213     0.9262        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.886       0.94      0.808\n",
      "\n",
      "10 epochs completed in 0.037 hours.\n",
      "Optimizer stripped from runs/detect/step_8_finetune24/weights/last.pt, 139.0MB\n",
      "Optimizer stripped from runs/detect/step_8_finetune24/weights/best.pt, 139.0MB\n",
      "\n",
      "Validating runs/detect/step_8_finetune24/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.948      0.882       0.94      0.808\n",
      "Speed: 0.1ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_8_finetune24\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 34583399 parameters, 0 gradients, 131.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.939      0.885       0.94      0.804\n",
      "Speed: 0.1ms preprocess, 12.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_8_post_val24\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8042272329558376\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "13.24470533478182\n",
      "After Pruning\n",
      "Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 33747610 parameters, 74176 gradients, 128.5 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.919      0.872      0.923      0.774\n",
      "Speed: 0.2ms preprocess, 13.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_9_pre_val23\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_9_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_9_finetune23\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 10: MACs=64.3900056 G, #Params=33.768007 M, mAP=0.77353892505729, speed up=1.2847709148203583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_9_finetune23/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_9_finetune23\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      12.6G     0.6022     0.3899     0.9207        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.921      0.881       0.93      0.784\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      12.1G     0.4755     0.3118      0.851        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.941      0.883      0.933      0.794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10        12G     0.5226     0.3441     0.8847        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.936      0.884      0.936      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      11.8G     0.5197     0.3324     0.8815         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.935      0.883      0.933      0.792\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10        12G     0.5239      0.353     0.8671         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.936      0.882      0.934      0.792\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10        12G     0.5413     0.3589     0.8919        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.93      0.877      0.934      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      12.1G     0.5753     0.3723     0.8863         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.873      0.933      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      12.1G     0.6104     0.3991     0.9113        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.95      0.868      0.931        0.8\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      12.1G     0.6059      0.395     0.9182        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.943      0.873      0.932      0.805\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10        12G     0.6558     0.4098     0.9218        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.945      0.871      0.933      0.805\n",
      "\n",
      "10 epochs completed in 0.033 hours.\n",
      "Optimizer stripped from runs/detect/step_9_finetune23/weights/last.pt, 135.6MB\n",
      "Optimizer stripped from runs/detect/step_9_finetune23/weights/best.pt, 135.6MB\n",
      "\n",
      "Validating runs/detect/step_9_finetune23/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.945      0.871      0.933      0.806\n",
      "Speed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_9_finetune23\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 33747610 parameters, 0 gradients, 128.5 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.943      0.875      0.932      0.804\n",
      "Speed: 0.2ms preprocess, 14.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_9_post_val23\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8042200149576527\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "14.060228108679125\n",
      "After Pruning\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 33209910 parameters, 74176 gradients, 126.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.933      0.855      0.928      0.782\n",
      "Speed: 0.2ms preprocess, 13.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_10_pre_val17\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_10_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_10_finetune17\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 11: MACs=63.4942128 G, #Params=33.230145 M, mAP=0.7824563352367453, speed up=1.302896795658832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_10_finetune17/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_10_finetune17\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      12.3G     0.5909     0.3739      0.911        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.938      0.863      0.931      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      11.9G     0.4459     0.2951     0.8389        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942       0.87      0.932      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      11.8G     0.5232     0.3395     0.8787        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.946      0.875      0.935      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      11.8G     0.4976     0.3318      0.877         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.945      0.878      0.934      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      11.8G     0.5079     0.3425     0.8612         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.946      0.875      0.934      0.798\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      11.8G     0.5287     0.3422     0.8902        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.951      0.875      0.937      0.804\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      11.9G     0.5654     0.3632     0.8837         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.951      0.879      0.938      0.803\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      11.9G     0.5918     0.3874     0.9027        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.948      0.878      0.937      0.802\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      11.9G     0.6008     0.3761      0.914        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.953      0.876      0.939      0.807\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      11.8G     0.6525     0.4039     0.9107        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.951      0.876      0.939      0.806\n",
      "\n",
      "10 epochs completed in 0.032 hours.\n",
      "Optimizer stripped from runs/detect/step_10_finetune17/weights/last.pt, 133.4MB\n",
      "Optimizer stripped from runs/detect/step_10_finetune17/weights/best.pt, 133.4MB\n",
      "\n",
      "Validating runs/detect/step_10_finetune17/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.952      0.876      0.937      0.807\n",
      "Speed: 0.1ms preprocess, 4.3ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_10_finetune17\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 33209910 parameters, 0 gradients, 126.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.949      0.873      0.938      0.803\n",
      "Speed: 0.2ms preprocess, 14.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_10_post_val17\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8030008974391184\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "14.519222631100824\n",
      "After Pruning\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32703049 parameters, 74176 gradients, 124.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.926      0.871      0.929      0.785\n",
      "Speed: 0.1ms preprocess, 15.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_11_pre_val15\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_11_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_11_finetune15\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 12: MACs=62.4345712 G, #Params=32.723122 M, mAP=0.7849986248769537, speed up=1.3250096030130178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_11_finetune15/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_11_finetune15\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      12.4G      0.592     0.3808     0.9108        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.912      0.894      0.937      0.794\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      12.5G     0.4274     0.2822     0.8394        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.936       0.88       0.94      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      11.6G     0.4962     0.3279     0.8649        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.934      0.885      0.942      0.807\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      11.6G     0.4768     0.3227     0.8737         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.946      0.877      0.938      0.807\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      11.5G     0.4901     0.3294     0.8562         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.884      0.939      0.804\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      11.5G     0.5087     0.3373     0.8861        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.883      0.939       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      11.5G     0.5481     0.3556     0.8798         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.919       0.89      0.936      0.804\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      11.6G     0.5694     0.3718     0.8979        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.937      0.888      0.938      0.804\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      11.6G     0.5756     0.3754     0.9038        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.886      0.939      0.806\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      11.5G     0.6536     0.3981     0.9224        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.95      0.883      0.939      0.809\n",
      "\n",
      "10 epochs completed in 0.030 hours.\n",
      "Optimizer stripped from runs/detect/step_11_finetune15/weights/last.pt, 131.4MB\n",
      "Optimizer stripped from runs/detect/step_11_finetune15/weights/best.pt, 131.4MB\n",
      "\n",
      "Validating runs/detect/step_11_finetune15/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.883      0.939      0.809\n",
      "Speed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_11_finetune15\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32703049 parameters, 0 gradients, 124.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.885      0.938      0.803\n",
      "Speed: 0.2ms preprocess, 15.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_11_post_val14\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8028105881367777\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "14.766719382862217\n",
      "After Pruning\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32669140 parameters, 74176 gradients, 124.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.945      0.883      0.942      0.806\n",
      "Speed: 0.1ms preprocess, 15.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_12_pre_val14\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_12_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_12_finetune13\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 13: MACs=62.4070664 G, #Params=32.689204 M, mAP=0.8058915915724488, speed up=1.325593577332454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_12_finetune13/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_12_finetune13\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      12.7G      0.499     0.3319     0.8801        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.933      0.887      0.937      0.809\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      11.6G     0.3689     0.2572     0.8209        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.932      0.886      0.938      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      11.6G     0.4539      0.302     0.8599        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.949      0.876      0.938       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      11.6G     0.4334     0.2969     0.8595         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.94      0.893      0.941      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      11.6G     0.4529     0.3123     0.8466         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.894      0.941      0.807\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      11.6G     0.4631     0.3128     0.8697        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.95      0.887      0.943      0.809\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      11.6G     0.5213     0.3408     0.8692         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.941      0.891      0.939      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      11.7G      0.539     0.3604     0.8853        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.939      0.892       0.94       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      11.7G     0.5515      0.358     0.8976        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.929      0.897       0.94      0.813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      11.6G     0.6402     0.3891     0.9106        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.941      0.897      0.943      0.816\n",
      "\n",
      "10 epochs completed in 0.029 hours.\n",
      "Optimizer stripped from runs/detect/step_12_finetune13/weights/last.pt, 131.3MB\n",
      "Optimizer stripped from runs/detect/step_12_finetune13/weights/best.pt, 131.3MB\n",
      "\n",
      "Validating runs/detect/step_12_finetune13/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.942      0.897      0.944      0.816\n",
      "Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_12_finetune13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 32669140 parameters, 0 gradients, 124.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.937      0.892      0.941      0.811\n",
      "Speed: 0.2ms preprocess, 16.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_12_post_val13\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8111457220640336\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "14.89709551315643\n",
      "After Pruning\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.939      0.886       0.94      0.805\n",
      "Speed: 0.2ms preprocess, 16.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_13_pre_val13\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_13_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_13_finetune13\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 14: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8050285863373501, speed up=1.3375568226839933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_13_finetune13/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_13_finetune13\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      12.1G     0.5096      0.332     0.8815        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.935      0.891      0.943      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      11.6G     0.3826     0.2558     0.8262        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.956       0.88      0.949      0.813\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      11.4G     0.4512     0.3126     0.8555        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.948      0.883      0.945      0.814\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      11.5G     0.4423      0.299     0.8562         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.95      0.882      0.944      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      11.5G     0.4557     0.3122     0.8492         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.935      0.896      0.945      0.804\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      11.5G     0.4734     0.3233     0.8728        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.93      0.892      0.941      0.805\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      11.5G     0.5367      0.352     0.8796         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.934      0.886      0.942      0.804\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      11.5G     0.5403     0.3508     0.8848        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.941      0.886      0.943       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      11.5G     0.5416     0.3534      0.889        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.893      0.947      0.814\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      11.5G     0.6465     0.4027     0.9187        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.945      0.891      0.947      0.818\n",
      "\n",
      "10 epochs completed in 0.030 hours.\n",
      "Optimizer stripped from runs/detect/step_13_finetune13/weights/last.pt, 130.3MB\n",
      "Optimizer stripped from runs/detect/step_13_finetune13/weights/best.pt, 130.3MB\n",
      "\n",
      "Validating runs/detect/step_13_finetune13/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.891      0.947      0.819\n",
      "Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_13_finetune13\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.951      0.887      0.946      0.815\n",
      "Speed: 0.2ms preprocess, 17.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_13_post_val13\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8146198835662797\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "14.96493134246744\n",
      "After Pruning\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32416863 parameters, 74176 gradients, 123.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.951      0.886      0.945      0.815\n",
      "Speed: 0.2ms preprocess, 17.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_14_pre_val13\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=None, data=coco128.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=step_14_finetune, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/step_14_finetune13\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After post-pruning Validation\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "After pruning iter 15: MACs=61.8488912 G, #Params=32.436843 M, mAP=0.8153698637584581, speed up=1.3375568226839933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgr\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "Plotting labels to runs/detect/step_14_finetune13/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 105 weight(decay=0.0), 112 weight(decay=0.0005), 111 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/step_14_finetune13\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10      11.4G     0.4922     0.3236     0.8733        122        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.955      0.882      0.946      0.822\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10      11.4G     0.3523     0.2478     0.8197        112        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.953       0.89      0.943      0.817\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10      11.4G     0.4283     0.2858     0.8487        116        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929       0.95       0.89      0.941       0.82\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10      11.4G      0.408     0.2829     0.8446         68        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.953      0.886      0.945      0.819\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10      11.4G     0.4299     0.2959     0.8395         96        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.943      0.902      0.944      0.823\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10      11.4G     0.4327     0.3007     0.8581        120        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.946      0.889      0.943       0.82\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10      11.4G     0.4762     0.3178     0.8584         69        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.944      0.892      0.945      0.814\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10      11.4G     0.5052     0.3337     0.8724        141        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.946      0.893      0.944      0.819\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10      11.4G     0.5167     0.3323     0.8776        104        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.943      0.898      0.944      0.819\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10      11.4G     0.5994     0.3758     0.8965        170        640: 100%|██████████| 8/8 [00:02\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.952      0.893      0.946      0.822\n",
      "\n",
      "10 epochs completed in 0.017 hours.\n",
      "Optimizer stripped from runs/detect/step_14_finetune13/weights/last.pt, 130.3MB\n",
      "Optimizer stripped from runs/detect/step_14_finetune13/weights/best.pt, 130.3MB\n",
      "\n",
      "Validating runs/detect/step_14_finetune13/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.943      0.902      0.944      0.823\n",
      "Speed: 0.1ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_14_finetune13\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24253MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrou\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████|\n",
      "                   all        128        929      0.947      0.899      0.943      0.819\n",
      "Speed: 0.2ms preprocess, 17.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/step_14_post_val13\u001b[0m\n",
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine tuning mAP=0.8192818206570706\n",
      "After post fine-tuning validation\n",
      "Model Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "Pruner Conv2d(3, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8l summary (fused): 285 layers, 32416863 parameters, 0 gradients, 123.4 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/detect/step_14_finetune13/weights/best.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (124.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.6s, saved as runs/detect/step_14_finetune13/weights/best.onnx (123.9 MB)\n",
      "\n",
      "Export complete (3.5s)\n",
      "Results saved to \u001b[1m/home/HubensN/fasterai/nbs/runs/detect/step_14_finetune13/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs/detect/step_14_finetune13/weights/best.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=runs/detect/step_14_finetune13/weights/best.onnx imgsz=640 data=/home/HubensN/miniconda3/envs/fasterai/lib/python3.9/site-packages/ultralytics/datasets/coco128.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "class Args(argparse.Namespace):\n",
    "  model = 'yolov8l.pt'\n",
    "  cfg = 'default.yaml'\n",
    "  iterative_steps = 15\n",
    "  target_prune_rate = 0.15\n",
    "  max_map_drop = 0.2\n",
    "  sched = Schedule(partial(sched_onecycle,  α=10, β=4))\n",
    "\n",
    "args=Args()\n",
    "prune(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afe98d-f6c4-4451-a838-991bb5993134",
   "metadata": {},
   "source": [
    "## Post-Training Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4390e-038c-42a5-a80a-872e35bf1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('/home/HubensN/fasterai/nbs/runs/detect/step_14_finetune4/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3527bf-bdca-40f0-93d3-42af28b01b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57692198400.0, 30077028)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model.model, example_inputs); base_macs, base_nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39190b40-3d3b-4844-9da7-c41e9b9aa6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "YOLOv8l summary (fused): 285 layers, 30057792 parameters, 0 gradients, 115.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/HubensN/fasterai/nbs/datasets/coco128/labels/tra\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R  \n",
      "                   all        128        929      0.917      0.907      0.945      0.809\n",
      "Speed: 0.2ms preprocess, 24.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val35\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.val(\n",
    "                data='coco128.yaml',\n",
    "                batch=1,\n",
    "                imgsz=640,\n",
    "                verbose=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb54734-cf16-4051-ae58-1e0b03435c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 11, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 79])\n",
       "box: ultralytics.yolo.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object>\n",
       "fitness: 0.8221835652536718\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.75668,     0.47387,      0.3594,     0.91994,     0.94661,     0.90211,     0.94289,     0.68531,     0.68927,     0.34436,     0.80851,      0.8955,     0.80851,     0.86091,     0.84563,     0.96863,     0.89911,      0.9501,     0.80851,     0.80851,     0.88959,       0.995,       0.995,     0.93382,\n",
       "            0.8273,     0.84511,      0.6686,     0.77723,     0.80558,      0.8234,      0.8955,     0.68357,     0.40784,     0.61583,     0.67229,     0.39977,     0.87814,     0.80851,     0.60498,     0.56705,       0.726,     0.76821,     0.76074,     0.56956,     0.72918,     0.79545,       0.995,     0.80851,\n",
       "             0.995,      0.8713,     0.73808,      0.7727,       0.995,     0.94354,     0.96781,      0.9387,     0.81318,     0.93981,     0.90203,       0.995,     0.80108,     0.90138,       0.995,      0.9641,     0.55702,     0.69798,     0.80851,     0.74254,     0.90157,     0.90353,     0.80851,     0.80032,\n",
       "           0.92735,     0.62032,     0.86949,     0.92389,       0.995,      0.9143,     0.80851,     0.94696])\n",
       "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       "plot: True\n",
       "results_dict: {'metrics/precision(B)': 0.9173606393289034, 'metrics/recall(B)': 0.9071337022261394, 'metrics/mAP50(B)': 0.9452653330065343, 'metrics/mAP50-95(B)': 0.8085078132811314, 'fitness': 0.8221835652536718}\n",
       "save_dir: Path('runs/detect/val35')\n",
       "speed: {'preprocess': 0.17499923706054688, 'inference': 24.442605674266815, 'loss': 0.0046156346797943115, 'postprocess': 0.376259908080101}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36d64b-13ff-4480-ae1b-7d302a7857e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132 🚀 Python-3.9.0 torch-2.2.1 CPU\n",
      "WARNING ⚠️ half=True only compatible with GPU export, i.e. use device=0\n",
      "YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov8l.pt with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (83.7 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.8s, saved as yolov8l.onnx (166.8 MB)\n",
      "\n",
      "Export complete (4.0s)\n",
      "Results saved to \u001b[1m/home/HubensN/fasterai/nbs\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8l.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=yolov8l.onnx imgsz=640 data=coco.yaml \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8l.onnx'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format = 'onnx', half = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
