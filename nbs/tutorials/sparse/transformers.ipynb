{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ea12c44a",
   "metadata": {},
   "source": [
    "---\n",
    "description: Prune transformers architecture with fasterai\n",
    "output-file: tutorial.transformers.html\n",
    "title: Prune Transformers\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-naples",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "This example code is taken from the fastai [docs](https://docs.fast.ai/tutorial.transformers.html)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from transformers.modeling_utils import Conv1D\n",
    "from fastai.text.all import *\n",
    "import fastcore\n",
    "from fasterai.sparse.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.WIKITEXT_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "df_train = pd.read_csv(path/'train.csv', header=None)\n",
    "df_valid = pd.read_csv(path/'test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "all_texts = np.concatenate([df_train[0].values, df_valid[0].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        toks = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "splits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]\n",
    "tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "bs,sl = 4,256\n",
    "dls = tls.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def tokenize(text):\n",
    "    toks = tokenizer.tokenize(text)\n",
    "    return tensor(tokenizer.convert_tokens_to_ids(toks))\n",
    "\n",
    "tokenized = [tokenize(t) for t in progress_bar(all_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        return x if isinstance(x, Tensor) else tokenize(x)\n",
    "        \n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "tls = TfmdLists(tokenized, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\n",
    "dls = tls.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "class DropOutput(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-graph",
   "metadata": {},
   "source": [
    "Let's create our fastai `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-tuner",
   "metadata": {},
   "source": [
    "And let's try to extend a given prompt with the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.model.generate(inp, max_length=40, num_beams=5, temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None]\n",
    "\n",
    "preds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n",
    "\n",
    "tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721f63e",
   "metadata": {},
   "source": [
    "## Make it sparse !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff50c46",
   "metadata": {},
   "source": [
    "Let's see now if we retrain our model, this time introducing sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95371238",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819d22f",
   "metadata": {},
   "source": [
    "Unfortunately, the transformer model uses a custom layer: `Conv1D`, which is not a part of PyTorch. To overcome this problem, we have to add this layer to our `Granularities` class, so that it knows what to sparsify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f362cc4",
   "metadata": {},
   "source": [
    "Here, the `Conv1D` behaves like a `Linear` layer, i.e. the weights are defined by a matrix of dimension `(nf,nx)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64493765",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(Conv1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3aa1cc",
   "metadata": {},
   "source": [
    "We can thus add the Conv1D granularity by using the `add_granularity` method, indicating the target module and the corresponding granularities that it can handle (the same as `Linear` so we can reuse it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a530677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Granularities.add_granularity(Conv1D, Granularities._granularities_Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909377e6",
   "metadata": {},
   "source": [
    "Let's now define our `SparsifyCallback`. Let's say we want to make our model 30% sparse, by removing the highest-norm weight in each attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ae3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(sparsity=30, granularity='weight', context='local', criteria=large_final, schedule=one_cycle, layer_type=Conv1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b21be",
   "metadata": {},
   "source": [
    "We now only have to pass our callback to fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-4, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49b313",
   "metadata": {},
   "source": [
    "And we can check the predicion to the same prompt as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None]\n",
    "\n",
    "preds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n",
    "\n",
    "tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c34e5",
   "metadata": {},
   "source": [
    "That's it ! You now have a sparse Transformer as performant as the whole model. However, this model is currently not more efficient speed and storage wise. To have such a speed-up, I suggest you to look at the [granularity](https://nathanhubens.github.io/fasterai/granularity.html) section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
