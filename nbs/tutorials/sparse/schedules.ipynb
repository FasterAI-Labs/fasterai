{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7a3bd9e5",
   "metadata": {},
   "source": [
    "---\n",
    "description: Make your neural network sparse with fastai\n",
    "output-file: tutorial.schedules.html\n",
    "title: Schedules\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-night",
   "metadata": {},
   "source": [
    "Neural Network Pruning usually follows one of the next 3 schedules:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-shannon",
   "metadata": {},
   "source": [
    "![](../../imgs/schedules.png \"Schedules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-fusion",
   "metadata": {},
   "source": [
    "In fasterai, all those 3 schedules can be applied from the **same** callback. We'll cover each below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339b363",
   "metadata": {},
   "source": [
    "In the SparsifyCallback, there are several parameters to 'shape' our pruning schedule:\n",
    "* `start_sparsity`: the initial sparsity of our model, generally kept at 0 as after initialization, our weights are generally non-zero.\n",
    "* `end_sparsity`: the target sparsity at the end of the training \n",
    "* `start_epoch`: we can decide to start pruning right from the beginning or let it train a bit before removing weights.\n",
    "* `sched_func`: this is where the general shape of the schedule is specified as it specifies how the sparsity evolves along the training. You can either use a schedule [available](https://docs.fast.ai/callback.schedule.html#Annealing) in fastai our even coming with your own !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-sherman",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "\n",
    "from fasterai.sparse.all import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(context='poster', style='white',\n",
    "        font='sans-serif', font_scale=1, color_codes=True, rc=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-passing",
   "metadata": {},
   "source": [
    "We will first train a network without any pruning, which will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acdb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.fit_one_cycle(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-passion",
   "metadata": {},
   "source": [
    "## One-Shot Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-reset",
   "metadata": {},
   "source": [
    "The simplest way to perform pruning is called One-Shot Pruning. It consists of the following three steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-ceremony",
   "metadata": {},
   "source": [
    "1. You first need to train a network\n",
    "2. You then need to remove some weights (depending on your criteria, needs,...)\n",
    "3. You fine-tune the remaining weights to recover from the loss of parameters.\n",
    "\n",
    "With fasterai, this is really easy to do. Let's illustrate it by an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-entry",
   "metadata": {},
   "source": [
    "In this case, your network needs to be trained before pruning. This training can be done independently from the pruning callback, or simulated by the `start_epoch` that will delay the pruning process.\n",
    "\n",
    "You thus only need to create the Callback with the `one_shot` schedule and set the `start_epoch` argument, i.e. how many epochs you want to train your network before pruning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=one_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-panama",
   "metadata": {},
   "source": [
    "Let's start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-channels",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-simon",
   "metadata": {},
   "source": [
    "## Iterative Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-indonesia",
   "metadata": {},
   "source": [
    "Researchers have come up with a better way to do pruning than pruning all the weigths in once (as in One-Shot Pruning). The idea is to perform several iterations of pruning and fine-tuning and is thus called Iterative Pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-symphony",
   "metadata": {},
   "source": [
    "1. You first need to train a network\n",
    "2. You then need to remove a part of the weights weights (depending on your criteria, needs,...)\n",
    "3. You fine-tune the remaining weights to recover from the loss of parameters.\n",
    "4. Back to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-peeing",
   "metadata": {},
   "source": [
    "In this case, your network needs to be trained before pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-parent",
   "metadata": {},
   "source": [
    "You only need to create the Callback with the `iterative` schedule and set the `start_epoch` argument, i.e. how many epochs you want to train your network before pruning it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-authority",
   "metadata": {},
   "source": [
    "The `iterative` schedules has a `n_steps`parameter, i.e. how many iterations of pruning/fine-tuning you want to perform. To modify its value, we can use the `partial` function like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-friday",
   "metadata": {},
   "source": [
    "```\n",
    "iterative = partial(iterative, n_steps=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=iterative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-sullivan",
   "metadata": {},
   "source": [
    "Let's start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-conditioning",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-detail",
   "metadata": {},
   "source": [
    "## Gradual Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763a794",
   "metadata": {},
   "source": [
    "Here is for example how to implement the [Automated Gradual Pruning](https://arxiv.org/pdf/1710.01878.pdf) schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb=SparsifyCallback(sparsity=90, granularity='weight', context='local', criteria=large_final, schedule=agp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-dispute",
   "metadata": {},
   "source": [
    "Let's start pruning after 3 epochs and train our model for 6 epochs to have the same total amount of training as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-backup",
   "metadata": {},
   "source": [
    "Even though they are often considered as different pruning methods, those 3 schedules can be captured by the same Callback. Here is how the sparsity in the network evolves for those methods;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-preparation",
   "metadata": {},
   "source": [
    "Let's take an example here. Let's say that we want to train our network for 3 epochs without pruning and then 7 epochs with pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "train = np.zeros(300)\n",
    "prune = np.linspace(0,1, 700) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-mapping",
   "metadata": {},
   "source": [
    "Then this is what our different pruning schedules will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,5), dpi=100)\n",
    "fig.patch.set_alpha(0.)\n",
    "ax.patch.set_alpha(0.)\n",
    "plt.plot(np.concatenate([train, sched_iterative(0,90, prune)]), label='Iterative', linestyle='-.', c='#89d6c9')\n",
    "plt.plot(np.concatenate([train, [sched_oneshot(0,90, p) for p in prune]]), label='One-Shot', linestyle=':', c='#89d6c9')\n",
    "plt.plot(np.concatenate([train, sched_agp(0,90, prune)]), label='Gradual', c='#89d6c9')\n",
    "ax.spines['bottom'].set_color('#808080')\n",
    "ax.spines['top'].set_color('#808080') \n",
    "ax.spines['right'].set_color('#808080')\n",
    "ax.spines['left'].set_color('#808080')\n",
    "ax.tick_params(axis='x', colors='#808080')\n",
    "ax.tick_params(axis='y', colors='#808080')\n",
    "ax.yaxis.label.set_color('#808080')\n",
    "ax.xaxis.label.set_color('#808080')\n",
    "plt.legend(framealpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-nudist",
   "metadata": {},
   "source": [
    "**You can also come up with your own pruning schedule !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
