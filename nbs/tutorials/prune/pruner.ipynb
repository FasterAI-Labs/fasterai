{
 "cells": [
  {
   "cell_type": "raw",
   "id": "frontmatter",
   "metadata": {},
   "source": [
    "---\n",
    "description: Structured pruning to create smaller, faster models\n",
    "output-file: tutorial.pruner.html\n",
    "title: Pruner Tutorial\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `Pruner` class performs **structured pruning** - physically removing entire filters and channels from your neural network. Unlike sparsification (which zeros weights but keeps the architecture), structured pruning creates a genuinely smaller model that runs faster on standard hardware.\n",
    "\n",
    "### Sparsifier vs Pruner\n",
    "\n",
    "| Aspect | Sparsifier | Pruner |\n",
    "|--------|------------|--------|\n",
    "| **What it removes** | Individual weights → zeros | Entire filters → gone |\n",
    "| **Architecture** | Unchanged (same shapes) | Smaller (fewer channels) |\n",
    "| **Speedup** | Needs sparse hardware | Immediate on any hardware |\n",
    "| **Use case** | Research, sparse accelerators | Production deployment |\n",
    "\n",
    "**When to use Pruner:**\n",
    "- You need a smaller model file\n",
    "- You want faster inference without special hardware\n",
    "- You're deploying to edge devices or mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from fasterai.prune.pruner import Pruner\n",
    "from fasterai.core.criteria import large_final, random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-header",
   "metadata": {},
   "source": [
    "## 1. Basic Pruning\n",
    "\n",
    "Let's start with a ResNet18 and prune 30% of its filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-before",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning:\n",
      "  conv1: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  layer1[0].conv1: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  Parameters: 11,689,512\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(weights=None)\n",
    "\n",
    "print('Before pruning:')\n",
    "print(f'  conv1: {model.conv1}')\n",
    "print(f'  layer1[0].conv1: {model.layer1[0].conv1}')\n",
    "print(f'  Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-prune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "\n",
      "After pruning:\n",
      "  conv1: Conv2d(3, 44, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  layer1[0].conv1: Conv2d(44, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  Parameters: 5,820,556\n",
      "  Reduction: 50.2%\n"
     ]
    }
   ],
   "source": [
    "pruner = Pruner(\n",
    "    model, \n",
    "    pruning_ratio=30,      # Remove 30% of filters\n",
    "    context='local',       # Prune each layer independently\n",
    "    criteria=large_final   # Keep filters with largest weights\n",
    ")\n",
    "pruner.prune_model()\n",
    "\n",
    "print('\\nAfter pruning:')\n",
    "print(f'  conv1: {model.conv1}')\n",
    "print(f'  layer1[0].conv1: {model.layer1[0].conv1}')\n",
    "params_after = sum(p.numel() for p in model.parameters())\n",
    "print(f'  Parameters: {params_after:,}')\n",
    "print(f'  Reduction: {100*(1 - params_after/11689512):.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notice-channels",
   "metadata": {},
   "source": [
    "Notice the channel counts changed: `Conv2d(3, 64, ...)` became `Conv2d(3, 44, ...)`. The model is genuinely smaller!\n",
    "\n",
    "**Key point:** The Pruner automatically handles layer dependencies. When you remove output channels from one layer, it removes the corresponding input channels from the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-header",
   "metadata": {},
   "source": [
    "## 2. Local vs Global Pruning\n",
    "\n",
    "The `context` parameter controls how filters are selected for pruning:\n",
    "\n",
    "| Context | Behavior | Best for |\n",
    "|---------|----------|----------|\n",
    "| `'local'` | Each layer loses same % of filters | Uniform compression |\n",
    "| `'global'` | Compare importance across all layers | Maximum accuracy retention |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "\n",
      "Local pruning (each layer loses 50%):\n",
      "  layer1[0].conv1: Conv2d(32, 32, ...)\n",
      "  layer2[0].conv1: Conv2d(32, 64, ...)\n",
      "  layer3[0].conv1: Conv2d(64, 128, ...)\n",
      "  layer4[0].conv1: Conv2d(128, 256, ...)\n"
     ]
    }
   ],
   "source": [
    "# Local: each layer pruned independently to 50%\n",
    "model_local = resnet18(weights=None)\n",
    "pruner = Pruner(model_local, 50, 'local', large_final)\n",
    "pruner.prune_model()\n",
    "\n",
    "print('\\nLocal pruning (each layer loses 50%):')\n",
    "print(f'  layer1[0].conv1: Conv2d({model_local.layer1[0].conv1.in_channels}, {model_local.layer1[0].conv1.out_channels}, ...)')\n",
    "print(f'  layer2[0].conv1: Conv2d({model_local.layer2[0].conv1.in_channels}, {model_local.layer2[0].conv1.out_channels}, ...)')\n",
    "print(f'  layer3[0].conv1: Conv2d({model_local.layer3[0].conv1.in_channels}, {model_local.layer3[0].conv1.out_channels}, ...)')\n",
    "print(f'  layer4[0].conv1: Conv2d({model_local.layer4[0].conv1.in_channels}, {model_local.layer4[0].conv1.out_channels}, ...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "\n",
      "Global pruning (importance compared across layers):\n",
      "  layer1[0].conv1: Conv2d(64, 64, ...)\n",
      "  layer2[0].conv1: Conv2d(64, 128, ...)\n",
      "  layer3[0].conv1: Conv2d(128, 69, ...)\n",
      "  layer4[0].conv1: Conv2d(256, 512, ...)\n"
     ]
    }
   ],
   "source": [
    "# Global: least important filters across entire network\n",
    "model_global = resnet18(weights=None)\n",
    "pruner = Pruner(model_global, 50, 'global', large_final)\n",
    "pruner.prune_model()\n",
    "\n",
    "print('\\nGlobal pruning (importance compared across layers):')\n",
    "print(f'  layer1[0].conv1: Conv2d({model_global.layer1[0].conv1.in_channels}, {model_global.layer1[0].conv1.out_channels}, ...)')\n",
    "print(f'  layer2[0].conv1: Conv2d({model_global.layer2[0].conv1.in_channels}, {model_global.layer2[0].conv1.out_channels}, ...)')\n",
    "print(f'  layer3[0].conv1: Conv2d({model_global.layer3[0].conv1.in_channels}, {model_global.layer3[0].conv1.out_channels}, ...)')\n",
    "print(f'  layer4[0].conv1: Conv2d({model_global.layer4[0].conv1.in_channels}, {model_global.layer4[0].conv1.out_channels}, ...)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-note",
   "metadata": {},
   "source": [
    "With global pruning, early layers often keep more filters (they're more important) while later layers with redundant features get pruned more aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iterative-header",
   "metadata": {},
   "source": [
    "## 3. Iterative Pruning\n",
    "\n",
    "For high compression ratios, iterative pruning works better than one-shot. The model gradually adapts to having fewer parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iterative-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "Iterative pruning (5 steps to reach 50%):\n",
      "  Step 1: 9,481,588 params (18.9% reduction)\n",
      "  Step 2: 7,534,380 params (35.5% reduction)\n",
      "  Step 3: 5,820,556 params (50.2% reduction)\n",
      "  Step 4: 4,318,898 params (63.1% reduction)\n",
      "  Step 5: 3,055,880 params (73.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(weights=None)\n",
    "params_orig = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Iterative pruning: 5 steps to reach 50% pruning\n",
    "pruner = Pruner(\n",
    "    model, \n",
    "    pruning_ratio=50,\n",
    "    context='local', \n",
    "    criteria=large_final,\n",
    "    iterative_steps=5  # Spread pruning over 5 steps\n",
    ")\n",
    "\n",
    "print('Iterative pruning (5 steps to reach 50%):')\n",
    "for i in range(5):\n",
    "    pruner.prune_model()\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'  Step {i+1}: {params:,} params ({100*(1-params/params_orig):.1f}% reduction)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iterative-note",
   "metadata": {},
   "source": [
    "**In practice:** When using `PruneCallback` during training, iterative pruning happens automatically - the model is pruned a little bit after each batch, allowing it to recover between steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perlayer-header",
   "metadata": {},
   "source": [
    "## 4. Per-Layer Pruning Ratios\n",
    "\n",
    "Different layers have different sensitivity to pruning. You can specify custom ratios using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perlayer-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "Using per-layer pruning with 8 layer-specific ratios\n",
      "\n",
      "Per-layer pruning results:\n",
      "  layer1.0.conv1: 51 channels (20% pruned from 64)\n",
      "  layer2.0.conv1: 76 channels (40% pruned from 128)\n",
      "  layer3.0.conv1: 102 channels (60% pruned from 256)\n",
      "  layer4.0.conv1: 102 channels (80% pruned from 512)\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(weights=None)\n",
    "\n",
    "# Conservative on early layers, aggressive on later layers\n",
    "per_layer_ratios = {\n",
    "    'layer1.0.conv1': 20,  'layer1.0.conv2': 20,  # 20% pruning\n",
    "    'layer2.0.conv1': 40,  'layer2.0.conv2': 40,  # 40% pruning  \n",
    "    'layer3.0.conv1': 60,  'layer3.0.conv2': 60,  # 60% pruning\n",
    "    'layer4.0.conv1': 80,  'layer4.0.conv2': 80,  # 80% pruning\n",
    "}\n",
    "\n",
    "pruner = Pruner(model, per_layer_ratios, 'local', large_final)\n",
    "pruner.prune_model()\n",
    "\n",
    "print('\\nPer-layer pruning results:')\n",
    "print(f'  layer1.0.conv1: {model.layer1[0].conv1.out_channels} channels (20% pruned from 64)')\n",
    "print(f'  layer2.0.conv1: {model.layer2[0].conv1.out_channels} channels (40% pruned from 128)')\n",
    "print(f'  layer3.0.conv1: {model.layer3[0].conv1.out_channels} channels (60% pruned from 256)')\n",
    "print(f'  layer4.0.conv1: {model.layer4[0].conv1.out_channels} channels (80% pruned from 512)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perlayer-note",
   "metadata": {},
   "source": "**Tip:** Use sensitivity analysis to determine which layers can tolerate more pruning. See the [Sensitivity Tutorial](../analyze/sensitivity.html) for details."
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "## 5. Verifying the Pruned Model\n",
    "\n",
    "After pruning, the model remains fully functional - it just has fewer parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "\n",
      "Forward pass verification:\n",
      "  Input shape:  torch.Size([1, 3, 224, 224])\n",
      "  Output shape: torch.Size([1, 1000])\n",
      "  Model works correctly after pruning!\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(weights=None)\n",
    "model.eval()\n",
    "\n",
    "# Prune 50%\n",
    "pruner = Pruner(model, 50, 'global', large_final)\n",
    "pruner.prune_model()\n",
    "\n",
    "# Verify forward pass works\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "\n",
    "print('\\nForward pass verification:')\n",
    "print(f'  Input shape:  {x.shape}')\n",
    "print(f'  Output shape: {output.shape}')\n",
    "print('  Model works correctly after pruning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criteria-header",
   "metadata": {},
   "source": [
    "## 6. Importance Criteria\n",
    "\n",
    "The `criteria` parameter determines how filter importance is calculated:\n",
    "\n",
    "| Criteria | Method | Best for |\n",
    "|----------|--------|----------|\n",
    "| `large_final` | Keep filters with largest L1 norm | General use, most common |\n",
    "| `small_final` | Keep filters with smallest L1 norm | Unusual, for experimentation |\n",
    "| `random` | Random selection | Baseline comparison |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criteria-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "Ignoring output layer: fc\n",
      "Total ignored layers: 1\n",
      "\n",
      "Same pruning ratio, different criteria:\n",
      "  large_final: 5,820,556 parameters\n",
      "  random: 5,820,556 parameters\n",
      "\n",
      "Note: Parameter counts are similar, but accuracy differs!\n",
      "large_final preserves important filters, random does not.\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name, criteria in [('large_final', large_final), ('random', random)]:\n",
    "    model = resnet18(weights=None)\n",
    "    pruner = Pruner(model, 30, 'local', criteria)\n",
    "    pruner.prune_model()\n",
    "    results[name] = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print('\\nSame pruning ratio, different criteria:')\n",
    "for name, params in results.items():\n",
    "    print(f'  {name}: {params:,} parameters')\n",
    "print('\\nNote: Parameter counts are similar, but accuracy differs!')\n",
    "print('large_final preserves important filters, random does not.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Structured pruning** | Removes entire filters, creating genuinely smaller models |\n",
    "| **Local context** | Each layer pruned by same percentage |\n",
    "| **Global context** | Compare importance across all layers |\n",
    "| **Iterative pruning** | Gradual pruning for better accuracy retention |\n",
    "| **Per-layer ratios** | Dictionary of custom ratios per layer |\n",
    "| **Auto dependency** | Handles layer connections automatically |\n",
    "\n",
    "### Typical Workflow\n",
    "\n",
    "```python\n",
    "# 1. One-shot pruning for quick experiments\n",
    "pruner = Pruner(model, 30, 'local', large_final)\n",
    "pruner.prune_model()\n",
    "\n",
    "# 2. During training with PruneCallback (recommended)\n",
    "cb = PruneCallback(pruning_ratio=50, schedule=agp, context='global', criteria=large_final)\n",
    "learn.fit(10, cbs=[cb])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [PruneCallback Tutorial](prune_callback.html) - Apply pruning during fastai training\n",
    "- [Sparsifier Tutorial](../sparse/sparsifier.html) - Unstructured pruning alternative\n",
    "- [Criteria](../../core/criteria.html) - Importance measures for filter selection\n",
    "- [Schedules](../../core/schedules.html) - Control pruning progression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
