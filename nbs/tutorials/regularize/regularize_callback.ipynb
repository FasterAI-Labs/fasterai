{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3fef4a5c",
   "metadata": {},
   "source": [
    "---\n",
    "description: Perform Group Regularization in fastai Callback system\n",
    "output-file: regularizer.html\n",
    "title: Regularize Callback\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9d82f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.core.criteria import *\n",
    "from fasterai.core.schedule import *\n",
    "from fasterai.regularize.all import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hddvpx4v5c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Group Regularization** is a technique that encourages structured sparsity in neural networks during training. Unlike standard L2 regularization (weight decay) which penalizes individual weights, group regularization penalizes *groups* of weights togetherâ€”such as entire filters, kernels, or channels.\n",
    "\n",
    "### Why Use Group Regularization?\n",
    "\n",
    "When preparing a model for **structured pruning**, you want entire structures (filters, channels) to become unimportant, not just individual weights. Group regularization pushes these structures toward zero *during training*, making subsequent pruning:\n",
    "\n",
    "1. **More effective** - Pruned structures are already near-zero, minimizing accuracy loss\n",
    "2. **Cleaner** - Clear separation between important and unimportant structures\n",
    "3. **Hardware-friendly** - Structured sparsity maps well to GPU/CPU acceleration\n",
    "\n",
    "### The RegularizeCallback\n",
    "\n",
    "The `RegularizeCallback` adds a regularization term to the loss function:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\lambda \\sum_{g \\in \\text{groups}} \\|W_g\\|_p$$\n",
    "\n",
    "Where $\\lambda$ is the regularization weight and $W_g$ are weight groups at your chosen granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccbffb9",
   "metadata": {},
   "source": [
    "## 1. Setup and Data\n",
    "\n",
    "Let's start by loading a dataset and establishing a baseline model without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e36125",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef73ea",
   "metadata": {},
   "source": [
    "## 2. Baseline Training (No Regularization)\n",
    "\n",
    "First, we train a model without any regularization to establish a baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad51bb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.682476</td>\n",
       "      <td>0.530806</td>\n",
       "      <td>0.847091</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.403894</td>\n",
       "      <td>0.268916</td>\n",
       "      <td>0.905277</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.235484</td>\n",
       "      <td>0.212882</td>\n",
       "      <td>0.918133</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.119361</td>\n",
       "      <td>0.198808</td>\n",
       "      <td>0.921516</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.067766</td>\n",
       "      <td>0.185810</td>\n",
       "      <td>0.928958</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f20d1",
   "metadata": {},
   "source": [
    "## 3. Training with Group Regularization\n",
    "\n",
    "Now let's train with `RegularizeCallback`. We'll configure it with:\n",
    "\n",
    "- **`criteria=squared_final`**: Uses squared weight magnitudes for regularization\n",
    "- **`granularity='weight'`**: Regularizes at individual weight level (try `'filter'` for structured pruning prep)\n",
    "- **`weight=3e-5`**: Regularization strength (higher = more aggressive)\n",
    "- **`schedule=one_cycle`**: Varies regularization strength during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05dab4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_cb = RegularizeCallback(squared_final, 'weight', 1e-3, schedule=one_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18def6d5-e684-48b7-91ec-3283639839c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9e0fd6-007c-411c-92dc-9d71a94aa009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.785396</td>\n",
       "      <td>1.031241</td>\n",
       "      <td>0.818674</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.362821</td>\n",
       "      <td>2.267276</td>\n",
       "      <td>0.893099</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.505133</td>\n",
       "      <td>4.478498</td>\n",
       "      <td>0.891069</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.259987</td>\n",
       "      <td>4.406100</td>\n",
       "      <td>0.927605</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.243036</td>\n",
       "      <td>4.337528</td>\n",
       "      <td>0.940460</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, cbs=reg_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0b212-4559-4553-923d-5cd2c1ab9085",
   "metadata": {},
   "source": [
    "## 4. Comparing Results\n",
    "\n",
    "After training, you should observe:\n",
    "- Similar or slightly lower accuracy (regularization adds a constraint)\n",
    "- Weights that are more concentrated around zero\n",
    "- Cleaner weight distribution for subsequent pruning\n",
    "\n",
    "To visualize the effect, you can plot weight histograms:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get all conv weights\n",
    "weights = torch.cat([m.weight.data.flatten() for m in learn.model.modules() \n",
    "                     if isinstance(m, nn.Conv2d)])\n",
    "\n",
    "plt.hist(weights.cpu().numpy(), bins=100, alpha=0.7)\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Weight Distribution After Group Regularization')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ag4l8qyex7",
   "metadata": {},
   "source": [
    "## 5. Parameter Guide\n",
    "\n",
    "### Choosing Granularity\n",
    "\n",
    "| Granularity | Effect | Best For |\n",
    "|-------------|--------|----------|\n",
    "| `'weight'` | Regularizes individual weights | Unstructured pruning, general sparsity |\n",
    "| `'filter'` | Regularizes entire Conv2d filters | **Structured pruning** (recommended) |\n",
    "| `'kernel'` | Regularizes 2D kernels within filters | Moderate structure |\n",
    "| `'channel'` | Regularizes input channels | Channel pruning |\n",
    "\n",
    "### Choosing Regularization Weight\n",
    "\n",
    "| Weight Range | Effect |\n",
    "|--------------|--------|\n",
    "| `1e-6 - 1e-5` | Very light regularization, minimal accuracy impact |\n",
    "| `1e-5 - 1e-4` | Moderate regularization, good balance |\n",
    "| `1e-4 - 1e-3` | Strong regularization, may reduce accuracy |\n",
    "| `> 1e-3` | Very aggressive, use with caution |\n",
    "\n",
    "**Tip:** Start with `1e-5` and increase if weights don't concentrate toward zero.\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```python\n",
    "# 1. Train with filter-level regularization\n",
    "reg_cb = RegularizeCallback(\n",
    "    criteria=large_final,      # or squared_final\n",
    "    granularity='filter',      # for structured pruning\n",
    "    weight=1e-4,\n",
    "    schedule=one_cycle,\n",
    "    verbose=True\n",
    ")\n",
    "learn.fit(epochs, cbs=[reg_cb])\n",
    "\n",
    "# 2. Prune the regularized model\n",
    "from fasterai.prune.all import *\n",
    "pruner = Pruner(learn.model, sparsity=0.3, context='local', criteria=large_final)\n",
    "pruner.prune_model()\n",
    "\n",
    "# 3. Fine-tune\n",
    "learn.fit(fine_tune_epochs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7w8flfdsw4m",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Group Regularization** | Penalizes groups of weights to encourage structured sparsity |\n",
    "| **RegularizeCallback** | fastai callback that adds regularization term to loss |\n",
    "| **Granularity** | Level at which to group weights (`'weight'`, `'filter'`, `'kernel'`) |\n",
    "| **Schedule** | Varies regularization strength during training |\n",
    "| **Typical Use** | Pre-pruning preparation to make structured pruning more effective |\n",
    "\n",
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Criteria](../../core/criteria.html) - Importance measures used for regularization\n",
    "- [Schedules](../../core/schedules.html) - Control regularization strength over training\n",
    "- [Pruner](../../prune/pruner.html) - Apply structured pruning after regularization\n",
    "- [Sparsifier](../../sparse/sparsifier.html) - Apply unstructured sparsification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
