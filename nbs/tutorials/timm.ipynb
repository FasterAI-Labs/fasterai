{
 "cells": [
  {
   "cell_type": "raw",
   "id": "685b253e-fd46-462e-a53c-46acd3ffad50",
   "metadata": {},
   "source": [
    "---\n",
    "description: TIMM Pruning\n",
    "output-file: timm_pruning.html\n",
    "title: TIMM Pruning\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835765a-6022-4896-b04a-86993a3c628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "from fasterai.core.criteria import *\n",
    "import torch_pruning as tp\n",
    "from torch_pruning.pruner import function\n",
    "import torch_pruning as tp\n",
    "import timm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b587a-93c6-47cd-8ca7-0c73245c1a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onecycle_scheduler(pruning_ratio_dict, steps, start=0, end=1, α=14, β=6):\n",
    "    return [\n",
    "        sched_onecycle(start, end, i / float(steps), α, β) * pruning_ratio_dict\n",
    "        for i in range(steps + 1)\n",
    "    ]\n",
    "\n",
    "def sched_onecycle(start, end, pos, α=14, β=6):\n",
    "    out = (1 + np.exp(-α + β)) / (1 + np.exp((-α * pos) + β))\n",
    "    return start + (end - start) * out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f136c-16fb-442a-be21-f572f692c7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1725a08-aecf-4253-a0bc-f1d754bbf3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(size, bs):\n",
    "    path = URLs.IMAGENETTE_160\n",
    "    source = untar_data(path)\n",
    "    blocks=(ImageBlock, CategoryBlock)\n",
    "    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n",
    "    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n",
    "\n",
    "    csv_file = 'noisy_imagenette.csv'\n",
    "    inp = pd.read_csv(source/csv_file)\n",
    "    dblock = DataBlock(blocks=blocks,\n",
    "               splitter=ColSplitter(),\n",
    "               get_x=ColReader('path', pref=source),\n",
    "               get_y=ColReader(f'noisy_labels_0'),\n",
    "               item_tfms=tfms,\n",
    "               batch_tfms=batch_tfms)\n",
    "\n",
    "    return dblock.dataloaders(inp, path=source, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea31be-e73f-43d9-b3bb-c0e8b986e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet18', pretrained=False, no_jit=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51141b-56d1-4df7-b287-1ae464647804",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(model.default_cfg['input_size'][2], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8edb7-d2de-4bef-9a00-5db38ac60ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = vision_learner(dls, 'bat_resnext26ts', metrics = [accuracy])\n",
    "#learn.unfreeze()\n",
    "\n",
    "model = timm.create_model('beit_base_patch16_224', pretrained=False, no_jit=True).eval()\n",
    "\n",
    "ignored_layers = []\n",
    "num_heads = {}\n",
    "pruning_ratio_dict = {}\n",
    "#ratios = [0.265625,0.234375,0.265625,0.265625,0.93359375,0.328125,0.2265625,0.58984375,0.54296875,0.701171875,0.919921875,0.04296875,0.796875,0.240966796875,0.07763671875]\n",
    "\n",
    "\n",
    "#k = 0\n",
    "for m in model.modules():\n",
    "    #if hasattr(m, 'head'): #isinstance(m, nn.Linear) and m.out_features == model.num_classes:\n",
    "    if isinstance(m, nn.Linear) and m.out_features == model.num_classes:\n",
    "        ignored_layers.append(m)\n",
    "        print(\"Ignore classifier layer: \", m)\n",
    "\n",
    "    # Attention layers\n",
    "    if hasattr(m, 'num_heads'):\n",
    "        if hasattr(m, 'qkv'):\n",
    "            num_heads[m.qkv] = m.num_heads\n",
    "            print(\"Attention layer: \", m.qkv, m.num_heads)\n",
    "        elif hasattr(m, 'qkv_proj'):\n",
    "            num_heads[m.qkv_proj] = m.num_heads\n",
    "    \n",
    "    #elif isinstance(m, nn.Conv2d):\n",
    "    #    pruning_ratio_dict[m] = ratios[k]\n",
    "    #    print(k)\n",
    "    #    k+=1\n",
    "\n",
    "learn = Learner(dls, model, metrics = [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45775057-7ee9-4c39-9ee6-b756865aa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, _ = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8324b-22c9-4045-904d-6fa0207f6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d3226-cb67-4ca0-916d-109c2cbbe235",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = tp.pruner.MetaPruner(\n",
    "                        model, \n",
    "                        xb.to('cpu'), \n",
    "                        global_pruning=False,\n",
    "                        importance=tp.importance.GroupNormImportance(), \n",
    "                        iterative_steps=10000,\n",
    "                        pruning_ratio=0.5,\n",
    "                        #pruning_ratio_dict=pruning_ratio_dict,\n",
    "                        num_heads=num_heads,\n",
    "                        ignored_layers=ignored_layers,\n",
    "                    )\n",
    "#for g in pruner.step(interactive=True):\n",
    "#    g.prune()\n",
    "pruner.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44332be1-c236-4da3-9d31-b07d8d52a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    # Attention layers\n",
    "    if hasattr(m, 'num_heads'):\n",
    "        if hasattr(m, 'qkv'):\n",
    "            m.num_heads = num_heads[m.qkv]\n",
    "            m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "        elif hasattr(m, 'qkv_proj'):\n",
    "            m.num_heads = num_heads[m.qqkv_projkv]\n",
    "            m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef09402-e838-4358-9f1c-079512227a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old = timm.create_model('convnext_xxlarge', pretrained=False, no_jit=True).eval()\n",
    "base_macs, base_params = tp.utils.count_ops_and_params(model_old, xb.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbdc44-abfb-4cdb-bef1-61b5053cfccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_macs, pruned_params = tp.utils.count_ops_and_params(model, xb.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10d6d0-638c-4a41-a1ca-35e2111389aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MACs: %.4f G => %.4f G\"%(base_macs/1e9, pruned_macs/1e9))\n",
    "print(\"Params: %.4f M => %.4f M\"%(base_params/1e6, pruned_params/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574790ea-cc63-4af4-87f6-1df0401b6ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d73804-57f7-4109-ac4c-f770bab6f1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a3b2b-1335-4f1f-aa4d-99d372018100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17945b76-2228-45e7-9742-725a67f198bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7255784-1329-4691-890e-566adc45ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruneCallback(Callback):\n",
    "    def __init__(self, pruning_ratio, schedule, criteria, ignored_layers, *args, **kwargs):\n",
    "        store_attr()\n",
    "        self.sparsity_levels = []\n",
    "        self.extra_args = args\n",
    "        self.extra_kwargs = kwargs\n",
    "\n",
    "    def before_fit(self):\n",
    "        n_batches_per_epoch = len(self.learn.dls.train)\n",
    "        total_training_steps = n_batches_per_epoch * self.learn.n_epoch\n",
    "\n",
    "        self.total_training_steps = total_training_steps \n",
    "        print(self.total_training_steps)\n",
    "        self.example_inputs, _ = self.learn.dls.one_batch()\n",
    "        self.sparsity_levels = self.schedule(self.pruning_ratio, total_training_steps)\n",
    "\n",
    "        self.pruner = tp.pruner.MetaPruner(\n",
    "        self.learn.model,\n",
    "        example_inputs= torch.randn(self.example_inputs.shape).to('cuda:0'),\n",
    "        importance=self.criteria,\n",
    "        pruning_ratio=self.pruning_ratio, \n",
    "        ignored_layers=self.ignored_layers,\n",
    "        iterative_steps= self.total_training_steps, \n",
    "        #iterative_steps= 1, \n",
    "        #iterative_pruning_ratio_scheduler=self.schedule,\n",
    "        #global_pruning=self.context, \n",
    "        *self.extra_args, \n",
    "        **self.extra_kwargs\n",
    "        )\n",
    "        \n",
    "    def before_step(self):\n",
    "        if self.training: \n",
    "           #self.pruner.step()\n",
    "            for g in self.pruner.step(interactive=True):\n",
    "                g.prune()\n",
    "            \n",
    "        #for m in self.pruner.model.modules():\n",
    "        #    # Attention layers\n",
    "        #    if hasattr(m, 'num_heads'):\n",
    "        #        if hasattr(m, 'qkv'):\n",
    "        #            m.num_heads = num_heads[m.qkv]\n",
    "        #            m.head_dim = m.qkv.out_features // (3 * m.num_heads)\n",
    "        #        elif hasattr(m, 'qkv_proj'):\n",
    "        #            m.num_heads = num_heads[m.qqkv_projkv]\n",
    "        #            m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        completed_steps = (self.epoch + 1) * len(self.learn.dls.train)\n",
    "        current_sparsity = self.sparsity_levels[completed_steps - 1]\n",
    "        print(f'Sparsity at the end of epoch {self.epoch}: {current_sparsity*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177c932-caff-4671-aac4-5aeef5e509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2695c9-8d0e-4a10-898c-8be84317f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet18', pretrained=True, no_jit=True).eval()\n",
    "\n",
    "ignored_layers = []\n",
    "num_heads = {}\n",
    "\n",
    "#k = 0\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Linear) and m.out_features == model.num_classes:\n",
    "        ignored_layers.append(m)\n",
    "        print(\"Ignore classifier layer: \", m)\n",
    "\n",
    "    # Attention layers\n",
    "    if hasattr(m, 'num_heads'):\n",
    "        if hasattr(m, 'qkv'):\n",
    "            num_heads[m.qkv] = m.num_heads\n",
    "            print(\"Attention layer: \", m.qkv, m.num_heads)\n",
    "        elif hasattr(m, 'qkv_proj'):\n",
    "            num_heads[m.qkv_proj] = m.num_heads\n",
    "\n",
    "learn = Learner(dls, model, metrics = [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c13b65-ebad-4468-b79b-d898fd76573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5235e-e3a1-4173-b476-c5f9eeb9cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_cb = PruneCallback(pruning_ratio=0.25, schedule=onecycle_scheduler, global_pruning=True, criteria=tp.importance.GroupNormImportance(normalizer=None, target_types=[nn.modules.conv._ConvNd, nn.Linear]), num_heads=num_heads, ignored_layers=ignored_layers)\n",
    "learn.fit_one_cycle(10, 1e-4, cbs=pr_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392dfe2-e7cc-439c-bfed-6e67200be32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96bd5f-c2d9-408d-8daf-f2dace590cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_cb = PruneCallback(pruning_ratio=0.25, schedule=onecycle_scheduler, global_pruning=True, criteria=GroupNormImportance(normalizer=None, target_types=[nn.modules.conv._ConvNd, nn.Linear]), num_heads=num_heads, ignored_layers=ignored_layers)\n",
    "learn.fit_one_cycle(15, 1e-4, cbs=pr_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab7cdc-c14e-48a0-be7c-629f3bdf1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('tf_efficientnet_b3', pretrained=False, no_jit=True).eval()\n",
    "base_macs, base_params = tp.utils.count_ops_and_params(model, xb.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1b898-a769-40e7-8d9f-d6fdbadb8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_macs, pruned_params = tp.utils.count_ops_and_params(learn.model, xb.to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0e372-302a-4287-a7b9-80d19171dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MACs: %.4f G => %.4f G\"%(base_macs/1e9, pruned_macs/1e9))\n",
    "print(\"Params: %.4f M => %.4f M\"%(base_params/1e6, pruned_params/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12d6f8-498d-405d-be61-4e982aa87b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ecc6c-1825-4458-a5a4-594a1e448953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd4cfa-4a7f-414b-8514-36eab964da7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555532a-e433-4774-a7a2-710b736b8a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299a495-675d-4bde-85ce-f922acf4a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import typing\n",
    "\n",
    "from torch_pruning import function\n",
    "from torch_pruning.dependency import Group\n",
    "\n",
    "class Importance(abc.ABC):\n",
    "    \"\"\" Estimate the importance of a tp.Dependency.Group, and return an 1-D per-channel importance score.\n",
    "\n",
    "        It should accept a group as inputs, and return a 1-D tensor with the same length as the number of channels.\n",
    "        All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups.\n",
    "\n",
    "        Example:\n",
    "            ```python\n",
    "            DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224)) \n",
    "            group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )    \n",
    "            scorer = MagnitudeImportance()    \n",
    "            imp_score = scorer(group)    \n",
    "            #imp_score is a 1-D tensor with length 3 for channels [2, 6, 9]  \n",
    "            min_score = imp_score.min() \n",
    "            ``` \n",
    "    \"\"\"\n",
    "    @abc.abstractclassmethod\n",
    "    def __call__(self, group: Group) -> torch.Tensor: \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class GroupNormImportance(Importance):\n",
    "\n",
    "    def __init__(self, \n",
    "                 p: int=2, \n",
    "                 group_reduction: str=\"mean\", \n",
    "                 normalizer: str='mean', \n",
    "                 bias=False,\n",
    "                 target_types:list=[nn.modules.conv._ConvNd, nn.Linear, nn.modules.batchnorm._BatchNorm, nn.LayerNorm]):\n",
    "        self.p = p\n",
    "        self.group_reduction = group_reduction\n",
    "        self.normalizer = normalizer\n",
    "        self.target_types = target_types\n",
    "        self.bias = bias\n",
    "\n",
    "    def _lamp(self, scores): # Layer-adaptive Sparsity for the Magnitude-based Pruning\n",
    "        \"\"\"\n",
    "        Normalizing scheme for LAMP.\n",
    "        \"\"\"\n",
    "        # sort scores in an ascending order\n",
    "        sorted_scores,sorted_idx = scores.view(-1).sort(descending=False)\n",
    "        # compute cumulative sum\n",
    "        scores_cumsum_temp = sorted_scores.cumsum(dim=0)\n",
    "        scores_cumsum = torch.zeros(scores_cumsum_temp.shape,device=scores.device)\n",
    "        scores_cumsum[1:] = scores_cumsum_temp[:len(scores_cumsum_temp)-1]\n",
    "        # normalize by cumulative sum\n",
    "        sorted_scores /= (scores.sum() - scores_cumsum)\n",
    "        # tidy up and output\n",
    "        new_scores = torch.zeros(scores_cumsum.shape,device=scores.device)\n",
    "        new_scores[sorted_idx] = sorted_scores\n",
    "        \n",
    "        return new_scores.view(scores.shape)\n",
    "    \n",
    "    def _normalize(self, group_importance, normalizer):\n",
    "        if normalizer is None:\n",
    "            return group_importance\n",
    "        elif isinstance(normalizer, typing.Callable):\n",
    "            return normalizer(group_importance)\n",
    "        elif normalizer == \"sum\":\n",
    "            return group_importance / group_importance.sum()\n",
    "        elif normalizer == \"standarization\":\n",
    "            return (group_importance - group_importance.min()) / (group_importance.max() - group_importance.min()+1e-8)\n",
    "        elif normalizer == \"mean\":\n",
    "            return group_importance / group_importance.mean()\n",
    "        elif normalizer == \"max\":\n",
    "            return group_importance / group_importance.max()\n",
    "        elif normalizer == 'gaussian':\n",
    "            return (group_importance - group_importance.mean()) / (group_importance.std()+1e-8)\n",
    "        elif normalizer.startswith('sentinel'): # normalize the score with the k-th smallest element. e.g. sentinel_0.5 means median normalization\n",
    "            sentinel = float(normalizer.split('_')[1]) * len(group_importance)\n",
    "            sentinel = torch.argsort(group_importance, dim=0, descending=False)[int(sentinel)]\n",
    "            return group_importance / (group_importance[sentinel]+1e-8)\n",
    "        elif normalizer=='lamp':\n",
    "            return self._lamp(group_importance)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _reduce(self, group_imp: typing.List[torch.Tensor], group_idxs: typing.List[typing.List[int]]):\n",
    "        if len(group_imp) == 0: return group_imp\n",
    "        if self.group_reduction == 'prod':\n",
    "            reduced_imp = torch.ones_like(group_imp[0])\n",
    "        elif self.group_reduction == 'max':\n",
    "            reduced_imp = torch.ones_like(group_imp[0]) * -99999\n",
    "        else:\n",
    "            reduced_imp = torch.zeros_like(group_imp[0])\n",
    "\n",
    "        for i, (imp, root_idxs) in enumerate(zip(group_imp, group_idxs)):\n",
    "            imp = imp.to(reduced_imp.device)\n",
    "            if self.group_reduction == \"sum\" or self.group_reduction == \"mean\":\n",
    "                reduced_imp.scatter_add_(0, torch.tensor(root_idxs, device=imp.device), imp) # accumulated importance\n",
    "            elif self.group_reduction == \"max\": # keep the max importance\n",
    "                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n",
    "                selected_imp = torch.maximum(input=selected_imp, other=imp)\n",
    "                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n",
    "            elif self.group_reduction == \"prod\": # product of importance\n",
    "                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n",
    "                torch.mul(selected_imp, imp, out=selected_imp)\n",
    "                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n",
    "            elif self.group_reduction == 'first':\n",
    "                if i == 0:\n",
    "                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n",
    "            elif self.group_reduction == 'gate':\n",
    "                if i == len(group_imp)-1:\n",
    "                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n",
    "            elif self.group_reduction is None:\n",
    "                reduced_imp = torch.stack(group_imp, dim=0) # no reduction\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        \n",
    "        if self.group_reduction == \"mean\":\n",
    "            reduced_imp /= len(group_imp)\n",
    "        return reduced_imp\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, group: Group):\n",
    "        group_imp = []\n",
    "        group_idxs = []\n",
    "        # Iterate over all groups and estimate group importance\n",
    "        for i, (dep, idxs) in enumerate(group):\n",
    "            layer = dep.layer\n",
    "            prune_fn = dep.pruning_fn\n",
    "            root_idxs = group[i].root_idxs\n",
    "            if not isinstance(layer, tuple(self.target_types)):\n",
    "                continue\n",
    "            ####################\n",
    "            # Conv/Linear Output\n",
    "            ####################\n",
    "            if prune_fn in [\n",
    "                function.prune_conv_out_channels,\n",
    "                function.prune_linear_out_channels,\n",
    "            ]:\n",
    "                if hasattr(layer, \"transposed\") and layer.transposed:\n",
    "                    w = layer.weight.data.transpose(1, 0)[idxs].flatten(1)\n",
    "                else:\n",
    "                    w = layer.weight.data[idxs].flatten(1)\n",
    "                #local_imp = w.abs().pow(self.p).sum(1)\n",
    "                local_imp = w.abs().pow(self.p).mean(1)\n",
    "                group_imp.append(local_imp)\n",
    "                group_idxs.append(root_idxs)\n",
    "\n",
    "                if self.bias and layer.bias is not None:\n",
    "                    local_imp = layer.bias.data[idxs].abs().pow(self.p)\n",
    "                    group_imp.append(local_imp)\n",
    "                    group_idxs.append(root_idxs)\n",
    "\n",
    "            ####################\n",
    "            # Conv/Linear Input\n",
    "            ####################\n",
    "            elif prune_fn in [\n",
    "                function.prune_conv_in_channels,\n",
    "                function.prune_linear_in_channels,\n",
    "            ]:\n",
    "                if hasattr(layer, \"transposed\") and layer.transposed:\n",
    "                    w = (layer.weight.data).flatten(1)\n",
    "                else:\n",
    "                    w = (layer.weight.data).transpose(0, 1).flatten(1)\n",
    "                #local_imp = w.abs().pow(self.p).sum(1)\n",
    "                local_imp = w.abs().pow(self.p).mean(1)\n",
    "\n",
    "                # repeat importance for group convolutions\n",
    "                if prune_fn == function.prune_conv_in_channels and layer.groups != layer.in_channels and layer.groups != 1:\n",
    "                    local_imp = local_imp.repeat(layer.groups)\n",
    "                \n",
    "                local_imp = local_imp[idxs]\n",
    "                group_imp.append(local_imp)\n",
    "                group_idxs.append(root_idxs)\n",
    "\n",
    "            ####################\n",
    "            # BatchNorm\n",
    "            ####################\n",
    "            elif prune_fn == function.prune_batchnorm_out_channels:\n",
    "                # regularize BN\n",
    "                if layer.affine:\n",
    "                    w = layer.weight.data[idxs]\n",
    "                    local_imp = w.abs().pow(self.p)\n",
    "                    group_imp.append(local_imp)\n",
    "                    group_idxs.append(root_idxs)\n",
    "\n",
    "                    if self.bias and layer.bias is not None:\n",
    "                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n",
    "                        group_imp.append(local_imp)\n",
    "                        group_idxs.append(root_idxs)\n",
    "            ####################\n",
    "            # LayerNorm\n",
    "            ####################\n",
    "            elif prune_fn == function.prune_layernorm_out_channels:\n",
    "\n",
    "                if layer.elementwise_affine:\n",
    "                    w = layer.weight.data[idxs]\n",
    "                    local_imp = w.abs().pow(self.p)\n",
    "                    group_imp.append(local_imp)\n",
    "                    group_idxs.append(root_idxs)\n",
    "\n",
    "                    if self.bias and layer.bias is not None:\n",
    "                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n",
    "                        group_imp.append(local_imp)\n",
    "                        group_idxs.append(root_idxs)\n",
    "\n",
    "        if len(group_imp) == 0: # skip groups without parameterized layers\n",
    "            return None\n",
    "\n",
    "        group_imp = self._reduce(group_imp, group_idxs)\n",
    "        group_imp = self._normalize(group_imp, self.normalizer)\n",
    "        return group_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a68c78-ba82-4355-b34d-b7ef25b283a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e0d38-ae15-41ad-b894-a77cc060fba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd348d-1201-4d1f-9610-c06ce9124aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8b7f9-3fa9-4060-9cec-b62863fa8fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eeb162-9e8b-465d-ade7-a9b2dacce192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5bc62-d0eb-4ece-9db3-5071950efe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ff06e9-35bf-4623-97ba-3326c7a29b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_models = timm.list_models(module='resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c292803-0387-4de3-bcb6-f0960e7fd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models(exclude_filters='*vit*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ba020-7d16-43e6-ad69-ffb21d0be4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee8fa4-fd07-4098-80af-0b177e0429ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b6751-76ff-414d-b0e4-c0ce00b88f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b256f1-42df-4edd-ac27-8862d5bc1648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908c777-042c-4337-9ba0-57f4d5be4234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20e1a1-3aa5-48f2-8017-2deda97ff3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962358c6-8cdf-4db7-bdf6-792436f1080d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7234a-1711-42ca-a333-2abf4852cec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e1992-1f22-4e90-9383-99e362d258b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = timm.create_model('seresnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ffa7c-15a4-4f24-a6ae-ad9f6ab3c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e805623-0226-4efc-97bf-38c61cff6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.prune.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d105013-4076-4a3f-9d8e-c62274e57a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(16, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f228819-fbae-47ad-8562-f2a144460223",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = timm.create_model('seresnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e7daa-a62d-4526-9389-e83851178310",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(m, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa95998-1a99-498d-a58a-60b80975c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = Pruner(m, 'local', large_final, layer_type=[nn.Conv2d])\n",
    "pr.prune_model(30, round_to=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df100666-0a6e-4e54-87fa-fc26a3eb52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(m, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6067115-2d3f-4a19-8c50-e2d2f862330a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c70a55-5afe-4d9e-820c-af7b58eedc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebb3f2-f85f-4fe4-ab0d-7d96e10fe542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b5271-62d0-429e-86bd-a19f70f726b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb931e0-6956-41c9-b46e-f6e634bc446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import typing\n",
    "\n",
    "from torch_pruning import function\n",
    "from torch_pruning.dependency import Group\n",
    "\n",
    "class Importance(abc.ABC):\n",
    "    \"\"\" Estimate the importance of a tp.Dependency.Group, and return an 1-D per-channel importance score.\n",
    "\n",
    "        It should accept a group as inputs, and return a 1-D tensor with the same length as the number of channels.\n",
    "        All groups must be pruned simultaneously and thus their importance should be accumulated across channel groups.\n",
    "\n",
    "        Example:\n",
    "            ```python\n",
    "            DG = tp.DependencyGraph().build_dependency(model, example_inputs=torch.randn(1,3,224,224)) \n",
    "            group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )    \n",
    "            scorer = MagnitudeImportance()    \n",
    "            imp_score = scorer(group)    \n",
    "            #imp_score is a 1-D tensor with length 3 for channels [2, 6, 9]  \n",
    "            min_score = imp_score.min() \n",
    "            ``` \n",
    "    \"\"\"\n",
    "    @abc.abstractclassmethod\n",
    "    def __call__(self, group: Group) -> torch.Tensor: \n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class GroupNormImportance(Importance):\n",
    "\n",
    "    def __init__(self, \n",
    "                 p: int=2, \n",
    "                 group_reduction: str=\"mean\", \n",
    "                 normalizer: str='mean', \n",
    "                 bias=False,\n",
    "                 target_types:list=[nn.modules.conv._ConvNd, nn.Linear, nn.modules.batchnorm._BatchNorm, nn.LayerNorm]):\n",
    "        self.p = p\n",
    "        self.group_reduction = group_reduction\n",
    "        self.normalizer = normalizer\n",
    "        self.target_types = target_types\n",
    "        self.bias = bias\n",
    "\n",
    "    def _lamp(self, scores): # Layer-adaptive Sparsity for the Magnitude-based Pruning\n",
    "        \"\"\"\n",
    "        Normalizing scheme for LAMP.\n",
    "        \"\"\"\n",
    "        # sort scores in an ascending order\n",
    "        sorted_scores,sorted_idx = scores.view(-1).sort(descending=False)\n",
    "        # compute cumulative sum\n",
    "        scores_cumsum_temp = sorted_scores.cumsum(dim=0)\n",
    "        scores_cumsum = torch.zeros(scores_cumsum_temp.shape,device=scores.device)\n",
    "        scores_cumsum[1:] = scores_cumsum_temp[:len(scores_cumsum_temp)-1]\n",
    "        # normalize by cumulative sum\n",
    "        sorted_scores /= (scores.sum() - scores_cumsum)\n",
    "        # tidy up and output\n",
    "        new_scores = torch.zeros(scores_cumsum.shape,device=scores.device)\n",
    "        new_scores[sorted_idx] = sorted_scores\n",
    "        \n",
    "        return new_scores.view(scores.shape)\n",
    "    \n",
    "    def _normalize(self, group_importance, normalizer):\n",
    "        if normalizer is None:\n",
    "            return group_importance\n",
    "        elif isinstance(normalizer, typing.Callable):\n",
    "            return normalizer(group_importance)\n",
    "        elif normalizer == \"sum\":\n",
    "            return group_importance / group_importance.sum()\n",
    "        elif normalizer == \"standarization\":\n",
    "            return (group_importance - group_importance.min()) / (group_importance.max() - group_importance.min()+1e-8)\n",
    "        elif normalizer == \"mean\":\n",
    "            return group_importance / group_importance.mean()\n",
    "        elif normalizer == \"max\":\n",
    "            return group_importance / group_importance.max()\n",
    "        elif normalizer == 'gaussian':\n",
    "            return (group_importance - group_importance.mean()) / (group_importance.std()+1e-8)\n",
    "        elif normalizer.startswith('sentinel'): # normalize the score with the k-th smallest element. e.g. sentinel_0.5 means median normalization\n",
    "            sentinel = float(normalizer.split('_')[1]) * len(group_importance)\n",
    "            sentinel = torch.argsort(group_importance, dim=0, descending=False)[int(sentinel)]\n",
    "            return group_importance / (group_importance[sentinel]+1e-8)\n",
    "        elif normalizer=='lamp':\n",
    "            return self._lamp(group_importance)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _reduce(self, group_imp: typing.List[torch.Tensor], group_idxs: typing.List[typing.List[int]]):\n",
    "        if len(group_imp) == 0: return group_imp\n",
    "        if self.group_reduction == 'prod':\n",
    "            reduced_imp = torch.ones_like(group_imp[0])\n",
    "        elif self.group_reduction == 'max':\n",
    "            reduced_imp = torch.ones_like(group_imp[0]) * -99999\n",
    "        else:\n",
    "            reduced_imp = torch.zeros_like(group_imp[0])\n",
    "\n",
    "        for i, (imp, root_idxs) in enumerate(zip(group_imp, group_idxs)):\n",
    "            imp = imp.to(reduced_imp.device)\n",
    "            if self.group_reduction == \"sum\" or self.group_reduction == \"mean\":\n",
    "                reduced_imp.scatter_add_(0, torch.tensor(root_idxs, device=imp.device), imp) # accumulated importance\n",
    "            elif self.group_reduction == \"max\": # keep the max importance\n",
    "                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n",
    "                selected_imp = torch.maximum(input=selected_imp, other=imp)\n",
    "                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n",
    "            elif self.group_reduction == \"prod\": # product of importance\n",
    "                selected_imp = torch.index_select(reduced_imp, 0, torch.tensor(root_idxs, device=imp.device))\n",
    "                torch.mul(selected_imp, imp, out=selected_imp)\n",
    "                reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), selected_imp)\n",
    "            elif self.group_reduction == 'first':\n",
    "                if i == 0:\n",
    "                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n",
    "            elif self.group_reduction == 'gate':\n",
    "                if i == len(group_imp)-1:\n",
    "                    reduced_imp.scatter_(0, torch.tensor(root_idxs, device=imp.device), imp)\n",
    "            elif self.group_reduction is None:\n",
    "                reduced_imp = torch.stack(group_imp, dim=0) # no reduction\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        \n",
    "        if self.group_reduction == \"mean\":\n",
    "            reduced_imp /= len(group_imp)\n",
    "        return reduced_imp\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, group: Group):\n",
    "        group_imp = []\n",
    "        group_idxs = []\n",
    "        # Iterate over all groups and estimate group importance\n",
    "        for i, (dep, idxs) in enumerate(group):\n",
    "            layer = dep.layer\n",
    "            prune_fn = dep.pruning_fn\n",
    "            root_idxs = group[i].root_idxs\n",
    "            if not isinstance(layer, tuple(self.target_types)):\n",
    "                continue\n",
    "            ####################\n",
    "            # Conv/Linear Output\n",
    "            ####################\n",
    "            if prune_fn in [\n",
    "                function.prune_conv_out_channels,\n",
    "                function.prune_linear_out_channels,\n",
    "            ]:\n",
    "                if hasattr(layer, \"transposed\") and layer.transposed:\n",
    "                    w = layer.weight.data.transpose(1, 0)[idxs].flatten(1)\n",
    "                else:\n",
    "                    w = layer.weight.data[idxs].flatten(1)\n",
    "                #local_imp = w.abs().pow(self.p).sum(1)\n",
    "                local_imp = w.abs().pow(self.p).mean(1)\n",
    "                group_imp.append(local_imp)\n",
    "                group_idxs.append(root_idxs)\n",
    "\n",
    "                if self.bias and layer.bias is not None:\n",
    "                    local_imp = layer.bias.data[idxs].abs().pow(self.p)\n",
    "                    group_imp.append(local_imp)\n",
    "                    group_idxs.append(root_idxs)\n",
    "\n",
    "            ####################\n",
    "            # Conv/Linear Input\n",
    "            ####################\n",
    "            elif prune_fn in [\n",
    "                function.prune_conv_in_channels,\n",
    "                function.prune_linear_in_channels,\n",
    "            ]:\n",
    "                if hasattr(layer, \"transposed\") and layer.transposed:\n",
    "                    w = (layer.weight.data).flatten(1)\n",
    "                else:\n",
    "                    w = (layer.weight.data).transpose(0, 1).flatten(1)\n",
    "                #local_imp = w.abs().pow(self.p).sum(1)\n",
    "                local_imp = w.abs().pow(self.p).mean(1)\n",
    "\n",
    "                # repeat importance for group convolutions\n",
    "                if prune_fn == function.prune_conv_in_channels and layer.groups != layer.in_channels and layer.groups != 1:\n",
    "                    local_imp = local_imp.repeat(layer.groups)\n",
    "                \n",
    "                local_imp = local_imp[idxs]\n",
    "                group_imp.append(local_imp)\n",
    "                group_idxs.append(root_idxs)\n",
    "\n",
    "            ####################\n",
    "            # BatchNorm\n",
    "            ####################\n",
    "            elif prune_fn == function.prune_batchnorm_out_channels:\n",
    "                # regularize BN\n",
    "                if layer.affine:\n",
    "                    w = layer.weight.data[idxs]\n",
    "                    local_imp = w.abs().pow(self.p)\n",
    "                    group_imp.append(local_imp)\n",
    "                    group_idxs.append(root_idxs)\n",
    "\n",
    "                    if self.bias and layer.bias is not None:\n",
    "                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n",
    "                        group_imp.append(local_imp)\n",
    "                        group_idxs.append(root_idxs)\n",
    "            ####################\n",
    "            # LayerNorm\n",
    "            ####################\n",
    "            elif prune_fn == function.prune_layernorm_out_channels:\n",
    "\n",
    "                if layer.elementwise_affine:\n",
    "                    w = layer.weight.data[idxs]\n",
    "                    local_imp = w.abs().pow(self.p)\n",
    "                    group_imp.append(local_imp)\n",
    "                    group_idxs.append(root_idxs)\n",
    "\n",
    "                    if self.bias and layer.bias is not None:\n",
    "                        local_imp = layer.bias.data[idxs].abs().pow(self.p)\n",
    "                        group_imp.append(local_imp)\n",
    "                        group_idxs.append(root_idxs)\n",
    "\n",
    "        if len(group_imp) == 0: # skip groups without parameterized layers\n",
    "            return None\n",
    "\n",
    "        group_imp = self._reduce(group_imp, group_idxs)\n",
    "        group_imp = self._normalize(group_imp, self.normalizer)\n",
    "        return group_imp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a36b8-a2a2-4767-8757-690eae72b8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
