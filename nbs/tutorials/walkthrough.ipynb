{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2bb19b73",
   "metadata": {},
   "source": [
    "---\n",
    "description: Walkthrough \n",
    "output-file: tutorial.walkthrough.html\n",
    "title: Walkthrough\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fasterbench.benchmark import evaluate_cpu_speed, get_model_size, get_num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd812e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "\n",
    "def get_dls(size, bs):\n",
    "    path = URLs.IMAGENETTE_160\n",
    "    source = untar_data(path)\n",
    "    blocks=(ImageBlock, CategoryBlock)\n",
    "    tfms = [RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)]\n",
    "    batch_tfms = [Normalize.from_stats(*imagenet_stats)]\n",
    "\n",
    "    csv_file = 'noisy_imagenette.csv'\n",
    "    inp = pd.read_csv(source/csv_file)\n",
    "    dblock = DataBlock(blocks=blocks,\n",
    "               splitter=ColSplitter(),\n",
    "               get_x=ColReader('path', pref=source),\n",
    "               get_y=ColReader(f'noisy_labels_0'),\n",
    "               item_tfms=tfms,\n",
    "               batch_tfms=batch_tfms)\n",
    "\n",
    "    return dblock.dataloaders(inp, path=source, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "size, bs = 128, 32\n",
    "dls = get_dls(size, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2375b",
   "metadata": {},
   "source": [
    "Let's start with a bit of context for the purpose of the demonstration. Imagine that we want to deploy a **VGG16** model on a mobile device that has limited storage capacity and that our task requires our model to run sufficiently fast. It is known that parameters and speed efficiency are not the strong points of **VGG16** but let's see what we can do with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4137a",
   "metadata": {},
   "source": [
    "Let's first check the number of parameters and the inference time of **VGG16**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 537.30 MB (disk), 134309962 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(learn.model)\n",
    "disk_size = get_model_size(learn.model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8894668-e387-426c-9c9f-c3348910d430",
   "metadata": {},
   "source": [
    "So our model has 134 millions parameters and needs 537MB of disk space in order to be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model.eval().to('cpu')\n",
    "x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d310a-e937-4873-8199-20151be6de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 25.30ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference Speed: {evaluate_cpu_speed(learn.model, x[0][None])[0]:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ed0cf-26bc-4b17-9033-18f730e4b3f5",
   "metadata": {},
   "source": [
    "And it takes **26ms** to perform inference on a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94bce0",
   "metadata": {},
   "source": [
    "Snap ! This is more than we can afford for deployment, ideally we would like our model to take only half of that...but should we give up ? Nope, there are actually a lot of techniques that we can use to help reducing the size and improve the speed of our models! Let's see how to apply them with **FasterAI**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466427f7",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7422e50",
   "metadata": {},
   "source": [
    "We will first train our **VGG16** model to have a **baseline** of what performance we should expect from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f5a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.090894</td>\n",
       "      <td>1.883946</td>\n",
       "      <td>0.350318</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.687373</td>\n",
       "      <td>1.445368</td>\n",
       "      <td>0.534013</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.349907</td>\n",
       "      <td>2.415507</td>\n",
       "      <td>0.379108</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.205638</td>\n",
       "      <td>1.525106</td>\n",
       "      <td>0.527134</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.087587</td>\n",
       "      <td>1.118419</td>\n",
       "      <td>0.635924</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.925177</td>\n",
       "      <td>0.992042</td>\n",
       "      <td>0.674650</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.781664</td>\n",
       "      <td>0.781098</td>\n",
       "      <td>0.756178</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.688618</td>\n",
       "      <td>0.643063</td>\n",
       "      <td>0.788790</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.637162</td>\n",
       "      <td>0.611780</td>\n",
       "      <td>0.806879</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.622672</td>\n",
       "      <td>0.598242</td>\n",
       "      <td>0.806369</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c07eb",
   "metadata": {},
   "source": [
    "So we would like our network to have comparable accuracy but fewer parameters and running faster... And the first technique that we will show how to use is called **Knowledge Distillation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e432c28",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503c5d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3370f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f627246",
   "metadata": {},
   "source": [
    "## **Knowledge Distillation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25eac04",
   "metadata": {},
   "source": [
    "Knowledge distillation is a simple yet very efficient way to train a model. It was introduced in 2006 by [Caruana et al.](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf). The main idea behind is to use a small model (called the **student**) to approximate the function learned by a larger and high-performing model (called the **teacher**). This can be done by using the large model to pseudo-label the data. This idea has been used very recently to [break the state-of-the-art accuracy on ImageNet](https://arxiv.org/abs/1911.04252).\n",
    "\n",
    "When we train our model for classification, we usually use a softmax as last layer. This softmax has the particularity to squish low value logits towards **0**, and the highest logit towards **1**. This has for effect to completely lose all the inter-class information, or what is sometimes called the *dark knowledge*. This is the information that is valuable and that we want to transfer from the teacher to the student.\n",
    "\n",
    "To do so, we still use a regular classification loss but at the same time, we'll use another loss, computed between the *softened* logits of the teacher (our *soft labels*) and the *softened* logits of the student (our *soft predictions*). Those soft values are obtained when you use a **soft-softmax**, that avoids squishing the values at its output. Our implementation follows [this paper](http://cs230.stanford.edu/files_winter_2018/projects/6940224.pdf) and the basic principle of training is represented in the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09a78e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "![](../imgs/distill.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e8947",
   "metadata": {},
   "source": [
    "To use **Knowledge Distillation** with FasterAI, you only need to use this callback when training your student model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149d8c9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<blockquote>\n",
    "<pre><b><i> KnowledgeDistillation(teacher.model, loss) </i></b></pre>\n",
    "<p style=\"font-size: 15px\"><i>\n",
    "You only need to give to the callback function your teacher learner. Behind the scenes, FasterAI will take care of making your model train using knowledge distillation.\n",
    "</i></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83011770",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.distill.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60472a",
   "metadata": {},
   "source": [
    "The first thing to do is to find a teacher, which can be any model, that preferrably performs well. We will chose **VGG19** for our demonstration. To make sure it performs better than our **VGG16** model, let's start from a pretrained version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be1f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.950298</td>\n",
       "      <td>0.347253</td>\n",
       "      <td>0.899618</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.492183</td>\n",
       "      <td>0.219459</td>\n",
       "      <td>0.932229</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422360</td>\n",
       "      <td>0.210169</td>\n",
       "      <td>0.934013</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher = vision_learner(dls, models.vgg19_bn, metrics=[accuracy])\n",
    "teacher.fit_one_cycle(3, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e84276",
   "metadata": {},
   "source": [
    "Our teacher has **94%** of accuracy which is pretty good, it is ready to take a student under its wing. So let's create our student model and train it with the **Knowledge Distillation** callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a3004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.685840</td>\n",
       "      <td>5.313475</td>\n",
       "      <td>0.425478</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.180610</td>\n",
       "      <td>3.716927</td>\n",
       "      <td>0.570955</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.470545</td>\n",
       "      <td>3.690420</td>\n",
       "      <td>0.596433</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.961368</td>\n",
       "      <td>2.822396</td>\n",
       "      <td>0.678726</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.564142</td>\n",
       "      <td>2.741690</td>\n",
       "      <td>0.673885</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.254859</td>\n",
       "      <td>2.236402</td>\n",
       "      <td>0.752102</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.985762</td>\n",
       "      <td>1.871943</td>\n",
       "      <td>0.780382</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.796154</td>\n",
       "      <td>1.660790</td>\n",
       "      <td>0.808917</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.572589</td>\n",
       "      <td>1.561121</td>\n",
       "      <td>0.819618</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.575104</td>\n",
       "      <td>1.572565</td>\n",
       "      <td>0.823185</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "student = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\n",
    "kd_cb = KnowledgeDistillationCallback(teacher.model, SoftTarget)\n",
    "student.fit_one_cycle(10, 1e-4, cbs=kd_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc60d7d",
   "metadata": {},
   "source": [
    "And we can see that indeed, the knowledge of the teacher was useful for the student, as it is clearly overperforming the vanilla **VGG16**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a138b",
   "metadata": {},
   "source": [
    "Ok, so now we are able to get more from a given model which is kind of cool ! With some experimentations we could come up with a model smaller than **VGG16** but able to reach the same performance as our baseline! You can try to find it by yourself later, but for now let's continue with the next technique !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddb841",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f2e6f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33622f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b17b3",
   "metadata": {},
   "source": [
    "## **Sparsifying**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87919d",
   "metadata": {},
   "source": [
    "Now that we have a student model that is performing better than our baseline, we have some room to compress it. And we'll start by making the network sparse. As explained in a previous [article](https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html), there are many ways leading to a sparse network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065c8dc",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93883e",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "Usually, the process of making a network sparse is called Pruning. I prefer using the term Pruning when parameters are **actually** removed from the network, which we will do in the next section.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b665f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "![](../imgs/schedules.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588836e",
   "metadata": {},
   "source": [
    "By default, FasterAI uses the **Automated Gradual Pruning** paradigm as it removes parameters as the model trains and doesn't require to pretrain the model, so it is usually much faster. In FasterAI, this is also managed by using a callback, that will replace the *least important* parameters of your model by zeroes during the training. The callback has a wide variety of parameters to tune your **Sparsifying** operation, let's take a look at them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e38ab",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc8b14",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "    <pre><b><i>SparsifyCallback(learn, sparsity, granularity, context, criteria, schedule)</i></b></pre>\n",
    "\n",
    "<ul><i>\n",
    "<li style=\"font-size:15px\"><b>sparsity</b>: the percentage of sparsity that you want in your network </li>\n",
    "<li style=\"font-size:15px\"><b>granularity</b>: on what granularity you want the sparsification to be operated</li>\n",
    "<li style=\"font-size:15px\"><b>context</b>: either <code>local</code> or <code>global</code>, will affect the selection of parameters to be choosen in each layer independently (<code>local</code>) or on the whole network (<code>global</code>).</li>\n",
    "<li style=\"font-size:15px\"><b>criteria</b>: the criteria used to select which parameters to remove (currently supported: <code>l1</code>, <code>taylor</code>)</li>\n",
    "<li style=\"font-size:15px\"><b>schedule</b>: which schedule you want to follow for the sparsification (currently supported: <a href=\"https://docs.fast.ai/callback.html#Annealing-functions\">any scheduling function of fastai</a>, i.e <code>linear</code>, <code>cosine</code>, ... and <code>gradual</code>, common schedules such as One-Shot, Iterative or <a href=\"https://openreview.net/pdf?id=Sy1iIDkPM\">Automated Gradual</a>)</li>\n",
    "</i></ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f91b4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe7ef5",
   "metadata": {},
   "source": [
    "**But let's come back to our example!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594743cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from fasterai.sparse.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271dd40c",
   "metadata": {},
   "source": [
    "Here, we will make our network **40%** sparse, and remove entire **filters**, selected **locally** and based on **L1 norm**. We will train with a learning rate a bit smaller to be gentle with our network because it has already been trained. The **scheduling** selected is cosinusoidal, so the pruning starts and ends quite slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58273b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of filter until a sparsity of [50]%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.575628</td>\n",
       "      <td>0.575193</td>\n",
       "      <td>0.819873</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.560112</td>\n",
       "      <td>0.558437</td>\n",
       "      <td>0.822166</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.567398</td>\n",
       "      <td>0.556176</td>\n",
       "      <td>0.826752</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.530564</td>\n",
       "      <td>0.554012</td>\n",
       "      <td>0.825223</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.547307</td>\n",
       "      <td>0.576913</td>\n",
       "      <td>0.815541</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.598721</td>\n",
       "      <td>0.588787</td>\n",
       "      <td>0.809682</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.627638</td>\n",
       "      <td>0.609079</td>\n",
       "      <td>0.806624</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.621662</td>\n",
       "      <td>0.626054</td>\n",
       "      <td>0.800764</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.645815</td>\n",
       "      <td>0.608645</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.639842</td>\n",
       "      <td>0.591584</td>\n",
       "      <td>0.806115</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: [1.22]%\n",
      "Sparsity at the end of epoch 1: [4.77]%\n",
      "Sparsity at the end of epoch 2: [10.31]%\n",
      "Sparsity at the end of epoch 3: [17.27]%\n",
      "Sparsity at the end of epoch 4: [25.0]%\n",
      "Sparsity at the end of epoch 5: [32.73]%\n",
      "Sparsity at the end of epoch 6: [39.69]%\n",
      "Sparsity at the end of epoch 7: [45.23]%\n",
      "Sparsity at the end of epoch 8: [48.78]%\n",
      "Sparsity at the end of epoch 9: [50.0]%\n",
      "Final Sparsity: [50.0]%\n",
      "Sparsity in Conv2d 2: 0.00%\n",
      "Sparsity in Conv2d 5: 0.00%\n",
      "Sparsity in Conv2d 9: 0.00%\n",
      "Sparsity in Conv2d 12: 0.00%\n",
      "Sparsity in Conv2d 16: 0.00%\n",
      "Sparsity in Conv2d 19: 0.00%\n",
      "Sparsity in Conv2d 22: 0.00%\n",
      "Sparsity in Conv2d 26: 62.11%\n",
      "Sparsity in Conv2d 29: 68.55%\n",
      "Sparsity in Conv2d 32: 74.41%\n",
      "Sparsity in Conv2d 36: 71.88%\n",
      "Sparsity in Conv2d 39: 68.55%\n",
      "Sparsity in Conv2d 42: 66.99%\n"
     ]
    }
   ],
   "source": [
    "sp_cb = SparsifyCallback(sparsity=50, granularity='filter', context='global', criteria=large_final, schedule=cos)\n",
    "student.fit(10, 1e-5, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bffc84",
   "metadata": {},
   "source": [
    "Our network now has **50%** of its filters composed entirely of zeroes, without even losing accuracy. Obviously, choosing a higher sparsity makes it more difficult for the network to keep a similar accuracy. Other parameters can also widely change the behaviour of our sparsification process. For example choosing a more fine-grained sparsity usually leads to better results but is then more difficult to take advantage of in terms of speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce8e37",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511cfb0",
   "metadata": {},
   "source": [
    "Let's now see how much we gained in terms of speed. Because we removed **50%** of convolution filters, we should expect crazy speed-up right ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f940d9-8bad-49de-ba48-8b40582a822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 26.31ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dce1ad-c470-4419-ba1d-986a76220601",
   "metadata": {},
   "source": [
    "Well actually, no. We didn't remove any parameters, we just replaced some by zeroes, remember? The amount of parameters is still the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2967a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 537.30 MB (disk), 134309962 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(student.model)\n",
    "disk_size = get_model_size(student.model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4be3ee",
   "metadata": {},
   "source": [
    "Which leads us to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004632d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa03dcd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8e603",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc00ce",
   "metadata": {},
   "source": [
    "## **Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf28720",
   "metadata": {},
   "source": [
    "Why don't we see any acceleration even though we removed half of the parameters? That's because natively, our **GPU** does not know that our matrices are sparse and thus isn't able to accelerate the computation. The easiest work around, is to **physically** remove the parameters we zeroed-out. But this operation requires to change the architecture of the network. \n",
    "\n",
    "This pruning only works if we remove entire filters as it is the only case where we can change the architecture accordingly. Hopefully, sparse computations will [soon be available](https://pytorch.org/docs/stable/sparse.html) on common deep learning librairies so this section will become useless in the future.\n",
    "\n",
    "<br>\n",
    "\n",
    "Here is what it looks like with fasterai:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33454908",
   "metadata": {},
   "source": [
    "![](../imgs/pruning_filters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff5626",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de369176",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "    <pre><b><i>PruneCallback(learn, sparsity, context, criteria, schedule)</i></b></pre>\n",
    "\n",
    "<ul><i>\n",
    "<li style=\"font-size:15px\"><b>sparsity</b>: the percentage of sparsity that you want in your network </li>\n",
    "<li style=\"font-size:15px\"><b>context</b>: either <code>local</code> or <code>global</code>, will affect the selection of parameters to be choosen in each layer independently (<code>local</code>) or on the whole network (<code>global</code>).</li>\n",
    "<li style=\"font-size:15px\"><b>criteria</b>: the criteria used to select which parameters to remove (currently supported: <code>l1</code>, <code>taylor</code>)</li>\n",
    "<li style=\"font-size:15px\"><b>schedule</b>: which schedule you want to follow for the sparsification (currently supported: <a href=\"https://docs.fast.ai/callback.html#Annealing-functions\">any scheduling function of fastai</a>, i.e <code>linear</code>, <code>cosine</code>, ... and <code>gradual</code>, common schedules such as One-Shot, Iterative or <a href=\"https://openreview.net/pdf?id=Sy1iIDkPM\">Automated Gradual</a>)</li>\n",
    "</i></ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97138db9",
   "metadata": {},
   "source": [
    "So in the case of our example, it gives: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a85c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.prune.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f467f",
   "metadata": {},
   "source": [
    "Let's now see what our model is capable of now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdfe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring output layer: Linear(in_features=4096, out_features=10, bias=True)\n",
      "Total ignored layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.566409</td>\n",
       "      <td>0.587551</td>\n",
       "      <td>0.810191</td>\n",
       "      <td>01:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.573262</td>\n",
       "      <td>0.604812</td>\n",
       "      <td>0.807643</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.752223</td>\n",
       "      <td>0.762330</td>\n",
       "      <td>0.807134</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.795834</td>\n",
       "      <td>0.791169</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.806404</td>\n",
       "      <td>0.787634</td>\n",
       "      <td>0.807134</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 1.94%\n",
      "Sparsity at the end of epoch 1: 19.96%\n",
      "Sparsity at the end of epoch 2: 45.82%\n",
      "Sparsity at the end of epoch 3: 49.74%\n",
      "Sparsity at the end of epoch 4: 50.00%\n"
     ]
    }
   ],
   "source": [
    "pr_cb = PruneCallback(pruning_ratio=50, context='global', criteria=large_final, schedule=one_cycle)\n",
    "student.fit(5, 1e-5, cbs=pr_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 186.72 MB (disk), 46667503 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(student.model)\n",
    "disk_size = get_model_size(student.model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27e756",
   "metadata": {},
   "source": [
    "And in terms of speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b321078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 15.21ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference Speed: {evaluate_cpu_speed(student.model, x[0][None])[0]:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a98866",
   "metadata": {},
   "source": [
    "Yay ! Now we can talk ! Let's just double check that our accuracy is unchanged and that we didn't mess up somewhere:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e346f8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829ab1a",
   "metadata": {},
   "source": [
    "And there is actually more that we can do ! Let's keep going ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec57c8b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fbc65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2f6f3",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c0ab6",
   "metadata": {},
   "source": [
    "## **Batch Normalization Folding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367fe86",
   "metadata": {},
   "source": [
    "**Batch Normalization Folding** is a really easy to implement and straightforward idea. The gist is that batch normalization is nothing more than a normalization of the input data at each layer. Moreover, at inference time, the batch statistics used for this normalization are fixed. We can thus incorporate the normalization process directly in the convolution by changing its weights and completely remove the batch normalization layers, which is a gain both in terms of parameters and in terms of computations. For a more in-depth explaination, see this [blog post](https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html). \n",
    "\n",
    "This is how to use it with FasterAI:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9080d",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "<pre><b><i>bn_folder = BN_Folder()\n",
    "bn_folder.fold(learn.model))</i></b></pre>\n",
    "<p style=\"font-size: 15px\"><i>\n",
    "Again, you only need to pass your model and FasterAI takes care of the rest. For models built using the nn.Sequential, you don't need to change anything. For others, if you want to see speedup and compression, you actually need to subclass your model to remove the batch norm from the parameters and from the <code>forward</code> method of your network.\n",
    "</i></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825f7f8",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "This operation should also be lossless as it redefines the convolution to take batch norm into account and is thus equivalent.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3428ae",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.misc.bn_folding import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47386fcb",
   "metadata": {},
   "source": [
    "Let's do this with our model ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_f = BN_Folder()\n",
    "folded_model = bn_f.fold(student.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac1b36",
   "metadata": {},
   "source": [
    "The parameters drop is generally not that significant, especially in a network such as **VGG** where almost all parameters are contained in the FC layers but, hey, any gain is good to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c4713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 186.66 MB (disk), 46662179 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(folded_model)\n",
    "disk_size = get_model_size(folded_model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1986747",
   "metadata": {},
   "source": [
    "Now that we removed the batch normalization layers, we should again see a speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56577a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 66.31ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference Speed: {evaluate_cpu_speed(folded_model, x[0][None])[0]:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4081c08",
   "metadata": {},
   "source": [
    "Again, let's double check that we didn't mess up somewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa8b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.7876350283622742,0.8068789839744568]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folded_learner = Learner(dls, folded_model, metrics=[accuracy])\n",
    "folded_learner.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9893ca",
   "metadata": {},
   "source": [
    "And we're still not done yet ! As we know for **VGG16**, most of the parameters are comprised in the fully-connected layers so there should be something that we can do about it, right ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d836c2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37add588",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f295e9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c617c7d",
   "metadata": {},
   "source": [
    "## **FC Layers Factorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31eed16",
   "metadata": {},
   "source": [
    "We can indeed, factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an **SVD** decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: $U \\Sigma V^T$. With $\\Sigma$ being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value $k$ of singular values to keep and modify matrices $U$ and $V^T$ accordingly. The resulting will be an approximation of the initial matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b47a5f",
   "metadata": {},
   "source": [
    "![](../imgs/svd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c682d",
   "metadata": {},
   "source": [
    "In FasterAI, to decompose the fully-connected layers of your model, here is what you need to do:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22633913",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "<pre><b><i>FCD = FCDecomposer()\n",
    "decomposed_model = FCD.decompose(model, percent_removed)</i></b></pre>\n",
    "<p style=\"font-size: 15px\"><i>\n",
    "    The <code>percent_removed</code> corresponds to the percentage of singular values removed (<i>k</i> value above).\n",
    "</i></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f923f",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "This time, the decomposition is not exact, so we expect a drop in performance afterwards and further retraining will be needed.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c05749",
   "metadata": {},
   "source": [
    "Which gives with our example, if we only want to keep half of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b20be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.misc.fc_decomposer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_decomposer = FC_Decomposer()\n",
    "decomposed_model = fc_decomposer.decompose(folded_learner.model, percent_removed=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d14c98",
   "metadata": {},
   "source": [
    "How many parameters do we have now ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 124.11 MB (disk), 28668858 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(decomposed_model)\n",
    "disk_size = get_model_size(decomposed_model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717dba4",
   "metadata": {},
   "source": [
    "And how much time did we gain ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 63.97ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference Speed: {evaluate_cpu_speed(decomposed_model, x[0][None])[0]:.2f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53ecb2",
   "metadata": {},
   "source": [
    "We actually get a network that is a little bit slower, but at the expense of reducing the by 10M the number of parameter. This is thus a matter of compromise between network weight and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f5299",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de463984",
   "metadata": {},
   "source": [
    "However, this technique is an approximation so it is not lossless, so we should retrain our network a bit to recover its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d2ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.930858</td>\n",
       "      <td>0.864517</td>\n",
       "      <td>0.708025</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.772294</td>\n",
       "      <td>0.793660</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.725475</td>\n",
       "      <td>0.724417</td>\n",
       "      <td>0.788790</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.665951</td>\n",
       "      <td>0.705446</td>\n",
       "      <td>0.796433</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.647845</td>\n",
       "      <td>0.682160</td>\n",
       "      <td>0.801274</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_learner = Learner(dls, decomposed_model, metrics=[accuracy])\n",
    "final_learner.fit_one_cycle(5, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5280d",
   "metadata": {},
   "source": [
    "This operation is usually less useful for more recent architectures as they usually do not have that many parameters in their fully-connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750f78f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a6e58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0ae5b",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.quantize.quantize_callback import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17cb74-360e-42de-92ee-55d56ecdede9",
   "metadata": {},
   "source": [
    "Now that we have removed every superfluous parameter that we could, we can still continue to compress our model. A common way to do so is now to reduce the precision of each parameter in the network, making it considerably smaller. Such an approach is called **Quantization** and won't affect the total number of parameter but will make each one of them smaller to store, on top of making computations faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732d78a-2613-4e4b-88a2-947a5acc984a",
   "metadata": {},
   "source": [
    "In **FasterAI**, quantization can be done in a static way, i.e. apply quantization to the model, also called Post-Training Quantization. It also can be applied dynamically during the training, also called Quantization-Aware Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938555c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.707208</td>\n",
       "      <td>0.709907</td>\n",
       "      <td>0.793121</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.672272</td>\n",
       "      <td>0.697823</td>\n",
       "      <td>0.796178</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.661783</td>\n",
       "      <td>0.658611</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.587255</td>\n",
       "      <td>0.646317</td>\n",
       "      <td>0.812229</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.565559</td>\n",
       "      <td>0.646682</td>\n",
       "      <td>0.811720</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_learner.fit_one_cycle(5, 1e-5, cbs=QuantizeCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390878b-4fab-4447-9fd8-77bf57a0dea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 10.46ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference Speed: {evaluate_cpu_speed(final_learner.model, x[0][None])[0]:.2f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad298b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def count_parameters_quantized(model):\n",
    "    total_params = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.modules.conv.Conv2d) or \\\n",
    "           isinstance(module, torch.nn.Linear) or \\\n",
    "           isinstance(module, torch.ao.nn.quantized.modules.conv.Conv2d) or \\\n",
    "           isinstance(module, torch.ao.nn.quantized.modules.linear.Linear):\n",
    "            \n",
    "            total_params += module.weight().numel()\n",
    "            \n",
    "            if module.bias() is not None:\n",
    "                total_params += module.bias().numel()\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d556d-24db-40d9-a5ae-21f73306a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 28.85 MB (disk), 28,668,858 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = count_parameters_quantized(final_learner.model)\n",
    "disk_size = get_model_size(final_learner.model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b7f18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d45e00-3c21-4ea2-98c2-aaf07ef9741f",
   "metadata": {},
   "source": [
    "## Extra Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35302d58-f9d0-489f-8c0a-46059c5c32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.misc.cpu_optimizer import accelerate_model_for_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0241dd-34c5-4dff-8821-7ff583e835da",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = accelerate_model_for_cpu(final_learner.model, x[0][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc8685-a358-464c-ad76-14c8e5985cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Speed: 9.33ms\n"
     ]
    }
   ],
   "source": [
    "print(f'Inference Speed: {evaluate_cpu_speed(final_model, x[0][None])[0]:.2f}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc5557-b356-4181-a156-d673e90f2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 28.85 MB (disk), 0 parameters\n"
     ]
    }
   ],
   "source": [
    "num_parameters = get_num_parameters(final_model)\n",
    "disk_size = get_model_size(final_model)\n",
    "print(f\"Model Size: {disk_size / 1e6:.2f} MB (disk), {num_parameters:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8d75b-a6b6-4e6e-b8a3-8bd98c306511",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ee7dc",
   "metadata": {},
   "source": [
    "So to recap, we saw in this article how to use fasterai to: <br>\n",
    "1. Make a student model learn from a teacher model (**Knowledge Distillation**) <br>\n",
    "2. Make our network sparse (**Sparsifying**) <br> \n",
    "3. Optionnaly physically remove the zero-filters (**Pruning**) <br>\n",
    "4. Remove the batch norm layers (**Batch Normalization Folding**) <br> \n",
    "5. Approximate our big fully-connected layers by smaller ones (**Fully-Connected Layers Factorization**) <br>\n",
    "6. Quantize the model to reduce the precision of the weights (**Quantization**) <br>\n",
    "7. Extra acceleration techniques to further optimize the speed of our network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374891a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26149076",
   "metadata": {},
   "source": [
    "And we saw that by applying those, we could reduce our **VGG16** model from **537 MB** of parameters down to **46 MB** (11x compression), and also speed-up the inference from **26.3ms** to **9.9ms** (2.6x speed-up) without any drop in accuracy compared to the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1aee0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d25af",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "Please keep in mind that the techniques presented above are not magic 🧙‍♂️, so do not expect to see a 200% speedup and compression everytime. What you can achieve highly depend on the architecture that you are using (some are already speed/parameter efficient by design) or the task it is doing (some datasets are so easy that you can remove almost all your network without seeing a drop in performance)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb27f61",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
