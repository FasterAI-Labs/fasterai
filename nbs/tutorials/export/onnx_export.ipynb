{
 "cells": [
  {
   "cell_type": "raw",
   "id": "frontmatter",
   "metadata": {},
   "source": [
    "---\n",
    "description: Export compressed models to ONNX for deployment\n",
    "output-file: tutorial.onnx_export.html\n",
    "title: ONNX Export Tutorial\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from fastai.vision.all import *\n",
    "from fasterai.sparse.all import *\n",
    "from fasterai.misc.all import *\n",
    "from fasterai.export.all import *\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "After compressing a model with fasterai, you'll want to **deploy** it. ONNX (Open Neural Network Exchange) is the standard format for deploying models across different platforms and runtimes.\n",
    "\n",
    "### Why Export to ONNX?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Portability** | Run on any platform: servers, mobile, edge devices, browsers |\n",
    "| **Performance** | ONNX Runtime is highly optimized for inference |\n",
    "| **Quantization** | Apply additional INT8 quantization during export |\n",
    "| **No Python needed** | Deploy without Python dependencies |\n",
    "\n",
    "### The Deployment Pipeline\n",
    "\n",
    "```\n",
    "Train → Compress (prune/sparsify/quantize) → Fold BN → Export ONNX → Deploy\n",
    "```\n",
    "\n",
    "This tutorial walks through the complete pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Training\n",
    "\n",
    "First, let's train a model that we'll later compress and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m files = get_image_files(path/\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlabel_func\u001b[39m(f): \u001b[38;5;28;01mreturn\u001b[39;00m f[\u001b[32m0\u001b[39m].isupper()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dls = \u001b[43mImageDataLoaders\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_name_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_tfms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/vision/data.py:150\u001b[39m, in \u001b[36mImageDataLoaders.from_name_func\u001b[39m\u001b[34m(cls, path, fnames, label_func, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mlabel_func couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be lambda function on Windows\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    149\u001b[39m f = using_attr(label_func, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_path_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/vision/data.py:136\u001b[39m, in \u001b[36mImageDataLoaders.from_path_func\u001b[39m\u001b[34m(cls, path, fnames, label_func, valid_pct, seed, item_tfms, batch_tfms, img_cls, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mCreate from list of `fnames` in `path`s with `label_func`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m dblock = DataBlock(blocks=(ImageBlock(img_cls), CategoryBlock),\n\u001b[32m    132\u001b[39m                    splitter=RandomSplitter(valid_pct, seed=seed),\n\u001b[32m    133\u001b[39m                    get_y=label_func,\n\u001b[32m    134\u001b[39m                    item_tfms=item_tfms,\n\u001b[32m    135\u001b[39m                    batch_tfms=batch_tfms)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_dblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:280\u001b[39m, in \u001b[36mDataLoaders.from_dblock\u001b[39m\u001b[34m(cls, dblock, source, path, bs, val_bs, shuffle, device, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_dblock\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \n\u001b[32m    271\u001b[39m     dblock, \u001b[38;5;66;03m# `DataBlock` object\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m     **kwargs\n\u001b[32m    279\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdblock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_bs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_bs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/block.py:159\u001b[39m, in \u001b[36mDataBlock.dataloaders\u001b[39m\u001b[34m(self, source, path, verbose, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m dsets = \u001b[38;5;28mself\u001b[39m.datasets(source, verbose=verbose)\n\u001b[32m    158\u001b[39m kwargs = {**\u001b[38;5;28mself\u001b[39m.dls_kwargs, **kwargs, \u001b[33m'\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m'\u001b[39m: verbose}\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdsets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_item\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mitem_tfms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_tfms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:333\u001b[39m, in \u001b[36mFilteredBase.dataloaders\u001b[39m\u001b[34m(self, bs, shuffle_train, shuffle, val_shuffle, n, path, dl_type, dl_kwargs, device, drop_last, val_bs, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m dl = dl_type(\u001b[38;5;28mself\u001b[39m.subset(\u001b[32m0\u001b[39m), **merge(kwargs,def_kwargs, dl_kwargs[\u001b[32m0\u001b[39m]))\n\u001b[32m    332\u001b[39m def_kwargs = {\u001b[33m'\u001b[39m\u001b[33mbs\u001b[39m\u001b[33m'\u001b[39m:bs \u001b[38;5;28;01mif\u001b[39;00m val_bs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m val_bs,\u001b[33m'\u001b[39m\u001b[33mshuffle\u001b[39m\u001b[33m'\u001b[39m:val_shuffle,\u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m:\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[33m'\u001b[39m\u001b[33mdrop_last\u001b[39m\u001b[33m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m dls = [dl] + [\u001b[43mdl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdef_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdl_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m               \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_subsets)]\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dbunch_type(*dls, path=path, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:104\u001b[39m, in \u001b[36mTfmdDL.new\u001b[39m\u001b[34m(self, dataset, cls, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_n_inp\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_types\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m         res._n_inp,res._types = \u001b[38;5;28mself\u001b[39m._n_inp,\u001b[38;5;28mself\u001b[39m._types\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/data/core.py:86\u001b[39m, in \u001b[36mTfmdDL._one_pass\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_one_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     85\u001b[39m     b = \u001b[38;5;28mself\u001b[39m.do_batch([\u001b[38;5;28mself\u001b[39m.do_item(\u001b[38;5;28;01mNone\u001b[39;00m)])\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: b = \u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     its = \u001b[38;5;28mself\u001b[39m.after_batch(b)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mself\u001b[39m._n_inp = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(its, (\u001b[38;5;28mlist\u001b[39m,\u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(its)==\u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(its)-\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:287\u001b[39m, in \u001b[36mto_device\u001b[39m\u001b[34m(b, device, non_blocking)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o,Tensor): \u001b[38;5;28;01mreturn\u001b[39;00m o.to(device, non_blocking=non_blocking)\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m o\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:224\u001b[39m, in \u001b[36mapply\u001b[39m\u001b[34m(func, x, *args, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(func, x, *args, **kwargs):\n\u001b[32m    223\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mApply `func` recursively to `x`, passing on args\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_listy(x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x)([\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x,(\u001b[38;5;28mdict\u001b[39m,MutableMapping)): \u001b[38;5;28;01mreturn\u001b[39;00m {k: apply(func, v, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m x.items()}\n\u001b[32m    226\u001b[39m     res = func(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:226\u001b[39m, in \u001b[36mapply\u001b[39m\u001b[34m(func, x, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_listy(x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(x)([apply(func, o, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x,(\u001b[38;5;28mdict\u001b[39m,MutableMapping)): \u001b[38;5;28;01mreturn\u001b[39;00m {k: apply(func, v, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m x.items()}\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m retain_type(res, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:285\u001b[39m, in \u001b[36mto_device.<locals>._inner\u001b[39m\u001b[34m(o)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_inner\u001b[39m(o):\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# ToDo: add TensorDict when released\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o,Tensor): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/fastai/torch_core.py:384\u001b[39m, in \u001b[36mTensorBase.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.debug \u001b[38;5;129;01mand\u001b[39;00m func.\u001b[34m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33m__str__\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m__repr__\u001b[39m\u001b[33m'\u001b[39m): \u001b[38;5;28mprint\u001b[39m(func, types, args, kwargs)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mcls\u001b[39m._opt, func): types = (torch.Tensor,)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m res = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m dict_objs = _find_args(args) \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m _find_args(\u001b[38;5;28mlist\u001b[39m(kwargs.values()))\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(res),TensorBase) \u001b[38;5;129;01mand\u001b[39;00m dict_objs: res.set_meta(dict_objs[\u001b[32m0\u001b[39m],as_copy=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dev/lib/python3.12/site-packages/torch/_tensor.py:1654\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1653\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1656\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compress-header",
   "metadata": {},
   "source": [
    "## 2. Compress the Model\n",
    "\n",
    "Apply sparsification to reduce model size. You could also use pruning, quantization, or any combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sparsify",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_cycle)\n",
    "learn.fit_one_cycle(2, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fold-header",
   "metadata": {},
   "source": [
    "## 3. Fold BatchNorm Layers\n",
    "\n",
    "Before export, fold batch normalization layers into convolutions for faster inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fold-bn",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_folder = BN_Folder()\n",
    "model = bn_folder.fold(learn.model)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 4. Export to ONNX\n",
    "\n",
    "Now export the optimized model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example input (batch_size=1, channels=3, height=64, width=64)\n",
    "sample = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = export_onnx(model.cpu(), sample, \"model.onnx\")\n",
    "print(f\"Exported to: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "### Verify the Export\n",
    "\n",
    "Always verify that the ONNX model produces the same outputs as the PyTorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid = verify_onnx(model, onnx_path, sample)\n",
    "print(f\"Verification {'passed' if is_valid else 'FAILED'}: ONNX outputs {'match' if is_valid else 'do not match'} PyTorch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantize-header",
   "metadata": {},
   "source": [
    "## 5. Export with INT8 Quantization\n",
    "\n",
    "For even smaller models and faster inference, apply INT8 quantization during export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantized-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization (no calibration data needed)\n",
    "quantized_path = export_onnx(\n",
    "    model.cpu(), sample, \"model_int8.onnx\",\n",
    "    quantize=True,\n",
    "    quantize_mode=\"dynamic\"\n",
    ")\n",
    "print(f\"Exported quantized model to: {quantized_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-quant",
   "metadata": {},
   "source": [
    "For better accuracy, use **static quantization** with calibration data:\n",
    "\n",
    "```python\n",
    "# Static quantization with calibration\n",
    "quantized_path = export_onnx(\n",
    "    model, sample, \"model_int8_static.onnx\",\n",
    "    quantize=True,\n",
    "    quantize_mode=\"static\",\n",
    "    calibration_data=dls.train  # Use training data for calibration\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 6. Compare Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-sizes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_size_mb(path):\n",
    "    return os.path.getsize(path) / 1e6\n",
    "\n",
    "# Save PyTorch model for comparison\n",
    "torch.save(model.state_dict(), \"model.pt\")\n",
    "\n",
    "pt_size = get_size_mb(\"model.pt\")\n",
    "onnx_size = get_size_mb(\"model.onnx\")\n",
    "int8_size = get_size_mb(quantized_path)\n",
    "\n",
    "print(f\"PyTorch model:    {pt_size:.2f} MB\")\n",
    "print(f\"ONNX model:       {onnx_size:.2f} MB\")\n",
    "print(f\"ONNX INT8 model:  {int8_size:.2f} MB ({pt_size/int8_size:.1f}x smaller)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## 7. Running Inference with ONNX Runtime\n",
    "\n",
    "Use the `ONNXModel` wrapper for easy inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "onnx_model = ONNXModel(\"model.onnx\", device=\"cpu\")\n",
    "\n",
    "# Run inference\n",
    "test_input = torch.randn(1, 3, 64, 64)\n",
    "output = onnx_model(test_input)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Predictions: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "### Benchmark Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(fn, input_tensor, warmup=10, runs=100):\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        fn(input_tensor)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(runs):\n",
    "        fn(input_tensor)\n",
    "    elapsed = (time.perf_counter() - start) / runs * 1000\n",
    "    return elapsed\n",
    "\n",
    "test_input = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# PyTorch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pt_time = benchmark(model, test_input)\n",
    "\n",
    "# ONNX\n",
    "onnx_model = ONNXModel(\"model.onnx\")\n",
    "onnx_time = benchmark(onnx_model, test_input)\n",
    "\n",
    "# ONNX INT8\n",
    "onnx_int8 = ONNXModel(quantized_path)\n",
    "int8_time = benchmark(onnx_int8, test_input)\n",
    "\n",
    "print(f\"PyTorch inference: {pt_time:.2f} ms\")\n",
    "print(f\"ONNX inference:    {onnx_time:.2f} ms ({pt_time/onnx_time:.1f}x faster)\")\n",
    "print(f\"ONNX INT8:         {int8_time:.2f} ms ({pt_time/int8_time:.1f}x faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params-header",
   "metadata": {},
   "source": [
    "## 8. Parameter Reference\n",
    "\n",
    "### export_onnx Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `model` | Required | PyTorch model to export |\n",
    "| `sample` | Required | Example input tensor (with batch dimension) |\n",
    "| `output_path` | Required | Output .onnx file path |\n",
    "| `opset_version` | `18` | ONNX opset version |\n",
    "| `quantize` | `False` | Apply INT8 quantization after export |\n",
    "| `quantize_mode` | `\"dynamic\"` | `\"dynamic\"` (no calibration) or `\"static\"` |\n",
    "| `calibration_data` | `None` | DataLoader for static quantization |\n",
    "| `optimize` | `True` | Run ONNX graph optimizer |\n",
    "| `dynamic_batch` | `True` | Allow variable batch size at runtime |\n",
    "\n",
    "### Quantization Mode Comparison\n",
    "\n",
    "| Mode | Calibration | Accuracy | Speed | Use Case |\n",
    "|------|-------------|----------|-------|----------|\n",
    "| `dynamic` | Not needed | Good | Fast export | Quick deployment |\n",
    "| `static` | Required | Better | Slower export | Production models |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Step | Tool | Purpose |\n",
    "|------|------|----------|\n",
    "| Compress | SparsifyCallback, PruneCallback, etc. | Reduce model complexity |\n",
    "| Fold BN | BN_Folder | Eliminate batch norm overhead |\n",
    "| Export | export_onnx | Convert to deployment format |\n",
    "| Verify | verify_onnx | Ensure correctness |\n",
    "| Quantize | `quantize=True` | Further reduce size (4x) |\n",
    "| Deploy | ONNXModel | Run inference |\n",
    "\n",
    "### Complete Pipeline Example\n",
    "\n",
    "```python\n",
    "from fasterai.sparse.all import *\n",
    "from fasterai.misc.all import *\n",
    "from fasterai.export.all import *\n",
    "\n",
    "# 1. Train with compression\n",
    "sp_cb = SparsifyCallback(sparsity=50, granularity='weight', ...)\n",
    "learn.fit_one_cycle(5, cbs=sp_cb)\n",
    "\n",
    "# 2. Fold batch norm\n",
    "model = BN_Folder().fold(learn.model)\n",
    "\n",
    "# 3. Export with quantization\n",
    "sample = torch.randn(1, 3, 224, 224)\n",
    "path = export_onnx(model, sample, \"model_int8.onnx\", quantize=True)\n",
    "\n",
    "# 4. Verify\n",
    "assert verify_onnx(model, path, sample)\n",
    "\n",
    "# 5. Deploy\n",
    "onnx_model = ONNXModel(path)\n",
    "output = onnx_model(input_tensor)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [ONNX Exporter API](../../export/onnx_exporter.html) - Detailed API reference\n",
    "- [BN Folding](../misc/bn_folding.html) - Fold batch norm before export\n",
    "- [CPU Optimizer](../../misc/cpu_optimizer.html) - Alternative: TorchScript for CPU deployment\n",
    "- [Sparsify Callback](../sparse/sparsify_callback.html) - Compress before export\n",
    "- [Quantize Callback](../quantize/quantize_callback.html) - QAT before export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Cleanup\n",
    "import os\n",
    "for f in [\"model.onnx\", \"model.pt\", \"model_int8.onnx\"]:\n",
    "    if os.path.exists(f): os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
