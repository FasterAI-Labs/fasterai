{
 "cells": [
  {
   "cell_type": "raw",
   "id": "frontmatter",
   "metadata": {},
   "source": [
    "---\n",
    "description: Export compressed models to ONNX for deployment\n",
    "output-file: tutorial.onnx_export.html\n",
    "title: ONNX Export Tutorial\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from fastai.vision.all import *\n",
    "from fasterai.sparse.all import *\n",
    "from fasterai.misc.all import *\n",
    "from fasterai.export.all import *\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "After compressing a model with fasterai, you'll want to **deploy** it. ONNX (Open Neural Network Exchange) is the standard format for deploying models across different platforms and runtimes.\n",
    "\n",
    "### Why Export to ONNX?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Portability** | Run on any platform: servers, mobile, edge devices, browsers |\n",
    "| **Performance** | ONNX Runtime is highly optimized for inference |\n",
    "| **Quantization** | Apply additional INT8 quantization during export |\n",
    "| **No Python needed** | Deploy without Python dependencies |\n",
    "\n",
    "### The Deployment Pipeline\n",
    "\n",
    "```\n",
    "Train → Compress (prune/sparsify/quantize) → Fold BN → Export ONNX → Deploy\n",
    "```\n",
    "\n",
    "This tutorial walks through the complete pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Training\n",
    "\n",
    "First, let's train a model that we'll later compress and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.641139</td>\n",
       "      <td>0.338822</td>\n",
       "      <td>0.868065</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.365181</td>\n",
       "      <td>0.225297</td>\n",
       "      <td>0.905954</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.195590</td>\n",
       "      <td>0.186686</td>\n",
       "      <td>0.914750</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compress-header",
   "metadata": {},
   "source": [
    "## 2. Compress the Model\n",
    "\n",
    "Apply sparsification to reduce model size. You could also use pruning, quantization, or any combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sparsify",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of [50]%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.263917</td>\n",
       "      <td>0.234192</td>\n",
       "      <td>0.901894</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.176362</td>\n",
       "      <td>0.202073</td>\n",
       "      <td>0.921516</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: [36.57]%\n",
      "Sparsity at the end of epoch 1: [50.0]%\n",
      "Final Sparsity: [50.0]%\n",
      "\n",
      "Sparsity Report:\n",
      "--------------------------------------------------------------------------------\n",
      "Layer                Type            Params     Zeros      Sparsity  \n",
      "--------------------------------------------------------------------------------\n",
      "Layer 0              Conv2d          9,408      4,704         50.00%\n",
      "Layer 1              Conv2d          36,864     18,432        50.00%\n",
      "Layer 2              Conv2d          36,864     18,432        50.00%\n",
      "Layer 3              Conv2d          36,864     18,432        50.00%\n",
      "Layer 4              Conv2d          36,864     18,432        50.00%\n",
      "Layer 5              Conv2d          73,728     36,863        50.00%\n",
      "Layer 6              Conv2d          147,456    73,726        50.00%\n",
      "Layer 7              Conv2d          8,192      4,096         50.00%\n",
      "Layer 8              Conv2d          147,456    73,726        50.00%\n",
      "Layer 9              Conv2d          147,456    73,726        50.00%\n",
      "Layer 10             Conv2d          294,912    147,452       50.00%\n",
      "Layer 11             Conv2d          589,824    294,905       50.00%\n",
      "Layer 12             Conv2d          32,768     16,384        50.00%\n",
      "Layer 13             Conv2d          589,824    294,905       50.00%\n",
      "Layer 14             Conv2d          589,824    294,905       50.00%\n",
      "Layer 15             Conv2d          1,179,648  589,810       50.00%\n",
      "Layer 16             Conv2d          2,359,296  1,179,619     50.00%\n",
      "Layer 17             Conv2d          131,072    65,534        50.00%\n",
      "Layer 18             Conv2d          2,359,296  1,179,619     50.00%\n",
      "Layer 19             Conv2d          2,359,296  1,179,618     50.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Overall              all             11,166,912 5,583,320     50.00%\n"
     ]
    }
   ],
   "source": [
    "sp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_cycle)\n",
    "learn.fit_one_cycle(2, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fold-header",
   "metadata": {},
   "source": [
    "## 3. Fold BatchNorm Layers\n",
    "\n",
    "Before export, fold batch normalization layers into convolutions for faster inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fold-bn",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_folder = BN_Folder()\n",
    "model = bn_folder.fold(learn.model)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 4. Export to ONNX\n",
    "\n",
    "Now export the optimized model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to: model.onnx\n"
     ]
    }
   ],
   "source": [
    "# Create example input (batch_size=1, channels=3, height=64, width=64)\n",
    "sample = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# Export to ONNX\n",
    "onnx_path = export_onnx(model.cpu(), sample, \"model.onnx\")\n",
    "print(f\"Exported to: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-header",
   "metadata": {},
   "source": [
    "### Verify the Export\n",
    "\n",
    "Always verify that the ONNX model produces the same outputs as the PyTorch model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification passed: ONNX outputs match PyTorch!\n"
     ]
    }
   ],
   "source": [
    "is_valid = verify_onnx(model, onnx_path, sample)\n",
    "print(f\"Verification {'passed' if is_valid else 'FAILED'}: ONNX outputs {'match' if is_valid else 'do not match'} PyTorch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantize-header",
   "metadata": {},
   "source": [
    "## 5. Export with INT8 Quantization\n",
    "\n",
    "For even smaller models and faster inference, apply INT8 quantization during export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantized-export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported quantized model to: model_int8_int8.onnx\n"
     ]
    }
   ],
   "source": [
    "# Dynamic quantization (no calibration data needed)\n",
    "quantized_path = export_onnx(\n",
    "    model.cpu(), sample, \"model_int8.onnx\",\n",
    "    quantize=True,\n",
    "    quantize_mode=\"dynamic\"\n",
    ")\n",
    "print(f\"Exported quantized model to: {quantized_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-quant",
   "metadata": {},
   "source": [
    "For better accuracy, use **static quantization** with calibration data:\n",
    "\n",
    "```python\n",
    "# Static quantization with calibration\n",
    "quantized_path = export_onnx(\n",
    "    model, sample, \"model_int8_static.onnx\",\n",
    "    quantize=True,\n",
    "    quantize_mode=\"static\",\n",
    "    calibration_data=dls.train  # Use training data for calibration\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 6. Compare Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-sizes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model:    46.83 MB\n",
      "ONNX model:       46.82 MB\n",
      "ONNX INT8 model:  11.78 MB (4.0x smaller)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_size_mb(path):\n",
    "    return os.path.getsize(path) / 1e6\n",
    "\n",
    "# Save PyTorch model for comparison\n",
    "torch.save(model.state_dict(), \"model.pt\")\n",
    "\n",
    "pt_size = get_size_mb(\"model.pt\")\n",
    "onnx_size = get_size_mb(\"model.onnx\")\n",
    "int8_size = get_size_mb(quantized_path)\n",
    "\n",
    "print(f\"PyTorch model:    {pt_size:.2f} MB\")\n",
    "print(f\"ONNX model:       {onnx_size:.2f} MB\")\n",
    "print(f\"ONNX INT8 model:  {int8_size:.2f} MB ({pt_size/int8_size:.1f}x smaller)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## 7. Running Inference with ONNX Runtime\n",
    "\n",
    "Use the `ONNXModel` wrapper for easy inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 2])\n",
      "Predictions: tensor([[0.6364, 0.3489]])\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "onnx_model = ONNXModel(\"model.onnx\", device=\"cpu\")\n",
    "\n",
    "# Run inference\n",
    "test_input = torch.randn(1, 3, 64, 64)\n",
    "output = onnx_model(test_input)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Predictions: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "### Benchmark Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch inference: 1.27 ms\n",
      "ONNX inference:    0.87 ms (1.5x faster)\n",
      "ONNX INT8:         2.90 ms (0.4x faster)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(fn, input_tensor, warmup=10, runs=100):\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        fn(input_tensor)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(runs):\n",
    "        fn(input_tensor)\n",
    "    elapsed = (time.perf_counter() - start) / runs * 1000\n",
    "    return elapsed\n",
    "\n",
    "test_input = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "# PyTorch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pt_time = benchmark(model, test_input)\n",
    "\n",
    "# ONNX\n",
    "onnx_model = ONNXModel(\"model.onnx\")\n",
    "onnx_time = benchmark(onnx_model, test_input)\n",
    "\n",
    "# ONNX INT8\n",
    "onnx_int8 = ONNXModel(quantized_path)\n",
    "int8_time = benchmark(onnx_int8, test_input)\n",
    "\n",
    "print(f\"PyTorch inference: {pt_time:.2f} ms\")\n",
    "print(f\"ONNX inference:    {onnx_time:.2f} ms ({pt_time/onnx_time:.1f}x faster)\")\n",
    "print(f\"ONNX INT8:         {int8_time:.2f} ms ({pt_time/int8_time:.1f}x faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params-header",
   "metadata": {},
   "source": [
    "## 8. Parameter Reference\n",
    "\n",
    "### export_onnx Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `model` | Required | PyTorch model to export |\n",
    "| `sample` | Required | Example input tensor (with batch dimension) |\n",
    "| `output_path` | Required | Output .onnx file path |\n",
    "| `opset_version` | `18` | ONNX opset version |\n",
    "| `quantize` | `False` | Apply INT8 quantization after export |\n",
    "| `quantize_mode` | `\"dynamic\"` | `\"dynamic\"` (no calibration) or `\"static\"` |\n",
    "| `calibration_data` | `None` | DataLoader for static quantization |\n",
    "| `optimize` | `True` | Run ONNX graph optimizer |\n",
    "| `dynamic_batch` | `True` | Allow variable batch size at runtime |\n",
    "\n",
    "### Quantization Mode Comparison\n",
    "\n",
    "| Mode | Calibration | Accuracy | Speed | Use Case |\n",
    "|------|-------------|----------|-------|----------|\n",
    "| `dynamic` | Not needed | Good | Fast export | Quick deployment |\n",
    "| `static` | Required | Better | Slower export | Production models |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Step | Tool | Purpose |\n",
    "|------|------|----------|\n",
    "| Compress | SparsifyCallback, PruneCallback, etc. | Reduce model complexity |\n",
    "| Fold BN | BN_Folder | Eliminate batch norm overhead |\n",
    "| Export | export_onnx | Convert to deployment format |\n",
    "| Verify | verify_onnx | Ensure correctness |\n",
    "| Quantize | `quantize=True` | Further reduce size (4x) |\n",
    "| Deploy | ONNXModel | Run inference |\n",
    "\n",
    "### Complete Pipeline Example\n",
    "\n",
    "```python\n",
    "from fasterai.sparse.all import *\n",
    "from fasterai.misc.all import *\n",
    "from fasterai.export.all import *\n",
    "\n",
    "# 1. Train with compression\n",
    "sp_cb = SparsifyCallback(sparsity=50, granularity='weight', ...)\n",
    "learn.fit_one_cycle(5, cbs=sp_cb)\n",
    "\n",
    "# 2. Fold batch norm\n",
    "model = BN_Folder().fold(learn.model)\n",
    "\n",
    "# 3. Export with quantization\n",
    "sample = torch.randn(1, 3, 224, 224)\n",
    "path = export_onnx(model, sample, \"model_int8.onnx\", quantize=True)\n",
    "\n",
    "# 4. Verify\n",
    "assert verify_onnx(model, path, sample)\n",
    "\n",
    "# 5. Deploy\n",
    "onnx_model = ONNXModel(path)\n",
    "output = onnx_model(input_tensor)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [ONNX Exporter API](../../export/onnx_exporter.html) - Detailed API reference\n",
    "- [BN Folding](../misc/bn_folding.html) - Fold batch norm before export\n",
    "- [CPU Optimizer](../../misc/cpu_optimizer.html) - Alternative: TorchScript for CPU deployment\n",
    "- [Sparsify Callback](../sparse/sparsify_callback.html) - Compress before export\n",
    "- [Quantize Callback](../quantize/quantize_callback.html) - QAT before export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# Cleanup\n",
    "import os\n",
    "for f in [\"model.onnx\", \"model.pt\", \"model_int8.onnx\"]:\n",
    "    if os.path.exists(f): os.remove(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
