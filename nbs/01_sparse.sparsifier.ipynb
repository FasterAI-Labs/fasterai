{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Make your neural network sparse\n",
    "output-file: sparsifier.html\n",
    "title: Sparsifier\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sparse.sparsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "from fastcore.basics import store_attr, listify\n",
    "from fasterai.core.criteria import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network's weight are mostly zeroes.\n",
    "\n",
    "With fasterai, you can do that thanks to the `Sparsifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sparsifier():\n",
    "    \"Class providing sparsifying capabilities\"\n",
    "    def __init__(self, model, granularity, context, criteria, layer_type=nn.Conv2d):\n",
    "        store_attr()\n",
    "        self._save_weights() # Save the original weights\n",
    "\n",
    "    def prune_layer(self, m, sparsity, round_to=None):\n",
    "        scores = self.criteria(m)\n",
    "        setattr(m, '_mask', self._compute_mask(m, scores, sparsity, round_to))\n",
    "        self._apply(m)\n",
    "        self.criteria.update_weights(m)\n",
    "\n",
    "    def prune_model(self, sparsity, round_to=None):\n",
    "        self.threshold=None\n",
    "        sparsity_list = listify(sparsity)\n",
    "        if len(sparsity_list)>1: assert self.context=='local', f\"A list of sparsities cannot be passed using: {self.context}\"\n",
    "        sparsities = cycle(sparsity_list) if len(sparsity_list)==1 else iter(sparsity_list)\n",
    "        mods = list(self.model.modules())\n",
    "        for k,m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, self.layer_type): \n",
    "                sp = next(sparsities)\n",
    "                self.prune_layer(m, sp, round_to)\n",
    "                if isinstance(mods[k+1], nn.modules.batchnorm._BatchNorm): self.prune_batchnorm(m, mods[k+1])\n",
    "                \n",
    "    def prune_batchnorm(self, m, bn):\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if self.granularity == 'filter' and mask is not None:\n",
    "            bn.weight.data.mul_(mask.squeeze())\n",
    "            bn.bias.data.mul_(mask.squeeze())\n",
    "            \n",
    "    def _apply_masks(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, self.layer_type):\n",
    "                self._apply(m)\n",
    "        \n",
    "    def _apply(self, m):\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if mask is not None: m.weight.data.mul_(mask)\n",
    "        if self.granularity == 'filter' and m.bias is not None:\n",
    "            if mask is not None: m.bias.data.mul_(mask.squeeze()) # We want to prune the bias when pruning filters\n",
    "    \n",
    "    def _reset_weights(self, model=None):\n",
    "        if not model: model=self.model\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                init_weights = getattr(m, \"_init_weights\", m.weight)\n",
    "                init_biases = getattr(m, \"_init_biases\", m.bias)\n",
    "                with torch.no_grad():\n",
    "                    if m.weight is not None: m.weight.copy_(init_weights)\n",
    "                    if m.bias is not None: m.bias.copy_(init_biases)\n",
    "                self._apply(m)\n",
    "            if isinstance(m, nn.modules.batchnorm._BatchNorm): m.reset_parameters()\n",
    "                \n",
    "    def _save_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'weight'):              \n",
    "                m.register_buffer(\"_init_weights\", m.weight.clone())\n",
    "                b = getattr(m, 'bias', None)\n",
    "                if b is not None: m.register_buffer(\"_init_biases\", b.clone())\n",
    "                    \n",
    "    def save_model(self, path, model=None):\n",
    "        if not model: model=self.model\n",
    "        tmp_model = pickle.loads(pickle.dumps(model))\n",
    "        self._reset_weights(tmp_model)\n",
    "        self._clean_buffers(tmp_model)\n",
    "        torch.save(tmp_model, path)\n",
    "\n",
    "    def _clean_buffers(self, model=None):\n",
    "        if not model: model=self.model\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                if hasattr(m, '_mask'): del m._buffers[\"_mask\"]\n",
    "                if hasattr(m, '_init_weights'): del m._buffers[\"_init_weights\"]\n",
    "                if hasattr(m, '_init_biases'): del m._buffers[\"_init_biases\"]\n",
    "    \n",
    "    def _compute_threshold(self, m, scores, sparsity):\n",
    "        if self.context == 'global':\n",
    "            if self.threshold is None: \n",
    "                global_criteria = torch.cat([self.criteria(m).view(-1) for m in self.model.modules() if isinstance(m, self.layer_type)]) # Get all scores\n",
    "                global_scores = torch.cat([self.criteria.get_scores(m, self.criteria(m), self.granularity, global_criteria.min()).view(-1) for m in self.model.modules() if isinstance(m, self.layer_type)])\n",
    "                self.threshold = torch.quantile(global_scores, sparsity/100) # Compute the threshold globally (only once per model pruning)\n",
    "            scores = self.criteria.get_scores(m, scores, self.granularity, self.criteria.min_value) # min_value is computed only once per prune_model\n",
    "            return self.threshold, scores\n",
    "        elif self.context == 'local':\n",
    "            scores = self.criteria.get_scores(m, scores, self.granularity)\n",
    "            return torch.quantile(scores.view(-1), sparsity/100), scores\n",
    "        else: raise NameError('Invalid Context')\n",
    "\n",
    "    def _rounded_sparsity(self, n_to_prune, round_to):\n",
    "        return max(round_to*torch.ceil(n_to_prune/round_to), round_to)\n",
    "    \n",
    "    def _compute_mask(self, m, scores, sparsity, round_to):\n",
    "        self.threshold, scores = self._compute_threshold(m, scores, sparsity)\n",
    "        if round_to:\n",
    "            n_to_keep = sum(scores.ge(self.threshold)).squeeze()\n",
    "            self.threshold = torch.topk(scores.squeeze(), int(self._rounded_sparsity(n_to_keep, round_to)))[0].min()\n",
    "        if self.threshold > scores.max(): self.threshold = scores.max() # Make sure we don't remove every weight of a given layer\n",
    "        return scores.ge(self.threshold).to(dtype=scores.dtype)\n",
    "    \n",
    "    def print_sparsity(self):\n",
    "        for k,m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, self.layer_type):\n",
    "                print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Sparsifier\n",
       "\n",
       ">      Sparsifier (model, granularity, context, criteria, layer_type=<class\n",
       ">                  'torch.nn.modules.conv.Conv2d'>)\n",
       "\n",
       "Class providing sparsifying capabilities"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Sparsifier\n",
       "\n",
       ">      Sparsifier (model, granularity, context, criteria, layer_type=<class\n",
       ">                  'torch.nn.modules.conv.Conv2d'>)\n",
       "\n",
       "Class providing sparsifying capabilities"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sparsifier` class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n",
    "\n",
    "- The `granularity`, i.e. the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\n",
    "- The `context`, i.e. if you want to consider each layer independently (`local`), or compare the parameters to remove across the whole network (`global`).\n",
    "- The `criteria`, i.e. the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can pass a single layer to prune by using the  `Sparsifier.prune_layer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Sparsifier.prune_layer\n",
       "\n",
       ">      Sparsifier.prune_layer (m, sparsity, round_to=None)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Sparsifier.prune_layer\n",
       "\n",
       ">      Sparsifier.prune_layer (m, sparsity, round_to=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.prune_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we may want to prune the whole model at once, using the `Sparsifier.prune_model` method, indicating the percentage of sparsity to you want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Sparsifier.prune_model\n",
       "\n",
       ">      Sparsifier.prune_model (sparsity, round_to=None)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Sparsifier.prune_model\n",
       "\n",
       ">      Sparsifier.prune_model (sparsity, round_to=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.prune_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some case, you may want to impose the remaining amount of parameters to be a multiple of a given number (e.g. 8), this can be done by passing the `round_to` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, instead of passing a single value of sparsity, a list of sparsities can also be provided. In that case, each value in the list is the sparsity that will be applied to all layers.\n",
    "\n",
    "**Example**: I have a 4-layer network and want to remove half of the parameters from the layers 2 and 3, I can provide the list: `sparsity = [0, 50, 50, 0]`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
