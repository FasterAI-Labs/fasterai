{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Make your neural network sparse\n",
    "output-file: sparsifier.html\n",
    "title: Sparsifier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sparse.sparsifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/HubensN/miniconda3/envs/prune/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from itertools import cycle\n",
    "from fastcore.basics import store_attr, listify, true\n",
    "from fasterai.core.criteria import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network's weight are mostly zeroes.\n",
    "\n",
    "With fasterai, you can do that thanks to the `Sparsifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sparsifier():\n",
    "    \"Class providing sparsifying capabilities\"\n",
    "    def __init__(self, model, granularity, context, criteria, layer_type=nn.Conv2d):\n",
    "        store_attr()\n",
    "        self._save_weights() # Save the original weights\n",
    "        self._reset_threshold()\n",
    "\n",
    "    def sparsify_layer(self, m, sparsity, round_to=None):\n",
    "        scores    = self._compute_scores(m, sparsity)\n",
    "        threshold = self._compute_threshold(scores, sparsity, round_to)\n",
    "        mask      = self._compute_mask(scores, threshold)\n",
    "        m.register_buffer('_mask', mask)\n",
    "        self._apply(m)\n",
    "        self.criteria.update_weights(m)\n",
    "\n",
    "    def sparsify_model(self, sparsity, round_to=None):\n",
    "        self._reset_threshold()\n",
    "        sparsity_list = listify(sparsity)\n",
    "        if len(sparsity_list)>1: assert self.context=='local', f\"A list of sparsities cannot be passed using: {self.context}\"\n",
    "        sparsities = cycle(sparsity_list) if len(sparsity_list)==1 else iter(sparsity_list)\n",
    "        mods = list(self.model.modules())\n",
    "        for k,m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, self.layer_type): \n",
    "                sp = next(sparsities)\n",
    "                self.sparsify_layer(m, sp, round_to)\n",
    "                if isinstance(mods[k+1], nn.modules.batchnorm._BatchNorm): self.sparsify_batchnorm(m, mods[k+1])\n",
    "                \n",
    "    def sparsify_batchnorm(self, m, bn):\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if self.granularity == 'filter' and true(mask):\n",
    "            bn.weight.data.mul_(mask.squeeze())\n",
    "            bn.bias.data.mul_(mask.squeeze())\n",
    "            \n",
    "    def _apply_masks(self):\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, self.layer_type):\n",
    "                self._apply(m)\n",
    "        \n",
    "    def _apply(self, m):\n",
    "        mask = getattr(m, \"_mask\", None)\n",
    "        if true(mask): m.weight.data.mul_(mask)\n",
    "        if self.granularity == 'filter' and true(m.bias):\n",
    "            if true(mask): m.bias.data.mul_(mask.squeeze()) # We want to sparsify the bias when pruning filters\n",
    "    \n",
    "    def _reset_weights(self, model=None):\n",
    "        model = model or self.model\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                init_weights = getattr(m, \"_init_weights\", m.weight)\n",
    "                init_biases = getattr(m, \"_init_biases\", m.bias)\n",
    "                with torch.no_grad():\n",
    "                    if true(m.weight): m.weight.copy_(init_weights)\n",
    "                    if true(m.bias): m.bias.copy_(init_biases)\n",
    "                self._apply(m)\n",
    "            if isinstance(m, nn.modules.batchnorm._BatchNorm): m.reset_parameters()\n",
    "                \n",
    "    def _save_weights(self):\n",
    "        for m in self.model.modules():\n",
    "            if hasattr(m, 'weight'):              \n",
    "                m.register_buffer(\"_init_weights\", m.weight.clone())\n",
    "                bias = getattr(m, 'bias', None)\n",
    "                if true(bias): m.register_buffer(\"_init_biases\", bias.clone())\n",
    "                    \n",
    "    def save_model(self, path, model=None):\n",
    "        model = model or self.model\n",
    "        tmp_model = pickle.loads(pickle.dumps(model))\n",
    "        self._reset_weights(tmp_model)\n",
    "        self._clean_buffers(tmp_model)\n",
    "        torch.save(tmp_model, path)\n",
    "\n",
    "    def _clean_buffers(self, model=None):\n",
    "        model = model or self.model\n",
    "        for m in model.modules():\n",
    "            if hasattr(m, 'weight'):\n",
    "                if hasattr(m, '_mask'): del m._buffers[\"_mask\"]\n",
    "                if hasattr(m, '_init_weights'): del m._buffers[\"_init_weights\"]\n",
    "                if hasattr(m, '_init_biases'): del m._buffers[\"_init_biases\"]\n",
    "                    \n",
    "    def _reset_threshold(self):\n",
    "        self.threshold=None\n",
    "            \n",
    "    def _rounded_sparsity(self, n_to_prune, round_to):\n",
    "        return max(round_to*torch.ceil(n_to_prune/round_to), round_to)\n",
    "    \n",
    "    def _compute_scores(self, m, sparsity):\n",
    "        if self.context == 'global':\n",
    "            if self.threshold == None: \n",
    "                global_scores  = torch.cat([self.criteria(m, self.granularity).view(-1) for m in self.model.modules() if isinstance(m, self.layer_type)])\n",
    "                self.threshold = torch.quantile(global_scores.view(-1), sparsity/100)\n",
    "            local_scores = self.criteria(m, self.granularity)\n",
    "        elif self.context == 'local': \n",
    "            local_scores = self.criteria(m, self.granularity)\n",
    "            self.threshold = torch.quantile(local_scores.view(-1), sparsity/100)\n",
    "        else: raise NameError('Invalid Context')\n",
    "        return local_scores\n",
    "                \n",
    "    def _compute_threshold(self, scores, sparsity, round_to):\n",
    "        if round_to:\n",
    "            n_to_keep = sum(scores.ge(self.threshold)).squeeze()\n",
    "            self.threshold = torch.topk(scores.squeeze(), int(self._rounded_sparsity(n_to_keep, round_to)))[0].min()\n",
    "        return self.threshold\n",
    "    \n",
    "    def _compute_mask(self, scores, threshold):\n",
    "        if threshold > scores.max(): threshold = scores.max() # Make sure we don't remove every weight of a given layer\n",
    "        return scores.ge(threshold).to(dtype=scores.dtype)\n",
    "    \n",
    "    def print_sparsity(self):\n",
    "        for k,m in enumerate(self.model.modules()):\n",
    "            if isinstance(m, self.layer_type):\n",
    "                print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Sparsifier\n",
       "\n",
       ">      Sparsifier (model, granularity, context, criteria, layer_type=<class\n",
       ">                  'torch.nn.modules.conv.Conv2d'>)\n",
       "\n",
       "Class providing sparsifying capabilities"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Sparsifier\n",
       "\n",
       ">      Sparsifier (model, granularity, context, criteria, layer_type=<class\n",
       ">                  'torch.nn.modules.conv.Conv2d'>)\n",
       "\n",
       "Class providing sparsifying capabilities"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sparsifier` class allows us to remove some weights, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\n",
    "\n",
    "- The `granularity`, i.e. the part of filters that you want to remove. Typically, we usually remove weights, vectors, kernels or even complete filters.\n",
    "- The `context`, i.e. if you want to consider each layer independently (`local`), or compare the parameters to remove across the whole network (`global`).\n",
    "- The `criteria`, i.e. the way to assess the usefulness of a parameter. Common methods compare parameters using their magnitude, the lowest magnitude ones considered to be less useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can pass a single layer to prune by using the  `Sparsifier.sparsify_layer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Sparsifier.sparsify_layer\n",
       "\n",
       ">      Sparsifier.sparsify_layer (m, sparsity, round_to=None)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Sparsifier.sparsify_layer\n",
       "\n",
       ">      Sparsifier.sparsify_layer (m, sparsity, round_to=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.sparsify_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, we may want to prune the whole model at once, using the `Sparsifier.prune_model` method, indicating the percentage of sparsity to you want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Sparsifier.sparsify_model\n",
       "\n",
       ">      Sparsifier.sparsify_model (sparsity, round_to=None)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Sparsifier.sparsify_model\n",
       "\n",
       ">      Sparsifier.sparsify_model (sparsity, round_to=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Sparsifier.sparsify_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some case, you may want to impose the remaining amount of parameters to be a multiple of a given number (e.g. 8), this can be done by passing the `round_to` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, instead of passing a single value of sparsity, a list of sparsities can also be provided. In that case, each value in the list is the sparsity that will be applied to all layers.\n",
    "\n",
    "**Example**: I have a 4-layer network and want to remove half of the parameters from the layers 2 and 3, I can provide the list: `sparsity = [0, 50, 50, 0]`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
