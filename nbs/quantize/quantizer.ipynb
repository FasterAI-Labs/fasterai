{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e9fbe7a6-be58-4fdb-9887-7ce64aabd73f",
   "metadata": {},
   "source": [
    "---\n",
    "description: Quantize your network \n",
    "output-file: quantizer.html\n",
    "title: Quantizer\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab5ccf-06ef-4f8d-b3e8-fc947cd92b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantize.quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80613b7a-9ee9-4729-80e0-a33e6406a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastcore.basics import store_attr\n",
    "from torch.ao.quantization import QConfig, get_default_qconfig_mapping, get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, prepare_qat_fx, convert_fx\n",
    "from torch.ao.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver\n",
    "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
    "from torch.quantization import quantize_dynamic\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from torch.ao.quantization.qconfig import default_dynamic_qconfig\n",
    "from typing import Optional, Dict, Any, Union\n",
    "import warnings\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fd84a-dcf6-4ec5-966e-6fdd01e1d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Quantizer:\n",
    "    def __init__(self, \n",
    "                 backend: str = \"x86\",                   # Target backend for quantization\n",
    "                 method: str = \"static\",                 # Quantization method: 'static', 'dynamic', or 'qat'\n",
    "                 qconfig_mapping: Optional[Dict] = None, # Optional custom quantization config\n",
    "                 custom_configs: Optional[Dict] = None,  # Custom module-specific configurations\n",
    "                 use_per_tensor: bool = False,           # Force per-tensor quantization\n",
    "                 verbose: bool = False                   # Enable verbose output\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize a quantizer with specified backend and options.\n",
    "        \"\"\"\n",
    "        store_attr()\n",
    "        \n",
    "        # Get the default config mapping for this backend\n",
    "        if qconfig_mapping is None:\n",
    "            if method == \"qat\":\n",
    "                self.qconfig_mapping = get_default_qat_qconfig_mapping(backend)\n",
    "            else:\n",
    "                self.qconfig_mapping = get_default_qconfig_mapping(backend)\n",
    "                \n",
    "            # If per-tensor quantization is enforced, update the global qconfig\n",
    "            if use_per_tensor:\n",
    "                self._update_qconfig_for_per_tensor()\n",
    "        else:\n",
    "            self.qconfig_mapping = qconfig_mapping\n",
    "            \n",
    "        # Set PyTorch's quantization engine\n",
    "        torch.backends.quantized.engine = backend\n",
    "\n",
    "    def _update_qconfig_for_per_tensor(self):\n",
    "        \"\"\"Replace per-channel with per-tensor quantization to avoid conversion issues\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Using per-tensor quantization instead of per-channel\")\n",
    "            \n",
    "        if self.method == \"qat\":            \n",
    "            weight_observer = MinMaxObserver.with_args(\n",
    "                dtype=torch.qint8,\n",
    "                qscheme=torch.per_tensor_symmetric,\n",
    "                quant_min=-128,\n",
    "                quant_max=127\n",
    "            )\n",
    "            \n",
    "            activation_observer = MovingAverageMinMaxObserver.with_args(\n",
    "                averaging_constant=0.01,\n",
    "                quant_min=0,\n",
    "                quant_max=255\n",
    "            )\n",
    "            \n",
    "            per_tensor_qconfig = QConfig(\n",
    "                activation=FakeQuantize.with_args(\n",
    "                    observer=activation_observer, \n",
    "                    quant_min=0, \n",
    "                    quant_max=255\n",
    "                ),\n",
    "                weight=FakeQuantize.with_args(\n",
    "                    observer=weight_observer, \n",
    "                    quant_min=-128, \n",
    "                    quant_max=127\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            activation_observer = MinMaxObserver.with_args(\n",
    "                dtype=torch.quint8,\n",
    "                qscheme=torch.per_tensor_affine,\n",
    "                quant_min=0,\n",
    "                quant_max=255\n",
    "            )\n",
    "            \n",
    "            weight_observer = MinMaxObserver.with_args(\n",
    "                dtype=torch.qint8,\n",
    "                qscheme=torch.per_tensor_symmetric,\n",
    "                quant_min=-128,\n",
    "                quant_max=127\n",
    "            )\n",
    "            \n",
    "            per_tensor_qconfig = QConfig(\n",
    "                activation=activation_observer,\n",
    "                weight=weight_observer\n",
    "            )\n",
    "            \n",
    "        # Update global qconfig\n",
    "        self.qconfig_mapping.global_qconfig = per_tensor_qconfig\n",
    "\n",
    "    def _apply_custom_configs(self):\n",
    "        \"\"\"Apply custom quantization configurations to specific modules\"\"\"\n",
    "        if not self.custom_configs:\n",
    "            return\n",
    "            \n",
    "        for module_name, config in self.custom_configs.items():\n",
    "            if self.verbose:\n",
    "                print(f\"Setting custom config for {module_name}\")\n",
    "            self.qconfig_mapping.set_module_name(module_name, config)\n",
    "    \n",
    "    def _prepare_model(self, \n",
    "                       model: nn.Module, \n",
    "                       example_inputs: Any\n",
    "                      ) -> nn.Module:\n",
    "        \"\"\"Prepare model for quantization based on selected method\"\"\"\n",
    "        model = model.cpu()  # Move to CPU first for quantization\n",
    "        \n",
    "        if self.method == \"qat\":\n",
    "            model = model.train()  # QAT needs train mode\n",
    "        else:\n",
    "            model = model.eval()  # PTQ needs eval mode\n",
    "        \n",
    "        try:\n",
    "            if self.method == \"static\":\n",
    "                return prepare_fx(model, self.qconfig_mapping, example_inputs)\n",
    "            elif self.method == \"dynamic\":                \n",
    "                # Setup dynamic qconfig for supported modules\n",
    "                self.qconfig_mapping.set_object_type(torch.nn.Linear, default_dynamic_qconfig)\n",
    "                self.qconfig_mapping.set_object_type(torch.nn.LSTM, default_dynamic_qconfig)\n",
    "                self.qconfig_mapping.set_object_type(torch.nn.GRU, default_dynamic_qconfig)\n",
    "                self.qconfig_mapping.set_object_type(torch.nn.RNN, default_dynamic_qconfig)\n",
    "                \n",
    "                # Apply any custom module configs\n",
    "                if self.custom_configs:\n",
    "                    for module_name, config in self.custom_configs.items():\n",
    "                        self.qconfig_mapping.set_module_name(module_name, config)\n",
    "                \n",
    "                return prepare_fx(model, self.qconfig_mapping, example_inputs)\n",
    "            elif self.method == \"qat\":\n",
    "                return prepare_qat_fx(model, self.qconfig_mapping, example_inputs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown quantization method: {self.method}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error preparing model for quantization: {e}\")\n",
    "    \n",
    "    def _calibrate_model(self, \n",
    "                        model: nn.Module, \n",
    "                        dataloader: Any, \n",
    "                        max_samples: Optional[int] = None, \n",
    "                        device: Union[str, torch.device] = 'cpu'\n",
    "                       ) -> None:\n",
    "        \"\"\"Calibrate the model\"\"\"\n",
    "        model.eval()\n",
    "        device = torch.device(device)\n",
    "        \n",
    "        # Move model to the specified device for calibration\n",
    "        orig_device = torch.device('cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Get dataset size from fastai dataloader\n",
    "        num_samples = getattr(dataloader, 'n', None)\n",
    "        \n",
    "        # Apply max samples limit if provided\n",
    "        if max_samples is not None and num_samples is not None:\n",
    "            num_samples = min(num_samples, max_samples)\n",
    "        \n",
    "        # Create progress bar if verbose\n",
    "        data_iter = dataloader if not self.verbose else tqdm(\n",
    "            dataloader, desc=\"Calibrating\", total=num_samples//dataloader.bs if num_samples else None\n",
    "        )\n",
    "        \n",
    "        # Run calibration\n",
    "        samples_seen = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                # Get inputs from the batch\n",
    "                if isinstance(batch, (list, tuple)) and len(batch) >= 1:\n",
    "                    inputs = batch[0]\n",
    "                else:\n",
    "                    inputs = batch\n",
    "                \n",
    "                # Handle fastai's TensorImage type\n",
    "                if hasattr(inputs, 'data'):\n",
    "                    inputs = inputs.data\n",
    "                    \n",
    "                # Move inputs to the device\n",
    "                if isinstance(inputs, (list, tuple)):\n",
    "                    inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "                \n",
    "                # Forward pass for calibration\n",
    "                model(inputs)\n",
    "                \n",
    "                # Check if we've processed enough samples\n",
    "                batch_size = inputs.shape[0] if isinstance(inputs, torch.Tensor) else inputs[0].shape[0]\n",
    "                samples_seen += batch_size\n",
    "                if max_samples is not None and samples_seen >= max_samples:\n",
    "                    break\n",
    "        \n",
    "        # Move model back to original device\n",
    "        model = model.to(orig_device)\n",
    "    \n",
    "    def _quantize_dynamic(self, \n",
    "                          model: nn.Module\n",
    "                         ):\n",
    "        \"\"\"Quantize a model with dynamic quantization\"\"\"\n",
    "        try:\n",
    "            # Create a deep copy of the model for quantization\n",
    "            model_copy = copy.deepcopy(model).cpu().eval()\n",
    "            \n",
    "            # Attempt to quantize all compatible module types\n",
    "            qconfig_spec = {module_class for module_class in [nn.Linear, nn.LSTM, nn.GRU, nn.RNN]}\n",
    "            quantized_model = quantize_dynamic(\n",
    "                model_copy, \n",
    "                qconfig_spec=qconfig_spec,\n",
    "                dtype=torch.qint8,\n",
    "                inplace=False\n",
    "            )\n",
    "            return quantized_model\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Dynamic quantization failed with error: {e}\")\n",
    "            return model\n",
    "        \n",
    "    def quantize(self, \n",
    "                model: nn.Module,                        # Model to quantize\n",
    "                calibration_dl: Any,                     # Dataloader for calibration\n",
    "                max_calibration_samples: int = 100,      # Maximum number of samples to use for calibration\n",
    "                device: Union[str, torch.device] = 'cpu' # Device to use for calibration\n",
    "               ) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Quantize a model using the specified method and settings.\n",
    "        \"\"\"\n",
    "        # For dynamic quantization, use a specialized approach\n",
    "        if self.method == \"dynamic\":\n",
    "            if self.verbose:\n",
    "                print(f\"Performing dynamic quantization approach with {self.backend} backend\")\n",
    "            \n",
    "            # Apply any custom configs\n",
    "            self._apply_custom_configs()\n",
    "            \n",
    "            # Use the dynamic quantization approach\n",
    "            return self._quantize_dynamic(model)\n",
    "        \n",
    "        # Apply any custom configs for static/QAT\n",
    "        self._apply_custom_configs()\n",
    "        \n",
    "        example_batch, _ = calibration_dl.one_batch()\n",
    "        \n",
    "        try:\n",
    "            # Prepare the model - prepare_fx and prepare_qat_fx will handle fusion automatically\n",
    "            if self.verbose:\n",
    "                print(f\"Preparing model for {self.method} quantization with {self.backend} backend\")\n",
    "            model_prepared = self._prepare_model(model, example_batch.cpu())\n",
    "            \n",
    "            # For static quantization, perform calibration\n",
    "            if self.method in [\"static\", \"qat\"]:\n",
    "                if self.verbose:\n",
    "                    print(f\"Calibrating with up to {max_calibration_samples} samples\")\n",
    "                self._calibrate_model(\n",
    "                    model_prepared, calibration_dl, \n",
    "                    max_samples=max_calibration_samples, device=device\n",
    "                )\n",
    "            \n",
    "            # Convert the model to a quantized version - convert_fx will handle final fusion\n",
    "            if self.verbose:\n",
    "                print(\"Converting to quantized model\")\n",
    "            \n",
    "            try:\n",
    "                quantized_model = convert_fx(model_prepared)\n",
    "            except RuntimeError as e:\n",
    "                if \"Unsupported qscheme: per_channel_affine\" in str(e) and not self.use_per_tensor:\n",
    "                    if self.verbose:\n",
    "                        print(\"Encountered per_channel_affine error, retrying with per-tensor quantization\")\n",
    "                    # Try again with per-tensor quantization\n",
    "                    self.use_per_tensor = True\n",
    "                    self._update_qconfig_for_per_tensor()\n",
    "                    return self.quantize(\n",
    "                        model, calibration_dl, max_calibration_samples, device\n",
    "                    )\n",
    "                else:\n",
    "                    raise e\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Quantization complete\")\n",
    "            \n",
    "            return quantized_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during quantization: {e}\")\n",
    "            if self.verbose:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            # Return the original model if quantization fails\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb7a13-e093-4392-b7d5-d16bc24c1ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
