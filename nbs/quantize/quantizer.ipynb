{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e9fbe7a6-be58-4fdb-9887-7ce64aabd73f",
   "metadata": {},
   "source": [
    "---\n",
    "description: Quantize your network \n",
    "output-file: quantizer.html\n",
    "title: Quantizer\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab5ccf-06ef-4f8d-b3e8-fc947cd92b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantize.quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350ff5a-2d02-4c82-8181-3e16f811ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80613b7a-9ee9-4729-80e0-a33e6406a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastcore.basics import store_attr\n",
    "from torch.ao.quantization import QConfig, get_default_qconfig_mapping, get_default_qat_qconfig_mapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, prepare_qat_fx, convert_fx\n",
    "from torch.ao.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver\n",
    "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
    "from torch.quantization import quantize_dynamic\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from torch.ao.quantization.qconfig import default_dynamic_qconfig\n",
    "from typing import Optional, Any, Union\n",
    "import warnings\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x8v51q8jaz",
   "metadata": {},
   "source": "## Overview\n\nThe `Quantizer` class provides model quantization capabilities to reduce model size and improve inference speed. Quantization converts floating-point weights and activations to lower precision integers (typically int8).\n\n**Supported Backends:**\n- `'x86'`: Optimized for Intel CPUs (default)\n- `'qnnpack'`: Optimized for ARM CPUs (mobile devices)\n- `'fbgemm'`: Facebook's quantization backend\n\n**Quantization Methods:**\n- `'static'`: Post-training quantization with calibration data - best accuracy, requires representative data\n- `'dynamic'`: Runtime quantization without calibration - easier to use, slightly lower accuracy\n- `'qat'`: Quantization-aware training - highest accuracy, requires retraining\n\n**Note:** PyTorch quantization produces CPU-only models. The quantized model will always run on CPU regardless of original device.\n\n### Choosing the Right Method\n\n| Method | Accuracy | Setup Effort | When to Use |\n|--------|----------|--------------|-------------|\n| **Static** | High | Medium (needs calibration data) | Production with representative dataset available |\n| **Dynamic** | Medium | Low (no calibration) | Quick experiments, NLP models with variable input |\n| **QAT** | Highest | High (requires retraining) | Maximum accuracy critical, have training resources |\n\n### Backend Selection Guide\n\n| Backend | Target Hardware | Best For |\n|---------|-----------------|----------|\n| `'x86'` | Intel/AMD CPUs | Desktop/server deployment |\n| `'qnnpack'` | ARM CPUs | Mobile (iOS/Android), Raspberry Pi |\n| `'fbgemm'` | Intel CPUs | Server-side with batch inference |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fd84a-dcf6-4ec5-966e-6fdd01e1d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import contextlib\n",
    "\n",
    "class Quantizer:\n",
    "    def __init__(self, \n",
    "                 backend: str = \"x86\",                   # Target backend for quantization\n",
    "                 method: str = \"static\",                 # Quantization method: 'static', 'dynamic', or 'qat'\n",
    "                 qconfig_mapping: Optional[dict] = None, # Optional custom quantization config\n",
    "                 custom_configs: Optional[dict] = None,  # Custom module-specific configurations\n",
    "                 use_per_tensor: bool = False,           # Force per-tensor quantization\n",
    "                 verbose: bool = False                   # Enable verbose output\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initialize a quantizer with specified backend and options.\n",
    "        \"\"\"\n",
    "        store_attr()\n",
    "        \n",
    "        # Get the default config mapping for this backend\n",
    "        if qconfig_mapping is None:\n",
    "            if method == \"qat\":\n",
    "                self.qconfig_mapping = get_default_qat_qconfig_mapping(backend)\n",
    "            else:\n",
    "                self.qconfig_mapping = get_default_qconfig_mapping(backend)\n",
    "                \n",
    "            # If per-tensor quantization is enforced, update the global qconfig\n",
    "            if use_per_tensor:\n",
    "                self._update_qconfig_for_per_tensor()\n",
    "        else:\n",
    "            self.qconfig_mapping = qconfig_mapping\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def _quantized_engine(self):\n",
    "        \"\"\"Context manager to temporarily set the quantization backend engine.\"\"\"\n",
    "        old_engine = torch.backends.quantized.engine\n",
    "        torch.backends.quantized.engine = self.backend\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            torch.backends.quantized.engine = old_engine\n",
    "\n",
    "    def _update_qconfig_for_per_tensor(self):\n",
    "        \"\"\"Replace per-channel with per-tensor quantization to avoid conversion issues\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Using per-tensor quantization instead of per-channel\")\n",
    "            \n",
    "        if self.method == \"qat\":            \n",
    "            weight_observer = MinMaxObserver.with_args(\n",
    "                dtype=torch.qint8,\n",
    "                qscheme=torch.per_tensor_symmetric,\n",
    "                quant_min=-128,\n",
    "                quant_max=127\n",
    "            )\n",
    "            \n",
    "            activation_observer = MovingAverageMinMaxObserver.with_args(\n",
    "                averaging_constant=0.01,\n",
    "                quant_min=0,\n",
    "                quant_max=255\n",
    "            )\n",
    "            \n",
    "            per_tensor_qconfig = QConfig(\n",
    "                activation=FakeQuantize.with_args(\n",
    "                    observer=activation_observer, \n",
    "                    quant_min=0, \n",
    "                    quant_max=255\n",
    "                ),\n",
    "                weight=FakeQuantize.with_args(\n",
    "                    observer=weight_observer, \n",
    "                    quant_min=-128, \n",
    "                    quant_max=127\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            activation_observer = MinMaxObserver.with_args(\n",
    "                dtype=torch.quint8,\n",
    "                qscheme=torch.per_tensor_affine,\n",
    "                quant_min=0,\n",
    "                quant_max=255\n",
    "            )\n",
    "            \n",
    "            weight_observer = MinMaxObserver.with_args(\n",
    "                dtype=torch.qint8,\n",
    "                qscheme=torch.per_tensor_symmetric,\n",
    "                quant_min=-128,\n",
    "                quant_max=127\n",
    "            )\n",
    "            \n",
    "            per_tensor_qconfig = QConfig(\n",
    "                activation=activation_observer,\n",
    "                weight=weight_observer\n",
    "            )\n",
    "            \n",
    "        # Update global qconfig\n",
    "        self.qconfig_mapping.global_qconfig = per_tensor_qconfig\n",
    "\n",
    "    def _apply_custom_configs(self):\n",
    "        \"\"\"Apply custom quantization configurations to specific modules\"\"\"\n",
    "        if not self.custom_configs:\n",
    "            return\n",
    "            \n",
    "        for module_name, config in self.custom_configs.items():\n",
    "            if self.verbose:\n",
    "                print(f\"Setting custom config for {module_name}\")\n",
    "            self.qconfig_mapping.set_module_name(module_name, config)\n",
    "    \n",
    "    def _prepare_model(self, \n",
    "                       model: nn.Module, \n",
    "                       example_inputs: Any\n",
    "                      ) -> nn.Module:\n",
    "        \"\"\"Prepare model for quantization based on selected method\"\"\"\n",
    "        model = model.cpu()  # Move to CPU first for quantization\n",
    "        \n",
    "        if self.method == \"qat\":\n",
    "            model = model.train()  # QAT needs train mode\n",
    "        else:\n",
    "            model = model.eval()  # PTQ needs eval mode\n",
    "        \n",
    "        try:\n",
    "            with self._quantized_engine():\n",
    "                if self.method == \"static\":\n",
    "                    return prepare_fx(model, self.qconfig_mapping, example_inputs)\n",
    "                elif self.method == \"dynamic\":                \n",
    "                    # Setup dynamic qconfig for supported modules\n",
    "                    self.qconfig_mapping.set_object_type(torch.nn.Linear, default_dynamic_qconfig)\n",
    "                    self.qconfig_mapping.set_object_type(torch.nn.LSTM, default_dynamic_qconfig)\n",
    "                    self.qconfig_mapping.set_object_type(torch.nn.GRU, default_dynamic_qconfig)\n",
    "                    self.qconfig_mapping.set_object_type(torch.nn.RNN, default_dynamic_qconfig)\n",
    "                    \n",
    "                    # Apply any custom module configs\n",
    "                    if self.custom_configs:\n",
    "                        for module_name, config in self.custom_configs.items():\n",
    "                            self.qconfig_mapping.set_module_name(module_name, config)\n",
    "                    \n",
    "                    return prepare_fx(model, self.qconfig_mapping, example_inputs)\n",
    "                elif self.method == \"qat\":\n",
    "                    return prepare_qat_fx(model, self.qconfig_mapping, example_inputs)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown quantization method: {self.method}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error preparing model for quantization: {e}\")\n",
    "    \n",
    "    def _calibrate_model(self, \n",
    "                        model: nn.Module, \n",
    "                        dataloader: Any, \n",
    "                        max_samples: Optional[int] = None, \n",
    "                        device: Union[str, torch.device] = 'cpu'\n",
    "                       ) -> None:\n",
    "        \"\"\"Calibrate the model on CPU (PyTorch quantization is CPU-only).\"\"\"\n",
    "        model.eval()\n",
    "        device = torch.device(device)\n",
    "        \n",
    "        # Quantized models must stay on CPU - PyTorch quantization backends are CPU-only\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Get dataset size from fastai dataloader\n",
    "        num_samples = getattr(dataloader, 'n', None)\n",
    "        \n",
    "        # Apply max samples limit if provided\n",
    "        if max_samples is not None and num_samples is not None:\n",
    "            num_samples = min(num_samples, max_samples)\n",
    "        \n",
    "        # Create progress bar if verbose\n",
    "        data_iter = dataloader if not self.verbose else tqdm(\n",
    "            dataloader, desc=\"Calibrating\", total=num_samples//dataloader.bs if num_samples else None\n",
    "        )\n",
    "        \n",
    "        # Run calibration\n",
    "        samples_seen = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                # Get inputs from the batch\n",
    "                if isinstance(batch, (list, tuple)) and len(batch) >= 1:\n",
    "                    inputs = batch[0]\n",
    "                else:\n",
    "                    inputs = batch\n",
    "                \n",
    "                # Handle fastai's TensorImage type\n",
    "                if hasattr(inputs, 'data'):\n",
    "                    inputs = inputs.data\n",
    "                    \n",
    "                # Move inputs to the device\n",
    "                if isinstance(inputs, (list, tuple)):\n",
    "                    inputs = [x.to(device) if isinstance(x, torch.Tensor) else x for x in inputs]\n",
    "                else:\n",
    "                    inputs = inputs.to(device)\n",
    "                \n",
    "                # Forward pass for calibration\n",
    "                model(inputs)\n",
    "                \n",
    "                # Check if we've processed enough samples\n",
    "                batch_size = inputs.shape[0] if isinstance(inputs, torch.Tensor) else inputs[0].shape[0]\n",
    "                samples_seen += batch_size\n",
    "                if max_samples is not None and samples_seen >= max_samples:\n",
    "                    break\n",
    "    \n",
    "    def _quantize_dynamic(self, \n",
    "                          model: nn.Module\n",
    "                         ):\n",
    "        \"\"\"Quantize a model with dynamic quantization\"\"\"\n",
    "        try:\n",
    "            # Create a deep copy of the model for quantization\n",
    "            model_copy = copy.deepcopy(model).cpu().eval()\n",
    "            \n",
    "            # Attempt to quantize all compatible module types\n",
    "            qconfig_spec = {module_class for module_class in [nn.Linear, nn.LSTM, nn.GRU, nn.RNN]}\n",
    "            with self._quantized_engine():\n",
    "                quantized_model = quantize_dynamic(\n",
    "                    model_copy, \n",
    "                    qconfig_spec=qconfig_spec,\n",
    "                    dtype=torch.qint8,\n",
    "                    inplace=False\n",
    "                )\n",
    "            return quantized_model\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Dynamic quantization failed with error: {e}\")\n",
    "            return model\n",
    "        \n",
    "    def quantize(self, \n",
    "                model: nn.Module,                        # Model to quantize\n",
    "                calibration_dl: Any,                     # Dataloader for calibration\n",
    "                max_calibration_samples: int = 100,      # Maximum number of samples to use for calibration\n",
    "                device: Union[str, torch.device] = 'cpu' # Device to use for calibration\n",
    "               ) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Quantize a model using the specified method and settings.\n",
    "        \n",
    "        Note: PyTorch quantization produces CPU-only models. The returned model\n",
    "        will always be on CPU regardless of the input model's device.\n",
    "        \"\"\"\n",
    "        # For dynamic quantization, use a specialized approach\n",
    "        if self.method == \"dynamic\":\n",
    "            if self.verbose:\n",
    "                print(f\"Performing dynamic quantization approach with {self.backend} backend\")\n",
    "            \n",
    "            # Apply any custom configs\n",
    "            self._apply_custom_configs()\n",
    "            \n",
    "            # Use the dynamic quantization approach\n",
    "            return self._quantize_dynamic(model)\n",
    "        \n",
    "        # Apply any custom configs for static/QAT\n",
    "        self._apply_custom_configs()\n",
    "        \n",
    "        example_batch, _ = calibration_dl.one_batch()\n",
    "        \n",
    "        try:\n",
    "            # Prepare the model - prepare_fx and prepare_qat_fx will handle fusion automatically\n",
    "            if self.verbose:\n",
    "                print(f\"Preparing model for {self.method} quantization with {self.backend} backend\")\n",
    "            model_prepared = self._prepare_model(model, example_batch.cpu())\n",
    "            \n",
    "            # For static quantization, perform calibration\n",
    "            if self.method in [\"static\", \"qat\"]:\n",
    "                if self.verbose:\n",
    "                    print(f\"Calibrating with up to {max_calibration_samples} samples\")\n",
    "                self._calibrate_model(\n",
    "                    model_prepared, calibration_dl, \n",
    "                    max_samples=max_calibration_samples, device=device\n",
    "                )\n",
    "            \n",
    "            # Convert the model to a quantized version - convert_fx will handle final fusion\n",
    "            if self.verbose:\n",
    "                print(\"Converting to quantized model\")\n",
    "            \n",
    "            try:\n",
    "                with self._quantized_engine():\n",
    "                    quantized_model = convert_fx(model_prepared)\n",
    "            except RuntimeError as e:\n",
    "                if \"Unsupported qscheme: per_channel_affine\" in str(e) and not self.use_per_tensor:\n",
    "                    if self.verbose:\n",
    "                        print(\"Encountered per_channel_affine error, retrying with per-tensor quantization\")\n",
    "                    # Try again with per-tensor quantization\n",
    "                    self.use_per_tensor = True\n",
    "                    self._update_qconfig_for_per_tensor()\n",
    "                    return self.quantize(\n",
    "                        model, calibration_dl, max_calibration_samples, device\n",
    "                    )\n",
    "                else:\n",
    "                    raise e\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"Quantization complete\")\n",
    "            \n",
    "            return quantized_model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during quantization: {e}\")\n",
    "            if self.verbose:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            # Return the original model if quantization fails\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb7a13-e093-4392-b7d5-d16bc24c1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Quantizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70o19a3qmhg",
   "metadata": {},
   "source": "**Parameters:**\n\n- `backend`: Target hardware backend (`'x86'`, `'qnnpack'`, `'fbgemm'`)\n- `method`: Quantization approach (`'static'`, `'dynamic'`, `'qat'`)\n- `qconfig_mapping`: Optional custom quantization configuration\n- `custom_configs`: Dict of module-specific configurations\n- `use_per_tensor`: Force per-tensor quantization (may help with conversion issues)\n- `verbose`: Enable detailed output during quantization\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c62a85-878f-4219-91e5-9ae67a049a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Quantizer.quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zhx3f62nbwe",
   "metadata": {},
   "source": "---\n\n## Usage Examples\n\n### Static Quantization (Recommended for best accuracy)\n\n```python\nfrom fasterai.quantize.quantizer import Quantizer\n\n# Create quantizer for static quantization\nquantizer = Quantizer(\n    backend='x86',\n    method='static',\n    verbose=True\n)\n\n# Quantize with calibration data\nquantized_model = quantizer.quantize(\n    model,\n    calibration_dl=dls.valid,\n    max_calibration_samples=100\n)\n```\n\n### Dynamic Quantization (No calibration needed)\n\n```python\nfrom fasterai.quantize.quantizer import Quantizer\n\n# Create quantizer for dynamic quantization\nquantizer = Quantizer(\n    backend='x86',\n    method='dynamic'\n)\n\n# Quantize - no dataloader needed\nquantized_model = quantizer.quantize(model, calibration_dl=dls.valid)\n```\n\n### Mobile Deployment (ARM devices)\n\n```python\nfrom fasterai.quantize.quantizer import Quantizer\n\n# Use qnnpack backend for mobile\nquantizer = Quantizer(\n    backend='qnnpack',\n    method='static'\n)\n\nquantized_model = quantizer.quantize(model, calibration_dl=dls.valid)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "3173e3a3-23f7-4fec-a54c-6f631d6dcaad",
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## See Also\n\n- [QuantizeCallback](quantize_callback.html) - Apply quantization during fastai training\n- [PyTorch Quantization Documentation](https://pytorch.org/docs/stable/quantization.html) - Official PyTorch quantization guide\n- [ONNX Exporter](../export/onnx_exporter.html) - Export models for cross-platform deployment"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
