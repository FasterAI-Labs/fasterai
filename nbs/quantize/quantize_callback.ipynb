{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba33972b-ea7c-441f-9a59-d5c4968b3783",
   "metadata": {},
   "source": [
    "---\n",
    "description: Quantize your network during training\n",
    "output-file: quantize_callback.html\n",
    "title: Quantize Callback\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c267aef-06bc-42a0-9626-bbfa7d7eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantize.quantize_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e92b35-953f-4656-997c-cbe5bea1c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a124f8-47cd-4df9-885f-df10887fc51b",
   "metadata": {},
   "outputs": [],
   "source": "#|export\nfrom fastai.callback.all import *\nfrom fastcore.basics import store_attr\nfrom fasterai.quantize.quantizer import Quantizer\nfrom torch.ao.quantization.quantize_fx import convert_fx\nimport torch\nimport copy"
  },
  {
   "cell_type": "markdown",
   "id": "yjn1ex4spah",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `QuantizeCallback` enables Quantization-Aware Training (QAT) within the fastai training loop. QAT simulates quantization effects during training, allowing the model to adapt its weights for better accuracy after quantization.\n",
    "\n",
    "**Why use QAT over post-training quantization?**\n",
    "- Higher accuracy on the quantized model\n",
    "- Model learns to be robust to quantization noise\n",
    "- Especially beneficial for models sensitive to precision loss\n",
    "\n",
    "**Trade-offs:**\n",
    "- Requires retraining (not just calibration)\n",
    "- Training is slower due to simulated quantization\n",
    "- Only for situations where you can afford additional training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc203ce-0e73-454e-8ecc-0ac07327bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class QuantizeCallback(Callback):\n",
    "    \"\"\"\n",
    "    Simple callback for Quantization-Aware Training (QAT) in fastai.\n",
    "    Uses the Quantizer class for configuration and conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 quantizer=None,        # Provide custom quantizer\n",
    "                 backend='x86',         # Target backend for quantization: 'x86', 'qnnpack'\n",
    "                 use_per_tensor=False,  # Force per-tensor quantization\n",
    "                 verbose=False          # Enable verbose output\n",
    "                ):\n",
    "        \"Initialize the QAT callback.\"\n",
    "        store_attr()\n",
    "        self.original_model = None\n",
    "    \n",
    "    def before_fit(self) -> None:\n",
    "        \"Prepare model for quantization-aware training\"\n",
    "        # Save original model\n",
    "        self.original_model = copy.deepcopy(self.learn.model)\n",
    "        \n",
    "        # Create quantizer if not provided\n",
    "        if self.quantizer is None:\n",
    "            self.quantizer = Quantizer(\n",
    "                backend=self.backend,\n",
    "                method=\"qat\",\n",
    "                use_per_tensor=self.use_per_tensor,\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "        \n",
    "        # Get example inputs\n",
    "        x, _ = self.learn.dls.one_batch()\n",
    "        original_device = next(self.learn.model.parameters()).device\n",
    "        \n",
    "        # Temporarily move to CPU for preparation\n",
    "        self.learn.model = self.learn.model.cpu()\n",
    "        \n",
    "        # Prepare model for QAT using the quantizer\n",
    "        try:\n",
    "            # First save the original state dict\n",
    "            orig_state_dict = self.learn.model.state_dict()\n",
    "            \n",
    "            # Use the _prepare_model method from the quantizer\n",
    "            prepared_model = self.quantizer._prepare_model(self.learn.model, x.cpu())\n",
    "            \n",
    "            # Move back to original device and update learner's model\n",
    "            self.learn.model = prepared_model.to(original_device)\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(\"Model prepared for QAT successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing model for QAT: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Restore original model on error\n",
    "            self.learn.model = self.original_model.to(original_device)\n",
    "    \n",
    "    def after_fit(self) -> None:\n",
    "        \"Convert QAT model to fully quantized model\"\n",
    "        # Get original device before try block to ensure it's available in except\n",
    "        original_device = next(self.learn.model.parameters()).device\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Converting QAT model to fully quantized model\")\n",
    "            \n",
    "            # Set model to eval mode and move to CPU for conversion\n",
    "            self.learn.model = self.learn.model.cpu().eval()\n",
    "            \n",
    "            # Save a copy of the trained QAT model\n",
    "            self.qat_model = copy.deepcopy(self.learn.model)\n",
    "            \n",
    "            # Convert to quantized model\n",
    "            quantized_model = convert_fx(self.learn.model)\n",
    "            \n",
    "            # Save the quantized model\n",
    "            self.learn.quantized_model = quantized_model\n",
    "            \n",
    "            # Keep the quantized model as the active model\n",
    "            # This is crucial - the quantized model IS the trained model\n",
    "            self.learn.model = quantized_model\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting QAT model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # If conversion fails, at least keep the QAT-trained model\n",
    "            if hasattr(self, 'qat_model'):\n",
    "                self.learn.model = self.qat_model.to(original_device)\n",
    "                print(\"Conversion failed, but QAT-trained model was kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b602ec-9ff5-4cd5-a411-da02f726e8a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'store_attr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mQuantizeCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mQuantizeCallback.__init__\u001b[39m\u001b[34m(self, quantizer, backend, use_per_tensor, verbose)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[32m      8\u001b[39m              quantizer=\u001b[38;5;28;01mNone\u001b[39;00m,        \u001b[38;5;66;03m# Provide custom quantizer\u001b[39;00m\n\u001b[32m      9\u001b[39m              backend=\u001b[33m'\u001b[39m\u001b[33mx86\u001b[39m\u001b[33m'\u001b[39m,         \u001b[38;5;66;03m# Target backend for quantization: 'x86', 'qnnpack'\u001b[39;00m\n\u001b[32m     10\u001b[39m              use_per_tensor=\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Force per-tensor quantization\u001b[39;00m\n\u001b[32m     11\u001b[39m              verbose=\u001b[38;5;28;01mFalse\u001b[39;00m          \u001b[38;5;66;03m# Enable verbose output\u001b[39;00m\n\u001b[32m     12\u001b[39m             ):\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mInitialize the QAT callback.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mstore_attr\u001b[49m()\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m.original_model = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'store_attr' is not defined"
     ]
    }
   ],
   "source": [
    "QuantizeCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dc527-cf87-4761-ade5-e34bbca9398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(QuantizeCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eiw60p3r7ff",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "\n",
    "- `quantizer`: Optional custom `Quantizer` instance for advanced configuration\n",
    "- `backend`: Target backend (`'x86'`, `'qnnpack'`) - only used if quantizer not provided\n",
    "- `use_per_tensor`: Force per-tensor quantization to avoid conversion issues\n",
    "- `verbose`: Enable detailed output during QAT\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "from fasterai.quantize.quantize_callback import QuantizeCallback\n",
    "\n",
    "# Basic QAT with default settings\n",
    "cb = QuantizeCallback(backend='x86', verbose=True)\n",
    "\n",
    "# Train with QAT\n",
    "learn.fit(5, cbs=[cb])\n",
    "\n",
    "# After training, the quantized model is available at:\n",
    "quantized_model = learn.quantized_model\n",
    "```\n",
    "\n",
    "### QAT Workflow\n",
    "\n",
    "1. **before_fit**: Model is prepared for QAT (fake quantization nodes inserted)\n",
    "2. **Training**: Model trains with simulated quantization effects\n",
    "3. **after_fit**: Model is converted to fully quantized form\n",
    "\n",
    "The final `learn.model` is the quantized model ready for CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd980254-ea26-43c7-bebe-2e7b94d8c637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ljoxbw41oy",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Quantizer](quantizer.html) - Core quantization class with backend/method options\n",
    "- [ONNX Exporter](../export/onnx_exporter.html) - Export quantized models for deployment\n",
    "- [PyTorch Quantization Docs](https://pytorch.org/docs/stable/quantization.html) - Official PyTorch guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
