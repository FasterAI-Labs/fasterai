{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba33972b-ea7c-441f-9a59-d5c4968b3783",
   "metadata": {},
   "source": [
    "---\n",
    "description: Quantize your network during training\n",
    "output-file: quantize_callback.html\n",
    "title: Quantize Callback\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c267aef-06bc-42a0-9626-bbfa7d7eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantize.quantize_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e92b35-953f-4656-997c-cbe5bea1c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a124f8-47cd-4df9-885f-df10887fc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.callback.all import *\n",
    "from fastcore.basics import store_attr\n",
    "from fasterai.quantize.quantizer import Quantizer\n",
    "from torch.ao.quantization.quantize_fx import convert_fx\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yjn1ex4spah",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `QuantizeCallback` enables Quantization-Aware Training (QAT) within the fastai training loop. QAT simulates quantization effects during training, allowing the model to adapt its weights for better accuracy after quantization.\n",
    "\n",
    "**Why use QAT over post-training quantization?**\n",
    "- Higher accuracy on the quantized model\n",
    "- Model learns to be robust to quantization noise\n",
    "- Especially beneficial for models sensitive to precision loss\n",
    "\n",
    "**Trade-offs:**\n",
    "- Requires retraining (not just calibration)\n",
    "- Training is slower due to simulated quantization\n",
    "- Only for situations where you can afford additional training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc203ce-0e73-454e-8ecc-0ac07327bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class QuantizeCallback(Callback):\n",
    "    \"\"\"\n",
    "    Simple callback for Quantization-Aware Training (QAT) in fastai.\n",
    "    Uses the Quantizer class for configuration and conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 quantizer=None,        # Provide custom quantizer\n",
    "                 backend='x86',         # Target backend for quantization: 'x86', 'qnnpack'\n",
    "                 use_per_tensor=False,  # Force per-tensor quantization\n",
    "                 verbose=False          # Enable verbose output\n",
    "                ):\n",
    "        \"Initialize the QAT callback.\"\n",
    "        store_attr()\n",
    "        self.original_model = None\n",
    "    \n",
    "    def before_fit(self) -> None:\n",
    "        \"Prepare model for quantization-aware training\"\n",
    "        # Save original model\n",
    "        self.original_model = copy.deepcopy(self.learn.model)\n",
    "        \n",
    "        # Create quantizer if not provided\n",
    "        if self.quantizer is None:\n",
    "            self.quantizer = Quantizer(\n",
    "                backend=self.backend,\n",
    "                method=\"qat\",\n",
    "                use_per_tensor=self.use_per_tensor,\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "        \n",
    "        # Get example inputs\n",
    "        x, _ = self.learn.dls.one_batch()\n",
    "        original_device = next(self.learn.model.parameters()).device\n",
    "        \n",
    "        # Temporarily move to CPU for preparation\n",
    "        self.learn.model = self.learn.model.cpu()\n",
    "        \n",
    "        # Prepare model for QAT using the quantizer\n",
    "        try:\n",
    "            # First save the original state dict\n",
    "            orig_state_dict = self.learn.model.state_dict()\n",
    "            \n",
    "            # Use the _prepare_model method from the quantizer\n",
    "            prepared_model = self.quantizer._prepare_model(self.learn.model, x.cpu())\n",
    "            \n",
    "            # Move back to original device and update learner's model\n",
    "            self.learn.model = prepared_model.to(original_device)\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(\"Model prepared for QAT successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing model for QAT: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Restore original model on error\n",
    "            self.learn.model = self.original_model.to(original_device)\n",
    "    \n",
    "    def after_fit(self) -> None:\n",
    "        \"Convert QAT model to fully quantized model\"\n",
    "        # Get original device before try block to ensure it's available in except\n",
    "        original_device = next(self.learn.model.parameters()).device\n",
    "        \n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Converting QAT model to fully quantized model\")\n",
    "            \n",
    "            # Set model to eval mode and move to CPU for conversion\n",
    "            self.learn.model = self.learn.model.cpu().eval()\n",
    "            \n",
    "            # Save a copy of the trained QAT model\n",
    "            self.qat_model = copy.deepcopy(self.learn.model)\n",
    "            \n",
    "            # Convert to quantized model\n",
    "            quantized_model = convert_fx(self.learn.model)\n",
    "            \n",
    "            # Save the quantized model\n",
    "            self.learn.quantized_model = quantized_model\n",
    "            \n",
    "            # Keep the quantized model as the active model\n",
    "            # This is crucial - the quantized model IS the trained model\n",
    "            self.learn.model = quantized_model\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting QAT model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # If conversion fails, at least keep the QAT-trained model\n",
    "            if hasattr(self, 'qat_model'):\n",
    "                self.learn.model = self.qat_model.to(original_device)\n",
    "                print(\"Conversion failed, but QAT-trained model was kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dc527-cf87-4761-ade5-e34bbca9398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(QuantizeCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eiw60p3r7ff",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "\n",
    "- `quantizer`: Optional custom `Quantizer` instance for advanced configuration\n",
    "- `backend`: Target backend (`'x86'`, `'qnnpack'`) - only used if quantizer not provided\n",
    "- `use_per_tensor`: Force per-tensor quantization to avoid conversion issues\n",
    "- `verbose`: Enable detailed output during QAT\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "from fasterai.quantize.quantize_callback import QuantizeCallback\n",
    "\n",
    "# Basic QAT with default settings\n",
    "cb = QuantizeCallback(backend='x86', verbose=True)\n",
    "\n",
    "# Train with QAT\n",
    "learn.fit(5, cbs=[cb])\n",
    "\n",
    "# After training, the quantized model is available at:\n",
    "quantized_model = learn.quantized_model\n",
    "```\n",
    "\n",
    "### QAT Workflow\n",
    "\n",
    "1. **before_fit**: Model is prepared for QAT (fake quantization nodes inserted)\n",
    "2. **Training**: Model trains with simulated quantization effects\n",
    "3. **after_fit**: Model is converted to fully quantized form\n",
    "\n",
    "The final `learn.model` is the quantized model ready for CPU inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ctui4863mse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# Construction with defaults\n",
    "cb = QuantizeCallback()\n",
    "test_eq(cb.backend, 'x86')\n",
    "test_eq(cb.use_per_tensor, False)\n",
    "test_eq(cb.verbose, False)\n",
    "assert cb.quantizer is None  # created lazily in before_fit\n",
    "assert cb.original_model is None\n",
    "\n",
    "# Construction with custom params\n",
    "cb2 = QuantizeCallback(backend='qnnpack', use_per_tensor=True, verbose=True)\n",
    "test_eq(cb2.backend, 'qnnpack')\n",
    "test_eq(cb2.use_per_tensor, True)\n",
    "test_eq(cb2.verbose, True)\n",
    "\n",
    "# Construction with pre-built quantizer\n",
    "from fasterai.quantize.quantizer import Quantizer\n",
    "q = Quantizer(backend='x86', method='qat')\n",
    "cb3 = QuantizeCallback(quantizer=q)\n",
    "test_eq(cb3.quantizer.method, 'qat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r25gzhzc9b",
   "metadata": {},
   "outputs": [],
   "source": "#| hide\n#| slow\n# Full QAT training loop â€” verify model is prepared and converted\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom fastai.data.core import DataLoaders\nfrom fastai.learner import Learner\n\n_model = nn.Sequential(\n    nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n    nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(16, 10)\n)\n_X = torch.randn(64, 3, 8, 8)\n_y = torch.randint(0, 10, (64,))\n_dls = DataLoaders.from_dsets(\n    TensorDataset(_X[:48], _y[:48]),\n    TensorDataset(_X[48:], _y[48:]),\n    bs=16, device='cpu'\n)\n\n_cb = QuantizeCallback(backend='x86', use_per_tensor=True)\n_learn = Learner(_dls, _model, loss_func=nn.CrossEntropyLoss(), cbs=[_cb])\n_learn.fit(2)\n\n# Verify QAT produced a quantized model\nassert hasattr(_learn, 'quantized_model'), \"quantized_model should be set by after_fit\"\nassert _cb.original_model is not None, \"original model backup should exist\"\n\n# Verify the quantized model can still produce output\nwith torch.no_grad():\n    _out = _learn.quantized_model(_X[:1].cpu())\ntest_eq(_out.shape, (1, 10))"
  },
  {
   "cell_type": "markdown",
   "id": "ljoxbw41oy",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Quantizer](quantizer.html) - Core quantization class with backend/method options\n",
    "- [ONNX Exporter](../export/onnx_exporter.html) - Export quantized models for deployment\n",
    "- [PyTorch Quantization Docs](https://pytorch.org/docs/stable/quantization.html) - Official PyTorch guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
