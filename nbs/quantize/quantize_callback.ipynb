{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba33972b-ea7c-441f-9a59-d5c4968b3783",
   "metadata": {},
   "source": [
    "---\n",
    "description: Quantize your network during training\n",
    "output-file: quantize_callback.html\n",
    "title: Quantize Callback\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c267aef-06bc-42a0-9626-bbfa7d7eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantize.quantize_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a124f8-47cd-4df9-885f-df10887fc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.callback.all import *\n",
    "from fasterai.quantize.all import *\n",
    "from torch.ao.quantization.quantize_fx import convert_fx\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc203ce-0e73-454e-8ecc-0ac07327bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class QuantizeCallback(Callback):\n",
    "    \"\"\"\n",
    "    Simple callback for Quantization-Aware Training (QAT) in fastai.\n",
    "    Uses the Quantizer class for configuration and conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self, quantizer=None, backend='x86', use_per_tensor=False, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the QAT callback.\n",
    "        \"\"\"\n",
    "        self.quantizer = quantizer\n",
    "        self.backend = backend\n",
    "        self.use_per_tensor = use_per_tensor\n",
    "        self.verbose = verbose\n",
    "        self.original_model = None\n",
    "    \n",
    "    def before_fit(self):\n",
    "        # Save original model\n",
    "        self.original_model = copy.deepcopy(self.learn.model)\n",
    "        \n",
    "        # Create quantizer if not provided\n",
    "        if self.quantizer is None:\n",
    "            self.quantizer = Quantizer(\n",
    "                backend=self.backend,\n",
    "                method=\"qat\",\n",
    "                use_per_tensor=self.use_per_tensor,\n",
    "                verbose=self.verbose\n",
    "            )\n",
    "        \n",
    "        # Get example inputs\n",
    "        x, _ = self.learn.dls.one_batch()\n",
    "        original_device = next(self.learn.model.parameters()).device\n",
    "        \n",
    "        # Temporarily move to CPU for preparation\n",
    "        self.learn.model = self.learn.model.cpu()\n",
    "        \n",
    "        # Prepare model for QAT using the quantizer\n",
    "        try:\n",
    "            # First save the original state dict\n",
    "            orig_state_dict = self.learn.model.state_dict()\n",
    "            \n",
    "            # Use the _prepare_model method from the quantizer\n",
    "            prepared_model = self.quantizer._prepare_model(self.learn.model, x.cpu())\n",
    "            \n",
    "            # Move back to original device and update learner's model\n",
    "            self.learn.model = prepared_model.to(original_device)\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(\"Model prepared for QAT successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing model for QAT: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Restore original model on error\n",
    "            self.learn.model = self.original_model.to(original_device)\n",
    "    \n",
    "    def after_fit(self):\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(\"Converting QAT model to fully quantized model\")\n",
    "            \n",
    "            # Remember the original device\n",
    "            original_device = next(self.learn.model.parameters()).device\n",
    "            \n",
    "            # Set model to eval mode and move to CPU for conversion\n",
    "            self.learn.model = self.learn.model.cpu().eval()\n",
    "            \n",
    "            # Save a copy of the trained QAT model\n",
    "            self.qat_model = copy.deepcopy(self.learn.model)\n",
    "            \n",
    "            # Convert to quantized model\n",
    "            quantized_model = convert_fx(self.learn.model)\n",
    "            \n",
    "            # Save the quantized model\n",
    "            self.learn.quantized_model = quantized_model\n",
    "            \n",
    "            # Keep the quantized model as the active model\n",
    "            # This is crucial - the quantized model IS the trained model\n",
    "            self.learn.model = quantized_model\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error converting QAT model: {e}\")\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            # If conversion fails, at least keep the QAT-trained model\n",
    "            if hasattr(self, 'qat_model'):\n",
    "                self.learn.model = self.qat_model.to(original_device)\n",
    "                print(\"Conversion failed, but QAT-trained model was kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dc527-cf87-4761-ade5-e34bbca9398c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd980254-ea26-43c7-bebe-2e7b94d8c637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
