{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba33972b-ea7c-441f-9a59-d5c4968b3783",
   "metadata": {},
   "source": [
    "---\n",
    "description: Quantize your network during training\n",
    "output-file: quantize_callback.html\n",
    "title: Quantize Callback\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c267aef-06bc-42a0-9626-bbfa7d7eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp quantize.quantize_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e92b35-953f-4656-997c-cbe5bea1c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a124f8-47cd-4df9-885f-df10887fc51b",
   "metadata": {},
   "outputs": [],
   "source": "#|export\nfrom fastai.callback.all import *\nfrom fasterai.quantize.quantizer import Quantizer\nfrom torch.ao.quantization.quantize_fx import convert_fx\nimport torch\nimport copy"
  },
  {
   "cell_type": "markdown",
   "id": "yjn1ex4spah",
   "metadata": {},
   "source": "## Overview\n\nThe `QuantizeCallback` enables Quantization-Aware Training (QAT) within the fastai training loop. QAT simulates quantization effects during training, allowing the model to adapt its weights for better accuracy after quantization.\n\n**Why use QAT over post-training quantization?**\n- Higher accuracy on the quantized model\n- Model learns to be robust to quantization noise\n- Especially beneficial for models sensitive to precision loss\n\n**Trade-offs:**\n- Requires retraining (not just calibration)\n- Training is slower due to simulated quantization\n- Only for situations where you can afford additional training time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc203ce-0e73-454e-8ecc-0ac07327bec0",
   "metadata": {},
   "outputs": [],
   "source": "#|export\nclass QuantizeCallback(Callback):\n    \"\"\"\n    Simple callback for Quantization-Aware Training (QAT) in fastai.\n    Uses the Quantizer class for configuration and conversion.\n    \"\"\"\n    def __init__(self, \n                 quantizer=None,        # Provide custom quantizer\n                 backend='x86',         # Target backend for quantization: 'x86', 'qnnpack'\n                 use_per_tensor=False,  # Force per-tensor quantization\n                 verbose=False          # Enable verbose output\n                ):\n        \"\"\"\n        Initialize the QAT callback.\n        \"\"\"\n        self.quantizer = quantizer\n        self.backend = backend\n        self.use_per_tensor = use_per_tensor\n        self.verbose = verbose\n        self.original_model = None\n    \n    def before_fit(self) -> None:\n        \"Prepare model for quantization-aware training\"\n        # Save original model\n        self.original_model = copy.deepcopy(self.learn.model)\n        \n        # Create quantizer if not provided\n        if self.quantizer is None:\n            self.quantizer = Quantizer(\n                backend=self.backend,\n                method=\"qat\",\n                use_per_tensor=self.use_per_tensor,\n                verbose=self.verbose\n            )\n        \n        # Get example inputs\n        x, _ = self.learn.dls.one_batch()\n        original_device = next(self.learn.model.parameters()).device\n        \n        # Temporarily move to CPU for preparation\n        self.learn.model = self.learn.model.cpu()\n        \n        # Prepare model for QAT using the quantizer\n        try:\n            # First save the original state dict\n            orig_state_dict = self.learn.model.state_dict()\n            \n            # Use the _prepare_model method from the quantizer\n            prepared_model = self.quantizer._prepare_model(self.learn.model, x.cpu())\n            \n            # Move back to original device and update learner's model\n            self.learn.model = prepared_model.to(original_device)\n                \n            if self.verbose:\n                print(\"Model prepared for QAT successfully\")\n                \n        except Exception as e:\n            print(f\"Error preparing model for QAT: {e}\")\n            import traceback\n            traceback.print_exc()\n            # Restore original model on error\n            self.learn.model = self.original_model.to(original_device)\n    \n    def after_fit(self) -> None:\n        \"Convert QAT model to fully quantized model\"\n        # Get original device before try block to ensure it's available in except\n        original_device = next(self.learn.model.parameters()).device\n        \n        try:\n            if self.verbose:\n                print(\"Converting QAT model to fully quantized model\")\n            \n            # Set model to eval mode and move to CPU for conversion\n            self.learn.model = self.learn.model.cpu().eval()\n            \n            # Save a copy of the trained QAT model\n            self.qat_model = copy.deepcopy(self.learn.model)\n            \n            # Convert to quantized model\n            quantized_model = convert_fx(self.learn.model)\n            \n            # Save the quantized model\n            self.learn.quantized_model = quantized_model\n            \n            # Keep the quantized model as the active model\n            # This is crucial - the quantized model IS the trained model\n            self.learn.model = quantized_model\n                \n        except Exception as e:\n            print(f\"Error converting QAT model: {e}\")\n            import traceback\n            traceback.print_exc()\n            \n            # If conversion fails, at least keep the QAT-trained model\n            if hasattr(self, 'qat_model'):\n                self.learn.model = self.qat_model.to(original_device)\n                print(\"Conversion failed, but QAT-trained model was kept\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00dc527-cf87-4761-ade5-e34bbca9398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(QuantizeCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eiw60p3r7ff",
   "metadata": {},
   "source": "**Parameters:**\n\n- `quantizer`: Optional custom `Quantizer` instance for advanced configuration\n- `backend`: Target backend (`'x86'`, `'qnnpack'`) - only used if quantizer not provided\n- `use_per_tensor`: Force per-tensor quantization to avoid conversion issues\n- `verbose`: Enable detailed output during QAT\n\n---\n\n## Usage Example\n\n```python\nfrom fasterai.quantize.quantize_callback import QuantizeCallback\n\n# Basic QAT with default settings\ncb = QuantizeCallback(backend='x86', verbose=True)\n\n# Train with QAT\nlearn.fit(5, cbs=[cb])\n\n# After training, the quantized model is available at:\nquantized_model = learn.quantized_model\n```\n\n### QAT Workflow\n\n1. **before_fit**: Model is prepared for QAT (fake quantization nodes inserted)\n2. **Training**: Model trains with simulated quantization effects\n3. **after_fit**: Model is converted to fully quantized form\n\nThe final `learn.model` is the quantized model ready for CPU inference."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd980254-ea26-43c7-bebe-2e7b94d8c637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ljoxbw41oy",
   "metadata": {},
   "source": "---\n\n## See Also\n\n- [Quantizer](quantizer.html) - Core quantization class with backend/method options\n- [ONNX Exporter](../export/onnx_exporter.html) - Export quantized models for deployment\n- [PyTorch Quantization Docs](https://pytorch.org/docs/stable/quantization.html) - Official PyTorch guide"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
