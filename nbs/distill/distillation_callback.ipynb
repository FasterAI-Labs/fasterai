{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f8290370",
   "metadata": {},
   "source": [
    "---\n",
    "description: Train a network in a teacher-student fashion\n",
    "output-file: knowledge_distillation.html\n",
    "title: Knowledge Distillation\n",
    "skip_showdoc: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distill.distillation_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nbdev.showdoc import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-congress",
   "metadata": {},
   "source": [
    "Knowledge Distillation, sometimes called teacher-student training, is a compression method in which a small (the student) model is trained to mimic the behaviour of a larger (the teacher) model.\n",
    "\n",
    "The main goal is to reveal what is called the **Dark Knowledge** hidden in the teacher model.\n",
    "\n",
    "If we take the same [example](https://www.ttic.edu/dl/dark14.pdf) provided by Geoffrey Hinton et al., we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-collection",
   "metadata": {},
   "source": [
    "The main problem of classification is that the output activation function (softmax) will, by design, make a single value really high and squash others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-celtic",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j} \\exp \\left(z_{j}\\right)}\n",
    "$$\n",
    "\n",
    "With $p_i$ the probability of class $i$, computed from the logits $z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-microwave",
   "metadata": {},
   "source": [
    "Here is an example to illustrate this phenomenon:\n",
    "\n",
    "Let's say that we have trained a model to discriminate between the following 5 classes: [cow, dog, plane, cat, car]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-access",
   "metadata": {},
   "source": [
    "And here is the output of the final layer (the logits) when the model is fed a new input image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([1.3, 3.1, 0.2, 1.9, -0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-humidity",
   "metadata": {},
   "source": [
    "By judging on the predictions, the model seems confident that the input data is a dog and quite confident that it is definitely not a plane nor a car, with predictions for cow and cat being moderately high.\n",
    "\n",
    "So the model not only has learned to recognize a dog in the image, but also that a dog is very different from a car and a plane and share similarities with cats and cows. This information is what is called **dark knowledge** !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-confusion",
   "metadata": {},
   "source": [
    "When passing those predictions through a softmax, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1063, 0.6431, 0.0354, 0.1937, 0.0215])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = F.softmax(logits, dim=-1); predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-shell",
   "metadata": {},
   "source": [
    "This is accuenting the differences that we had earlier, discarding some of the dark knowledge acquired earlier. The way to keep this knowledge is to \"soften\" our softmax outputs, by adding a **temperature** parameter. The higher the temperature, the softer the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1879, 0.3423, 0.1302, 0.2294, 0.1102])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_predictions = F.softmax(logits/3, dim=-1); soft_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-recipient",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "if the Temperature is equal to 1, then we have regular softmax\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-senate",
   "metadata": {},
   "source": [
    "When applying Knowledge Distillation, we want to keep the **Dark Knowledge** that the teacher model has acquired during its training but not rely entirely on it. So we combine two losses: \n",
    "\n",
    "- The Teacher loss between the softened predictions of the teacher and the softened predictions of the student\n",
    "- The Classification loss, which is the regular loss between hard labels and hard predictions\n",
    "\n",
    "The combination between those losses are weighted by an additional parameter Î±, as:\n",
    "\n",
    "$$\n",
    "L_{K D}=\\alpha  * \\text { CrossEntropy }\\left(p_{S}^{\\tau}, p_{T}^{\\tau}\\right)+(1-\\alpha) * \\text { CrossEntropy }\\left(p_{S}, y_{\\text {true }}\\right)\n",
    "$$\n",
    "\n",
    "With $p^{\\tau}$ being the softened predictions of the student and teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-pulse",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "In practice, the distillation loss will be a [bit different](http://cs230.stanford.edu/files_winter_2018/projects/6940224.pdf) in the implementation\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-hepatitis",
   "metadata": {},
   "source": [
    "![](../imgs/distill.png \"Knowledge Distillation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import reduce\n",
    "from typing import Callable, Any\n",
    "from fasterai.core.schedule import Schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-hardware",
   "metadata": {},
   "source": [
    "This can be done with fastai, using the Callback system !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eeecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class KnowledgeDistillationCallback(Callback):\n",
    "    def __init__(self, \n",
    "                 teacher: nn.Module,                                           # Teacher model\n",
    "                 loss: Callable,                                               # Distillation loss function\n",
    "                 activations_student: str | list[str] | None = None,           # Student activation layers to match\n",
    "                 activations_teacher: str | list[str] | None = None,           # Teacher activation layers to match\n",
    "                 weight: float = 0.5,                                          # Weight for distillation loss\n",
    "                 schedule: Schedule | None = None                              # Optional schedule for weight progression\n",
    "    ):\n",
    "        \"Implement knowledge distillation from a teacher model to the student being trained\"\n",
    "        self.stored_activation_student, self.stored_activation_teacher  = {}, {}\n",
    "        store_attr()\n",
    "        if self.activations_student is not None:\n",
    "            self.activations_student, self.activations_teacher = listify(activations_student), listify(activations_teacher)\n",
    "        self.current_weight = weight\n",
    "        \n",
    "    def before_fit(self) -> None:\n",
    "        \"Setup hooks and prepare teacher before training\"\n",
    "        if self.activations_student and self.activations_teacher: self.register_hooks()\n",
    "        self.teacher.eval()\n",
    "\n",
    "    def before_batch(self) -> None:\n",
    "        \"Update distillation weight if scheduled\"\n",
    "        if self.schedule is not None:\n",
    "            progress = self.schedule.progress(self.pct_train)\n",
    "            self.current_weight = self.weight * progress\n",
    "\n",
    "    def after_batch(self) -> None:\n",
    "        \"Clear activations after each batch to prevent memory buildup\"\n",
    "        self.stored_activation_student.clear()\n",
    "        self.stored_activation_teacher.clear()\n",
    "\n",
    "    def after_loss(self) -> None:\n",
    "        \"Apply distillation loss using teacher predictions\"\n",
    "        teacher_pred = self.teacher(self.x)\n",
    "        new_loss = self.loss(pred=self.pred, teacher_pred=teacher_pred, fm_s=self.stored_activation_student, fm_t=self.stored_activation_teacher)\n",
    "        self.learn.loss_grad = torch.lerp(self.learn.loss_grad, new_loss, self.current_weight)\n",
    "        self.learn.loss = self.learn.loss_grad.clone()\n",
    "    \n",
    "    def register_hooks(self) -> None:\n",
    "        \"Set up forward hooks to capture activations\"\n",
    "        self.handles_st, self.handles_t = {}, {}\n",
    "        for name_st, name_t in zip(self.activations_student, self.activations_teacher):\n",
    "            self.handles_st[name_st] = get_module_by_name(self.learn, name_st).register_forward_hook(self.get_activation(self.stored_activation_student, name_st))\n",
    "            self.handles_t[name_t] = get_module_by_name(self.teacher, name_t).register_forward_hook(self.get_activation(self.stored_activation_teacher, name_t))\n",
    "        \n",
    "    def get_activation(self, \n",
    "                       activation: dict[str, torch.Tensor],  # Dictionary to store activations\n",
    "                       name: str                             # Name of the layer\n",
    "    ) -> Callable:\n",
    "        \"Create a hook function to store activations\"\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output\n",
    "        return hook\n",
    "    \n",
    "    def find_hook(self, \n",
    "                  m: nn.Module\n",
    "    ) -> list[tuple[str, int, str]]:\n",
    "        \"Find all hooks registered in a module\"\n",
    "        save = []\n",
    "        module_name = type(m).__name__\n",
    "        for k, v in m._forward_hooks.items():\n",
    "            save.append((module_name, k, v.__name__))\n",
    "        return save\n",
    "    \n",
    "    def remove_hooks(self, \n",
    "                     handles: dict[str, Any]\n",
    "    ) -> None:\n",
    "        \"Remove all registered hooks\"\n",
    "        for handle in handles.values():\n",
    "            handle.remove()\n",
    "    \n",
    "    def after_fit(self) -> None:\n",
    "        \"Clean up hooks after training\"\n",
    "        if self.activations_student and self.activations_teacher:\n",
    "            self.remove_hooks(self.handles_t)\n",
    "            self.remove_hooks(self.handles_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e012122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/distillation_callback.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### KnowledgeDistillationCallback\n",
       "\n",
       "```python\n",
       "\n",
       "def KnowledgeDistillationCallback(\n",
       "    teacher:nn.Module, # Teacher model\n",
       "    loss:Callable, # Distillation loss function\n",
       "    activations_student:str | list[str] | None=None, # Student activation layers to match\n",
       "    activations_teacher:str | list[str] | None=None, # Teacher activation layers to match\n",
       "    weight:float=0.5, # Weight for distillation loss\n",
       "    schedule:Schedule | None=None, # Optional schedule for weight progression\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def KnowledgeDistillationCallback(\n",
       "    teacher:nn.Module, # Teacher model\n",
       "    loss:Callable, # Distillation loss function\n",
       "    activations_student:str | list[str] | None=None, # Student activation layers to match\n",
       "    activations_teacher:str | list[str] | None=None, # Teacher activation layers to match\n",
       "    weight:float=0.5, # Weight for distillation loss\n",
       "    schedule:Schedule | None=None, # Optional schedule for weight progression\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Basic class handling tweaks of the training loop by changing a `Learner` in various events*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(KnowledgeDistillationCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3378f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_model_layers(\n",
    "    model: nn.Module,             # Model to inspect\n",
    "    getLayerRepr: bool = False    # Whether to return layer representations\n",
    ") -> list[str] | dict[str, str]:\n",
    "    \"Get all layer names in a model, optionally with their representations\"\n",
    "    layers = OrderedDict() if getLayerRepr else []\n",
    "    \n",
    "    def get_layers(net, prefix=[]):\n",
    "        if hasattr(net, \"_modules\"):\n",
    "            for name, layer in net._modules.items():\n",
    "                if layer is None:\n",
    "                    continue\n",
    "                if getLayerRepr:\n",
    "                    layers[\".\".join(prefix+[name])] = layer.__repr__()\n",
    "                else:\n",
    "                    layers.append(\".\".join(prefix + [name]))\n",
    "                get_layers(layer, prefix=prefix+[name])\n",
    "\n",
    "    get_layers(model)\n",
    "    return layers\n",
    "\n",
    "\n",
    "\n",
    "def get_module_by_name(\n",
    "    module: torch.Tensor | nn.Module,  # Module to search in\n",
    "    access_string: str                 # Dot-separated path to the submodule\n",
    ") -> nn.Module | None:\n",
    "    \"Access a nested submodule by its name path\"\n",
    "    try:\n",
    "        names = access_string.split(sep='.')\n",
    "        return reduce(getattr, names, module)\n",
    "    except AttributeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r4r7hlx19ho",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/distillation_callback.py#L121){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_module_by_name\n",
       "\n",
       "```python\n",
       "\n",
       "def get_module_by_name(\n",
       "    module:torch.Tensor | nn.Module, # Module to search in\n",
       "    access_string:str, # Dot-separated path to the submodule\n",
       ")->nn.Module | None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Access a nested submodule by its name path*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def get_module_by_name(\n",
       "    module:torch.Tensor | nn.Module, # Module to search in\n",
       "    access_string:str, # Dot-separated path to the submodule\n",
       ")->nn.Module | None:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Access a nested submodule by its name path*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_model_layers)\n",
    "show_doc(get_module_by_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2adffb",
   "metadata": {},
   "source": [
    "The loss function that is used may depend on the use case. For classification, we usually use the one presented above, named `SoftTarget` in fasterai. But for regression cases, we may want to perform regression on the logits directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f72f3-9a5a-4605-8996-6e49e70fc6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r9q5hndaxd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "def _test_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 3, padding=1),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "model = _test_model()\n",
    "\n",
    "# get_model_layers returns list of strings\n",
    "layers = get_model_layers(model)\n",
    "assert isinstance(layers, list)\n",
    "assert all(isinstance(n, str) for n in layers)\n",
    "assert len(layers) > 0\n",
    "\n",
    "# get_model_layers with repr returns dict (OrderedDict)\n",
    "layers_d = get_model_layers(model, getLayerRepr=True)\n",
    "assert isinstance(layers_d, OrderedDict)\n",
    "assert len(layers_d) > 0\n",
    "\n",
    "# get_module_by_name returns correct module\n",
    "m0 = get_module_by_name(model, '0')\n",
    "test_is(m0, model[0])\n",
    "\n",
    "# get_module_by_name returns None for nonexistent\n",
    "test_eq(get_module_by_name(model, 'nonexistent'), None)\n",
    "\n",
    "# KnowledgeDistillationCallback construction\n",
    "teacher = _test_model()\n",
    "from fasterai.distill.losses import SoftTarget\n",
    "cb = KnowledgeDistillationCallback(\n",
    "    teacher=teacher,\n",
    "    loss=SoftTarget,\n",
    "    weight=0.5\n",
    ")\n",
    "test_eq(cb.weight, 0.5)\n",
    "test_eq(cb.current_weight, 0.5)\n",
    "assert cb.activations_student is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iesi7mbl80p",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.191925</td>\n",
       "      <td>1.231740</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.190237</td>\n",
       "      <td>1.229488</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "#| slow\n",
    "# Teacher-student training with KnowledgeDistillationCallback\n",
    "from torch.utils.data import TensorDataset\n",
    "from fastai.data.core import DataLoaders\n",
    "from fasterai.distill.losses import SoftTarget\n",
    "\n",
    "_teacher = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(16, 10)\n",
    ")\n",
    "_student = nn.Sequential(\n",
    "    nn.Conv2d(3, 8, 3, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(8, 10)\n",
    ")\n",
    "\n",
    "_X = torch.randn(64, 3, 8, 8)\n",
    "_y = torch.randint(0, 10, (64,))\n",
    "_dls = DataLoaders.from_dsets(\n",
    "    TensorDataset(_X[:48], _y[:48]),\n",
    "    TensorDataset(_X[48:], _y[48:]),\n",
    "    bs=16, device='cpu'\n",
    ")\n",
    "\n",
    "_cb = KnowledgeDistillationCallback(teacher=_teacher, loss=SoftTarget)\n",
    "_learn = Learner(_dls, _student, loss_func=nn.CrossEntropyLoss(), cbs=[_cb])\n",
    "_learn.fit(2)  # verify it runs end-to-end without error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36tfm1wumw",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage with Schedule\n",
    "\n",
    "You can now gradually increase the distillation weight during training:\n",
    "\n",
    "```python\n",
    "from fasterai.core.schedule import cos\n",
    "\n",
    "# Gradually increase teacher influence from 0 to 0.8 using cosine schedule\n",
    "cb = KnowledgeDistillationCallback(\n",
    "    teacher=teacher_model,\n",
    "    loss=SoftTarget,\n",
    "    weight=0.8,\n",
    "    schedule=cos\n",
    ")\n",
    "\n",
    "learn.fit(10, cbs=[cb])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [Distillation Losses](losses.html) - Available loss functions (Attention, FitNet, PKT, etc.)\n",
    "- [Distillation Tutorial](../tutorials/distill/distill_callback.html) - Step-by-step guide to knowledge distillation\n",
    "- [Schedules](../core/schedules.html) - Control distillation weight progression\n",
    "- [Pruner](../prune/pruner.html) - Combine with pruning for maximum compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
