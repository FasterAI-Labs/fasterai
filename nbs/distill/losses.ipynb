{
 "cells": [
  {
   "cell_type": "raw",
   "id": "losses-raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Knowledge distillation loss functions\n",
    "output-file: losses.html\n",
    "title: Distillation Losses\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-default-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distill.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8qckg8v05zd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0f4lmh6aa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This module provides loss functions for knowledge distillation. These losses enable training a smaller \"student\" network to mimic a larger \"teacher\" network.\n",
    "\n",
    "**Loss Categories:**\n",
    "- **Output-based**: `SoftTarget`, `Logits`, `Mutual` - compare final predictions\n",
    "- **Feature-based**: `Attention`, `FitNet`, `Similarity`, `ActivationBoundaries` - compare intermediate representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bq8qwwe07d",
   "metadata": {},
   "source": [
    "## Output-Based Losses\n",
    "\n",
    "These losses compare the final output predictions between student and teacher networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soft-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def SoftTarget(pred: torch.Tensor,          # Student predictions\n",
    "               teacher_pred: torch.Tensor,  # Teacher predictions\n",
    "               T: float = 5,                # Temperature for softening\n",
    "               **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"Knowledge distillation with softened distributions (Hinton et al.)\"\n",
    "    student_soft = F.log_softmax(pred / T, dim=1)\n",
    "    teacher_soft = F.softmax(teacher_pred / T, dim=1)\n",
    "    return nn.KLDivLoss(reduction='batchmean')(student_soft, teacher_soft) * (T * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ws667laa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/losses.py#L13){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### SoftTarget\n",
       "\n",
       "```python\n",
       "\n",
       "def SoftTarget(\n",
       "    pred:torch.Tensor, # Student predictions\n",
       "    teacher_pred:torch.Tensor, # Teacher predictions\n",
       "    T:float=5, # Temperature for softening\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Knowledge distillation with softened distributions (Hinton et al.)*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def SoftTarget(\n",
       "    pred:torch.Tensor, # Student predictions\n",
       "    teacher_pred:torch.Tensor, # Teacher predictions\n",
       "    T:float=5, # Temperature for softening\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Knowledge distillation with softened distributions (Hinton et al.)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SoftTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logits",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Logits(pred: torch.Tensor,          # Student predictions\n",
    "           teacher_pred: torch.Tensor,  # Teacher predictions\n",
    "           **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"Direct logit matching between student and teacher\"\n",
    "    return F.mse_loss(pred, teacher_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jz7a5lp8zb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/losses.py#L24){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Logits\n",
       "\n",
       "```python\n",
       "\n",
       "def Logits(\n",
       "    pred:torch.Tensor, # Student predictions\n",
       "    teacher_pred:torch.Tensor, # Teacher predictions\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Direct logit matching between student and teacher*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def Logits(\n",
       "    pred:torch.Tensor, # Student predictions\n",
       "    teacher_pred:torch.Tensor, # Teacher predictions\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Direct logit matching between student and teacher*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mutual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Mutual(pred: torch.Tensor,          # Student predictions\n",
    "           teacher_pred: torch.Tensor,  # Teacher predictions\n",
    "           **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"KL divergence between student and teacher\"\n",
    "    student_log_prob = F.log_softmax(pred, dim=1)\n",
    "    teacher_prob = F.softmax(teacher_pred, dim=1)\n",
    "    return nn.KLDivLoss(reduction='batchmean')(student_log_prob, teacher_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6ejukxx3oo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/losses.py#L32){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Mutual\n",
       "\n",
       "```python\n",
       "\n",
       "def Mutual(\n",
       "    pred:torch.Tensor, # Student predictions\n",
       "    teacher_pred:torch.Tensor, # Teacher predictions\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*KL divergence between student and teacher*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def Mutual(\n",
       "    pred:torch.Tensor, # Student predictions\n",
       "    teacher_pred:torch.Tensor, # Teacher predictions\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*KL divergence between student and teacher*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Mutual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa19gr273h",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature-Based Losses\n",
    "\n",
    "These losses compare intermediate feature representations, enabling the student to learn internal representations similar to the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Attention(fm_s: dict[str, torch.Tensor],  # Student feature maps {name: tensor}\n",
    "              fm_t: dict[str, torch.Tensor],  # Teacher feature maps {name: tensor}\n",
    "              p: int = 2,                     # Power for attention computation\n",
    "              **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"Attention transfer loss (Zagoruyko & Komodakis)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        student_attention = fm_s[name_st].pow(p).mean(1)\n",
    "        teacher_attention = fm_t[name_t].pow(p).mean(1)\n",
    "        student_norm = F.normalize(student_attention, dim=(1, 2))\n",
    "        teacher_norm = F.normalize(teacher_attention, dim=(1, 2))\n",
    "        total_loss += F.mse_loss(student_norm, teacher_norm)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zk5fx238tq",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/losses.py#L42){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Attention\n",
       "\n",
       "```python\n",
       "\n",
       "def Attention(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps {name: tensor}\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps {name: tensor}\n",
       "    p:int=2, # Power for attention computation\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Attention transfer loss (Zagoruyko & Komodakis)*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def Attention(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps {name: tensor}\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps {name: tensor}\n",
       "    p:int=2, # Power for attention computation\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Attention transfer loss (Zagoruyko & Komodakis)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-boundaries",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ActivationBoundaries(fm_s: dict[str, torch.Tensor],  # Student feature maps\n",
    "                         fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n",
    "                         m: float = 2,                   # Boundary margin\n",
    "                         **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"Boundary-based knowledge distillation (Heo et al.)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        student_act = fm_s[name_st]\n",
    "        teacher_act = fm_t[name_t]\n",
    "        positive_boundary = (student_act + m).pow(2) * ((student_act > -m) & (teacher_act <= 0)).float()\n",
    "        negative_boundary = (student_act - m).pow(2) * ((student_act <= m) & (teacher_act > 0)).float()\n",
    "        total_loss += (positive_boundary + negative_boundary).mean()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uy3y8ao6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/losses.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ActivationBoundaries\n",
       "\n",
       "```python\n",
       "\n",
       "def ActivationBoundaries(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n",
       "    m:float=2, # Boundary margin\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Boundary-based knowledge distillation (Heo et al.)*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def ActivationBoundaries(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n",
       "    m:float=2, # Boundary margin\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Boundary-based knowledge distillation (Heo et al.)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ActivationBoundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def FitNet(fm_s: dict[str, torch.Tensor],  # Student feature maps\n",
    "           fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n",
    "           **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"FitNets: direct feature map matching (Romero et al.)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        total_loss += F.mse_loss(fm_s[name_st], fm_t[name_t])\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uhfr1l3zaw8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/losses.py#L74){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FitNet\n",
       "\n",
       "```python\n",
       "\n",
       "def FitNet(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*FitNets: direct feature map matching (Romero et al.)*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def FitNet(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*FitNets: direct feature map matching (Romero et al.)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FitNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Similarity(fm_s: dict[str, torch.Tensor],  # Student feature maps\n",
    "               fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n",
    "               pred: torch.Tensor,             # Student predictions (unused, for API consistency)\n",
    "               p: int = 2,                     # Normalization power\n",
    "               **kwargs\n",
    ") -> torch.Tensor:\n",
    "    \"Similarity-preserving knowledge distillation (Tung & Mori)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        student_flat = fm_s[name_st].view(fm_s[name_st].size(0), -1)\n",
    "        teacher_flat = fm_t[name_t].view(fm_t[name_t].size(0), -1)\n",
    "        student_sim = F.normalize(student_flat @ student_flat.t(), p=p, dim=1)\n",
    "        teacher_sim = F.normalize(teacher_flat @ teacher_flat.t(), p=p, dim=1)\n",
    "        total_loss += F.mse_loss(student_sim, teacher_sim)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1u9czk2qe1g",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/distill/losses.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Similarity\n",
       "\n",
       "```python\n",
       "\n",
       "def Similarity(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n",
       "    pred:torch.Tensor, # Student predictions (unused, for API consistency)\n",
       "    p:int=2, # Normalization power\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Similarity-preserving knowledge distillation (Tung & Mori)*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def Similarity(\n",
       "    fm_s:dict[str, torch.Tensor], # Student feature maps\n",
       "    fm_t:dict[str, torch.Tensor], # Teacher feature maps\n",
       "    pred:torch.Tensor, # Student predictions (unused, for API consistency)\n",
       "    p:int=2, # Normalization power\n",
       "    kwargs:VAR_KEYWORD\n",
       ")->torch.Tensor:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Similarity-preserving knowledge distillation (Tung & Mori)*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufjczbvysk9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# Output-based losses return scalars\n",
    "pred_s, pred_t = torch.randn(4, 10), torch.randn(4, 10)\n",
    "\n",
    "test_eq(SoftTarget(pred_s, pred_t).dim(), 0)\n",
    "test_eq(Logits(pred_s, pred_t).dim(), 0)\n",
    "test_eq(Mutual(pred_s, pred_t).dim(), 0)\n",
    "\n",
    "# Different temperature → different loss\n",
    "test_ne(SoftTarget(pred_s, pred_t, T=1), SoftTarget(pred_s, pred_t, T=10))\n",
    "\n",
    "# Feature-based losses return scalars\n",
    "fm_s = {'l1': torch.randn(4, 32, 8, 8), 'l2': torch.randn(4, 64, 4, 4)}\n",
    "fm_t = {'l1': torch.randn(4, 32, 8, 8), 'l2': torch.randn(4, 64, 4, 4)}\n",
    "\n",
    "test_eq(Attention(fm_s, fm_t).dim(), 0)\n",
    "test_eq(FitNet(fm_s, fm_t).dim(), 0)\n",
    "\n",
    "# Identical inputs → ~0 loss\n",
    "fm_id = {'l1': torch.randn(4, 32, 8, 8)}\n",
    "test_close(FitNet(fm_id, fm_id).item(), 0.0, eps=1e-5)\n",
    "test_close(Attention(fm_id, fm_id).item(), 0.0, eps=1e-4)\n",
    "\n",
    "# All losses non-negative\n",
    "assert SoftTarget(pred_s, pred_t) >= 0\n",
    "assert Attention(fm_s, fm_t) >= 0\n",
    "assert FitNet(fm_s, fm_t) >= 0\n",
    "\n",
    "# ActivationBoundaries returns scalar\n",
    "test_eq(ActivationBoundaries(fm_s, fm_t).dim(), 0)\n",
    "assert ActivationBoundaries(fm_s, fm_t) >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9l18cjsg784",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [KnowledgeDistillationCallback](distillation_callback.html) - Apply these losses during training\n",
    "- [Distillation Tutorial](../tutorials/distill/distill_callback.html) - Practical examples with different losses\n",
    "\n",
    "### Loss Selection Guide\n",
    "\n",
    "| Loss | Best For | Complexity |\n",
    "|------|----------|------------|\n",
    "| **SoftTarget** | General distillation, logit matching | Low |\n",
    "| **Attention** | When attention patterns matter | Low |\n",
    "| **FitNet** | Intermediate feature matching | Medium |\n",
    "| **PKT** | Probability distribution matching | Medium |\n",
    "| **RKD** | Relational knowledge transfer | High |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
