{
 "cells": [
  {
   "cell_type": "raw",
   "id": "losses-raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Knowledge distillation loss functions\n",
    "output-file: losses.html\n",
    "title: Distillation Losses\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-default-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distill.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8qckg8v05zd",
   "metadata": {},
   "outputs": [],
   "source": "#| include: false\nfrom nbdev.showdoc import *"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-imports",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom __future__ import annotations\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
   "cell_type": "markdown",
   "id": "v0f4lmh6aa",
   "metadata": {},
   "source": "## Overview\n\nThis module provides loss functions for knowledge distillation. These losses enable training a smaller \"student\" network to mimic a larger \"teacher\" network.\n\n**Loss Categories:**\n- **Output-based**: `SoftTarget`, `Logits`, `Mutual` - compare final predictions\n- **Feature-based**: `Attention`, `FitNet`, `Similarity`, `ActivationBoundaries` - compare intermediate representations"
  },
  {
   "cell_type": "markdown",
   "id": "bq8qwwe07d",
   "metadata": {},
   "source": "## Output-Based Losses\n\nThese losses compare the final output predictions between student and teacher networks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soft-target",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef SoftTarget(pred: torch.Tensor,          # Student predictions\n               teacher_pred: torch.Tensor,  # Teacher predictions\n               T: float = 5,                # Temperature for softening\n               **kwargs\n) -> torch.Tensor:\n    \"Knowledge distillation with softened distributions (Hinton et al.)\"\n    student_soft = F.log_softmax(pred / T, dim=1)\n    teacher_soft = F.softmax(teacher_pred / T, dim=1)\n    return nn.KLDivLoss(reduction='batchmean')(student_soft, teacher_soft) * (T * T)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ws667laa6e",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SoftTarget)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logits",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Logits(pred: torch.Tensor,          # Student predictions\n           teacher_pred: torch.Tensor,  # Teacher predictions\n           **kwargs\n) -> torch.Tensor:\n    \"Direct logit matching between student and teacher\"\n    return F.mse_loss(pred, teacher_pred)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jz7a5lp8zb",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(Logits)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mutual",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Mutual(pred: torch.Tensor,          # Student predictions\n           teacher_pred: torch.Tensor,  # Teacher predictions\n           **kwargs\n) -> torch.Tensor:\n    \"KL divergence between student and teacher\"\n    student_log_prob = F.log_softmax(pred, dim=1)\n    teacher_prob = F.softmax(teacher_pred, dim=1)\n    return nn.KLDivLoss(reduction='batchmean')(student_log_prob, teacher_prob)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w6ejukxx3oo",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(Mutual)"
  },
  {
   "cell_type": "markdown",
   "id": "aa19gr273h",
   "metadata": {},
   "source": "---\n\n## Feature-Based Losses\n\nThese losses compare intermediate feature representations, enabling the student to learn internal representations similar to the teacher."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Attention(fm_s: dict[str, torch.Tensor],  # Student feature maps {name: tensor}\n              fm_t: dict[str, torch.Tensor],  # Teacher feature maps {name: tensor}\n              p: int = 2,                     # Power for attention computation\n              **kwargs\n) -> torch.Tensor:\n    \"Attention transfer loss (Zagoruyko & Komodakis)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        student_attention = fm_s[name_st].pow(p).mean(1)\n        teacher_attention = fm_t[name_t].pow(p).mean(1)\n        student_norm = F.normalize(student_attention, dim=(1, 2))\n        teacher_norm = F.normalize(teacher_attention, dim=(1, 2))\n        total_loss += F.mse_loss(student_norm, teacher_norm)\n    return total_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zk5fx238tq",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(Attention)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-boundaries",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef ActivationBoundaries(fm_s: dict[str, torch.Tensor],  # Student feature maps\n                         fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n                         m: float = 2,                   # Boundary margin\n                         **kwargs\n) -> torch.Tensor:\n    \"Boundary-based knowledge distillation (Heo et al.)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        student_act = fm_s[name_st]\n        teacher_act = fm_t[name_t]\n        positive_boundary = (student_act + m).pow(2) * ((student_act > -m) & (teacher_act <= 0)).float()\n        negative_boundary = (student_act - m).pow(2) * ((student_act <= m) & (teacher_act > 0)).float()\n        total_loss += (positive_boundary + negative_boundary).mean()\n    return total_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uy3y8ao6f",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(ActivationBoundaries)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitnet",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef FitNet(fm_s: dict[str, torch.Tensor],  # Student feature maps\n           fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n           **kwargs\n) -> torch.Tensor:\n    \"FitNets: direct feature map matching (Romero et al.)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        total_loss += F.mse_loss(fm_s[name_st], fm_t[name_t])\n    return total_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uhfr1l3zaw8",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(FitNet)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Similarity(fm_s: dict[str, torch.Tensor],  # Student feature maps\n               fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n               pred: torch.Tensor,             # Student predictions (unused, for API consistency)\n               p: int = 2,                     # Normalization power\n               **kwargs\n) -> torch.Tensor:\n    \"Similarity-preserving knowledge distillation (Tung & Mori)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        student_flat = fm_s[name_st].view(fm_s[name_st].size(0), -1)\n        teacher_flat = fm_t[name_t].view(fm_t[name_t].size(0), -1)\n        student_sim = F.normalize(student_flat @ student_flat.t(), p=p, dim=1)\n        teacher_sim = F.normalize(teacher_flat @ teacher_flat.t(), p=p, dim=1)\n        total_loss += F.mse_loss(student_sim, teacher_sim)\n    return total_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1u9czk2qe1g",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(Similarity)"
  },
  {
   "cell_type": "markdown",
   "id": "9l18cjsg784",
   "metadata": {},
   "source": "---\n\n## See Also\n\n- [KnowledgeDistillationCallback](distillation_callback.html) - Apply these losses during training\n- [Distillation Tutorial](../tutorials/distill/distill_callback.html) - Practical examples with different losses\n\n### Loss Selection Guide\n\n| Loss | Best For | Complexity |\n|------|----------|------------|\n| **SoftTarget** | General distillation, logit matching | Low |\n| **Attention** | When attention patterns matter | Low |\n| **FitNet** | Intermediate feature matching | Medium |\n| **PKT** | Probability distribution matching | Medium |\n| **RKD** | Relational knowledge transfer | High |"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
