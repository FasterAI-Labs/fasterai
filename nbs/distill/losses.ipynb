{
 "cells": [
  {
   "cell_type": "raw",
   "id": "losses-raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Knowledge distillation loss functions\n",
    "output-file: losses.html\n",
    "title: Distillation Losses\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-default-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distill.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-imports",
   "metadata": {},
   "outputs": [],
   "source": "#| export\nfrom __future__ import annotations\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soft-target",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef SoftTarget(pred: torch.Tensor,          # Student predictions\n               teacher_pred: torch.Tensor,  # Teacher predictions\n               T: float = 5,                # Temperature for softening\n               **kwargs\n) -> torch.Tensor:\n    \"Knowledge distillation with softened distributions (Hinton et al.)\"\n    student_soft = F.log_softmax(pred / T, dim=1)\n    teacher_soft = F.softmax(teacher_pred / T, dim=1)\n    return nn.KLDivLoss(reduction='batchmean')(student_soft, teacher_soft) * (T * T)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logits",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Logits(pred: torch.Tensor,          # Student predictions\n           teacher_pred: torch.Tensor,  # Teacher predictions\n           **kwargs\n) -> torch.Tensor:\n    \"Direct logit matching between student and teacher\"\n    return F.mse_loss(pred, teacher_pred)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mutual",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Mutual(pred: torch.Tensor,          # Student predictions\n           teacher_pred: torch.Tensor,  # Teacher predictions\n           **kwargs\n) -> torch.Tensor:\n    \"KL divergence between student and teacher\"\n    student_log_prob = F.log_softmax(pred, dim=1)\n    teacher_prob = F.softmax(teacher_pred, dim=1)\n    return nn.KLDivLoss(reduction='batchmean')(student_log_prob, teacher_prob)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Attention(fm_s: dict[str, torch.Tensor],  # Student feature maps {name: tensor}\n              fm_t: dict[str, torch.Tensor],  # Teacher feature maps {name: tensor}\n              p: int = 2,                     # Power for attention computation\n              **kwargs\n) -> torch.Tensor:\n    \"Attention transfer loss (Zagoruyko & Komodakis)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        student_attention = fm_s[name_st].pow(p).mean(1)\n        teacher_attention = fm_t[name_t].pow(p).mean(1)\n        student_norm = F.normalize(student_attention, dim=(1, 2))\n        teacher_norm = F.normalize(teacher_attention, dim=(1, 2))\n        total_loss += F.mse_loss(student_norm, teacher_norm)\n    return total_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-boundaries",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef ActivationBoundaries(fm_s: dict[str, torch.Tensor],  # Student feature maps\n                         fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n                         m: float = 2,                   # Boundary margin\n                         **kwargs\n) -> torch.Tensor:\n    \"Boundary-based knowledge distillation (Heo et al.)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        student_act = fm_s[name_st]\n        teacher_act = fm_t[name_t]\n        positive_boundary = (student_act + m).pow(2) * ((student_act > -m) & (teacher_act <= 0)).float()\n        negative_boundary = (student_act - m).pow(2) * ((student_act <= m) & (teacher_act > 0)).float()\n        total_loss += (positive_boundary + negative_boundary).mean()\n    return total_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitnet",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef FitNet(fm_s: dict[str, torch.Tensor],  # Student feature maps\n           fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n           **kwargs\n) -> torch.Tensor:\n    \"FitNets: direct feature map matching (Romero et al.)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        total_loss += F.mse_loss(fm_s[name_st], fm_t[name_t])\n    return total_loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef Similarity(fm_s: dict[str, torch.Tensor],  # Student feature maps\n               fm_t: dict[str, torch.Tensor],  # Teacher feature maps\n               pred: torch.Tensor,             # Student predictions (unused, for API consistency)\n               p: int = 2,                     # Normalization power\n               **kwargs\n) -> torch.Tensor:\n    \"Similarity-preserving knowledge distillation (Tung & Mori)\"\n    total_loss = 0.0\n    for name_st, name_t in zip(fm_s, fm_t):\n        student_flat = fm_s[name_st].view(fm_s[name_st].size(0), -1)\n        teacher_flat = fm_t[name_t].view(fm_t[name_t].size(0), -1)\n        student_sim = F.normalize(student_flat @ student_flat.t(), p=p, dim=1)\n        teacher_sim = F.normalize(teacher_flat @ teacher_flat.t(), p=p, dim=1)\n        total_loss += F.mse_loss(student_sim, teacher_sim)\n    return total_loss"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
