{
 "cells": [
  {
   "cell_type": "raw",
   "id": "losses-raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Knowledge distillation loss functions\n",
    "output-file: losses.html\n",
    "title: Distillation Losses\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-default-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distill.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "losses-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soft-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def SoftTarget(pred,          # Student predictions\n",
    "               teacher_pred,  # Teacher predictions\n",
    "               T=5,           # Temperature for softening\n",
    "               **kwargs\n",
    "):\n",
    "    \"Knowledge distillation with softened distributions (Hinton et al.)\"\n",
    "    student_soft = F.log_softmax(pred / T, dim=1)\n",
    "    teacher_soft = F.softmax(teacher_pred / T, dim=1)\n",
    "    return nn.KLDivLoss(reduction='batchmean')(student_soft, teacher_soft) * (T * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logits",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Logits(pred,          # Student predictions\n",
    "           teacher_pred,  # Teacher predictions\n",
    "           **kwargs\n",
    "):\n",
    "    \"Direct logit matching between student and teacher\"\n",
    "    return F.mse_loss(pred, teacher_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mutual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Mutual(pred,          # Student predictions\n",
    "           teacher_pred,  # Teacher predictions\n",
    "           **kwargs\n",
    "):\n",
    "    \"KL divergence between student and teacher\"\n",
    "    student_log_prob = F.log_softmax(pred, dim=1)\n",
    "    teacher_prob = F.softmax(teacher_pred, dim=1)\n",
    "    return nn.KLDivLoss(reduction='batchmean')(student_log_prob, teacher_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Attention(fm_s,  # Student feature maps {name: tensor}\n",
    "              fm_t,  # Teacher feature maps {name: tensor}\n",
    "              p=2,   # Power for attention computation\n",
    "              **kwargs\n",
    "):\n",
    "    \"Attention transfer loss (Zagoruyko & Komodakis)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        student_attention = fm_s[name_st].pow(p).mean(1)\n",
    "        teacher_attention = fm_t[name_t].pow(p).mean(1)\n",
    "        student_norm = F.normalize(student_attention, dim=(1, 2))\n",
    "        teacher_norm = F.normalize(teacher_attention, dim=(1, 2))\n",
    "        total_loss += F.mse_loss(student_norm, teacher_norm)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activation-boundaries",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ActivationBoundaries(fm_s,  # Student feature maps\n",
    "                         fm_t,  # Teacher feature maps\n",
    "                         m=2,   # Boundary margin\n",
    "                         **kwargs\n",
    "):\n",
    "    \"Boundary-based knowledge distillation (Heo et al.)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        student_act = fm_s[name_st]\n",
    "        teacher_act = fm_t[name_t]\n",
    "        positive_boundary = (student_act + m).pow(2) * ((student_act > -m) & (teacher_act <= 0)).float()\n",
    "        negative_boundary = (student_act - m).pow(2) * ((student_act <= m) & (teacher_act > 0)).float()\n",
    "        total_loss += (positive_boundary + negative_boundary).mean()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def FitNet(fm_s,  # Student feature maps\n",
    "           fm_t,  # Teacher feature maps\n",
    "           **kwargs\n",
    "):\n",
    "    \"FitNets: direct feature map matching (Romero et al.)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        total_loss += F.mse_loss(fm_s[name_st], fm_t[name_t])\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def Similarity(fm_s,  # Student feature maps\n",
    "               fm_t,  # Teacher feature maps\n",
    "               pred,  # Student predictions (unused, for API consistency)\n",
    "               p=2,   # Normalization power\n",
    "               **kwargs\n",
    "):\n",
    "    \"Similarity-preserving knowledge distillation (Tung & Mori)\"\n",
    "    total_loss = 0.0\n",
    "    for name_st, name_t in zip(fm_s, fm_t):\n",
    "        student_flat = fm_s[name_st].view(fm_s[name_st].size(0), -1)\n",
    "        teacher_flat = fm_t[name_t].view(fm_t[name_t].size(0), -1)\n",
    "        student_sim = F.normalize(student_flat @ student_flat.t(), p=p, dim=1)\n",
    "        teacher_sim = F.normalize(teacher_flat @ teacher_flat.t(), p=p, dim=1)\n",
    "        total_loss += F.mse_loss(student_sim, teacher_sim)\n",
    "    return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
