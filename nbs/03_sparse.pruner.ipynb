{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5a33dc9c",
   "metadata": {},
   "source": [
    "---\n",
    "description: Remove useless filters to recreate a dense network\n",
    "output-file: pruner.html\n",
    "title: Pruner\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp sparse.pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce321e7",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "The Pruner method currently works on fully-feedforward ConvNets, e.g. VGG16. Support for residual connections, e.g. ResNets is under development.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b5a0d",
   "metadata": {},
   "source": [
    "When our network has filters containing zero values, there is an additional step that we may take. Indeed, those zero-filters can be **physically** removed from our network, allowing us to get a new, dense, architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf84c3e",
   "metadata": {},
   "source": [
    "This can be done by reexpressing each layer, reducing the number of filter, to match the number of non-zero filters. However, when we remove a filter in a layer, this means that there will be a missing activation map, which should be used by all the filters in the next layer. So, not only should we physically remove the filter, but also its corresponding kernel in each of the filters in the next layer (see Fig. below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e028277",
   "metadata": {},
   "source": [
    "![](imgs/pruning_filters.png \"Pruning Filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b7a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathan/opt/miniconda3/envs/nbdev/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac59313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Pruner():\n",
    "    \"Remove zero filters from a model\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def filters_to_keep(self, layer, nxt_layer):\n",
    "        \n",
    "        ixs = self._get_nz_ixs(layer)\n",
    "    \n",
    "        filters_keep = layer.weight.index_select(0, ixs[0]).data # keep only the non_zero filters\n",
    "        biases_keep = layer.bias.index_select(0, ixs[0]).data\n",
    "        \n",
    "        nxt_filters_keep = nxt_layer.weight.index_select(1, ixs[0]).data if nxt_layer is not None else None\n",
    "            \n",
    "        return filters_keep, biases_keep, nxt_filters_keep\n",
    "    \n",
    "    def prune_conv(self, layer, nxt_layer):\n",
    "        assert layer.__class__.__name__ == 'Conv2d'\n",
    "    \n",
    "        new_weights, new_biases, new_next_weights = self.filters_to_keep(layer, nxt_layer)\n",
    "    \n",
    "        layer.out_channels = new_weights.shape[0]\n",
    "        layer.in_channels = new_weights.shape[1]\n",
    "    \n",
    "        layer.weight = nn.Parameter(new_weights)\n",
    "        layer.bias = nn.Parameter(new_biases)\n",
    "\n",
    "        if new_next_weights is not None:\n",
    "            new_next_in_channels = new_next_weights.shape[1]\n",
    "            nxt_layer.weight = nn.Parameter(new_next_weights)\n",
    "            nxt_layer.in_channels = new_next_in_channels\n",
    "    \n",
    "        return layer, nxt_layer\n",
    "    \n",
    "    def prune_bn(self, layer, prev_conv):\n",
    "        \n",
    "        ixs = self._get_nz_ixs(prev_conv)\n",
    "        \n",
    "        weights_keep = layer.weight.data.index_select(0, ixs[0]).data\n",
    "    \n",
    "        layer.num_features = weights_keep.shape[0]\n",
    "        layer.weight = nn.Parameter(weights_keep)\n",
    "        layer.bias = nn.Parameter(layer.bias.data.index_select(0, ixs[0]).data)\n",
    "        layer.running_mean = layer.running_mean.data.index_select(0, ixs[0]).data\n",
    "        layer.running_var = layer.running_var.data.index_select(0, ixs[0]).data\n",
    "        \n",
    "        return layer\n",
    "\n",
    "    def delete_fc_weights(self, layer, last_conv, pool_shape):\n",
    "        \n",
    "        ixs = self._get_nz_ixs(last_conv)\n",
    "        \n",
    "        new_ixs = torch.cat([torch.arange(i*pool_shape**2,((i+1)*pool_shape**2)) for i in ixs[0]]) if pool_shape else ixs[0]\n",
    "        new_ixs = torch.LongTensor(new_ixs).cuda()\n",
    "\n",
    "        weights_keep = layer.weight.data.index_select(1, new_ixs).data\n",
    "        \n",
    "        layer.in_features = weights_keep.shape[1]\n",
    "        layer.weight = nn.Parameter(weights_keep)\n",
    "    \n",
    "        return layer\n",
    "    \n",
    "    def _get_nz_ixs(self, layer):\n",
    "        filters = layer.weight\n",
    "        nz_filters = filters.data.sum(dim=(1,2,3)) # Flatten the filters to compare them\n",
    "        ixs = torch.nonzero(nz_filters).T\n",
    "        return ixs.cuda()\n",
    "    \n",
    "    def _find_next_conv(self, model, conv_ix):\n",
    "        for k,m in enumerate(model.modules()):\n",
    "            if k > conv_ix and isinstance(m, nn.Conv2d):\n",
    "                next_conv_ix = k\n",
    "                break\n",
    "            else:\n",
    "                next_conv_ix = None\n",
    "        return next_conv_ix\n",
    "    \n",
    "    def _find_previous_conv(self, model, layer_ix):\n",
    "        for k,m in reversed(list(enumerate(model.modules()))):\n",
    "            if k < layer_ix and isinstance(m, nn.Conv2d):\n",
    "                prev_conv_ix = k\n",
    "                break\n",
    "            else:\n",
    "                prev_conv_ix = None\n",
    "        return prev_conv_ix    \n",
    "    \n",
    "    def _get_last_conv_ix(self, model):\n",
    "        for k,m in enumerate(list(model.modules())):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                last_conv_ix = k\n",
    "        return last_conv_ix\n",
    "    \n",
    "    def _get_first_fc_ix(self, model):\n",
    "        for k,m in enumerate(list(model.modules())):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                first_fc_ix = k\n",
    "                break       \n",
    "        return first_fc_ix\n",
    "    \n",
    "    def _find_pool_shape(self, model):\n",
    "        for k,m in enumerate(model.modules()):\n",
    "            if isinstance(m, nn.AdaptiveAvgPool2d):\n",
    "                output_shape = m.output_size\n",
    "                break\n",
    "            else: output_shape=None\n",
    "        return output_shape    \n",
    "    \n",
    "    def prune_model(self, model):\n",
    "        pruned_model = copy.deepcopy(model)\n",
    "        \n",
    "        layer_names = list(dict(pruned_model.named_modules()).keys())\n",
    "        layers = dict(pruned_model.named_modules())\n",
    "        old_layers = dict(model.named_modules())\n",
    "        \n",
    "        last_conv_ix = self._get_last_conv_ix(pruned_model)\n",
    "        first_fc_ix = self._get_first_fc_ix(pruned_model)\n",
    "        \n",
    "        for k,m in enumerate(list(pruned_model.modules())):\n",
    "            \n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                next_conv_ix = self._find_next_conv(model, k)\n",
    "                if next_conv_ix is not None: # The conv layer is not the last one\n",
    "                    new_m, new_next_m = self.prune_conv(m, layers[layer_names[next_conv_ix]]) # Prune the current conv layer\n",
    "                else:\n",
    "                    new_m, _ = self.prune_conv(m, None) # Prune the current conv layer without changing the next one\n",
    "                    \n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                new_m = self.prune_bn(m, old_layers[layer_names[self._find_previous_conv(model, k)]])             \n",
    "                    \n",
    "            if isinstance(m, nn.Linear) and k==first_fc_ix:\n",
    "                pool_shape = self._find_pool_shape(model)\n",
    "                new_m = self.delete_fc_weights(m, old_layers[layer_names[last_conv_ix]], pool_shape[0])\n",
    "\n",
    "        return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652db13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Pruner.prune_model\n",
       "\n",
       ">      Pruner.prune_model (model)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Pruner.prune_model\n",
       "\n",
       ">      Pruner.prune_model (model)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Pruner.prune_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
