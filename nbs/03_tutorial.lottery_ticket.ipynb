{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promising-healing",
   "metadata": {},
   "source": [
    "# Lottery Ticket Hypothesis\n",
    "\n",
    "> How to find winning tickets with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-highlight",
   "metadata": {},
   "source": [
    "## The Lottery Ticket Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-counter",
   "metadata": {},
   "source": [
    "The [Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635) is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n",
    "\n",
    "> A randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n",
    "\n",
    "Meaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\n",
    "\n",
    "The way authors propose to find those subnetwork is as follows:\n",
    "\n",
    "1. Initialize the neural network\n",
    "2. Train it to convergence\n",
    "3. Prune the smallest magnitude weights by creating a mask $m$\n",
    "4. Reinitialize the weights to their original value; i.e at iteration $0$.\n",
    "5. Repeat from step 2 until reaching the desired level of sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterai.sparse.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-money",
   "metadata": {},
   "source": [
    "What we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy $a_B > a_A$, in a training time $t_B < t_A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-campaign",
   "metadata": {},
   "source": [
    "Let's get the baseline for network A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbe050",
   "metadata": {},
   "source": [
    "Let's save original weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = deepcopy(learn.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-treasurer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.594088</td>\n",
       "      <td>0.607128</td>\n",
       "      <td>0.679296</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.538883</td>\n",
       "      <td>0.577014</td>\n",
       "      <td>0.721922</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498966</td>\n",
       "      <td>0.563139</td>\n",
       "      <td>0.746279</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.465249</td>\n",
       "      <td>0.519879</td>\n",
       "      <td>0.734100</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.428827</td>\n",
       "      <td>0.477736</td>\n",
       "      <td>0.760487</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-nutrition",
   "metadata": {},
   "source": [
    "To find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-capability",
   "metadata": {},
   "source": [
    "We will restart from the same initialization to be sure to not get lucky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-holly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n",
    "learn.model.load_state_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf00529",
   "metadata": {},
   "source": [
    "We can pass the parameters `lth=True` to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found \"lottery ticket\" after the pruning phase. This can be done by setting the `end_epoch` parameter, which will control the epoch at which we stop the pruning process. The fine-tuning phase thus happening during `total_epoch-end_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(50, 'weight', 'local', large_final, iterative, start_epoch=5, lth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9fc57f",
   "metadata": {},
   "source": [
    "So here, we will first pretrain our model for 1 epoch (`start_epoch=1`), perform the LTH process during 3 epochs (`end_epoch-start_epoch`), then the found lottery ticket for 6 epochs (`total_epoch-end_epoch`). If the final accuracy is higher than the baseline then we have found a \"winning lottery ticket\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-ecology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.595973</td>\n",
       "      <td>0.605056</td>\n",
       "      <td>0.684032</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.546129</td>\n",
       "      <td>0.590174</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.516857</td>\n",
       "      <td>0.587353</td>\n",
       "      <td>0.717862</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.474746</td>\n",
       "      <td>0.493197</td>\n",
       "      <td>0.767253</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.440702</td>\n",
       "      <td>0.472007</td>\n",
       "      <td>0.774019</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.553329</td>\n",
       "      <td>0.724072</td>\n",
       "      <td>0.715832</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.515671</td>\n",
       "      <td>0.524586</td>\n",
       "      <td>0.730717</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.477123</td>\n",
       "      <td>0.622931</td>\n",
       "      <td>0.645467</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.437616</td>\n",
       "      <td>0.535363</td>\n",
       "      <td>0.712449</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.391450</td>\n",
       "      <td>0.697298</td>\n",
       "      <td>0.648850</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.500410</td>\n",
       "      <td>0.464378</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.445288</td>\n",
       "      <td>0.454092</td>\n",
       "      <td>0.777402</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.402433</td>\n",
       "      <td>0.481071</td>\n",
       "      <td>0.784168</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.360295</td>\n",
       "      <td>0.474284</td>\n",
       "      <td>0.801083</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.330312</td>\n",
       "      <td>0.448178</td>\n",
       "      <td>0.805819</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.410723</td>\n",
       "      <td>0.504001</td>\n",
       "      <td>0.767930</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.363790</td>\n",
       "      <td>0.412130</td>\n",
       "      <td>0.813261</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.332738</td>\n",
       "      <td>0.460551</td>\n",
       "      <td>0.793640</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.286119</td>\n",
       "      <td>0.544373</td>\n",
       "      <td>0.807172</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.270594</td>\n",
       "      <td>0.473709</td>\n",
       "      <td>0.794993</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 0.00%\n",
      "Sparsity at the end of epoch 4: 0.00%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 5: 16.67%\n",
      "Sparsity at the end of epoch 6: 16.67%\n",
      "Sparsity at the end of epoch 7: 16.67%\n",
      "Sparsity at the end of epoch 8: 16.67%\n",
      "Sparsity at the end of epoch 9: 16.67%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 10: 33.33%\n",
      "Sparsity at the end of epoch 11: 33.33%\n",
      "Sparsity at the end of epoch 12: 33.33%\n",
      "Sparsity at the end of epoch 13: 33.33%\n",
      "Sparsity at the end of epoch 14: 33.33%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 15: 50.00%\n",
      "Sparsity at the end of epoch 16: 50.00%\n",
      "Sparsity at the end of epoch 17: 50.00%\n",
      "Sparsity at the end of epoch 18: 50.00%\n",
      "Sparsity at the end of epoch 19: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit(20, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-wheat",
   "metadata": {},
   "source": [
    "We indeed have a network B, whose accuracy $a_B > a_A$ in the same training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-samoa",
   "metadata": {},
   "source": [
    "## Lottery Ticket Hypothesis with Rewinding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29afe3a",
   "metadata": {},
   "source": [
    "In some case, LTH fails for deeper networks, author then propose a [solution](https://arxiv.org/pdf/1903.01611.pdf), which is to rewind the weights to a more advanced iteration instead of the initialization value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922beeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n",
    "learn.model.load_state_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5607ce3",
   "metadata": {},
   "source": [
    "This can be done in fasterai by passing the `rewind_epoch` parameter, that will save the weights at that epoch, then resetting the weights accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(50, 'weight', 'local', large_final, iterative, start_epoch=5, lth=True, rewind_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.595904</td>\n",
       "      <td>1.070335</td>\n",
       "      <td>0.673884</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.556773</td>\n",
       "      <td>0.685454</td>\n",
       "      <td>0.689445</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.514651</td>\n",
       "      <td>0.509246</td>\n",
       "      <td>0.743572</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.464843</td>\n",
       "      <td>0.508990</td>\n",
       "      <td>0.761840</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.435481</td>\n",
       "      <td>0.568965</td>\n",
       "      <td>0.723275</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.511907</td>\n",
       "      <td>0.565787</td>\n",
       "      <td>0.673207</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.464483</td>\n",
       "      <td>0.556267</td>\n",
       "      <td>0.695535</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.424676</td>\n",
       "      <td>0.692757</td>\n",
       "      <td>0.711096</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.375262</td>\n",
       "      <td>0.787988</td>\n",
       "      <td>0.717185</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.341823</td>\n",
       "      <td>0.404487</td>\n",
       "      <td>0.818674</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.435166</td>\n",
       "      <td>1.205433</td>\n",
       "      <td>0.686062</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.394866</td>\n",
       "      <td>0.454268</td>\n",
       "      <td>0.792963</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.342811</td>\n",
       "      <td>0.428764</td>\n",
       "      <td>0.799053</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.319831</td>\n",
       "      <td>0.431812</td>\n",
       "      <td>0.786874</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.276488</td>\n",
       "      <td>0.405543</td>\n",
       "      <td>0.822733</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.351722</td>\n",
       "      <td>0.552171</td>\n",
       "      <td>0.713802</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.305281</td>\n",
       "      <td>0.388482</td>\n",
       "      <td>0.827470</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.414270</td>\n",
       "      <td>0.834912</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.237284</td>\n",
       "      <td>0.462773</td>\n",
       "      <td>0.828146</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.212240</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.831529</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Saving Weights at epoch 1\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 0.00%\n",
      "Sparsity at the end of epoch 4: 0.00%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 5: 16.67%\n",
      "Sparsity at the end of epoch 6: 16.67%\n",
      "Sparsity at the end of epoch 7: 16.67%\n",
      "Sparsity at the end of epoch 8: 16.67%\n",
      "Sparsity at the end of epoch 9: 16.67%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 10: 33.33%\n",
      "Sparsity at the end of epoch 11: 33.33%\n",
      "Sparsity at the end of epoch 12: 33.33%\n",
      "Sparsity at the end of epoch 13: 33.33%\n",
      "Sparsity at the end of epoch 14: 33.33%\n",
      "Resetting Weights to their epoch 1 values\n",
      "Sparsity at the end of epoch 15: 50.00%\n",
      "Sparsity at the end of epoch 16: 50.00%\n",
      "Sparsity at the end of epoch 17: 50.00%\n",
      "Sparsity at the end of epoch 18: 50.00%\n",
      "Sparsity at the end of epoch 19: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit(20, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da0cc9a",
   "metadata": {},
   "source": [
    "## Super-Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b348c",
   "metadata": {},
   "source": [
    "Researchers from Uber AI [investigated](https://arxiv.org/pdf/1905.01067.pdf) the LTH and found the existence of what they call \"Super-Masks\", i.e. masks that, applied on a untrained neural network, allows to reach better-than-random results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0df259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n",
    "learn.model.load_state_dict(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2272d5",
   "metadata": {},
   "source": [
    "To find supermasks, authors perform the LTH method then apply the mask on the original, untrained network. In fasterai, you can pass the parameter `reset_end=True`, which will reset the weights to their original value at the end of the training, but keeping the pruned weights (i.e. the mask) unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4729720",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(50, 'weight', 'local', large_final, iterative, start_epoch=5, lth=True, reset_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e5115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.587182</td>\n",
       "      <td>0.617454</td>\n",
       "      <td>0.653586</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.543216</td>\n",
       "      <td>0.689823</td>\n",
       "      <td>0.718539</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.516071</td>\n",
       "      <td>0.550590</td>\n",
       "      <td>0.705683</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.470385</td>\n",
       "      <td>0.759532</td>\n",
       "      <td>0.566982</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.443852</td>\n",
       "      <td>0.608897</td>\n",
       "      <td>0.751015</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.552131</td>\n",
       "      <td>0.564474</td>\n",
       "      <td>0.715832</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.510690</td>\n",
       "      <td>0.546589</td>\n",
       "      <td>0.732747</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.468867</td>\n",
       "      <td>0.571830</td>\n",
       "      <td>0.679296</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.425871</td>\n",
       "      <td>0.535260</td>\n",
       "      <td>0.734777</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.386654</td>\n",
       "      <td>0.495325</td>\n",
       "      <td>0.764547</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.486236</td>\n",
       "      <td>0.549312</td>\n",
       "      <td>0.707037</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.433474</td>\n",
       "      <td>0.543789</td>\n",
       "      <td>0.707037</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.380443</td>\n",
       "      <td>0.673880</td>\n",
       "      <td>0.777402</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.366553</td>\n",
       "      <td>0.387486</td>\n",
       "      <td>0.813938</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.317037</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>0.788904</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.400861</td>\n",
       "      <td>0.507487</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.338845</td>\n",
       "      <td>0.458387</td>\n",
       "      <td>0.792963</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.300142</td>\n",
       "      <td>0.608675</td>\n",
       "      <td>0.773342</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.284395</td>\n",
       "      <td>0.445842</td>\n",
       "      <td>0.783491</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.237855</td>\n",
       "      <td>0.495823</td>\n",
       "      <td>0.809202</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 0.00%\n",
      "Sparsity at the end of epoch 4: 0.00%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 5: 16.67%\n",
      "Sparsity at the end of epoch 6: 16.67%\n",
      "Sparsity at the end of epoch 7: 16.67%\n",
      "Sparsity at the end of epoch 8: 16.67%\n",
      "Sparsity at the end of epoch 9: 16.67%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 10: 33.33%\n",
      "Sparsity at the end of epoch 11: 33.33%\n",
      "Sparsity at the end of epoch 12: 33.33%\n",
      "Sparsity at the end of epoch 13: 33.33%\n",
      "Sparsity at the end of epoch 14: 33.33%\n",
      "Resetting Weights to their epoch 0 values\n",
      "Sparsity at the end of epoch 15: 50.00%\n",
      "Sparsity at the end of epoch 16: 50.00%\n",
      "Sparsity at the end of epoch 17: 50.00%\n",
      "Sparsity at the end of epoch 18: 50.00%\n",
      "Sparsity at the end of epoch 19: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit(20, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37bbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [3.123504400253296,0.3288227319717407]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603cbae",
   "metadata": {},
   "source": [
    "So now we have an untrained model which is better than random (50%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
