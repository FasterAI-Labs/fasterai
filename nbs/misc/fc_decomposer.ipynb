{
 "cells": [
  {
   "cell_type": "raw",
   "id": "452d040d",
   "metadata": {},
   "source": [
    "---\n",
    "description: Factorize heavy FC layers into smaller ones\n",
    "output-file: fc_decomposer.html\n",
    "title: Fully-Connected Layers Decomposer\n",
    "skip_showdoc: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd975549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp misc.fc_decomposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ixjoviwta",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `FC_Decomposer` class reduces model size by factorizing large fully-connected (Linear) layers into two smaller layers using Singular Value Decomposition (SVD). This is particularly effective for models with large FC layers like VGG or older architectures with big classifier heads.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Reduces parameter count without changing model architecture externally\n",
    "- No retraining required (though fine-tuning may improve accuracy)\n",
    "- Works on any model with Linear layers\n",
    "\n",
    "### When to Use FC Decomposition\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| Large classifier heads (e.g., VGG's 4096→4096→1000) | **Highly recommended** - significant savings |\n",
    "| Modern architectures (ResNet, EfficientNet) | Limited benefit - already efficient |\n",
    "| Transformer attention layers | Use with caution - may hurt performance |\n",
    "| Pre-deployment optimization | Good complement to pruning/quantization |\n",
    "\n",
    "### Compression Ratio\n",
    "\n",
    "For a Linear layer with shape `(out_features, in_features)`:\n",
    "- **Original parameters**: `out_features × in_features + out_features` (with bias)\n",
    "- **After decomposition** (keeping `k` singular values): `k × in_features + out_features × k + out_features`\n",
    "- **Compression ratio**: roughly `1 / (1 - percent_removed)` for square layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f88207",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "SVD decomposes a weight matrix into three matrices: $W = U \\Sigma V^T$\n",
    "\n",
    "Where:\n",
    "- $U$ contains left singular vectors (output features)\n",
    "- $\\Sigma$ is diagonal with singular values (importance scores)\n",
    "- $V^T$ contains right singular vectors (input features)\n",
    "\n",
    "By keeping only the top $k$ singular values, we approximate $W$ with two smaller matrices, trading accuracy for compression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50bd48",
   "metadata": {},
   "source": [
    "![](../imgs/svd.png \"SVD Decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbccd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FC_Decomposer:\n",
    "    \"Decompose fully-connected layers using SVD to reduce parameters\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def decompose(self, \n",
    "                  model: nn.Module,            # The model to decompose\n",
    "                  percent_removed: float = 0.5 # Fraction of singular values to remove [0, 1)\n",
    "    ) -> nn.Module:\n",
    "        \"Recursively decompose all Linear layers in the model using SVD\"\n",
    "        if not (0 <= percent_removed < 1):\n",
    "            raise ValueError(f\"percent_removed must be in range [0, 1), got {percent_removed}\")\n",
    "\n",
    "        new_model = copy.deepcopy(model)\n",
    "        module_names = list(new_model._modules)\n",
    "\n",
    "        for k, name in enumerate(module_names):\n",
    "            if len(list(new_model._modules[name]._modules)) > 0:\n",
    "                new_model._modules[name] = self.decompose(new_model._modules[name], percent_removed)\n",
    "            else:\n",
    "                if isinstance(new_model._modules[name], nn.Linear):\n",
    "                    layer = self.SVD(new_model._modules[name], percent_removed)\n",
    "                    new_model._modules[name] = layer\n",
    "        return new_model\n",
    "\n",
    "\n",
    "    def SVD(self, \n",
    "            layer: nn.Linear,       # The Linear layer to decompose\n",
    "            percent_removed: float  # Fraction of singular values to remove\n",
    "    ) -> nn.Sequential:\n",
    "        \"Perform SVD decomposition on a single Linear layer\"\n",
    "        W = layer.weight.data\n",
    "        U, S, V = torch.svd(W)\n",
    "        L = max(1, int((1.-percent_removed) * S.shape[0]))\n",
    "        W1 = U[:,:L]\n",
    "        W2 = torch.diag(S[:L]) @ V[:,:L].t()\n",
    "        layer_1 = nn.Linear(in_features=layer.in_features, \n",
    "                    out_features=L, bias=False)\n",
    "        layer_1.weight.data = W2\n",
    "\n",
    "        layer_2 = nn.Linear(in_features=L, \n",
    "                    out_features=layer.out_features, bias=True)\n",
    "        layer_2.weight.data = W1\n",
    "\n",
    "        if layer.bias is None: \n",
    "            layer_2.bias.data = torch.zeros(layer.out_features)\n",
    "        else:\n",
    "            layer_2.bias.data = layer.bias.data\n",
    "\n",
    "        return nn.Sequential(layer_1, layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50222d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/misc/fc_decomposer.py#L19){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FC_Decomposer.decompose\n",
       "\n",
       "```python\n",
       "\n",
       "def decompose(\n",
       "    model:Module, # The model to decompose\n",
       "    percent_removed:float=0.5, # Fraction of singular values to remove [0, 1)\n",
       ")->Module:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Recursively decompose all Linear layers in the model using SVD*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def decompose(\n",
       "    model:Module, # The model to decompose\n",
       "    percent_removed:float=0.5, # Fraction of singular values to remove [0, 1)\n",
       ")->Module:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Recursively decompose all Linear layers in the model using SVD*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FC_Decomposer.decompose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfn0ivy4b4o",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Example\n",
    "\n",
    "```python\n",
    "from fasterai.misc.fc_decomposer import FC_Decomposer\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# Load a model with large FC layers\n",
    "model = vgg16(pretrained=True)\n",
    "\n",
    "# Decompose, removing 50% of singular values\n",
    "decomposer = FC_Decomposer()\n",
    "compressed_model = decomposer.decompose(model, percent_removed=0.5)\n",
    "\n",
    "# Check parameter reduction\n",
    "original_params = sum(p.numel() for p in model.parameters())\n",
    "compressed_params = sum(p.numel() for p in compressed_model.parameters())\n",
    "print(f\"Compression: {original_params/compressed_params:.2f}x\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xwk977e4ia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "\n",
    "# SVD decomposition preserves output approximately\n",
    "model = nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 10))\n",
    "x = torch.randn(4, 32)\n",
    "out_orig = model(x)\n",
    "\n",
    "decomposer = FC_Decomposer()\n",
    "model_dec = decomposer.decompose(model, percent_removed=0.5)\n",
    "out_dec = model_dec(x)\n",
    "test_close(out_orig, out_dec, eps=1.0)  # 50% SVD removal has significant reconstruction error\n",
    "\n",
    "# Decomposed structure: Linear → Sequential(Linear, Linear)\n",
    "assert isinstance(model_dec[0], nn.Sequential)\n",
    "assert len(model_dec[0]) == 2\n",
    "\n",
    "# percent_removed=0 → very close output\n",
    "m2 = nn.Sequential(nn.Linear(32, 64))\n",
    "x2 = torch.randn(4, 32)\n",
    "out2 = m2(x2)\n",
    "m2_dec = decomposer.decompose(m2, percent_removed=0.0)\n",
    "test_close(out2, m2_dec(x2), eps=1e-4)\n",
    "\n",
    "# L >= 1 always (even at extreme removal)\n",
    "m3 = nn.Sequential(nn.Linear(10, 20))\n",
    "m3_dec = decomposer.decompose(m3, percent_removed=0.95)\n",
    "assert m3_dec[0][0].out_features >= 1\n",
    "\n",
    "# Invalid percent_removed raises ValueError\n",
    "with ExceptionExpected(ValueError):\n",
    "    decomposer.decompose(nn.Sequential(nn.Linear(10, 10)), percent_removed=1.0)\n",
    "with ExceptionExpected(ValueError):\n",
    "    decomposer.decompose(nn.Sequential(nn.Linear(10, 10)), percent_removed=-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca024eed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## See Also\n",
    "\n",
    "- [FC Decomposer Tutorial](../tutorials/misc/fc_decomposer.html) - Step-by-step walkthrough with examples\n",
    "- [BN Folding](bn_folding.html) - Another optimization technique to reduce inference overhead\n",
    "- [Pruner](../prune/pruner.html) - Remove entire filters for structured compression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
