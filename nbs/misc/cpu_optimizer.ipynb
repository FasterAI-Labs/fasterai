{
 "cells": [
  {
   "cell_type": "raw",
   "id": "452d040d",
   "metadata": {},
   "source": [
    "---\n",
    "description: Further optimize for CPU inference\n",
    "output-file: cpu_optimizer.html\n",
    "title: Further optimize for CPU inference\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd975549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp misc.cpu_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbccd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hbzsrd6sl1h",
   "metadata": {},
   "source": "## Overview\n\nThe `accelerate_model_for_cpu` function applies optimizations to prepare a PyTorch model for efficient CPU inference. It combines several techniques:\n\n1. **Channels-last memory format**: Optimizes memory layout for CNN operations on CPU\n2. **TorchScript compilation**: JIT compiles the model for faster execution\n3. **Mobile optimization**: Applies `optimize_for_mobile` for operator fusion and other optimizations\n\n**When to use:**\n- Deploying models on CPU-only servers\n- Edge deployment without GPU\n- After quantization for maximum CPU performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def accelerate_model_for_cpu(model: nn.Module, example_input: torch.Tensor):\n",
    "    model.eval()\n",
    "    example_input = example_input.to(memory_format=torch.channels_last)\n",
    "    \n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    model = torch.jit.script(model)\n",
    "    model = optimize_for_mobile(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50222d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(accelerate_model_for_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78818w1gh87",
   "metadata": {},
   "source": "**Parameters:**\n\n- `model`: The PyTorch model to optimize\n- `example_input`: A sample input tensor (used for tracing)\n\n**Returns:** An optimized TorchScript model\n\n---\n\n## Usage Example\n\n```python\nfrom fasterai.misc.cpu_optimizer import accelerate_model_for_cpu\nimport torch\n\n# Create example input matching your model's expected shape\nexample_input = torch.randn(1, 3, 224, 224)\n\n# Optimize model for CPU inference\noptimized_model = accelerate_model_for_cpu(model, example_input)\n\n# Use the optimized model\nwith torch.no_grad():\n    output = optimized_model(input_tensor)\n```\n\n**Note:** The returned model is a TorchScript model. Some dynamic Python features may not be supported."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
