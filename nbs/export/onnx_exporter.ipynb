{
 "cells": [
  {
   "cell_type": "raw",
   "id": "frontmatter",
   "metadata": {},
   "source": [
    "---\n",
    "description: Export PyTorch models to ONNX format with optional INT8 quantization\n",
    "output-file: onnx_exporter.html\n",
    "title: ONNX Exporter\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "default-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp export.onnx_exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dev-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-md",
   "metadata": {},
   "source": [
    "# ONNX Export\n",
    "\n",
    "Export PyTorch models to ONNX format for deployment. Supports:\n",
    "- Basic ONNX export with graph optimization\n",
    "- Dynamic INT8 quantization (no calibration needed)\n",
    "- Static INT8 quantization (with calibration data)\n",
    "- Output verification against original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dep-checks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@lru_cache(maxsize=None)\n",
    "def _has_package(name: str) -> bool:\n",
    "    \"Check if a package is available (cached)\"\n",
    "    from importlib.util import find_spec\n",
    "    return find_spec(name) is not None\n",
    "\n",
    "\n",
    "def _require(*packages: str, install_hint: str | None = None) -> None:\n",
    "    \"Raise ImportError if any package is missing\"\n",
    "    missing = [p for p in packages if not _has_package(p)]\n",
    "    if missing:\n",
    "        hint = install_hint or f\"pip install {' '.join(missing)}\"\n",
    "        raise ImportError(f\"Missing packages: {missing}. Install with: {hint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-md",
   "metadata": {},
   "source": [
    "## Export Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-onnx",
   "metadata": {},
   "outputs": [],
   "source": "#| export\ndef export_onnx(\n    model: nn.Module,                     # PyTorch model to export\n    sample: torch.Tensor,                 # Example input for tracing (with batch dim)\n    output_path: str | Path,              # Output .onnx file path\n    *,\n    opset_version: int = 17,              # ONNX opset version (17 recommended for compatibility)\n    quantize: bool = False,               # Apply INT8 quantization after export\n    quantize_mode: str = \"dynamic\",       # \"dynamic\" (no calibration) or \"static\"\n    calibration_data: Iterable | None = None,  # DataLoader for static quantization\n    optimize: bool = True,                # Run ONNX graph optimizer\n    dynamic_batch: bool = True,           # Allow variable batch size at runtime\n    input_names: list[str] | None = None, # Names for input tensors\n    output_names: list[str] | None = None,# Names for output tensors\n) -> Path:\n    \"Export a PyTorch model to ONNX format with optional quantization\"\n    _require(\"onnx\", install_hint=\"pip install onnx onnxruntime\")\n    import onnx\n    \n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    # Defaults\n    input_names = input_names or [\"input\"]\n    output_names = output_names or [\"output\"]\n    \n    # Quantization requires fixed batch size for shape inference\n    dynamic_axes = None\n    if dynamic_batch and not quantize:\n        dynamic_axes = {\n            input_names[0]: {0: \"batch_size\"},\n            output_names[0]: {0: \"batch_size\"},\n        }\n    \n    # Export to ONNX using legacy TorchScript exporter for better operator coverage\n    # The new dynamo-based exporter (PyTorch 2.x default) has limited op support\n    model.eval()\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n        torch.onnx.export(\n            model, sample, str(output_path),\n            opset_version=opset_version,\n            input_names=input_names,\n            output_names=output_names,\n            dynamic_axes=dynamic_axes,\n            do_constant_folding=True,\n            dynamo=False,  # Use legacy TorchScript exporter for broader op support\n        )\n    \n    # Optimize the graph (optional)\n    if optimize and _has_package(\"onnxoptimizer\"):\n        import onnxoptimizer\n        onnx_model = onnx.load(str(output_path))\n        onnx_model = onnxoptimizer.optimize(onnx_model)\n        onnx.save(onnx_model, str(output_path))\n    \n    # Apply quantization if requested\n    if quantize:\n        output_path = _quantize_onnx(output_path, quantize_mode, calibration_data, input_names[0])\n    \n    return output_path"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantize-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _quantize_onnx(\n",
    "    onnx_path: Path,\n",
    "    mode: str,\n",
    "    calibration_data: Iterable | None,\n",
    "    input_name: str,\n",
    ") -> Path:\n",
    "    \"Apply INT8 quantization to an ONNX model\"\n",
    "    _require(\"onnxruntime\", install_hint=\"pip install onnxruntime\")\n",
    "    \n",
    "    from onnxruntime.quantization import QuantFormat, QuantType, quantize_dynamic, quantize_static, shape_inference\n",
    "    \n",
    "    # Preprocess for shape inference\n",
    "    preprocessed = onnx_path.with_stem(f\"{onnx_path.stem}_preprocessed\")\n",
    "    shape_inference.quant_pre_process(str(onnx_path), str(preprocessed))\n",
    "    \n",
    "    quantized = onnx_path.with_stem(f\"{onnx_path.stem}_int8\")\n",
    "    \n",
    "    if mode == \"dynamic\":\n",
    "        quantize_dynamic(str(preprocessed), str(quantized), weight_type=QuantType.QUInt8)\n",
    "    elif mode == \"static\":\n",
    "        if calibration_data is None:\n",
    "            raise ValueError(\"Static quantization requires calibration_data\")\n",
    "        \n",
    "        from onnxruntime.quantization import CalibrationDataReader\n",
    "        \n",
    "        class _DataReader(CalibrationDataReader):\n",
    "            def __init__(self, data_iter, name):\n",
    "                self.it, self.name = iter(data_iter), name\n",
    "            def get_next(self):\n",
    "                try:\n",
    "                    batch = next(self.it)\n",
    "                    if isinstance(batch, (tuple, list)): batch = batch[0]\n",
    "                    return {self.name: batch.numpy()}\n",
    "                except StopIteration:\n",
    "                    return None\n",
    "        \n",
    "        quantize_static(\n",
    "            str(preprocessed), str(quantized),\n",
    "            calibration_data_reader=_DataReader(calibration_data, input_name),\n",
    "            quant_format=QuantFormat.QDQ,\n",
    "            activation_type=QuantType.QUInt8,\n",
    "            weight_type=QuantType.QInt8,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown quantize_mode: {mode}. Use 'dynamic' or 'static'.\")\n",
    "    \n",
    "    preprocessed.unlink(missing_ok=True)\n",
    "    return quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-md",
   "metadata": {},
   "source": [
    "## Inference Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onnx-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ONNXModel:\n",
    "    \"Wrapper for ONNX Runtime inference with PyTorch-like interface\"\n",
    "    \n",
    "    def __init__(self, path: str | Path, device: str = \"cpu\"):\n",
    "        _require(\"onnxruntime\", install_hint=\"pip install onnxruntime\")\n",
    "        import onnxruntime as ort\n",
    "        \n",
    "        self.path, self.device = Path(path), device\n",
    "        providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if device == \"cuda\" else [\"CPUExecutionProvider\"]\n",
    "        self.session = ort.InferenceSession(str(self.path), providers=providers)\n",
    "        self.input_name = self.session.get_inputs()[0].name\n",
    "        self.output_name = self.session.get_outputs()[0].name\n",
    "    \n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"Run inference on input tensor\"\n",
    "        x_np = x.cpu().numpy() if x.is_cuda else x.numpy()\n",
    "        result = torch.from_numpy(self.session.run([self.output_name], {self.input_name: x_np})[0])\n",
    "        return result.cuda() if self.device == \"cuda\" else result\n",
    "    \n",
    "    def warmup(self, sample: torch.Tensor, n: int = 10) -> None:\n",
    "        \"Run warmup iterations to stabilize inference timing\"\n",
    "        for _ in range(n): self(sample)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"ONNXModel(path='{self.path}', device='{self.device}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-md",
   "metadata": {},
   "source": [
    "## Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-onnx",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def verify_onnx(\n",
    "    model: nn.Module,        # Original PyTorch model\n",
    "    onnx_path: str | Path,   # Path to exported ONNX model\n",
    "    sample: torch.Tensor,    # Test input tensor\n",
    "    rtol: float = 1e-3,      # Relative tolerance\n",
    "    atol: float = 1e-5,      # Absolute tolerance\n",
    ") -> bool:\n",
    "    \"Verify ONNX model outputs match PyTorch model within tolerance\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pt_out = model(sample).cpu().numpy()\n",
    "    onnx_out = ONNXModel(onnx_path)(sample.cpu()).numpy()\n",
    "    return np.allclose(pt_out, onnx_out, rtol=rtol, atol=atol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-md",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "```python\n",
    "from fasterai.export.all import export_onnx, ONNXModel, verify_onnx\n",
    "\n",
    "# Basic export\n",
    "path = export_onnx(model, sample, \"model.onnx\")\n",
    "\n",
    "# With quantization\n",
    "path = export_onnx(model, sample, \"model.onnx\", quantize=True)\n",
    "\n",
    "# Inference\n",
    "onnx_model = ONNXModel(\"model.onnx\")\n",
    "output = onnx_model(input_tensor)\n",
    "\n",
    "# Verify\n",
    "assert verify_onnx(model, \"model.onnx\", sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "showdoc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/nathan/miniconda3/envs/dev/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "W0202 11:00:43.171000 421313 site-packages/torch/utils/cpp_extension.py:117] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-12.8'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found permutation search CUDA kernels\n",
      "[ASP][Info] permutation_search_kernels can be imported.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/export/onnx_exporter.py#L34){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### export_onnx\n",
       "\n",
       "```python\n",
       "\n",
       "def export_onnx(\n",
       "    model:nn.Module, # PyTorch model to export\n",
       "    sample:torch.Tensor, # Example input for tracing (with batch dim)\n",
       "    output_path:str | Path, # Output .onnx file path\n",
       "    opset_version:int=18, # ONNX opset version\n",
       "    quantize:bool=False, # Apply INT8 quantization after export\n",
       "    quantize_mode:str='dynamic', # \"dynamic\" (no calibration) or \"static\"\n",
       "    calibration_data:Iterable | None=None, # DataLoader for static quantization\n",
       "    optimize:bool=True, # Run ONNX graph optimizer\n",
       "    dynamic_batch:bool=True, # Allow variable batch size at runtime\n",
       "    input_names:list[str] | None=None, # Names for input tensors\n",
       "    output_names:list[str] | None=None, # Names for output tensors\n",
       ")->Path:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Export a PyTorch model to ONNX format with optional quantization*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def export_onnx(\n",
       "    model:nn.Module, # PyTorch model to export\n",
       "    sample:torch.Tensor, # Example input for tracing (with batch dim)\n",
       "    output_path:str | Path, # Output .onnx file path\n",
       "    opset_version:int=18, # ONNX opset version\n",
       "    quantize:bool=False, # Apply INT8 quantization after export\n",
       "    quantize_mode:str='dynamic', # \"dynamic\" (no calibration) or \"static\"\n",
       "    calibration_data:Iterable | None=None, # DataLoader for static quantization\n",
       "    optimize:bool=True, # Run ONNX graph optimizer\n",
       "    dynamic_batch:bool=True, # Allow variable batch size at runtime\n",
       "    input_names:list[str] | None=None, # Names for input tensors\n",
       "    output_names:list[str] | None=None, # Names for output tensors\n",
       ")->Path:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Export a PyTorch model to ONNX format with optional quantization*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(export_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "showdoc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/export/onnx_exporter.py#L145){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ONNXModel\n",
       "\n",
       "```python\n",
       "\n",
       "def ONNXModel(\n",
       "    path:str | Path, device:str='cpu'\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Wrapper for ONNX Runtime inference with PyTorch-like interface*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def ONNXModel(\n",
       "    path:str | Path, device:str='cpu'\n",
       "):\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Wrapper for ONNX Runtime inference with PyTorch-like interface*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ONNXModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "showdoc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/FasterAI-Labs/fasterai/tree/master/blob/master/fasterai/export/onnx_exporter.py#L172){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### verify_onnx\n",
       "\n",
       "```python\n",
       "\n",
       "def verify_onnx(\n",
       "    model:nn.Module, # Original PyTorch model\n",
       "    onnx_path:str | Path, # Path to exported ONNX model\n",
       "    sample:torch.Tensor, # Test input tensor\n",
       "    rtol:float=0.001, # Relative tolerance\n",
       "    atol:float=1e-05, # Absolute tolerance\n",
       ")->bool:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Verify ONNX model outputs match PyTorch model within tolerance*"
      ],
      "text/plain": [
       "```python\n",
       "\n",
       "def verify_onnx(\n",
       "    model:nn.Module, # Original PyTorch model\n",
       "    onnx_path:str | Path, # Path to exported ONNX model\n",
       "    sample:torch.Tensor, # Test input tensor\n",
       "    rtol:float=0.001, # Relative tolerance\n",
       "    atol:float=1e-05, # Absolute tolerance\n",
       ")->bool:\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "*Verify ONNX model outputs match PyTorch model within tolerance*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(verify_onnx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
