{
 "cells": [
  {
   "cell_type": "raw",
   "id": "93178b57",
   "metadata": {},
   "source": [
    "---\n",
    "description: Per-layer sensitivity analysis for compression methods\n",
    "output-file: sensitivity.html\n",
    "title: Sensitivity Analysis\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "## Overview\n\nNot all layers in a neural network are equally important. Some are fragile â€” compressing them even slightly degrades performance â€” while others are robust and can be heavily compressed with minimal impact. **Sensitivity analysis** measures this per-layer fragility, enabling smarter compression strategies.\n\nThe `SensitivityAnalyzer` works by compressing one layer at a time, measuring the impact on a user-provided evaluation metric, and ranking layers by their sensitivity (delta from baseline).\n\n**Key Features:**\n\n- Supports three compression types: **sparsity** (weight zeroing), **pruning** (structural filter removal), and **quantization** (precision reduction)\n- Generates **non-uniform per-layer targets** via `to_layer_targets()` â€” fragile layers get less compression, robust layers get more\n- Uses fasterai's `Sparsifier` and `Pruner` internally for consistent results\n- Visualization with `plot()` and export to pandas with `to_dataframe()`\n\n### When to Use\n\n| Compression Type | `level` parameter | What it tests | Best for |\n|------------------|-------------------|---------------|----------|\n| **`\"sparsity\"`** | % of weights zeroed (e.g., 50) | Unstructured weight removal | Generating per-layer sparsity targets |\n| **`\"pruning\"`** | % of filters removed (e.g., 30) | Structural filter pruning | Identifying layers to protect via `ignored_layers` |\n| **`\"quantization\"`** | Bit width (e.g., 8) | Precision reduction | Finding layers that need higher precision |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "default-exp",
   "metadata": {},
   "outputs": [],
   "source": "#| default_exp analyze.sensitivity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Callable, Any, Literal\n",
    "from collections import OrderedDict\n",
    "from fastcore.basics import store_attr\n",
    "\n",
    "# fasterai imports (relative within fasterai package)\n",
    "from fasterai.sparse.all import Sparsifier\n",
    "from fasterai.prune.all import Pruner\n",
    "from fasterai.core.all import large_final, Criteria, Granularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hide-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataclasses-header",
   "metadata": {},
   "source": "## Data Classes\n\nSensitivity results are returned as structured dataclasses for easy inspection, sorting, and export."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataclasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(slots=True)\n",
    "class LayerSensitivity:\n",
    "    \"\"\"Sensitivity result for a single layer.\"\"\"\n",
    "    name: str                    # layer name\n",
    "    layer_type: str              # e.g., \"Conv2d\", \"Linear\"\n",
    "    params: int                  # number of parameters\n",
    "    baseline_metric: float       # metric before compression\n",
    "    compressed_metric: float     # metric after compression\n",
    "    delta: float                 # metric change (positive = degradation)\n",
    "    \n",
    "    def as_dict(self) -> dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class SensitivityResult:\n",
    "    \"\"\"Structured result from sensitivity analysis.\"\"\"\n",
    "    compression_type: str                   # \"sparsity\", \"pruning\", \"quantization\"\n",
    "    compression_level: float                # e.g., 50 for 50% sparsity\n",
    "    baseline_metric: float                  # overall baseline metric\n",
    "    layers: list[LayerSensitivity]          # per-layer results\n",
    "    metric_name: str = \"accuracy\"           # name of the metric\n",
    "    higher_is_better: bool = True           # whether higher metric is better\n",
    "    _results: list[LayerSensitivity] = field(default=None, init=False, repr=False)  # for top() compatibility\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self._results = self.layers  # for compatibility with top() pattern\n",
    "    \n",
    "    def as_dict(self) -> dict[str, Any]:\n",
    "        \"\"\"Convert to flat dictionary.\"\"\"\n",
    "        return {\n",
    "            \"compression_type\": self.compression_type,\n",
    "            \"compression_level\": self.compression_level,\n",
    "            \"baseline_metric\": self.baseline_metric,\n",
    "            \"metric_name\": self.metric_name,\n",
    "            \"higher_is_better\": self.higher_is_better,\n",
    "            \"layers\": [l.as_dict() for l in self.layers],\n",
    "        }\n",
    "    \n",
    "    def top(\n",
    "        self,\n",
    "        n: int = 5,                    # number of layers to return\n",
    "        *,\n",
    "        most_sensitive: bool = True,   # True=highest delta (fragile), False=lowest (robust)\n",
    "    ) -> list[LayerSensitivity]:\n",
    "        \"\"\"Return top N most or least sensitive layers.\"\"\"\n",
    "        sorted_layers = sorted(self.layers, key=lambda x: x.delta, reverse=most_sensitive)\n",
    "        return sorted_layers[:n]\n",
    "    \n",
    "    def summary(\n",
    "        self,\n",
    "        *,\n",
    "        top: int = 5,  # number of layers to show per category\n",
    "    ) -> None:\n",
    "        \"\"\"Print a formatted summary of sensitivity analysis.\"\"\"\n",
    "        print(f\"{'â•' * 60}\")\n",
    "        print(f\"Sensitivity Analysis: {self.compression_type} @ {self.compression_level}%\")\n",
    "        print(f\"{'â•' * 60}\")\n",
    "        print(f\"  Baseline {self.metric_name}: {self.baseline_metric:.4f}\")\n",
    "        print(f\"  Layers analyzed: {len(self.layers)}\")\n",
    "        print()\n",
    "        \n",
    "        # Most sensitive (fragile) layers\n",
    "        print(f\"  ðŸ”´ Most Sensitive (fragile):\")\n",
    "        for i, layer in enumerate(self.top(top, most_sensitive=True), 1):\n",
    "            sign = \"+\" if layer.delta > 0 else \"\"\n",
    "            print(f\"     {i}. {layer.name:30} Î”={sign}{layer.delta:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Most robust layers\n",
    "        print(f\"  ðŸŸ¢ Most Robust (compressible):\")\n",
    "        for i, layer in enumerate(self.top(top, most_sensitive=False), 1):\n",
    "            sign = \"+\" if layer.delta > 0 else \"\"\n",
    "            print(f\"     {i}. {layer.name:30} Î”={sign}{layer.delta:.4f}\")\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"Convert to pandas DataFrame.\"\"\"\n",
    "        import pandas as pd\n",
    "        rows = [layer.as_dict() for layer in self.layers]\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def to_layer_targets(\n",
    "        self,\n",
    "        model: nn.Module,          # model (used for parameter counts)\n",
    "        target_pct: float = 50,    # target mean compression percentage\n",
    "        min_pct: float = 0,        # minimum compression for any layer\n",
    "        max_pct: float = 90,       # maximum compression for any layer\n",
    "        gamma: float = 1.0,        # exponent for sensitivity scaling (higher = more differentiation)\n",
    "    ) -> dict[str, float]:\n",
    "        \"\"\"Convert sensitivity to non-uniform per-layer compression targets.\n",
    "        \n",
    "        High sensitivity layers get lower compression, robust layers get higher.\n",
    "        Uses parameter-weighted optimization to hit target_pct exactly.\n",
    "        \"\"\"\n",
    "        if not self.layers:\n",
    "            return {}\n",
    "        \n",
    "        # Convert to fractions\n",
    "        target = target_pct / 100.0\n",
    "        smin = min_pct / 100.0\n",
    "        smax = max_pct / 100.0\n",
    "        \n",
    "        # Get sensitivity scores\n",
    "        names = [l.name for l in self.layers]\n",
    "        deltas = np.array([max(0.0, l.delta) for l in self.layers], dtype=float)\n",
    "        weights = np.array([float(l.params) for l in self.layers], dtype=float)\n",
    "        \n",
    "        if weights.sum() == 0:\n",
    "            return {n: target_pct for n in names}\n",
    "        \n",
    "        # Normalize sensitivity and invert (high sensitivity -> low compression)\n",
    "        if np.allclose(deltas, deltas[0]):\n",
    "            s0 = np.full_like(deltas, target)\n",
    "        else:\n",
    "            norm = (deltas - deltas.min()) / (np.ptp(deltas) + 1e-12)\n",
    "            inv = (1.0 - norm) ** gamma\n",
    "            s0 = smin + (smax - smin) * inv\n",
    "        \n",
    "        # Binary search for lambda to hit target weighted mean\n",
    "        W = weights.sum()\n",
    "        tgt = target * W\n",
    "        \n",
    "        def f(lam):\n",
    "            s = np.clip(s0 + lam, smin, smax)\n",
    "            return float(np.dot(weights, s))\n",
    "        \n",
    "        # Find lambda via bisection\n",
    "        lam_lo, lam_hi = -1.0, 1.0\n",
    "        while f(lam_lo) > tgt:\n",
    "            lam_lo *= 2\n",
    "        while f(lam_hi) < tgt:\n",
    "            lam_hi *= 2\n",
    "        \n",
    "        for _ in range(60):\n",
    "            lam_mid = 0.5 * (lam_lo + lam_hi)\n",
    "            if f(lam_mid) < tgt:\n",
    "                lam_lo = lam_mid\n",
    "            else:\n",
    "                lam_hi = lam_mid\n",
    "        \n",
    "        final_s = np.clip(s0 + 0.5 * (lam_lo + lam_hi), smin, smax)\n",
    "        \n",
    "        return {name: round(s * 100, 2) for name, s in zip(names, final_s)}\n",
    "    \n",
    "    def plot(\n",
    "        self,\n",
    "        figsize: tuple = (12, 5),  # figure size (width, height)\n",
    "    ) -> None:\n",
    "        \"\"\"Plot sensitivity as a bar chart.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        names = [l.name for l in self.layers]\n",
    "        deltas = np.array([l.delta for l in self.layers], dtype=float)\n",
    "        \n",
    "        # Color by sensitivity\n",
    "        norm = (deltas - deltas.min()) / (np.ptp(deltas) + 1e-9)\n",
    "        colors = plt.cm.RdYlGn_r(norm)  # Red=sensitive, Green=robust\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(range(len(deltas)), deltas, color=colors)\n",
    "        plt.axhline(0, color='gray', linewidth=1.2, linestyle='--')\n",
    "        plt.xticks(range(len(names)), names, rotation=60, ha='right')\n",
    "        plt.ylabel(f\"{self.metric_name} drop (Î”)\")\n",
    "        plt.title(f\"Layer Sensitivity to {self.compression_type} @ {self.compression_level}%\", \n",
    "                  pad=12, weight='bold')\n",
    "        plt.grid(axis='y', linestyle=':', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "me8c3ut7k7a",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(LayerSensitivity)"
  },
  {
   "cell_type": "markdown",
   "id": "3avy0eebxon",
   "metadata": {},
   "source": "`LayerSensitivity` holds the result for a single layer: the metric before and after compression, the delta (positive = degradation), and layer metadata. Use `as_dict()` to serialize.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7ms987f6b",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SensitivityResult)"
  },
  {
   "cell_type": "markdown",
   "id": "c5skl3as33",
   "metadata": {},
   "source": "`SensitivityResult` aggregates all per-layer results. It provides methods to inspect, rank, visualize, and convert sensitivity data into actionable compression targets.\n\n### Key Methods"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dsde2w6uaiq",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SensitivityResult.top)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fkfqbj9et8",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SensitivityResult.to_layer_targets)"
  },
  {
   "cell_type": "markdown",
   "id": "mroawy2h1mb",
   "metadata": {},
   "source": "The `to_layer_targets()` method converts sensitivity scores into a `dict[str, float]` mapping layer names to compression percentages. This dict can be passed directly to `Sparsifier.sparsify_model()` or `SparsifyCallback(sparsity=...)` for non-uniform compression.\n\nThe `gamma` parameter controls differentiation: higher values protect fragile layers more aggressively.\n\n```python\ntargets = result.to_layer_targets(model, target_pct=50, min_pct=10, max_pct=80, gamma=1.5)\n# {'conv1': 62.5, 'layer1.0.conv1': 65.3, 'layer1.1.conv2': 10.0, ...}\n```\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5tjjn2ajnq",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SensitivityResult.summary)\nshow_doc(SensitivityResult.plot)\nshow_doc(SensitivityResult.to_dataframe)"
  },
  {
   "cell_type": "markdown",
   "id": "analyzer-header",
   "metadata": {},
   "source": "---\n\n## SensitivityAnalyzer\n\nThe `SensitivityAnalyzer` class provides full control over the analysis process. For quick one-off analysis, see `analyze_sensitivity()` below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SensitivityAnalyzer:\n",
    "    \"\"\"Analyze per-layer sensitivity to compression methods.\n",
    "    \n",
    "    Uses fasterai's Sparsifier for sparsity analysis and Pruner for structural pruning.\n",
    "    Supports sparsity (weight zeroing), pruning (structural), and quantization.\n",
    "    \"\"\"\n",
    "    \n",
    "    VALID_COMPRESSIONS = frozenset({\"sparsity\", \"pruning\", \"quantization\"})\n",
    "    COMPRESSIBLE_LAYERS = Granularities.available_modules()  # Use fasterai's layer registry\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,                              # model to analyze\n",
    "        sample: torch.Tensor,                          # example input (for Pruner dependency analysis)\n",
    "        eval_fn: Callable[[nn.Module], float],         # evaluation function returning metric\n",
    "        *,\n",
    "        criteria: Criteria = large_final,              # fasterai criteria for importance scoring\n",
    "        higher_is_better: bool = True,                 # whether higher metric values are better\n",
    "        metric_name: str = \"accuracy\",                 # name of the metric for display\n",
    "        device: str | torch.device | None = None,      # device for computation\n",
    "        calibration_data: torch.Tensor | None = None,  # for observer-based quantization\n",
    "    ):\n",
    "        store_attr()\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self._results: SensitivityResult | None = None\n",
    "        self._sparsifier: Sparsifier | None = None\n",
    "        self._activation_hooks: list[Any] = []\n",
    "        self._activation_quantize_config: dict[str, bool] = {}\n",
    "    \n",
    "    def _get_compressible_layers(self) -> list[tuple[str, nn.Module]]:\n",
    "        \"\"\"Get all compressible layers (Conv2d, Linear, etc.).\"\"\"\n",
    "        return [\n",
    "            (name, module) \n",
    "            for name, module in self.model.named_modules()\n",
    "            if isinstance(module, self.COMPRESSIBLE_LAYERS)\n",
    "            and hasattr(module, 'weight') and module.weight is not None\n",
    "        ]\n",
    "    \n",
    "    def _init_sparsifier(\n",
    "        self,\n",
    "        granularity: str = \"weight\",  # sparsity granularity\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize fasterai Sparsifier (saves initial weights for all layers).\"\"\"\n",
    "        if self._sparsifier is None:\n",
    "            self._sparsifier = Sparsifier(\n",
    "                self.model,\n",
    "                granularity=granularity,\n",
    "                context='local',\n",
    "                criteria=self.criteria,\n",
    "            )\n",
    "    \n",
    "    def _cleanup_sparsifier(self) -> None:\n",
    "        \"\"\"Remove sparsifier buffers from model.\"\"\"\n",
    "        if self._sparsifier is not None:\n",
    "            self._sparsifier._clean_buffers()\n",
    "            self._sparsifier = None\n",
    "    \n",
    "    def _apply_sparsity(\n",
    "        self,\n",
    "        module: nn.Module,  # layer to sparsify\n",
    "        level: float,       # sparsity percentage (0-100)\n",
    "    ) -> None:\n",
    "        \"\"\"Apply sparsity using fasterai Sparsifier.\"\"\"\n",
    "        self._sparsifier.sparsify_layer(module, level)\n",
    "    \n",
    "    def _restore_layer(\n",
    "        self,\n",
    "        module: nn.Module,  # layer to restore\n",
    "    ) -> None:\n",
    "        \"\"\"Restore a single layer from saved initial weights.\"\"\"\n",
    "        if hasattr(module, '_init_weights'):\n",
    "            module.weight.data.copy_(module._init_weights)\n",
    "        if hasattr(module, '_init_biases') and module._init_biases is not None:\n",
    "            module.bias.data.copy_(module._init_biases)\n",
    "        if hasattr(module, '_mask'):\n",
    "            del module._buffers['_mask']\n",
    "    \n",
    "    def _clone_model(self) -> nn.Module:\n",
    "        \"\"\"Create a clean copy of the model (avoids deepcopy issues with non-leaf tensors).\"\"\"\n",
    "        import io\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(self.model, buffer)\n",
    "        buffer.seek(0)\n",
    "        model_copy = torch.load(buffer, weights_only=False)\n",
    "        model_copy.eval()\n",
    "        return model_copy\n",
    "    \n",
    "    def _apply_structural_pruning(\n",
    "        self, \n",
    "        target_name: str,  # name of layer to prune\n",
    "        level: float,      # pruning ratio (0-100)\n",
    "    ) -> nn.Module:\n",
    "        \"\"\"Apply structural pruning to a single layer using fasterai Pruner.\n",
    "        \n",
    "        Returns a copy of the model with only the target layer pruned.\n",
    "        Uses state_dict cloning to avoid deepcopy issues with non-leaf tensors.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model_copy = self._clone_model()\n",
    "        except Exception:\n",
    "            # Fallback to deepcopy if clone fails\n",
    "            model_copy = deepcopy(self.model)\n",
    "        \n",
    "        all_layers = []\n",
    "        target_module = None\n",
    "        for name, module in model_copy.named_modules():\n",
    "            if isinstance(module, self.COMPRESSIBLE_LAYERS):\n",
    "                all_layers.append(module)\n",
    "                if name == target_name:\n",
    "                    target_module = module\n",
    "        \n",
    "        ignored_layers = [m for m in all_layers if m is not target_module]\n",
    "        \n",
    "        try:\n",
    "            pruner = Pruner(\n",
    "                model_copy,\n",
    "                pruning_ratio=level,\n",
    "                context='local',\n",
    "                criteria=self.criteria,\n",
    "                ignored_layers=ignored_layers,\n",
    "                example_inputs=self.sample,\n",
    "            )\n",
    "            pruner.prune_model()\n",
    "        except Exception as e:\n",
    "            import warnings\n",
    "            warnings.warn(f\"Structural pruning failed for {target_name}: {e}\")\n",
    "            return model_copy\n",
    "        \n",
    "        return model_copy\n",
    "    \n",
    "    # â”€â”€â”€ Quantization helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \n",
    "    def _compute_qparams_symmetric(\n",
    "        self,\n",
    "        tensor: torch.Tensor,       # tensor to compute qparams for\n",
    "        bits: int = 8,              # quantization bits\n",
    "        per_channel: bool = False,  # per-channel or per-tensor\n",
    "        channel_axis: int = 0,      # axis for per-channel quantization\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute scale and zero_point for symmetric quantization.\"\"\"\n",
    "        qmin, qmax = -(2 ** (bits - 1)), 2 ** (bits - 1) - 1\n",
    "        \n",
    "        if per_channel and tensor.dim() > 1:\n",
    "            dims = list(range(tensor.dim()))\n",
    "            dims.remove(channel_axis)\n",
    "            amax = tensor.abs()\n",
    "            for dim in sorted(dims, reverse=True):\n",
    "                amax = amax.max(dim=dim).values\n",
    "            scale = amax / qmax\n",
    "            scale = torch.clamp(scale, min=1e-8)\n",
    "            zero_point = torch.zeros_like(scale, dtype=torch.int32)\n",
    "        else:\n",
    "            amax = tensor.abs().max()\n",
    "            scale = torch.tensor([max(amax.item() / qmax, 1e-8)], device=tensor.device)\n",
    "            zero_point = torch.tensor([0], dtype=torch.int32, device=tensor.device)\n",
    "        \n",
    "        return scale, zero_point\n",
    "    \n",
    "    def _compute_qparams_asymmetric(\n",
    "        self,\n",
    "        tensor: torch.Tensor,       # tensor to compute qparams for\n",
    "        bits: int = 8,              # quantization bits\n",
    "        per_channel: bool = False,  # per-channel or per-tensor\n",
    "        channel_axis: int = 0,      # axis for per-channel quantization\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute scale and zero_point for asymmetric quantization.\"\"\"\n",
    "        qmin, qmax = 0, 2 ** bits - 1\n",
    "        \n",
    "        if per_channel and tensor.dim() > 1:\n",
    "            dims = list(range(tensor.dim()))\n",
    "            dims.remove(channel_axis)\n",
    "            t_min, t_max = tensor.clone(), tensor.clone()\n",
    "            for dim in sorted(dims, reverse=True):\n",
    "                t_min = t_min.min(dim=dim).values\n",
    "                t_max = t_max.max(dim=dim).values\n",
    "        else:\n",
    "            t_min, t_max = tensor.min(), tensor.max()\n",
    "        \n",
    "        scale = (t_max - t_min) / (qmax - qmin)\n",
    "        scale = torch.clamp(scale, min=1e-8)\n",
    "        zero_point = torch.clamp(torch.round(-t_min / scale), qmin, qmax).to(torch.int32)\n",
    "        \n",
    "        if not per_channel or tensor.dim() <= 1:\n",
    "            scale = scale.view(1) if scale.dim() == 0 else scale\n",
    "            zero_point = zero_point.view(1) if zero_point.dim() == 0 else zero_point\n",
    "        \n",
    "        return scale, zero_point\n",
    "    \n",
    "    def _fake_quantize_per_channel(\n",
    "        self,\n",
    "        tensor: torch.Tensor,      # tensor to quantize\n",
    "        bits: int = 8,             # quantization bits\n",
    "        symmetric: bool = True,    # symmetric or asymmetric\n",
    "        channel_axis: int = 0,     # axis for per-channel quantization\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply fake quantization (per-channel).\"\"\"\n",
    "        qmin = -(2 ** (bits - 1)) if symmetric else 0\n",
    "        qmax = (2 ** (bits - 1)) - 1 if symmetric else (2 ** bits) - 1\n",
    "        \n",
    "        if symmetric:\n",
    "            scale, zero_point = self._compute_qparams_symmetric(\n",
    "                tensor, bits, per_channel=True, channel_axis=channel_axis\n",
    "            )\n",
    "        else:\n",
    "            scale, zero_point = self._compute_qparams_asymmetric(\n",
    "                tensor, bits, per_channel=True, channel_axis=channel_axis\n",
    "            )\n",
    "        \n",
    "        return torch.fake_quantize_per_channel_affine(\n",
    "            tensor, scale, zero_point, channel_axis, qmin, qmax\n",
    "        )\n",
    "    \n",
    "    def _fake_quantize_per_tensor(\n",
    "        self,\n",
    "        tensor: torch.Tensor,    # tensor to quantize\n",
    "        bits: int = 8,           # quantization bits\n",
    "        symmetric: bool = True,  # symmetric or asymmetric\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply fake quantization (per-tensor).\"\"\"\n",
    "        qmin = -(2 ** (bits - 1)) if symmetric else 0\n",
    "        qmax = (2 ** (bits - 1)) - 1 if symmetric else (2 ** bits) - 1\n",
    "        \n",
    "        if symmetric:\n",
    "            scale, zero_point = self._compute_qparams_symmetric(tensor, bits, per_channel=False)\n",
    "        else:\n",
    "            scale, zero_point = self._compute_qparams_asymmetric(tensor, bits, per_channel=False)\n",
    "        \n",
    "        return torch.fake_quantize_per_tensor_affine(\n",
    "            tensor, scale.item(), int(zero_point.item()), qmin, qmax\n",
    "        )\n",
    "    \n",
    "    def _apply_weight_quantization(\n",
    "        self, \n",
    "        module: nn.Module,       # layer to quantize\n",
    "        bits: int = 8,           # quantization bits\n",
    "        per_channel: bool = True,  # per-channel or per-tensor\n",
    "    ) -> None:\n",
    "        \"\"\"Apply weight quantization using fake_quantize.\"\"\"\n",
    "        weight = module.weight.data\n",
    "        if per_channel and weight.dim() > 1:\n",
    "            quantized = self._fake_quantize_per_channel(weight, bits, symmetric=True, channel_axis=0)\n",
    "        else:\n",
    "            quantized = self._fake_quantize_per_tensor(weight, bits, symmetric=True)\n",
    "        weight.copy_(quantized)\n",
    "    \n",
    "    def _create_activation_quantize_hook(\n",
    "        self,\n",
    "        layer_name: str,  # layer name for config lookup\n",
    "        bits: int = 8,    # quantization bits\n",
    "    ):\n",
    "        \"\"\"Create a forward hook that quantizes activations.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if self._activation_quantize_config.get(layer_name, False):\n",
    "                return self._fake_quantize_per_tensor(output, bits, symmetric=False)\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    def _setup_activation_hooks(\n",
    "        self,\n",
    "        bits: int = 8,  # quantization bits\n",
    "    ) -> None:\n",
    "        \"\"\"Register activation quantization hooks on all layers.\"\"\"\n",
    "        self._remove_activation_hooks()\n",
    "        for name, module in self._get_compressible_layers():\n",
    "            hook = self._create_activation_quantize_hook(name, bits)\n",
    "            handle = module.register_forward_hook(hook)\n",
    "            self._activation_hooks.append(handle)\n",
    "            self._activation_quantize_config[name] = False\n",
    "    \n",
    "    def _remove_activation_hooks(self) -> None:\n",
    "        \"\"\"Remove all activation quantization hooks.\"\"\"\n",
    "        for handle in self._activation_hooks:\n",
    "            handle.remove()\n",
    "        self._activation_hooks = []\n",
    "        self._activation_quantize_config = {}\n",
    "    \n",
    "    # â”€â”€â”€ Main analysis method â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \n",
    "    def analyze(\n",
    "        self,\n",
    "        compression: Literal[\"sparsity\", \"pruning\", \"quantization\"] = \"sparsity\",  # compression type\n",
    "        level: float = 50,                    # compression level (% for sparsity/pruning, bits for quant)\n",
    "        *,\n",
    "        granularity: str = \"weight\",          # granularity for sparsity (fasterai granularities)\n",
    "        layers: list[str] | None = None,      # specific layer names to analyze (None = all)\n",
    "        quant_per_channel: bool = True,       # use per-channel quantization\n",
    "        quant_activations: bool = False,      # also quantize activations\n",
    "        verbose: bool = True,                 # print progress\n",
    "    ) -> SensitivityResult:\n",
    "        \"\"\"Analyze per-layer sensitivity to compression.\"\"\"\n",
    "        if compression not in self.VALID_COMPRESSIONS:\n",
    "            raise ValueError(f\"compression must be one of {self.VALID_COMPRESSIONS}\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        if compression == \"sparsity\":\n",
    "            self._init_sparsifier(granularity)\n",
    "        \n",
    "        if compression == \"quantization\" and quant_activations:\n",
    "            bits = int(level) if level > 1 else 8\n",
    "            self._setup_activation_hooks(bits)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Computing baseline {self.metric_name}...\", end=\" \", flush=True)\n",
    "        baseline = self.eval_fn(self.model)\n",
    "        if verbose:\n",
    "            print(f\"{baseline:.4f}\")\n",
    "        \n",
    "        all_layers = self._get_compressible_layers()\n",
    "        if layers is not None:\n",
    "            all_layers = [(n, m) for n, m in all_layers if n in layers]\n",
    "        \n",
    "        mode_info = \"\"\n",
    "        if compression == \"quantization\":\n",
    "            mode_info = f\" (per-{'channel' if quant_per_channel else 'tensor'}\"\n",
    "            mode_info += f\", {'weights+activations' if quant_activations else 'weights only'})\"\n",
    "        elif compression == \"sparsity\":\n",
    "            mode_info = f\" (granularity={granularity}, criteria={self.criteria.f.__name__})\"\n",
    "        elif compression == \"pruning\":\n",
    "            mode_info = f\" (structural, criteria={self.criteria.f.__name__})\"\n",
    "        \n",
    "        if verbose:\n",
    "            unit = 'bits' if compression == 'quantization' else '%'\n",
    "            print(f\"Analyzing {len(all_layers)} layers for {compression} @ {level}{unit}{mode_info}\")\n",
    "        \n",
    "        results: list[LayerSensitivity] = []\n",
    "        \n",
    "        for i, (name, module) in enumerate(all_layers):\n",
    "            if verbose:\n",
    "                print(f\"  [{i+1}/{len(all_layers)}] {name}...\", end=\" \", flush=True)\n",
    "            \n",
    "            if compression == \"sparsity\":\n",
    "                self._apply_sparsity(module, level)\n",
    "                compressed_metric = self.eval_fn(self.model)\n",
    "                self._restore_layer(module)\n",
    "                param_count = module.weight.numel()\n",
    "                \n",
    "            elif compression == \"pruning\":\n",
    "                pruned_model = self._apply_structural_pruning(name, level)\n",
    "                compressed_metric = self.eval_fn(pruned_model)\n",
    "                param_count = module.weight.numel()\n",
    "                del pruned_model\n",
    "                \n",
    "            elif compression == \"quantization\":\n",
    "                saved_weight = module.weight.data.clone()\n",
    "                saved_bias = module.bias.data.clone() if module.bias is not None else None\n",
    "                \n",
    "                bits = int(level) if level > 1 else 8\n",
    "                self._apply_weight_quantization(module, bits, per_channel=quant_per_channel)\n",
    "                \n",
    "                if quant_activations:\n",
    "                    self._activation_quantize_config[name] = True\n",
    "                \n",
    "                compressed_metric = self.eval_fn(self.model)\n",
    "                \n",
    "                if quant_activations:\n",
    "                    self._activation_quantize_config[name] = False\n",
    "                \n",
    "                module.weight.data.copy_(saved_weight)\n",
    "                if saved_bias is not None:\n",
    "                    module.bias.data.copy_(saved_bias)\n",
    "                param_count = module.weight.numel()\n",
    "            \n",
    "            if self.higher_is_better:\n",
    "                delta = baseline - compressed_metric\n",
    "            else:\n",
    "                delta = compressed_metric - baseline\n",
    "            \n",
    "            if verbose:\n",
    "                sign = \"+\" if delta > 0 else \"\"\n",
    "                print(f\"Î”={sign}{delta:.4f}\")\n",
    "            \n",
    "            results.append(LayerSensitivity(\n",
    "                name=name,\n",
    "                layer_type=module.__class__.__name__,\n",
    "                params=param_count,\n",
    "                baseline_metric=baseline,\n",
    "                compressed_metric=compressed_metric,\n",
    "                delta=delta,\n",
    "            ))\n",
    "        \n",
    "        if compression == \"sparsity\":\n",
    "            self._cleanup_sparsifier()\n",
    "        if compression == \"quantization\" and quant_activations:\n",
    "            self._remove_activation_hooks()\n",
    "        \n",
    "        compression_desc = compression\n",
    "        if compression == \"quantization\":\n",
    "            compression_desc = f\"quantization-{int(level) if level > 1 else 8}bit\"\n",
    "            if quant_activations:\n",
    "                compression_desc += \"+act\"\n",
    "        \n",
    "        self._results = SensitivityResult(\n",
    "            compression_type=compression_desc,\n",
    "            compression_level=level,\n",
    "            baseline_metric=baseline,\n",
    "            layers=results,\n",
    "            metric_name=self.metric_name,\n",
    "            higher_is_better=self.higher_is_better,\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âœ“ Analysis complete\")\n",
    "        \n",
    "        return self._results\n",
    "    \n",
    "    def sweep(\n",
    "        self,\n",
    "        compression: Literal[\"sparsity\", \"pruning\", \"quantization\"] = \"sparsity\",  # compression type\n",
    "        levels: list[float] | None = None,  # compression levels to test (default: [25, 50, 75])\n",
    "        **kwargs,\n",
    "    ) -> list[SensitivityResult]:\n",
    "        \"\"\"Run sensitivity analysis at multiple compression levels.\"\"\"\n",
    "        if levels is None:\n",
    "            levels = [25, 50, 75]\n",
    "        results = []\n",
    "        for level in levels:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            unit = 'bits' if compression == 'quantization' else '%'\n",
    "            print(f\"Sweep: {compression} @ {level}{unit}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            result = self.analyze(compression, level, **kwargs)\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4q5hi4zx5zh",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SensitivityAnalyzer)"
  },
  {
   "cell_type": "markdown",
   "id": "dmm4r4vwvhm",
   "metadata": {},
   "source": "The `eval_fn` should take a `nn.Module` and return a scalar metric (e.g., accuracy, loss). The analyzer will call it once per layer, so it should be reasonably fast â€” a forward pass on a small validation batch is typical.\n\n### Key Methods"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkscpjtnhoc",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SensitivityAnalyzer.analyze)"
  },
  {
   "cell_type": "markdown",
   "id": "e0c5hyu284p",
   "metadata": {},
   "source": "For **quantization** analysis, additional parameters control the behavior:\n\n- `quant_per_channel=True` â€” per-channel quantization (more accurate, standard for weights)\n- `quant_activations=False` â€” set to `True` to also quantize activations (slower but more realistic)\n- `level` is interpreted as **bit width** (e.g., 8 for INT8) instead of percentage\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2e32ob21ma",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(SensitivityAnalyzer.sweep)"
  },
  {
   "cell_type": "markdown",
   "id": "convenience-header",
   "metadata": {},
   "source": "---\n\n## Convenience Function\n\nFor quick one-off analysis without creating an analyzer instance:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def analyze_sensitivity(\n",
    "    model: nn.Module,                    # model to analyze\n",
    "    sample: torch.Tensor,                # example input tensor\n",
    "    eval_fn: Callable[[nn.Module], float],  # evaluation function returning metric\n",
    "    compression: Literal[\"sparsity\", \"pruning\", \"quantization\"] = \"sparsity\",  # compression type\n",
    "    level: float = 50,                   # compression level (% for sparsity/pruning, bits for quant)\n",
    "    *,\n",
    "    criteria: Criteria = large_final,    # fasterai criteria for importance scoring\n",
    "    higher_is_better: bool = True,       # whether higher metric values are better\n",
    "    metric_name: str = \"accuracy\",       # name of the metric for display\n",
    "    granularity: str = \"weight\",         # granularity for sparsity\n",
    "    verbose: bool = True,                # print progress\n",
    "    **kwargs,\n",
    ") -> SensitivityResult:\n",
    "    \"\"\"One-line sensitivity analysis using fasterai compression methods.\"\"\"\n",
    "    analyzer = SensitivityAnalyzer(\n",
    "        model, sample, eval_fn, \n",
    "        criteria=criteria, \n",
    "        higher_is_better=higher_is_better,\n",
    "        metric_name=metric_name,\n",
    "    )\n",
    "    return analyzer.analyze(compression, level, granularity=granularity, verbose=verbose, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2k207hhds",
   "metadata": {},
   "outputs": [],
   "source": "show_doc(analyze_sensitivity)"
  },
  {
   "cell_type": "markdown",
   "id": "09g23bbjnc35",
   "metadata": {},
   "source": "### Usage Example\n\n```python\nfrom fasterai.analyze.sensitivity import analyze_sensitivity\n\nresult = analyze_sensitivity(model, sample, eval_fn, compression=\"sparsity\", level=50)\n\n# Inspect results\nresult.summary()                          # formatted console output\nfragile = result.top(5, most_sensitive=True)  # most sensitive layers\n\n# Generate per-layer targets for non-uniform compression\ntargets = result.to_layer_targets(model, target_pct=50, min_pct=10, max_pct=80)\n# Pass directly to Sparsifier: sparsifier.sparsify_model(sparsity=targets)\n```"
  },
  {
   "cell_type": "markdown",
   "id": "kptxpwxi3b",
   "metadata": {},
   "source": "---\n\n## See Also\n\n- [Sensitivity Tutorial](tutorials/analyze/sensitivity.html) - Step-by-step guide with real examples on ResNet18\n- [Sparsifier](../sparse/sparsifier.html) - Apply non-uniform sparsity using `to_layer_targets()` output\n- [Pruner](../prune/pruner.html) - Structural pruning (use `top()` to find layers to protect)\n- [Criteria](../core/criteria.html) - Importance scoring methods used during analysis\n- [Schedules](../core/schedules.html) - Control compression progression during training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "khcwer3hi0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "import torch.nn as nn\n",
    "\n",
    "def _test_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3, 16, 3, padding=1),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, 3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32, 10)\n",
    "    )\n",
    "\n",
    "model = _test_model()\n",
    "sample = torch.randn(2, 3, 8, 8)\n",
    "def _eval(m):\n",
    "    m.eval()\n",
    "    with torch.no_grad(): return m(sample).abs().mean().item()\n",
    "\n",
    "analyzer = SensitivityAnalyzer(model, sample, _eval)\n",
    "\n",
    "# Invalid compression raises ValueError\n",
    "with ExceptionExpected(ValueError):\n",
    "    analyzer.analyze('invalid', 50, verbose=False)\n",
    "\n",
    "# Result structure for sparsity analysis\n",
    "result = analyzer.analyze('sparsity', 30, verbose=False)\n",
    "assert isinstance(result, SensitivityResult)\n",
    "test_eq(result.compression_level, 30)\n",
    "assert len(result.layers) > 0\n",
    "assert isinstance(result.layers[0], LayerSensitivity)\n",
    "\n",
    "# top() sorted correctly (most sensitive first)\n",
    "top3 = result.top(3, most_sensitive=True)\n",
    "for i in range(len(top3)-1):\n",
    "    assert top3[i].delta >= top3[i+1].delta\n",
    "\n",
    "# to_layer_targets returns dict with layer names\n",
    "sched = result.to_layer_targets(model, target_pct=50)\n",
    "assert isinstance(sched, dict)\n",
    "assert len(sched) == len(result.layers)\n",
    "# All values should be percentages in valid range\n",
    "for v in sched.values():\n",
    "    assert 0 <= v <= 90\n",
    "\n",
    "# LayerSensitivity.as_dict returns proper dict\n",
    "d = result.layers[0].as_dict()\n",
    "assert 'name' in d\n",
    "assert 'delta' in d\n",
    "assert 'params' in d\n",
    "\n",
    "# SensitivityResult.as_dict works\n",
    "full_d = result.as_dict()\n",
    "assert 'compression_type' in full_d\n",
    "assert 'layers' in full_d\n",
    "\n",
    "# analyze_sensitivity convenience function works\n",
    "result2 = analyze_sensitivity(\n",
    "    _test_model(), sample, _eval,\n",
    "    compression='sparsity', level=20, verbose=False\n",
    ")\n",
    "assert isinstance(result2, SensitivityResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zkpwcyede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| slow\n",
    "# Full sensitivity analysis on a larger model\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "_model_lg = resnet18(weights=None)\n",
    "_sample_lg = torch.randn(2, 3, 32, 32)\n",
    "def _eval_lg(m):\n",
    "    m.eval()\n",
    "    with torch.no_grad(): return m(_sample_lg).abs().mean().item()\n",
    "\n",
    "_result_lg = analyze_sensitivity(_model_lg, _sample_lg, _eval_lg,\n",
    "                                 compression='sparsity', level=30, verbose=False)\n",
    "assert isinstance(_result_lg, SensitivityResult)\n",
    "assert len(_result_lg.layers) > 3  # resnet18 has many conv layers\n",
    "\n",
    "_sched_lg = _result_lg.to_layer_targets(_model_lg, target_pct=30)\n",
    "assert len(_sched_lg) > 3\n",
    "for v in _sched_lg.values():\n",
    "    assert 0 <= v <= 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
