{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mounted-racing",
   "metadata": {},
   "source": [
    "# Prune Transformers\n",
    "\n",
    "> How to prune a transformer with fasterai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-naples",
   "metadata": {},
   "source": [
    "> Note: This example code is taken from the fastai [docs](https://docs.fast.ai/tutorial.transformers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from transformers.modeling_utils import Conv1D\n",
    "from fastai.text.all import *\n",
    "import fastcore\n",
    "from fasterai.sparse.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = 'gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.WIKITEXT_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df_train = pd.read_csv(path/'train.csv', header=None)\n",
    "df_valid = pd.read_csv(path/'test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "all_texts = np.concatenate([df_train[0].values, df_valid[0].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        toks = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-phrase",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4576 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "splits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]\n",
    "tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "bs,sl = 4,256\n",
    "dls = tls.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='662' class='' max='662' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [662/662 00:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "def tokenize(text):\n",
    "    toks = tokenizer.tokenize(text)\n",
    "    return tensor(tokenizer.convert_tokens_to_ids(toks))\n",
    "\n",
    "tokenized = [tokenize(t) for t in progress_bar(all_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer): self.tokenizer = tokenizer\n",
    "    def encodes(self, x): \n",
    "        return x if isinstance(x, Tensor) else tokenize(x)\n",
    "        \n",
    "    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tls = TfmdLists(tokenized, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\n",
    "dls = tls.dataloaders(bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "class DropOutput(Callback):\n",
    "    def after_pred(self): self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-graph",
   "metadata": {},
   "source": [
    "Let's create our fastai `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-tuner",
   "metadata": {},
   "source": [
    "And let's try to extend a given prompt with the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-puzzle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "preds = learn.model.generate(inp, max_length=40, num_beams=5, temperature=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-grill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn on its head.\\n\\nA unicorn is a magical creature with a rainbow tail and a horn'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-trust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [3.695716381072998,40.2744140625]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-lithuania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.139103</td>\n",
       "      <td>2.843017</td>\n",
       "      <td>17.167484</td>\n",
       "      <td>07:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn on its head. It is a member of the <unk> <unk> <unk>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None]\n",
    "\n",
    "preds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n",
    "\n",
    "tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721f63e",
   "metadata": {},
   "source": [
    "## Make it sparse !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff50c46",
   "metadata": {},
   "source": [
    "Let's see now if we retrain our model, this time introducing sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95371238",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e6673",
   "metadata": {},
   "source": [
    "Also, when working with text, fastai defines the number of processed batches differently, so we have to adjust our `SparsifyCallback` accordingly (luckily, fastai makes it available as the `n_batches` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(SparsifyCallback)     \n",
    "def before_fit(self):\n",
    "    print(f'Pruning of {self.granularity} until a sparsity of {self.end_sparsity}%')\n",
    "    self.end_epoch = self.n_epoch if self.end_epoch is None else self.end_epoch\n",
    "    assert self.end_epoch <= self.n_epoch, 'Your end_epoch must be smaller than total number of epoch'\n",
    "\n",
    "    model = self.learn.model if self.model is None else self.model # Pass a model if you don't want the whole model to be pruned\n",
    "    self.sparsifier = Sparsifier(model, self.granularity, self.method, self.criteria, self.layer_type)\n",
    "    self.total_iters = self.end_epoch * self.dls.n_batches\n",
    "    self.start_iter = self.start_epoch * self.dls.n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909377e6",
   "metadata": {},
   "source": [
    "Let's define our `SparsifyCallback`. Let's say we want to make our model 30% sparse, by removing the highest-norm weight in each attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ae3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb = SparsifyCallback(end_sparsity=30, granularity='weight', method='local', criteria=large_final, sched_func=sched_onecycle, layer_type=Conv1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b21be",
   "metadata": {},
   "source": [
    "We now only have to pass our callback to fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of [30]%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.004998</td>\n",
       "      <td>2.860594</td>\n",
       "      <td>17.471899</td>\n",
       "      <td>12:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: [30.0]%\n",
      "Final Sparsity: [30.0]%\n",
      "Sparsity in Conv1D 9: 30.00%\n",
      "Sparsity in Conv1D 10: 30.00%\n",
      "Sparsity in Conv1D 15: 30.00%\n",
      "Sparsity in Conv1D 16: 30.00%\n",
      "Sparsity in Conv1D 21: 30.00%\n",
      "Sparsity in Conv1D 22: 30.00%\n",
      "Sparsity in Conv1D 27: 30.00%\n",
      "Sparsity in Conv1D 28: 30.00%\n",
      "Sparsity in Conv1D 33: 30.00%\n",
      "Sparsity in Conv1D 34: 30.00%\n",
      "Sparsity in Conv1D 39: 30.00%\n",
      "Sparsity in Conv1D 40: 30.00%\n",
      "Sparsity in Conv1D 45: 30.00%\n",
      "Sparsity in Conv1D 46: 30.00%\n",
      "Sparsity in Conv1D 51: 30.00%\n",
      "Sparsity in Conv1D 52: 30.00%\n",
      "Sparsity in Conv1D 57: 30.00%\n",
      "Sparsity in Conv1D 58: 30.00%\n",
      "Sparsity in Conv1D 63: 30.00%\n",
      "Sparsity in Conv1D 64: 30.00%\n",
      "Sparsity in Conv1D 69: 30.00%\n",
      "Sparsity in Conv1D 70: 30.00%\n",
      "Sparsity in Conv1D 75: 30.00%\n",
      "Sparsity in Conv1D 76: 30.00%\n",
      "Sparsity in Conv1D 81: 30.00%\n",
      "Sparsity in Conv1D 82: 30.00%\n",
      "Sparsity in Conv1D 87: 30.00%\n",
      "Sparsity in Conv1D 88: 30.00%\n",
      "Sparsity in Conv1D 93: 30.00%\n",
      "Sparsity in Conv1D 94: 30.00%\n",
      "Sparsity in Conv1D 99: 30.00%\n",
      "Sparsity in Conv1D 100: 30.00%\n",
      "Sparsity in Conv1D 105: 30.00%\n",
      "Sparsity in Conv1D 106: 30.00%\n",
      "Sparsity in Conv1D 111: 30.00%\n",
      "Sparsity in Conv1D 112: 30.00%\n",
      "Sparsity in Conv1D 117: 30.00%\n",
      "Sparsity in Conv1D 118: 30.00%\n",
      "Sparsity in Conv1D 123: 30.00%\n",
      "Sparsity in Conv1D 124: 30.00%\n",
      "Sparsity in Conv1D 129: 30.00%\n",
      "Sparsity in Conv1D 130: 30.00%\n",
      "Sparsity in Conv1D 135: 30.00%\n",
      "Sparsity in Conv1D 136: 30.00%\n",
      "Sparsity in Conv1D 141: 30.00%\n",
      "Sparsity in Conv1D 142: 30.00%\n",
      "Sparsity in Conv1D 147: 30.00%\n",
      "Sparsity in Conv1D 148: 30.00%\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-4, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49b313",
   "metadata": {},
   "source": [
    "And we can check the predicion to the same prompt as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn @-@ like head. The unicorn is a member of the <unk> <unk>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids = tokenizer.encode(prompt)\n",
    "inp = tensor(prompt_ids)[None]\n",
    "\n",
    "preds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n",
    "\n",
    "tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def print_sparsity(model):\n",
    "    for k,m in enumerate(learn.model.modules()):\n",
    "        if m.__class__.__name__ == 'Conv1D':\n",
    "            print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv1D 9: 30.00%\n",
      "Sparsity in Conv1D 10: 30.00%\n",
      "Sparsity in Conv1D 15: 30.00%\n",
      "Sparsity in Conv1D 16: 30.00%\n",
      "Sparsity in Conv1D 21: 30.00%\n",
      "Sparsity in Conv1D 22: 30.00%\n",
      "Sparsity in Conv1D 27: 30.00%\n",
      "Sparsity in Conv1D 28: 30.00%\n",
      "Sparsity in Conv1D 33: 30.00%\n",
      "Sparsity in Conv1D 34: 30.00%\n",
      "Sparsity in Conv1D 39: 30.00%\n",
      "Sparsity in Conv1D 40: 30.00%\n",
      "Sparsity in Conv1D 45: 30.00%\n",
      "Sparsity in Conv1D 46: 30.00%\n",
      "Sparsity in Conv1D 51: 30.00%\n",
      "Sparsity in Conv1D 52: 30.00%\n",
      "Sparsity in Conv1D 57: 30.00%\n",
      "Sparsity in Conv1D 58: 30.00%\n",
      "Sparsity in Conv1D 63: 30.00%\n",
      "Sparsity in Conv1D 64: 30.00%\n",
      "Sparsity in Conv1D 69: 30.00%\n",
      "Sparsity in Conv1D 70: 30.00%\n",
      "Sparsity in Conv1D 75: 30.00%\n",
      "Sparsity in Conv1D 76: 30.00%\n",
      "Sparsity in Conv1D 81: 30.00%\n",
      "Sparsity in Conv1D 82: 30.00%\n",
      "Sparsity in Conv1D 87: 30.00%\n",
      "Sparsity in Conv1D 88: 30.00%\n",
      "Sparsity in Conv1D 93: 30.00%\n",
      "Sparsity in Conv1D 94: 30.00%\n",
      "Sparsity in Conv1D 99: 30.00%\n",
      "Sparsity in Conv1D 100: 30.00%\n",
      "Sparsity in Conv1D 105: 30.00%\n",
      "Sparsity in Conv1D 106: 30.00%\n",
      "Sparsity in Conv1D 111: 30.00%\n",
      "Sparsity in Conv1D 112: 30.00%\n",
      "Sparsity in Conv1D 117: 30.00%\n",
      "Sparsity in Conv1D 118: 30.00%\n",
      "Sparsity in Conv1D 123: 30.00%\n",
      "Sparsity in Conv1D 124: 30.00%\n",
      "Sparsity in Conv1D 129: 30.00%\n",
      "Sparsity in Conv1D 130: 30.00%\n",
      "Sparsity in Conv1D 135: 30.00%\n",
      "Sparsity in Conv1D 136: 30.00%\n",
      "Sparsity in Conv1D 141: 30.00%\n",
      "Sparsity in Conv1D 142: 30.00%\n",
      "Sparsity in Conv1D 147: 30.00%\n",
      "Sparsity in Conv1D 148: 30.00%\n"
     ]
    }
   ],
   "source": [
    "print_sparsity(learn.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c34e5",
   "metadata": {},
   "source": [
    "That's it ! You now have a sparse Transformer as performant as the whole model. However, this model is currently not more efficient speed and storage wise. To have such a speed-up, I suggest you to look at the [granularity](https://nathanhubens.github.io/fasterai/granularity.html) section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
