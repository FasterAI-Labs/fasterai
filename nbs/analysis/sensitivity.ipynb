{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis\n",
    "\n",
    "> Per-layer sensitivity analysis for compression methods (sparsity, pruning, quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "default-exp",
   "metadata": {},
   "outputs": [],
   "source": "#| default_exp analysis.sensitivity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Callable, Any, Literal\n",
    "from collections import OrderedDict\n",
    "\n",
    "# fasterai imports (relative within fasterai package)\n",
    "from fasterai.sparse.all import Sparsifier\n",
    "from fasterai.prune.all import Pruner\n",
    "from fasterai.core.all import large_final, Criteria, Granularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hide-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataclasses-header",
   "metadata": {},
   "source": [
    "## Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataclasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(slots=True)\n",
    "class LayerSensitivity:\n",
    "    \"\"\"Sensitivity result for a single layer.\"\"\"\n",
    "    name: str                    # layer name\n",
    "    layer_type: str              # e.g., \"Conv2d\", \"Linear\"\n",
    "    params: int                  # number of parameters\n",
    "    baseline_metric: float       # metric before compression\n",
    "    compressed_metric: float     # metric after compression\n",
    "    delta: float                 # metric change (positive = degradation)\n",
    "    \n",
    "    def as_dict(self) -> dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class SensitivityResult:\n",
    "    \"\"\"Structured result from sensitivity analysis.\"\"\"\n",
    "    compression_type: str                   # \"sparsity\", \"pruning\", \"quantization\"\n",
    "    compression_level: float                # e.g., 50 for 50% sparsity\n",
    "    baseline_metric: float                  # overall baseline metric\n",
    "    layers: list[LayerSensitivity]          # per-layer results\n",
    "    metric_name: str = \"accuracy\"           # name of the metric\n",
    "    higher_is_better: bool = True           # whether higher metric is better\n",
    "    _results: list[LayerSensitivity] = field(default=None, init=False, repr=False)  # for top() compatibility\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self._results = self.layers  # for compatibility with top() pattern\n",
    "    \n",
    "    def as_dict(self) -> dict[str, Any]:\n",
    "        \"\"\"Convert to flat dictionary.\"\"\"\n",
    "        return {\n",
    "            \"compression_type\": self.compression_type,\n",
    "            \"compression_level\": self.compression_level,\n",
    "            \"baseline_metric\": self.baseline_metric,\n",
    "            \"metric_name\": self.metric_name,\n",
    "            \"higher_is_better\": self.higher_is_better,\n",
    "            \"layers\": [l.as_dict() for l in self.layers],\n",
    "        }\n",
    "    \n",
    "    def top(\n",
    "        self,\n",
    "        n: int = 5,                    # number of layers to return\n",
    "        *,\n",
    "        most_sensitive: bool = True,   # True=highest delta (fragile), False=lowest (robust)\n",
    "    ) -> list[LayerSensitivity]:\n",
    "        \"\"\"Return top N most or least sensitive layers.\"\"\"\n",
    "        sorted_layers = sorted(self.layers, key=lambda x: x.delta, reverse=most_sensitive)\n",
    "        return sorted_layers[:n]\n",
    "    \n",
    "    def summary(\n",
    "        self,\n",
    "        *,\n",
    "        top: int = 5,  # number of layers to show per category\n",
    "    ) -> None:\n",
    "        \"\"\"Print a formatted summary of sensitivity analysis.\"\"\"\n",
    "        print(f\"{'\u2550' * 60}\")\n",
    "        print(f\"Sensitivity Analysis: {self.compression_type} @ {self.compression_level}%\")\n",
    "        print(f\"{'\u2550' * 60}\")\n",
    "        print(f\"  Baseline {self.metric_name}: {self.baseline_metric:.4f}\")\n",
    "        print(f\"  Layers analyzed: {len(self.layers)}\")\n",
    "        print()\n",
    "        \n",
    "        # Most sensitive (fragile) layers\n",
    "        print(f\"  \ud83d\udd34 Most Sensitive (fragile):\")\n",
    "        for i, layer in enumerate(self.top(top, most_sensitive=True), 1):\n",
    "            sign = \"+\" if layer.delta > 0 else \"\"\n",
    "            print(f\"     {i}. {layer.name:30} \u0394={sign}{layer.delta:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Most robust layers\n",
    "        print(f\"  \ud83d\udfe2 Most Robust (compressible):\")\n",
    "        for i, layer in enumerate(self.top(top, most_sensitive=False), 1):\n",
    "            sign = \"+\" if layer.delta > 0 else \"\"\n",
    "            print(f\"     {i}. {layer.name:30} \u0394={sign}{layer.delta:.4f}\")\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"Convert to pandas DataFrame.\"\"\"\n",
    "        import pandas as pd\n",
    "        rows = [layer.as_dict() for layer in self.layers]\n",
    "        return pd.DataFrame(rows)\n",
    "    \n",
    "    def to_schedule(\n",
    "        self,\n",
    "        model: nn.Module,          # model (used for parameter counts)\n",
    "        target_pct: float = 50,    # target mean compression percentage\n",
    "        min_pct: float = 0,        # minimum compression for any layer\n",
    "        max_pct: float = 90,       # maximum compression for any layer\n",
    "        gamma: float = 1.0,        # exponent for sensitivity scaling (higher = more differentiation)\n",
    "    ) -> dict[str, float]:\n",
    "        \"\"\"Convert sensitivity to non-uniform compression schedule.\n",
    "        \n",
    "        High sensitivity layers get lower compression, robust layers get higher.\n",
    "        Uses parameter-weighted optimization to hit target_pct exactly.\n",
    "        \"\"\"\n",
    "        if not self.layers:\n",
    "            return {}\n",
    "        \n",
    "        # Convert to fractions\n",
    "        target = target_pct / 100.0\n",
    "        smin = min_pct / 100.0\n",
    "        smax = max_pct / 100.0\n",
    "        \n",
    "        # Get sensitivity scores\n",
    "        names = [l.name for l in self.layers]\n",
    "        deltas = np.array([max(0.0, l.delta) for l in self.layers], dtype=float)\n",
    "        weights = np.array([float(l.params) for l in self.layers], dtype=float)\n",
    "        \n",
    "        if weights.sum() == 0:\n",
    "            return {n: target_pct for n in names}\n",
    "        \n",
    "        # Normalize sensitivity and invert (high sensitivity -> low compression)\n",
    "        if np.allclose(deltas, deltas[0]):\n",
    "            s0 = np.full_like(deltas, target)\n",
    "        else:\n",
    "            norm = (deltas - deltas.min()) / (np.ptp(deltas) + 1e-12)\n",
    "            inv = (1.0 - norm) ** gamma\n",
    "            s0 = smin + (smax - smin) * inv\n",
    "        \n",
    "        # Binary search for lambda to hit target weighted mean\n",
    "        W = weights.sum()\n",
    "        tgt = target * W\n",
    "        \n",
    "        def f(lam):\n",
    "            s = np.clip(s0 + lam, smin, smax)\n",
    "            return float(np.dot(weights, s))\n",
    "        \n",
    "        # Find lambda via bisection\n",
    "        lam_lo, lam_hi = -1.0, 1.0\n",
    "        while f(lam_lo) > tgt:\n",
    "            lam_lo *= 2\n",
    "        while f(lam_hi) < tgt:\n",
    "            lam_hi *= 2\n",
    "        \n",
    "        for _ in range(60):\n",
    "            lam_mid = 0.5 * (lam_lo + lam_hi)\n",
    "            if f(lam_mid) < tgt:\n",
    "                lam_lo = lam_mid\n",
    "            else:\n",
    "                lam_hi = lam_mid\n",
    "        \n",
    "        final_s = np.clip(s0 + 0.5 * (lam_lo + lam_hi), smin, smax)\n",
    "        \n",
    "        return {name: round(s * 100, 2) for name, s in zip(names, final_s)}\n",
    "    \n",
    "    def plot(\n",
    "        self,\n",
    "        figsize: tuple = (12, 5),  # figure size (width, height)\n",
    "    ) -> None:\n",
    "        \"\"\"Plot sensitivity as a bar chart.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        names = [l.name for l in self.layers]\n",
    "        deltas = np.array([l.delta for l in self.layers], dtype=float)\n",
    "        \n",
    "        # Color by sensitivity\n",
    "        norm = (deltas - deltas.min()) / (np.ptp(deltas) + 1e-9)\n",
    "        colors = plt.cm.RdYlGn_r(norm)  # Red=sensitive, Green=robust\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.bar(range(len(deltas)), deltas, color=colors)\n",
    "        plt.axhline(0, color='gray', linewidth=1.2, linestyle='--')\n",
    "        plt.xticks(range(len(names)), names, rotation=60, ha='right')\n",
    "        plt.ylabel(f\"{self.metric_name} drop (\u0394)\")\n",
    "        plt.title(f\"Layer Sensitivity to {self.compression_type} @ {self.compression_level}%\", \n",
    "                  pad=12, weight='bold')\n",
    "        plt.grid(axis='y', linestyle=':', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzer-header",
   "metadata": {},
   "source": [
    "## SensitivityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SensitivityAnalyzer:\n",
    "    \"\"\"Analyze per-layer sensitivity to compression methods.\n",
    "    \n",
    "    Uses fasterai's Sparsifier for sparsity analysis and Pruner for structural pruning.\n",
    "    Supports sparsity (weight zeroing), pruning (structural), and quantization.\n",
    "    \"\"\"\n",
    "    \n",
    "    VALID_COMPRESSIONS = frozenset({\"sparsity\", \"pruning\", \"quantization\"})\n",
    "    COMPRESSIBLE_LAYERS = Granularities.available_modules()  # Use fasterai's layer registry\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,                              # model to analyze\n",
    "        sample: torch.Tensor,                          # example input (for Pruner dependency analysis)\n",
    "        eval_fn: Callable[[nn.Module], float],         # evaluation function returning metric\n",
    "        *,\n",
    "        criteria: Criteria = large_final,              # fasterai criteria for importance scoring\n",
    "        higher_is_better: bool = True,                 # whether higher metric values are better\n",
    "        metric_name: str = \"accuracy\",                 # name of the metric for display\n",
    "        device: str | torch.device | None = None,      # device for computation\n",
    "        calibration_data: torch.Tensor | None = None,  # for observer-based quantization\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.sample = sample\n",
    "        self.eval_fn = eval_fn\n",
    "        self.criteria = criteria\n",
    "        self.higher_is_better = higher_is_better\n",
    "        self.metric_name = metric_name\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self.calibration_data = calibration_data\n",
    "        self._results: SensitivityResult | None = None\n",
    "        self._sparsifier: Sparsifier | None = None\n",
    "        self._activation_hooks: list[Any] = []\n",
    "        self._activation_quantize_config: dict[str, bool] = {}\n",
    "    \n",
    "    def _get_compressible_layers(self) -> list[tuple[str, nn.Module]]:\n",
    "        \"\"\"Get all compressible layers (Conv2d, Linear, etc.).\"\"\"\n",
    "        return [\n",
    "            (name, module) \n",
    "            for name, module in self.model.named_modules()\n",
    "            if isinstance(module, self.COMPRESSIBLE_LAYERS)\n",
    "            and hasattr(module, 'weight') and module.weight is not None\n",
    "        ]\n",
    "    \n",
    "    def _init_sparsifier(\n",
    "        self,\n",
    "        granularity: str = \"weight\",  # sparsity granularity\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize fasterai Sparsifier (saves initial weights for all layers).\"\"\"\n",
    "        if self._sparsifier is None:\n",
    "            self._sparsifier = Sparsifier(\n",
    "                self.model,\n",
    "                granularity=granularity,\n",
    "                context='local',\n",
    "                criteria=self.criteria,\n",
    "            )\n",
    "    \n",
    "    def _cleanup_sparsifier(self) -> None:\n",
    "        \"\"\"Remove sparsifier buffers from model.\"\"\"\n",
    "        if self._sparsifier is not None:\n",
    "            self._sparsifier._clean_buffers()\n",
    "            self._sparsifier = None\n",
    "    \n",
    "    def _apply_sparsity(\n",
    "        self,\n",
    "        module: nn.Module,  # layer to sparsify\n",
    "        level: float,       # sparsity percentage (0-100)\n",
    "    ) -> None:\n",
    "        \"\"\"Apply sparsity using fasterai Sparsifier.\"\"\"\n",
    "        self._sparsifier.sparsify_layer(module, level)\n",
    "    \n",
    "    def _restore_layer(\n",
    "        self,\n",
    "        module: nn.Module,  # layer to restore\n",
    "    ) -> None:\n",
    "        \"\"\"Restore a single layer from saved initial weights.\"\"\"\n",
    "        if hasattr(module, '_init_weights'):\n",
    "            module.weight.data.copy_(module._init_weights)\n",
    "        if hasattr(module, '_init_biases') and module._init_biases is not None:\n",
    "            module.bias.data.copy_(module._init_biases)\n",
    "        if hasattr(module, '_mask'):\n",
    "            del module._buffers['_mask']\n",
    "    \n",
    "    def _apply_structural_pruning(\n",
    "        self, \n",
    "        target_name: str,  # name of layer to prune\n",
    "        level: float,      # pruning ratio (0-100)\n",
    "    ) -> nn.Module:\n",
    "        \"\"\"Apply structural pruning to a single layer using fasterai Pruner.\n",
    "        \n",
    "        Returns a deep copy of the model with only the target layer pruned.\n",
    "        \"\"\"\n",
    "        model_copy = deepcopy(self.model)\n",
    "        \n",
    "        all_layers = []\n",
    "        target_module = None\n",
    "        for name, module in model_copy.named_modules():\n",
    "            if isinstance(module, self.COMPRESSIBLE_LAYERS):\n",
    "                all_layers.append(module)\n",
    "                if name == target_name:\n",
    "                    target_module = module\n",
    "        \n",
    "        ignored_layers = [m for m in all_layers if m is not target_module]\n",
    "        \n",
    "        try:\n",
    "            pruner = Pruner(\n",
    "                model_copy,\n",
    "                pruning_ratio=level,\n",
    "                context='local',\n",
    "                criteria=self.criteria,\n",
    "                ignored_layers=ignored_layers,\n",
    "                example_inputs=self.sample,\n",
    "            )\n",
    "            pruner.prune_model()\n",
    "        except Exception as e:\n",
    "            import warnings\n",
    "            warnings.warn(f\"Structural pruning failed for {target_name}: {e}\")\n",
    "            return model_copy\n",
    "        \n",
    "        return model_copy\n",
    "    \n",
    "    # \u2500\u2500\u2500 Quantization helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    \n",
    "    def _compute_qparams_symmetric(\n",
    "        self,\n",
    "        tensor: torch.Tensor,       # tensor to compute qparams for\n",
    "        bits: int = 8,              # quantization bits\n",
    "        per_channel: bool = False,  # per-channel or per-tensor\n",
    "        channel_axis: int = 0,      # axis for per-channel quantization\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute scale and zero_point for symmetric quantization.\"\"\"\n",
    "        qmin, qmax = -(2 ** (bits - 1)), 2 ** (bits - 1) - 1\n",
    "        \n",
    "        if per_channel and tensor.dim() > 1:\n",
    "            dims = list(range(tensor.dim()))\n",
    "            dims.remove(channel_axis)\n",
    "            amax = tensor.abs()\n",
    "            for dim in sorted(dims, reverse=True):\n",
    "                amax = amax.max(dim=dim).values\n",
    "            scale = amax / qmax\n",
    "            scale = torch.clamp(scale, min=1e-8)\n",
    "            zero_point = torch.zeros_like(scale, dtype=torch.int32)\n",
    "        else:\n",
    "            amax = tensor.abs().max()\n",
    "            scale = torch.tensor([max(amax.item() / qmax, 1e-8)], device=tensor.device)\n",
    "            zero_point = torch.tensor([0], dtype=torch.int32, device=tensor.device)\n",
    "        \n",
    "        return scale, zero_point\n",
    "    \n",
    "    def _compute_qparams_asymmetric(\n",
    "        self,\n",
    "        tensor: torch.Tensor,       # tensor to compute qparams for\n",
    "        bits: int = 8,              # quantization bits\n",
    "        per_channel: bool = False,  # per-channel or per-tensor\n",
    "        channel_axis: int = 0,      # axis for per-channel quantization\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute scale and zero_point for asymmetric quantization.\"\"\"\n",
    "        qmin, qmax = 0, 2 ** bits - 1\n",
    "        \n",
    "        if per_channel and tensor.dim() > 1:\n",
    "            dims = list(range(tensor.dim()))\n",
    "            dims.remove(channel_axis)\n",
    "            t_min, t_max = tensor.clone(), tensor.clone()\n",
    "            for dim in sorted(dims, reverse=True):\n",
    "                t_min = t_min.min(dim=dim).values\n",
    "                t_max = t_max.max(dim=dim).values\n",
    "        else:\n",
    "            t_min, t_max = tensor.min(), tensor.max()\n",
    "        \n",
    "        scale = (t_max - t_min) / (qmax - qmin)\n",
    "        scale = torch.clamp(scale, min=1e-8)\n",
    "        zero_point = torch.clamp(torch.round(-t_min / scale), qmin, qmax).to(torch.int32)\n",
    "        \n",
    "        if not per_channel or tensor.dim() <= 1:\n",
    "            scale = scale.view(1) if scale.dim() == 0 else scale\n",
    "            zero_point = zero_point.view(1) if zero_point.dim() == 0 else zero_point\n",
    "        \n",
    "        return scale, zero_point\n",
    "    \n",
    "    def _fake_quantize_per_channel(\n",
    "        self,\n",
    "        tensor: torch.Tensor,      # tensor to quantize\n",
    "        bits: int = 8,             # quantization bits\n",
    "        symmetric: bool = True,    # symmetric or asymmetric\n",
    "        channel_axis: int = 0,     # axis for per-channel quantization\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply fake quantization (per-channel).\"\"\"\n",
    "        qmin = -(2 ** (bits - 1)) if symmetric else 0\n",
    "        qmax = (2 ** (bits - 1)) - 1 if symmetric else (2 ** bits) - 1\n",
    "        \n",
    "        if symmetric:\n",
    "            scale, zero_point = self._compute_qparams_symmetric(\n",
    "                tensor, bits, per_channel=True, channel_axis=channel_axis\n",
    "            )\n",
    "        else:\n",
    "            scale, zero_point = self._compute_qparams_asymmetric(\n",
    "                tensor, bits, per_channel=True, channel_axis=channel_axis\n",
    "            )\n",
    "        \n",
    "        return torch.fake_quantize_per_channel_affine(\n",
    "            tensor, scale, zero_point, channel_axis, qmin, qmax\n",
    "        )\n",
    "    \n",
    "    def _fake_quantize_per_tensor(\n",
    "        self,\n",
    "        tensor: torch.Tensor,    # tensor to quantize\n",
    "        bits: int = 8,           # quantization bits\n",
    "        symmetric: bool = True,  # symmetric or asymmetric\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Apply fake quantization (per-tensor).\"\"\"\n",
    "        qmin = -(2 ** (bits - 1)) if symmetric else 0\n",
    "        qmax = (2 ** (bits - 1)) - 1 if symmetric else (2 ** bits) - 1\n",
    "        \n",
    "        if symmetric:\n",
    "            scale, zero_point = self._compute_qparams_symmetric(tensor, bits, per_channel=False)\n",
    "        else:\n",
    "            scale, zero_point = self._compute_qparams_asymmetric(tensor, bits, per_channel=False)\n",
    "        \n",
    "        return torch.fake_quantize_per_tensor_affine(\n",
    "            tensor, scale.item(), int(zero_point.item()), qmin, qmax\n",
    "        )\n",
    "    \n",
    "    def _apply_weight_quantization(\n",
    "        self, \n",
    "        module: nn.Module,       # layer to quantize\n",
    "        bits: int = 8,           # quantization bits\n",
    "        per_channel: bool = True,  # per-channel or per-tensor\n",
    "    ) -> None:\n",
    "        \"\"\"Apply weight quantization using fake_quantize.\"\"\"\n",
    "        weight = module.weight.data\n",
    "        if per_channel and weight.dim() > 1:\n",
    "            quantized = self._fake_quantize_per_channel(weight, bits, symmetric=True, channel_axis=0)\n",
    "        else:\n",
    "            quantized = self._fake_quantize_per_tensor(weight, bits, symmetric=True)\n",
    "        weight.copy_(quantized)\n",
    "    \n",
    "    def _create_activation_quantize_hook(\n",
    "        self,\n",
    "        layer_name: str,  # layer name for config lookup\n",
    "        bits: int = 8,    # quantization bits\n",
    "    ):\n",
    "        \"\"\"Create a forward hook that quantizes activations.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if self._activation_quantize_config.get(layer_name, False):\n",
    "                return self._fake_quantize_per_tensor(output, bits, symmetric=False)\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    def _setup_activation_hooks(\n",
    "        self,\n",
    "        bits: int = 8,  # quantization bits\n",
    "    ) -> None:\n",
    "        \"\"\"Register activation quantization hooks on all layers.\"\"\"\n",
    "        self._remove_activation_hooks()\n",
    "        for name, module in self._get_compressible_layers():\n",
    "            hook = self._create_activation_quantize_hook(name, bits)\n",
    "            handle = module.register_forward_hook(hook)\n",
    "            self._activation_hooks.append(handle)\n",
    "            self._activation_quantize_config[name] = False\n",
    "    \n",
    "    def _remove_activation_hooks(self) -> None:\n",
    "        \"\"\"Remove all activation quantization hooks.\"\"\"\n",
    "        for handle in self._activation_hooks:\n",
    "            handle.remove()\n",
    "        self._activation_hooks = []\n",
    "        self._activation_quantize_config = {}\n",
    "    \n",
    "    # \u2500\u2500\u2500 Main analysis method \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    \n",
    "    def analyze(\n",
    "        self,\n",
    "        compression: Literal[\"sparsity\", \"pruning\", \"quantization\"] = \"sparsity\",  # compression type\n",
    "        level: float = 50,                    # compression level (% for sparsity/pruning, bits for quant)\n",
    "        *,\n",
    "        granularity: str = \"weight\",          # granularity for sparsity (fasterai granularities)\n",
    "        layers: list[str] | None = None,      # specific layer names to analyze (None = all)\n",
    "        quant_per_channel: bool = True,       # use per-channel quantization\n",
    "        quant_activations: bool = False,      # also quantize activations\n",
    "        verbose: bool = True,                 # print progress\n",
    "    ) -> SensitivityResult:\n",
    "        \"\"\"Analyze per-layer sensitivity to compression.\"\"\"\n",
    "        if compression not in self.VALID_COMPRESSIONS:\n",
    "            raise ValueError(f\"compression must be one of {self.VALID_COMPRESSIONS}\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        if compression == \"sparsity\":\n",
    "            self._init_sparsifier(granularity)\n",
    "        \n",
    "        if compression == \"quantization\" and quant_activations:\n",
    "            bits = int(level) if level > 1 else 8\n",
    "            self._setup_activation_hooks(bits)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Computing baseline {self.metric_name}...\", end=\" \", flush=True)\n",
    "        baseline = self.eval_fn(self.model)\n",
    "        if verbose:\n",
    "            print(f\"{baseline:.4f}\")\n",
    "        \n",
    "        all_layers = self._get_compressible_layers()\n",
    "        if layers is not None:\n",
    "            all_layers = [(n, m) for n, m in all_layers if n in layers]\n",
    "        \n",
    "        mode_info = \"\"\n",
    "        if compression == \"quantization\":\n",
    "            mode_info = f\" (per-{'channel' if quant_per_channel else 'tensor'}\"\n",
    "            mode_info += f\", {'weights+activations' if quant_activations else 'weights only'})\"\n",
    "        elif compression == \"sparsity\":\n",
    "            mode_info = f\" (granularity={granularity}, criteria={self.criteria.f.__name__})\"\n",
    "        elif compression == \"pruning\":\n",
    "            mode_info = f\" (structural, criteria={self.criteria.f.__name__})\"\n",
    "        \n",
    "        if verbose:\n",
    "            unit = 'bits' if compression == 'quantization' else '%'\n",
    "            print(f\"Analyzing {len(all_layers)} layers for {compression} @ {level}{unit}{mode_info}\")\n",
    "        \n",
    "        results: list[LayerSensitivity] = []\n",
    "        \n",
    "        for i, (name, module) in enumerate(all_layers):\n",
    "            if verbose:\n",
    "                print(f\"  [{i+1}/{len(all_layers)}] {name}...\", end=\" \", flush=True)\n",
    "            \n",
    "            if compression == \"sparsity\":\n",
    "                self._apply_sparsity(module, level)\n",
    "                compressed_metric = self.eval_fn(self.model)\n",
    "                self._restore_layer(module)\n",
    "                param_count = module.weight.numel()\n",
    "                \n",
    "            elif compression == \"pruning\":\n",
    "                pruned_model = self._apply_structural_pruning(name, level)\n",
    "                compressed_metric = self.eval_fn(pruned_model)\n",
    "                param_count = module.weight.numel()\n",
    "                del pruned_model\n",
    "                \n",
    "            elif compression == \"quantization\":\n",
    "                saved_weight = module.weight.data.clone()\n",
    "                saved_bias = module.bias.data.clone() if module.bias is not None else None\n",
    "                \n",
    "                bits = int(level) if level > 1 else 8\n",
    "                self._apply_weight_quantization(module, bits, per_channel=quant_per_channel)\n",
    "                \n",
    "                if quant_activations:\n",
    "                    self._activation_quantize_config[name] = True\n",
    "                \n",
    "                compressed_metric = self.eval_fn(self.model)\n",
    "                \n",
    "                if quant_activations:\n",
    "                    self._activation_quantize_config[name] = False\n",
    "                \n",
    "                module.weight.data.copy_(saved_weight)\n",
    "                if saved_bias is not None:\n",
    "                    module.bias.data.copy_(saved_bias)\n",
    "                param_count = module.weight.numel()\n",
    "            \n",
    "            if self.higher_is_better:\n",
    "                delta = baseline - compressed_metric\n",
    "            else:\n",
    "                delta = compressed_metric - baseline\n",
    "            \n",
    "            if verbose:\n",
    "                sign = \"+\" if delta > 0 else \"\"\n",
    "                print(f\"\u0394={sign}{delta:.4f}\")\n",
    "            \n",
    "            results.append(LayerSensitivity(\n",
    "                name=name,\n",
    "                layer_type=module.__class__.__name__,\n",
    "                params=param_count,\n",
    "                baseline_metric=baseline,\n",
    "                compressed_metric=compressed_metric,\n",
    "                delta=delta,\n",
    "            ))\n",
    "        \n",
    "        if compression == \"sparsity\":\n",
    "            self._cleanup_sparsifier()\n",
    "        if compression == \"quantization\" and quant_activations:\n",
    "            self._remove_activation_hooks()\n",
    "        \n",
    "        compression_desc = compression\n",
    "        if compression == \"quantization\":\n",
    "            compression_desc = f\"quantization-{int(level) if level > 1 else 8}bit\"\n",
    "            if quant_activations:\n",
    "                compression_desc += \"+act\"\n",
    "        \n",
    "        self._results = SensitivityResult(\n",
    "            compression_type=compression_desc,\n",
    "            compression_level=level,\n",
    "            baseline_metric=baseline,\n",
    "            layers=results,\n",
    "            metric_name=self.metric_name,\n",
    "            higher_is_better=self.higher_is_better,\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\u2713 Analysis complete\")\n",
    "        \n",
    "        return self._results\n",
    "    \n",
    "    def sweep(\n",
    "        self,\n",
    "        compression: Literal[\"sparsity\", \"pruning\", \"quantization\"] = \"sparsity\",  # compression type\n",
    "        levels: list[float] | None = None,  # compression levels to test (default: [25, 50, 75])\n",
    "        **kwargs,\n",
    "    ) -> list[SensitivityResult]:\n",
    "        \"\"\"Run sensitivity analysis at multiple compression levels.\"\"\"\n",
    "        if levels is None:\n",
    "            levels = [25, 50, 75]\n",
    "        results = []\n",
    "        for level in levels:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            unit = 'bits' if compression == 'quantization' else '%'\n",
    "            print(f\"Sweep: {compression} @ {level}{unit}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            result = self.analyze(compression, level, **kwargs)\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenience-header",
   "metadata": {},
   "source": [
    "## Convenience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def analyze_sensitivity(\n",
    "    model: nn.Module,                    # model to analyze\n",
    "    sample: torch.Tensor,                # example input tensor\n",
    "    eval_fn: Callable[[nn.Module], float],  # evaluation function returning metric\n",
    "    compression: Literal[\"sparsity\", \"pruning\", \"quantization\"] = \"sparsity\",  # compression type\n",
    "    level: float = 50,                   # compression level (% for sparsity/pruning, bits for quant)\n",
    "    *,\n",
    "    criteria: Criteria = large_final,    # fasterai criteria for importance scoring\n",
    "    higher_is_better: bool = True,       # whether higher metric values are better\n",
    "    metric_name: str = \"accuracy\",       # name of the metric for display\n",
    "    granularity: str = \"weight\",         # granularity for sparsity\n",
    "    verbose: bool = True,                # print progress\n",
    "    **kwargs,\n",
    ") -> SensitivityResult:\n",
    "    \"\"\"One-line sensitivity analysis using fasterai compression methods.\"\"\"\n",
    "    analyzer = SensitivityAnalyzer(\n",
    "        model, sample, eval_fn, \n",
    "        criteria=criteria, \n",
    "        higher_is_better=higher_is_better,\n",
    "        metric_name=metric_name,\n",
    "    )\n",
    "    return analyzer.analyze(compression, level, granularity=granularity, verbose=verbose, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}