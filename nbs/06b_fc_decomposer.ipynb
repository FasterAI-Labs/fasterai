{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd975549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp misc.fc_decomposer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5644a0",
   "metadata": {},
   "source": [
    "# Fully-Connected Layers Decomposer\n",
    "\n",
    "> Factorize heavy FC layers into smaller ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f88207",
   "metadata": {},
   "source": [
    "We can factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: $U \\Sigma V^T$\n",
    "With $\\Sigma$ being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value $k$ of singular values to keep and modify matrices $U$ and $V^T$ accordingly. The resulting will be an approximation of the initial matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50bd48",
   "metadata": {},
   "source": [
    "![alt text](imgs/svd.pdf \"SVD Decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "files = get_image_files(path/\"images\")\n",
    "\n",
    "def label_func(f): return f[0].isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, vgg16_bn(num_classes=2), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.886644</td>\n",
       "      <td>0.652151</td>\n",
       "      <td>0.685386</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.692583</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.685386</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.646516</td>\n",
       "      <td>0.622866</td>\n",
       "      <td>0.685386</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Decomposer:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def decompose(self, model, percent_removed=0.5):\n",
    "\n",
    "        new_model = copy.deepcopy(model)\n",
    "\n",
    "        module_names = list(new_model._modules)\n",
    "\n",
    "        for k, name in enumerate(module_names):\n",
    "\n",
    "            if len(list(new_model._modules[name]._modules)) > 0:\n",
    "                new_model._modules[name] = self.decompose(new_model._modules[name], percent_removed)\n",
    "\n",
    "            else:\n",
    "                if isinstance(new_model._modules[name], nn.Linear):\n",
    "                    # Folded BN\n",
    "                    layer = self.SVD(new_model._modules[name], percent_removed)\n",
    "\n",
    "                    # Replace old weight values\n",
    "                    new_model._modules[name] = layer # Replace the FC Layer by the decomposed version\n",
    "        return new_model\n",
    "\n",
    "\n",
    "    def SVD(self, layer, percent_removed):\n",
    "\n",
    "        W = layer.weight.data\n",
    "        U, S, V = torch.svd(W)\n",
    "        L = int((1.-percent_removed)*U.shape[0])\n",
    "        W1 = U[:,:L]\n",
    "        W2 = torch.diag(S[:L]) @ V[:,:L].t()\n",
    "        layer_1 = nn.Linear(in_features=layer.in_features, \n",
    "                    out_features=L, bias=False)\n",
    "        layer_1.weight.data = W2\n",
    "\n",
    "        layer_2 = nn.Linear(in_features=L, \n",
    "                    out_features=layer.out_features, bias=True)\n",
    "        layer_2.weight.data = W1\n",
    "\n",
    "        if layer.bias.data is None: \n",
    "            layer_2.bias.data = torch.zeros(*layer.out_features.shape)\n",
    "        else:\n",
    "            layer_2.bias.data = layer.bias.data\n",
    "\n",
    "        return nn.Sequential(layer_1, layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50222d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FC_Decomposer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5da2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = fc.decompose(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6fdce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=2048, bias=False)\n",
       "      (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    )\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "      (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    )\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=1, bias=False)\n",
       "      (1): Linear(in_features=1, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f017f",
   "metadata": {},
   "source": [
    "We can see compare the amount of parameters before/after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2952bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134277186"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfa006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91281476"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86174d3e",
   "metadata": {},
   "source": [
    "This represents a decrease of ~40M parameters ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4eecd",
   "metadata": {},
   "source": [
    "Now this is an approximation, so it isn't really lossless and we should expect to see a performance drop, which will be bigger as we keep fewer singular values. Here we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b865afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_learn = Learner(dls, new_model, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#2) [0.6868855357170105,0.6853856444358826]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_learn.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
