{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "listed-pursuit",
   "metadata": {},
   "source": [
    "# Neural Network Pruning\n",
    "\n",
    "> Make your neural network sparse with fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-quarter",
   "metadata": {},
   "source": [
    "## **Pruning**\n",
    "\n",
    "The [inspiration](https://en.wikipedia.org/wiki/Synaptic_pruning) behind neural network pruning is taken from how our own brain evolves during our life. Indeed, between our birth and adulthood, the amount of synapses (the structures that allows the neurons to transmit an electrical or chemical signal to another neuron) greatly varies. Our brain experience a large amount of growth during infancy, then basically follows a “use it or lose it” process. It results in a synaptic pruning which will remove any synapse that is not needed, in order to reach an optimal amount depending on our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-astrology",
   "metadata": {},
   "source": [
    "In the case of neural networks, the principle of pruning is to remove network connections that are considered *unimportant* to keep the network performance unchanged. Pruning is actually a quite old idea (like most ideas of deep learning) but that is an active field of research nowadays.\n",
    "\n",
    "It dates back to 1990s namely, with most popular work at that time being [Optimal Brain Damage](https://papers.nips.cc/paper/250-optimal-brain-damage.pdf) and  [Optimal Brain Surgeon](https://papers.nips.cc/paper/749-optimal-brain-surgeon-extensions-and-performance-comparisons.pdf). Pruning has been popularized by [Han et al.](https://arxiv.org/abs/1506.02626) with their 2015 paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-landscape",
   "metadata": {},
   "source": [
    "!['Pruning'](imgs/pruning.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-cement",
   "metadata": {},
   "source": [
    "### **Granularity**\n",
    "\n",
    "Neural network pruning can come in many fashion, represented in the image below: \n",
    "\n",
    "\n",
    "!['Synapses'](imgs/granularity.pdf \"Inspired by Mao et al., Exploring the Regularity of Sparse Structure in Convolutional Neural Networks\")\n",
    "\n",
    "\n",
    "You can either be very precise and remove each weight independently or remove bigger chunks at a time. The more fine-grained (or *unstructured*) the pruning, the more precise the process will be, but the more difficult it will be to get any acceleration. On the other hand, removing bigger chunks at a time (*structured pruning*) will be less accurate, but will make life easier for any sparse matrix computation libraries. So granularity of pruning will be a trade-off between precision and acceleration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-mexico",
   "metadata": {},
   "source": [
    "Apart from the granularity of pruning, you also have to choose **when** you will remove the weights.\n",
    "\n",
    "\n",
    "### **Scheduling**\n",
    "\n",
    "The timing and scheduling that you will adopt to prune your network will highly impact its final performance. \n",
    "\n",
    "The three most commonly used schedulings are:\n",
    "- **One-shot Pruning**\n",
    "- **Iterative Pruning**\n",
    "- **Gradual Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-antarctica",
   "metadata": {},
   "source": [
    "The most basic idea is to start from a trained model, prune it to the desired sparsity, then optionally fine-tune the network to accommodate from the removal of some of its parameters. This technique is known as ***One-shot Pruning***. However, a simple change in that technique is able to provide way better results. The idea is simply to perform the pruning phase over several steps, all followed by some fine-tuning. That technique is called ***Iterative Pruning*** and, while leading to better results, can sometimes be very time-consuming and computationally intensive, especially if the number of parameters removed at each iteration is low. There has also been [some research](https://arxiv.org/abs/1710.01878) in incorporating weight pruning directly **inside** of the training step, periodically removing the weights. This technique is called ***Gradual Pruning***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-celebration",
   "metadata": {},
   "source": [
    "### **Criteria**\n",
    "\n",
    "Now that we know how and when to remove our parameters, we have to know **which** ones to choose.\n",
    "\n",
    "There exist many ways to evaluate weights importance, but the two most common ways are:\n",
    "\n",
    "- **Weight Magnitude Pruning**\n",
    "- **Gradient Magnitude Pruning**\n",
    "\n",
    "While being extremely simple, weight magnitude pruning has been found to be very effective. It simply consists of computing the $L1$-norm, i.e $\\sum_{i}\\left|x_{i}\\right|$, of the weights (or group/kernel/filter depending on the granularity), and to remove the ones with the lowest values. In the case of gradient magnitude pruning, the only change is that we will multiply our weights by their corresponding gradients before computing the $L1$-norm on the result.\n",
    "\n",
    "Those criteria can be evaluated **locally**, i.e. each channel is pruned until the desired sparsity is reached, resulting in equally sparse layers, or **globally**, i.e. we evaluate the weights over the whole network, resulting in a sparse network, but with layers having different sparsity values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
