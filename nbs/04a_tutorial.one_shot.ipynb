{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consecutive-floating",
   "metadata": {},
   "source": [
    "# One-Shot Pruning\n",
    "\n",
    "> Make your neural network sparse with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-shift",
   "metadata": {},
   "source": [
    "The simplest way to perform pruning is called One-Shot Pruning. It consists of the following three steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-booth",
   "metadata": {},
   "source": [
    "![alt text](imgs/one_shot.pdf \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-browse",
   "metadata": {},
   "source": [
    "1. You first need to train a network\n",
    "2. You then need to remove some weights (depending on your criteria, needs,...)\n",
    "3. You fine-tune the remaining weights to recover from the loss of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-circular",
   "metadata": {},
   "source": [
    "With fasterai, this is really easy to do. Let's illustrate it by an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.all import *\n",
    "\n",
    "from fasterai.sparsifier import *\n",
    "from fasterai.criteria import *\n",
    "from fasterai.sparsify_callback import *\n",
    "from fasterai.schedule import one_shot\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_image_files(path/\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(f): return f[0].isupper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-measurement",
   "metadata": {},
   "source": [
    "We will first train a network without any pruning, which will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.679310</td>\n",
       "      <td>0.442378</td>\n",
       "      <td>0.832206</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.370691</td>\n",
       "      <td>0.304751</td>\n",
       "      <td>0.861299</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.212456</td>\n",
       "      <td>0.226414</td>\n",
       "      <td>0.910014</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-consideration",
   "metadata": {},
   "source": [
    "## One-Shot Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-chemical",
   "metadata": {},
   "source": [
    "There are two main ways that you can perform One-Shot Pruning with fasterai. \n",
    "\n",
    "1. You already possess a trained network and want to prune it\n",
    "2. You don't possess such a network and have to train it from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-desire",
   "metadata": {},
   "source": [
    "### 1. You possess a trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-printer",
   "metadata": {},
   "source": [
    "In this case, the step 1) of the One-Shot Pruning process is already done. But you still need to prune the network and then fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-cleanup",
   "metadata": {},
   "source": [
    "Let's say we want to remove $80 \\%$ of the weights of our network. This can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = Sparsifier(learn.model, 'weight', 'global', l1_norm)\n",
    "sp.prune(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-employee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8037889003753662"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc = learn.validate(); acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-harvard",
   "metadata": {},
   "source": [
    "Obviously, as we removed a good part of trained weights, the perfomance of the network is degraded. This can be solved by retraining our pruned network, making sure that the pruned weights keep their 0 value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-puzzle",
   "metadata": {},
   "source": [
    "We don't want to update the sparsity level anymore so we have to create a schedule that returns a constant value. Such a schedule exists in fasterai and is called `one_shot` and is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot(start, end, pos): return end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-quick",
   "metadata": {},
   "source": [
    "We can pass the same arguments to our callback than those used by the Sparsifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb=SparsifyCallback(sparsity=80, granularity='weight', method='global', criteria=l1_norm, sched_func=sched_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 80%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.224453</td>\n",
       "      <td>0.354158</td>\n",
       "      <td>0.866712</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.183741</td>\n",
       "      <td>0.218558</td>\n",
       "      <td>0.919486</td>\n",
       "      <td>00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101941</td>\n",
       "      <td>0.225590</td>\n",
       "      <td>0.920839</td>\n",
       "      <td>00:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 80.00%\n",
      "Sparsity at the end of epoch 1: 80.00%\n",
      "Sparsity at the end of epoch 2: 80.00%\n",
      "Final Sparsity: 80.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-sarah",
   "metadata": {},
   "source": [
    "We can also check where the pruned weights are in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d 2: 38.65%\n",
      "Sparsity in Conv2d 8: 55.38%\n",
      "Sparsity in Conv2d 11: 50.37%\n",
      "Sparsity in Conv2d 14: 48.50%\n",
      "Sparsity in Conv2d 17: 50.10%\n",
      "Sparsity in Conv2d 21: 53.52%\n",
      "Sparsity in Conv2d 24: 61.05%\n",
      "Sparsity in Conv2d 27: 42.68%\n",
      "Sparsity in Conv2d 30: 60.19%\n",
      "Sparsity in Conv2d 33: 62.56%\n",
      "Sparsity in Conv2d 37: 65.79%\n",
      "Sparsity in Conv2d 40: 70.87%\n",
      "Sparsity in Conv2d 43: 60.93%\n",
      "Sparsity in Conv2d 46: 74.63%\n",
      "Sparsity in Conv2d 49: 77.23%\n",
      "Sparsity in Conv2d 53: 77.70%\n",
      "Sparsity in Conv2d 56: 82.72%\n",
      "Sparsity in Conv2d 59: 60.01%\n",
      "Sparsity in Conv2d 62: 80.68%\n",
      "Sparsity in Conv2d 65: 91.67%\n"
     ]
    }
   ],
   "source": [
    "for k,m in enumerate(learn.model.modules()):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-warren",
   "metadata": {},
   "source": [
    "> Note: Using Sparsifier to prune the network is not necessary as it will also be called in the Callback. This was used here to better illustrate all the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-removal",
   "metadata": {},
   "source": [
    "### 2. You don't possess a trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=accuracy)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-remove",
   "metadata": {},
   "source": [
    "In this case, your network needs to be trained before pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-depression",
   "metadata": {},
   "source": [
    "You only need to create the Callback with the `one_shot` schedule and set the `start_epoch` argument, i.e. how many epochs you want to train your network before pruning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_cb=SparsifyCallback(sparsity=80, granularity='weight', method='global', criteria=l1_norm, sched_func=one_shot, start_epoch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-factor",
   "metadata": {},
   "source": [
    "Let's start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 80%\n",
      "Saving Weights at epoch 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.714634</td>\n",
       "      <td>0.769432</td>\n",
       "      <td>0.811908</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.439999</td>\n",
       "      <td>0.663998</td>\n",
       "      <td>0.786874</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265372</td>\n",
       "      <td>0.231254</td>\n",
       "      <td>0.907984</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148409</td>\n",
       "      <td>0.208199</td>\n",
       "      <td>0.923545</td>\n",
       "      <td>00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.092647</td>\n",
       "      <td>0.218885</td>\n",
       "      <td>0.932341</td>\n",
       "      <td>00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054311</td>\n",
       "      <td>0.206575</td>\n",
       "      <td>0.934371</td>\n",
       "      <td>00:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity at the end of epoch 0: 0.00%\n",
      "Sparsity at the end of epoch 1: 0.00%\n",
      "Sparsity at the end of epoch 2: 0.00%\n",
      "Sparsity at the end of epoch 3: 80.00%\n",
      "Sparsity at the end of epoch 4: 80.00%\n",
      "Sparsity at the end of epoch 5: 80.00%\n",
      "Final Sparsity: 80.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(6, cbs=sp_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-extent",
   "metadata": {},
   "source": [
    "Actually, doing the training and pruning in a single cycle works even better !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-headquarters",
   "metadata": {},
   "source": [
    "We can check if we get similar sparsity values across the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d 2: 38.33%\n",
      "Sparsity in Conv2d 8: 55.25%\n",
      "Sparsity in Conv2d 11: 50.01%\n",
      "Sparsity in Conv2d 14: 47.87%\n",
      "Sparsity in Conv2d 17: 49.38%\n",
      "Sparsity in Conv2d 21: 53.15%\n",
      "Sparsity in Conv2d 24: 60.62%\n",
      "Sparsity in Conv2d 27: 42.43%\n",
      "Sparsity in Conv2d 30: 59.82%\n",
      "Sparsity in Conv2d 33: 62.18%\n",
      "Sparsity in Conv2d 37: 65.48%\n",
      "Sparsity in Conv2d 40: 70.72%\n",
      "Sparsity in Conv2d 43: 60.64%\n",
      "Sparsity in Conv2d 46: 74.58%\n",
      "Sparsity in Conv2d 49: 77.17%\n",
      "Sparsity in Conv2d 53: 77.68%\n",
      "Sparsity in Conv2d 56: 82.79%\n",
      "Sparsity in Conv2d 59: 59.78%\n",
      "Sparsity in Conv2d 62: 80.73%\n",
      "Sparsity in Conv2d 65: 91.79%\n"
     ]
    }
   ],
   "source": [
    "for k,m in enumerate(learn.model.modules()):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
