[
  {
    "objectID": "tutorial.schedules.html",
    "href": "tutorial.schedules.html",
    "title": "Pruning Schedules",
    "section": "",
    "text": "Neural Network Pruning usually follows one of the next 3 schedules:\nIn fasterai, all those 3 schedules can be applied from the same callback. We’ll cover each below\nIn the SparsifyCallback, there are several parameters to ‘shape’ our pruning schedule: * start_sparsity: the initial sparsity of our model, generally kept at 0 as after initialization, our weights are generally non-zero. * end_sparsity: the target sparsity at the end of the training * start_epoch: we can decide to start pruning right from the beginning or let it train a bit before removing weights. * sched_func: this is where the general shape of the schedule is specified as it specifies how the sparsity evolves along the training. You can either use a schedule available in fastai our even coming with your own !\nWe will first train a network without any pruning, which will serve as a baseline."
  },
  {
    "objectID": "tutorial.schedules.html#one-shot-pruning",
    "href": "tutorial.schedules.html#one-shot-pruning",
    "title": "Pruning Schedules",
    "section": "One-Shot Pruning",
    "text": "One-Shot Pruning\nThe simplest way to perform pruning is called One-Shot Pruning. It consists of the following three steps:\n\nYou first need to train a network\nYou then need to remove some weights (depending on your criteria, needs,…)\nYou fine-tune the remaining weights to recover from the loss of parameters.\n\nWith fasterai, this is really easy to do. Let’s illustrate it by an example:\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning. This training can be done independently from the pruning callback, or simulated by the start_epoch that will delay the pruning process.\nYou thus only need to create the Callback with the one_shot schedule and set the start_epoch argument, i.e. how many epochs you want to train your network before pruning it.\n\nsp_cb=SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=one_shot)\n\nLet’s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit_one_cycle(6, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.671942\n      0.521863\n      0.809878\n      00:07\n    \n    \n      1\n      0.433165\n      0.335386\n      0.868742\n      00:07\n    \n    \n      2\n      0.252873\n      0.223187\n      0.906631\n      00:07\n    \n    \n      3\n      0.151653\n      0.195924\n      0.922869\n      00:07\n    \n    \n      4\n      0.091317\n      0.169764\n      0.929635\n      00:07\n    \n    \n      5\n      0.055428\n      0.161210\n      0.934371\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [50.0]%\nSparsity at the end of epoch 3: [50.0]%\nSparsity at the end of epoch 4: [50.0]%\nSparsity at the end of epoch 5: [50.0]%\nFinal Sparsity: [50]%\nSparsity in Conv2d 2: 50.00%\nSparsity in Conv2d 8: 50.00%\nSparsity in Conv2d 11: 50.00%\nSparsity in Conv2d 14: 50.00%\nSparsity in Conv2d 17: 50.00%\nSparsity in Conv2d 21: 50.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 50.00%\nSparsity in Conv2d 53: 50.00%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 59: 50.00%\nSparsity in Conv2d 62: 50.00%\nSparsity in Conv2d 65: 50.00%"
  },
  {
    "objectID": "tutorial.schedules.html#iterative-pruning",
    "href": "tutorial.schedules.html#iterative-pruning",
    "title": "Pruning Schedules",
    "section": "Iterative Pruning",
    "text": "Iterative Pruning\nResearchers have come up with a better way to do pruning than pruning all the weigths in once (as in One-Shot Pruning). The idea is to perform several iterations of pruning and fine-tuning and is thus called Iterative Pruning.\n\nYou first need to train a network\nYou then need to remove a part of the weights weights (depending on your criteria, needs,…)\nYou fine-tune the remaining weights to recover from the loss of parameters.\nBack to step 2.\n\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nIn this case, your network needs to be trained before pruning.\nYou only need to create the Callback with the iterative schedule and set the start_epoch argument, i.e. how many epochs you want to train your network before pruning it.\nThe iterative schedules has a n_stepsparameter, i.e. how many iterations of pruning/fine-tuning you want to perform. To modify its value, we can use the partial function like this:\niterative = partial(iterative, n_steps=5)\n\nsp_cb=SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=iterative)\n\nLet’s start pruningn after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit_one_cycle(6, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.747520\n      0.534082\n      0.823410\n      00:07\n    \n    \n      1\n      0.406695\n      0.292009\n      0.876184\n      00:07\n    \n    \n      2\n      0.248393\n      0.211495\n      0.912720\n      00:07\n    \n    \n      3\n      0.138455\n      0.226409\n      0.912720\n      00:07\n    \n    \n      4\n      0.092650\n      0.205549\n      0.926928\n      00:07\n    \n    \n      5\n      0.056216\n      0.195934\n      0.931664\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [16.67]%\nSparsity at the end of epoch 2: [33.33]%\nSparsity at the end of epoch 3: [33.33]%\nSparsity at the end of epoch 4: [50.0]%\nSparsity at the end of epoch 5: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 50.00%\nSparsity in Conv2d 8: 50.00%\nSparsity in Conv2d 11: 50.00%\nSparsity in Conv2d 14: 50.00%\nSparsity in Conv2d 17: 50.00%\nSparsity in Conv2d 21: 50.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 50.00%\nSparsity in Conv2d 53: 50.00%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 59: 50.00%\nSparsity in Conv2d 62: 50.00%\nSparsity in Conv2d 65: 50.00%"
  },
  {
    "objectID": "tutorial.schedules.html#gradual-pruning",
    "href": "tutorial.schedules.html#gradual-pruning",
    "title": "Pruning Schedules",
    "section": "Gradual Pruning",
    "text": "Gradual Pruning\nHere is for example how to implement the Automated Gradual Pruning schedule.\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsp_cb=SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=agp)\n\nLet’s start pruning after 3 epochs and train our model for 6 epochs to have the same total amount of training as before\n\nlearn.fit_one_cycle(6, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.675533\n      0.671494\n      0.815291\n      00:07\n    \n    \n      1\n      0.414929\n      0.261510\n      0.888363\n      00:07\n    \n    \n      2\n      0.261279\n      0.247027\n      0.903924\n      00:07\n    \n    \n      3\n      0.151988\n      0.198519\n      0.914750\n      00:07\n    \n    \n      4\n      0.088916\n      0.157761\n      0.933694\n      00:07\n    \n    \n      5\n      0.043516\n      0.148362\n      0.940460\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [21.02]%\nSparsity at the end of epoch 2: [37.79]%\nSparsity at the end of epoch 3: [46.39]%\nSparsity at the end of epoch 4: [49.55]%\nSparsity at the end of epoch 5: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 50.00%\nSparsity in Conv2d 8: 50.00%\nSparsity in Conv2d 11: 50.00%\nSparsity in Conv2d 14: 50.00%\nSparsity in Conv2d 17: 50.00%\nSparsity in Conv2d 21: 50.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 50.00%\nSparsity in Conv2d 53: 50.00%\nSparsity in Conv2d 56: 50.00%\nSparsity in Conv2d 59: 50.00%\nSparsity in Conv2d 62: 50.00%\nSparsity in Conv2d 65: 50.00%\n\n\nEven though they are often considered as different pruning methods, those 3 schedules can be captured by the same Callback. Here is how the sparsity in the network evolves for those methods;\nLet’s take an example here. Let’s say that we want to train our network for 3 epochs without pruning and then 7 epochs with pruning.\nThen this is what our different pruning schedules will look like:\n\n\n\n\n\nYou can also come up with your own pruning schedule !"
  },
  {
    "objectID": "knowledge_distillation.html",
    "href": "knowledge_distillation.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge Distillation, sometimes called teacher-student training, is a compression method in which a small (the student) model is trained to mimic the behaviour of a larger (the teacher) model.\nThe main goal is to reveal what is called the Dark Knowledge hidden in the teacher model.\nIf we take the same example provided by Geoffrey Hinton et al., we have\nThe main problem of classification is that the output activation function (softmax) will, by design, make a single value really high and squash others.\n\\[\np_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j} \\exp \\left(z_{j}\\right)}\n\\]\nWith \\(p_i\\) the probability of class \\(i\\), computed from the logits \\(z\\)\nHere is an example to illustrate this phenomenon:\nLet’s say that we have trained a model to discriminate between the following 5 classes: [cow, dog, plane, cat, car]\nAnd here is the output of the final layer (the logits) when the model is fed a new input image:\n\nlogits = torch.tensor([1.3, 3.1, 0.2, 1.9, -0.3])\n\nBy judging on the predictions, the model seems confident that the input data is a dog and quite confident that it is definitely not a plane nor a car, with predictions for cow and cat being moderately high.\nSo the model not only has learned to recognize a dog in the image, but also that a dog is very different from a car and a plane and share similarities with cats and cows. This information is what is called dark knowledge !\nWhen passing those predictions through a softmax, we have:\n\npredictions = F.softmax(logits, dim=-1); predictions\n\ntensor([0.1063, 0.6431, 0.0354, 0.1937, 0.0215])\n\n\nThis is accuenting the differences that we had earlier, discarding some of the dark knowledge acquired earlier. The way to keep this knowledge is to “soften” our softmax outputs, by adding a temperature parameter. The higher the temperature, the softer the predictions.\n\nsoft_predictions = F.softmax(logits/3, dim=-1); soft_predictions\n\ntensor([0.1879, 0.3423, 0.1302, 0.2294, 0.1102])\n\n\n\n\n\n\n\n\nNote\n\n\n\nif the Temperature is equal to 1, then we have regular softmax\n\n\nWhen applying Knowledge Distillation, we want to keep the Dark Knowledge that the teacher model has acquired during its training but not rely entirely on it. So we combine two losses:\n\nThe Teacher loss between the softened predictions of the teacher and the softened predictions of the student\nThe Classification loss, which is the regular loss between hard labels and hard predictions\n\nThe combination between those losses are weighted by an additional parameter α, as:\n\\[\nL_{K D}=\\alpha  * \\text { CrossEntropy }\\left(p_{S}^{\\tau}, p_{T}^{\\tau}\\right)+(1-\\alpha) * \\text { CrossEntropy }\\left(p_{S}, y_{\\text {true }}\\right)\n\\]\nWith \\(p^{\\tau}\\) being the softened predictions of the student and teacher\n\n\n\n\n\n\nNote\n\n\n\nIn practice, the distillation loss will be a bit different in the implementation\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom fastai.vision.all import *\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom functools import reduce\nfrom typing import Union\n:::\nThis can be done with fastai, using the Callback system !\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass KnowledgeDistillationCallback(Callback):\n    def __init__(self, teacher, loss, activations_student=None, activations_teacher=None, weight=0.5):\n        self.stored_activation_student, self.stored_activation_teacher  = {}, {}\n        store_attr()\n        if self.activations_student is not None:\n            self.activations_student, self.activations_teacher = listify(activations_student), listify(activations_teacher)\n        \n    def before_fit(self):\n        if self.activations_student and self.activations_teacher : self.register_hooks()\n        self.teacher.eval()\n\n    def after_loss(self):\n        teacher_pred = self.teacher(self.x)\n        new_loss = self.loss(pred=self.pred, teacher_pred=teacher_pred, fm_s=self.stored_activation_student, fm_t=self.stored_activation_teacher)\n        self.learn.loss_grad = torch.lerp(self.learn.loss_grad, new_loss, self.weight)\n        self.learn.loss = self.learn.loss_grad.clone()\n    \n    def register_hooks(self):\n        self.handles_st, self.handles_t = {}, {}\n        for name_st, name_t in zip(self.activations_student, self.activations_teacher):\n            self.handles_st[name_st] = get_module_by_name(self.learn, name_st).register_forward_hook(self.get_activation(self.stored_activation_student, name_st))\n            self.handles_t[name_t] = get_module_by_name(self.teacher, name_t).register_forward_hook(self.get_activation(self.stored_activation_teacher, name_t))\n        \n    def get_activation(self, activation, name):\n        def hook(model, input, output):\n            activation[name] = output\n        return hook\n    \n    def find_hook(self, m):\n        save = []\n        module_name = type(m).__name__\n        for k, v in m._forward_hooks.items():\n            save.append((module_name, k, v.__name__))\n        return save\n    \n    def remove_hooks(self, handles):\n        for k, v in handles.items():\n            handles[k].remove()\n    \n    def after_fit(self):\n        if self.activations_student and self.activations_teacher:\n            self.remove_hooks(self.handles_t)\n            self.remove_hooks(self.handles_st)\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef get_model_layers(model, getLayerRepr=False):\n    layers = OrderedDict() if getLayerRepr else []\n    def get_layers(net, prefix=[]):\n        if hasattr(net, \"_modules\"):\n            for name, layer in net._modules.items():\n                if layer is None:\n                    continue\n                if getLayerRepr:\n                    layers[\".\".join(prefix+[name])] = layer.__repr__()\n                else:\n                    layers.append(\".\".join(prefix + [name]))\n                get_layers(layer, prefix=prefix+[name])\n\n    get_layers(model)\n    return layers\n\n\ndef get_module_by_name(module: Union[torch.Tensor, nn.Module],\n                       access_string: str):\n\n    names = access_string.split(sep='.')\n    return reduce(getattr, names, module)\n:::\nThe loss function that is used may depend on the use case. For classification, we usually use the one presented above, named SoftTarget in fasterai. But for regression cases, we may want to perform regression on the logits directly.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef SoftTarget(pred, teacher_pred, T=5, **kwargs):\n    return nn.KLDivLoss(reduction='batchmean')(F.log_softmax(pred/T, dim=1), F.softmax(teacher_pred/T, dim=1)) * (T*T)\n\ndef Logits(pred, teacher_pred, **kwargs):\n    return F.mse_loss(pred, teacher_pred)\n\ndef Mutual(pred, teacher_pred, **kwargs):\n    return nn.KLDivLoss(reduction='batchmean')(F.log_softmax(pred, dim=1), F.softmax(teacher_pred, dim=1))\n\n\ndef Attention(fm_s, fm_t, p=2, **kwargs):\n    return sum([F.mse_loss(F.normalize(fm_s[name_st].pow(p).mean(1),dim=(1,2)), F.normalize(fm_t[name_t].pow(p).mean(1),dim=(1,2))) for name_st, name_t in zip(fm_s, fm_t)])\n\ndef ActivationBoundaries(fm_s, fm_t, m=2, **kwargs):\n    return sum([((fm_s[name_st] + m).pow(2) * ((fm_s[name_st] > -m) & (fm_t[name_t] <= 0)).float() + (fm_s[name_st] - m).pow(2) * ((fm_s[name_st] <= m) & (fm_t[name_t] > 0)).float()).mean() for name_st, name_t in zip(fm_s, fm_t)])\n\ndef FitNet(fm_s, fm_t, **kwargs):\n    return sum([F.mse_loss(fm_s[name_st],fm_t[name_t]) for name_st, name_t in zip(fm_s, fm_t)])\n\ndef Similarity(fm_s, fm_t, pred, p=2, **kwargs):\n    return sum([F.mse_loss(F.normalize(fm_s[name_st].view(fm_s[name_st].size(0), -1) @ fm_s[name_st].view(fm_s[name_st].size(0), -1).t(), p=p, dim=1), F.normalize(fm_t[name_t].view(fm_t[name_t].size(0), -1) @ fm_t[name_t].view(fm_t[name_t].size(0), -1).t(), p=p, dim=1)) for name_st, name_t in zip(fm_s, fm_t)])\n:::"
  },
  {
    "objectID": "tutorial.knowledge_distillation.html",
    "href": "tutorial.knowledge_distillation.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "We’ll illustrate how to use Knowledge Distillation to distill the knowledge of a Resnet34 (the teacher), to a Resnet18 (the student)\nLet’s us grab some data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nThe first step is then to train the teacher model. We’ll start from a pretrained model, ensuring to get good results on our dataset.\n\nteacher = cnn_learner(dls, resnet34, metrics=accuracy)\nteacher.unfreeze()\nteacher.fit_one_cycle(10, 1e-3)\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.721918\n      0.643276\n      0.841678\n      00:09\n    \n    \n      1\n      0.484658\n      0.604135\n      0.828146\n      00:08\n    \n    \n      2\n      0.401239\n      1.103915\n      0.815291\n      00:08\n    \n    \n      3\n      0.394400\n      0.318618\n      0.860622\n      00:08\n    \n    \n      4\n      0.276733\n      0.276223\n      0.878890\n      00:08\n    \n    \n      5\n      0.187687\n      0.515996\n      0.851150\n      00:08\n    \n    \n      6\n      0.127520\n      0.230542\n      0.911367\n      00:08\n    \n    \n      7\n      0.071110\n      0.233229\n      0.924222\n      00:08\n    \n    \n      8\n      0.044975\n      0.199706\n      0.931664\n      00:08\n    \n    \n      9\n      0.031355\n      0.177644\n      0.939784\n      00:08\n    \n  \n\n\n\n\nWithout KD\nWe’ll now train a Resnet18 from scratch, and without any help from the teacher model, to get that as a baseline\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nstudent.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.608119\n      0.594279\n      0.679296\n      00:07\n    \n    \n      1\n      0.577984\n      0.637746\n      0.690798\n      00:07\n    \n    \n      2\n      0.543163\n      0.532345\n      0.732070\n      00:07\n    \n    \n      3\n      0.508363\n      0.468151\n      0.772666\n      00:07\n    \n    \n      4\n      0.464459\n      0.442890\n      0.780108\n      00:07\n    \n    \n      5\n      0.405926\n      0.410481\n      0.816644\n      00:07\n    \n    \n      6\n      0.355392\n      0.429471\n      0.821380\n      00:07\n    \n    \n      7\n      0.278941\n      0.365873\n      0.838972\n      00:07\n    \n    \n      8\n      0.218126\n      0.366222\n      0.855886\n      00:07\n    \n    \n      9\n      0.165694\n      0.367872\n      0.857239\n      00:07\n    \n  \n\n\n\n\n\nWith KD\nAnd now we train the same model, but with the help of the teacher. The chosen loss is a combination of the regular classification loss (Cross-Entropy) and a loss pushing the student to learn from the teacher’s predictions.\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.335700\n      1.860445\n      0.692828\n      00:09\n    \n    \n      1\n      2.241398\n      1.773348\n      0.727334\n      00:09\n    \n    \n      2\n      2.055018\n      1.710084\n      0.723951\n      00:09\n    \n    \n      3\n      1.851421\n      1.632465\n      0.761840\n      00:09\n    \n    \n      4\n      1.620585\n      1.675239\n      0.755751\n      00:09\n    \n    \n      5\n      1.393245\n      1.410955\n      0.774019\n      00:09\n    \n    \n      6\n      1.155736\n      1.087842\n      0.826116\n      00:09\n    \n    \n      7\n      0.908853\n      0.983743\n      0.838972\n      00:09\n    \n    \n      8\n      0.696537\n      0.852848\n      0.857916\n      00:09\n    \n    \n      9\n      0.564625\n      0.854901\n      0.857239\n      00:09\n    \n  \n\n\n\nWhen helped, the student model performs better !\nThere exist more complicated KD losses, such as the one coming from Paying Attention to Attention, where the student tries to replicate the same attention maps of the teacher at intermediate layers.\nUsing such a loss requires to be able to specify from which layer we want to replicate those attention maps. To do so, we have to specify them from their string name, which can be obtained with the get_model_layers function.\nFor example, we set the loss to be applied after each Residual block of our models:\n\nstudent = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nkd = KnowledgeDistillationCallback(teacher.model, Attention, ['layer1', 'layer2', 'layer3', 'layer4'], ['0.4', '0.5', '0.6', '0.7'], weight=0.9)\nstudent.fit_one_cycle(10, 1e-3, cbs=kd)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.088313\n      0.088667\n      0.679973\n      00:09\n    \n    \n      1\n      0.079737\n      0.077369\n      0.719892\n      00:09\n    \n    \n      2\n      0.070380\n      0.065641\n      0.765223\n      00:09\n    \n    \n      3\n      0.061056\n      0.061554\n      0.792963\n      00:09\n    \n    \n      4\n      0.055300\n      0.058515\n      0.790934\n      00:09\n    \n    \n      5\n      0.048522\n      0.052656\n      0.830853\n      00:09\n    \n    \n      6\n      0.040360\n      0.047567\n      0.847767\n      00:09\n    \n    \n      7\n      0.032288\n      0.046334\n      0.855210\n      00:09\n    \n    \n      8\n      0.023988\n      0.045383\n      0.868065\n      00:09\n    \n    \n      9\n      0.020456\n      0.044370\n      0.866712\n      00:09"
  },
  {
    "objectID": "sparsify_callback.html",
    "href": "sparsify_callback.html",
    "title": "SparsifyCallback",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nfrom fastai.vision.all import *\nfrom fastai.callback.all import *\nfrom fasterai.sparse.sparsifier import *\nfrom fasterai.sparse.criteria import *\nfrom fasterai.sparse.schedule import *\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n:::\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\nclass SparsifyCallback(Callback):\n    def __init__(self, sparsity, granularity, context, criteria, schedule, lth=False, rewind_epoch=0, reset_end=False, save_tickets=False, model=None, round_to=None, layer_type=nn.Conv2d):\n        store_attr()\n        self.sparsity = listify(self.sparsity)\n\n    def before_fit(self):\n        print(f'Pruning of {self.granularity} until a sparsity of {self.sparsity}%')\n        assert self.schedule.start_pct*self.n_epoch>=self.rewind_epoch, 'You must rewind to an epoch before the start of the pruning process'\n        model = self.model if self.model else self.learn.model\n        self.sparsifier = Sparsifier(model, self.granularity, self.context, self.criteria, self.layer_type)\n\n    def before_epoch(self):\n        if self.epoch == self.rewind_epoch:\n            print(f'Saving Weights at epoch {self.epoch}')\n            self.sparsifier._save_weights()\n\n    def before_batch(self):\n        self.current_sparsity = self.schedule(self.sparsity, round(self.pct_train,3))\n        if self.schedule.pruned and self.training:\n            if self.lth and self.save_tickets:\n                print('Saving Intermediate Ticket')\n                self.sparsifier.save_model(f'winning_ticket_{self.previous_sparsity[0]:.2f}.pth', self.learn.model)\n            self.sparsifier.prune_model(self.current_sparsity, self.round_to)\n\n    def after_step(self):\n        if self.lth and self.schedule.pruned:\n            print(f'Resetting Weights to their epoch {self.rewind_epoch} values')\n            self.sparsifier._reset_weights(self.learn.model)\n        self.schedule.after_pruned()\n        self.sparsifier._apply_masks()\n\n    def after_epoch(self):\n        sparsity_str = [float(f\"%0.2f\"%sp) for sp in self.current_sparsity]\n        print(f'Sparsity at the end of epoch {self.epoch}: {sparsity_str}%')\n\n    def after_fit(self):\n        if self.save_tickets:\n            print('Saving Final Ticket')\n            self.sparsifier.save_model(f'winning_ticket_{self.previous_sparsity[0]:.2f}.pth', self.learn.model)\n        print(f'Final Sparsity: {self.schedule.current_sparsity:}%')\n        if self.reset_end: self.sparsifier._reset_weights()\n        self.sparsifier._clean_buffers()\n        self.schedule.reset()\n        self.sparsifier.print_sparsity()\n:::\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\n\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.714170\n      0.534177\n      0.802436\n      00:08\n    \n    \n      1\n      0.405863\n      0.466950\n      0.861976\n      00:07\n    \n    \n      2\n      0.229647\n      0.234999\n      0.902571\n      00:07\n    \n    \n      3\n      0.141966\n      0.198904\n      0.924222\n      00:07\n    \n    \n      4\n      0.073327\n      0.191152\n      0.930988\n      00:07\n    \n  \n\n\n\nLet’s now try adding some sparsity in our model\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=cos)\n\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.597135\n      0.568558\n      0.694181\n      00:09\n    \n    \n      1\n      0.543739\n      0.527585\n      0.730717\n      00:07\n    \n    \n      2\n      0.508932\n      0.507831\n      0.748309\n      00:07\n    \n    \n      3\n      0.451922\n      0.454692\n      0.799053\n      00:07\n    \n    \n      4\n      0.427453\n      0.434664\n      0.801759\n      00:07\n    \n    \n      5\n      0.377218\n      0.402817\n      0.823410\n      00:07\n    \n    \n      6\n      0.340924\n      0.410856\n      0.820027\n      00:07\n    \n    \n      7\n      0.319503\n      0.363846\n      0.837618\n      00:07\n    \n    \n      8\n      0.271233\n      0.377996\n      0.853180\n      00:07\n    \n    \n      9\n      0.228336\n      0.334722\n      0.865359\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [1.22]%\nSparsity at the end of epoch 1: [4.77]%\nSparsity at the end of epoch 2: [10.31]%\nSparsity at the end of epoch 3: [17.27]%\nSparsity at the end of epoch 4: [25.0]%\nSparsity at the end of epoch 5: [32.73]%\nSparsity at the end of epoch 6: [39.69]%\nSparsity at the end of epoch 7: [45.23]%\nSparsity at the end of epoch 8: [48.78]%\nSparsity at the end of epoch 9: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\nSurprisingly, our network that is composed of \\(50 \\%\\) of zeroes performs reasonnably well when compared to our plain and dense network.\nThe SparsifyCallback also accepts a list of sparsities, corresponding to each layer of layer_type to be pruned. Below, we show how to prune only the intermediate layers of ResNet-18.\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsparsities = [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]\n\n\nsp_cb = SparsifyCallback(sparsity=sparsities, granularity='weight', context='local', criteria=large_final, schedule=cos)\n\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.748184\n      0.876642\n      0.826116\n      00:08\n    \n    \n      1\n      0.422033\n      0.255813\n      0.889039\n      00:08\n    \n    \n      2\n      0.262884\n      0.234100\n      0.904601\n      00:08\n    \n    \n      3\n      0.132767\n      0.228366\n      0.921516\n      00:08\n    \n    \n      4\n      0.075110\n      0.210104\n      0.930311\n      00:08\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 4: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nFinal Sparsity: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity in Conv2d 2: 0.00%\nSparsity in Conv2d 8: 0.00%\nSparsity in Conv2d 11: 0.00%\nSparsity in Conv2d 14: 0.00%\nSparsity in Conv2d 17: 0.00%\nSparsity in Conv2d 21: 0.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 0.00%\nSparsity in Conv2d 53: 0.00%\nSparsity in Conv2d 56: 0.00%\nSparsity in Conv2d 59: 0.00%\nSparsity in Conv2d 62: 0.00%\nSparsity in Conv2d 65: 0.00%\n\n\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nstart_sparsity: the sparsity that the schedule will use as a starting point (default to 0)\nstart_epoch: the epoch at which the schedule will start pruning (default to 0)\nend_epoch: the epoch at which the schedule will stop pruning (default to the training epochs passed in fit)\nlth: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you don’t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`\n\nFor example, we correctly pruned the convolution layers of our model, but we could imagine pruning the Linear Layers of even only the BatchNorm ones !"
  },
  {
    "objectID": "granularity.html",
    "href": "granularity.html",
    "title": "Granularity",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=2}\n:::"
  },
  {
    "objectID": "granularity.html#conv2d-pruning",
    "href": "granularity.html#conv2d-pruning",
    "title": "Granularity",
    "section": "Conv2d Pruning",
    "text": "Conv2d Pruning\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=4}\n_granularities_Conv2d = {'weight':0, 'shared_weight':1, 'channel':2, 'column':3, 'row':4, 'kernel':(3,4), 'filter':(2,3,4), 'shared_channel':(1,2), 'shared_column': (1,3), 'shared_row': (1,4), 'vertical_slice': (2,3), 'horizontal_slice': (2,4), 'shared_vertical_slice': (1,2,3), 'shared_horizontal_slice': (1,2,4), 'shared_kernel': (1,3,4), 'layer':(1,2,3,4)}\n:::\nA Conv2d layer possess a 4d-tensor as weights. This means that there exist many ways of removing blocks from it.\n\n0-D Blocks\nIn the case of convolution filters, removing 0-D elements is equivalent to removing individual weights.\n\nweight granularity\n\n\nget_pruned_conv('weight')\n\n\n\n\n\n\n1-D Blocks\n1-D blocks of elements is equivalent to removing vectors from the convolution filters. There are several ways to chose the vectors, that will be represented below.\n\nshared_weight: this granularity is very particular as it removes individual weights from a filter, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_weight')\n\n\n\n\n\nchannel: remove vector of weights along the channel axis.\n\n\nget_pruned_conv('channel')\n\n\n\n\n\ncolumn: remove vector of weights along the height axis.\n\n\nget_pruned_conv('column')\n\n\n\n\n\nrow: remove vector of weights along the width axis.\n\n\nget_pruned_conv('row')\n\n\n\n\n\n\n2-D Blocks\n\nshared_channel: remove vector of weight along the channel axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_channel')\n\n\n\n\n\nshared_column: remove vector of weight along the height axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_column')\n\n\n\n\n\nshared_row: remove vector of weight along the width axis, but with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_row')\n\n\n\n\n\nvertical_slice: remove vertical slices of weight along the height axis.\n\n\nget_pruned_conv('vertical_slice')\n\n\n\n\n\nhorizontal_slice: remove vertical slices of weight along the width axis.\n\n\nget_pruned_conv('horizontal_slice')\n\n\n\n\n\nkernel: remove kernels of from the convolution filters.\n\n\nget_pruned_conv('kernel')\n\n\n\n\n\n\n3-D Blocks\n\nshared_vertical_slice: remove vertical slices of weight along the height axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_vertical_slice')\n\n\n\n\n\nshared_horizontal_slice: remove horizontal slices of weight along the width axis, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_horizontal_slice')\n\n\n\n\n\nshared_kernel: remove kernels of weight from the convolution filters, with a pattern that is shared across all filters.\n\n\nget_pruned_conv('shared_kernel')\n\n\n\n\n\nfilter: remove entire filters.\n\n\nget_pruned_conv('filter')"
  },
  {
    "objectID": "granularity.html#linear-pruning",
    "href": "granularity.html#linear-pruning",
    "title": "Granularity",
    "section": "Linear Pruning",
    "text": "Linear Pruning\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=22}\n_granularities_Linear = {'weight':0, 'column':1, 'row':2, 'layer':(1,2)}\n:::\n\n0-D Blocks\nAs for the convolution filters, weights from a Linear layer can be removed independently.\n\nweight: remove individual weights.\n\n\nget_pruned_linear('weight')\n\n\n\n\n\n\n1-D Blocks\n\ncolumn: remove column of weight, which corresponds to removing input neurons.\n\n\nget_pruned_linear('column')\n\n\n\n\n\nrow: remove rows of weight, which corresponds to removing output neurons.\n\n\nget_pruned_linear('row')"
  },
  {
    "objectID": "granularity.html#transformer-pruning",
    "href": "granularity.html#transformer-pruning",
    "title": "Granularity",
    "section": "Transformer Pruning",
    "text": "Transformer Pruning\n\n\n\n\n\n\nNote\n\n\n\nThis is an experimental part of the library\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=28}\ngranularities = {'Conv2d': _granularities_Conv2d, 'QuantConv2d': _granularities_Conv2d , 'Linear': _granularities_Linear, 'Conv1D': _granularities_Linear}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=29}\ndef available_granularities():\n    for k in granularities.keys():\n        print('Layer Type:', k)\n        print('Granularities:', list(granularities[k].keys()))\n        print('\\n')\n:::"
  },
  {
    "objectID": "tutorial.lottery_ticket.html",
    "href": "tutorial.lottery_ticket.html",
    "title": "Lottery Ticket Hypothesis",
    "section": "",
    "text": "The Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B > a_A\\), in a training time \\(t_B < t_A\\).\nLet’s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLet’s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.624942\n      0.559285\n      0.711096\n      00:08\n    \n    \n      1\n      0.554435\n      0.629745\n      0.699594\n      00:07\n    \n    \n      2\n      0.540063\n      0.562307\n      0.723951\n      00:07\n    \n    \n      3\n      0.500982\n      0.548926\n      0.704330\n      00:07\n    \n    \n      4\n      0.459079\n      0.565009\n      0.675913\n      00:07\n    \n  \n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n<All keys matched successfully>\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found “lottery ticket” after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.584362\n      0.607169\n      0.635995\n      00:07\n    \n    \n      1\n      0.547975\n      0.663318\n      0.719215\n      00:07\n    \n    \n      2\n      0.518505\n      0.544549\n      0.716509\n      00:07\n    \n    \n      3\n      0.472541\n      0.470786\n      0.775372\n      00:07\n    \n    \n      4\n      0.425963\n      0.673728\n      0.730717\n      00:07\n    \n    \n      5\n      0.573806\n      0.529624\n      0.723275\n      00:07\n    \n    \n      6\n      0.507284\n      0.458147\n      0.777402\n      00:07\n    \n    \n      7\n      0.447610\n      0.629908\n      0.652233\n      00:07\n    \n    \n      8\n      0.412829\n      0.394333\n      0.823410\n      00:07\n    \n    \n      9\n      0.381195\n      0.413377\n      0.815291\n      00:07\n    \n    \n      10\n      0.480223\n      0.518234\n      0.761840\n      00:07\n    \n    \n      11\n      0.415105\n      0.399813\n      0.823410\n      00:07\n    \n    \n      12\n      0.376185\n      0.387731\n      0.822733\n      00:07\n    \n    \n      13\n      0.353330\n      0.462406\n      0.771989\n      00:07\n    \n    \n      14\n      0.319414\n      0.355309\n      0.837618\n      00:07\n    \n    \n      15\n      0.392155\n      0.446360\n      0.792287\n      00:07\n    \n    \n      16\n      0.360181\n      0.461158\n      0.786874\n      00:07\n    \n    \n      17\n      0.325754\n      0.406724\n      0.824087\n      00:07\n    \n    \n      18\n      0.270658\n      0.589742\n      0.767930\n      00:07\n    \n    \n      19\n      0.263195\n      0.392320\n      0.832882\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B > a_A\\) in the same training time."
  },
  {
    "objectID": "tutorial.lottery_ticket.html#the-lottery-ticket-hypothesis",
    "href": "tutorial.lottery_ticket.html#the-lottery-ticket-hypothesis",
    "title": "Lottery Ticket Hypothesis",
    "section": "The Lottery Ticket Hypothesis",
    "text": "The Lottery Ticket Hypothesis\nThe Lottery Ticket Hypothesis is a really intriguing discovery made in 2019 by Frankle & Carbin. It states that:\n\nA randomly-initialized, dense neural network contains a subnetwork that is initialised such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n\nMeaning that, once we find that subnetwork. Every other parameter in the network becomes useless.\nThe way authors propose to find those subnetwork is as follows:\n\nInitialize the neural network\nTrain it to convergence\nPrune the smallest magnitude weights by creating a mask \\(m\\)\nReinitialize the weights to their original value; i.e at iteration \\(0\\).\nRepeat from step 2 until reaching the desired level of sparsity.\n\n\nfrom fasterai.sparse.all import *\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64), device=device)\n\nWhat we are trying to prove is that: in a neural network A, there exists a subnetwork B able to get an accuracy \\(a_B > a_A\\), in a training time \\(t_B < t_A\\).\nLet’s get the baseline for network A:\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\nLet’s save original weights\n\ninitial_weights = deepcopy(learn.model.state_dict())\n\n\nlearn.fit(5, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.624942\n      0.559285\n      0.711096\n      00:08\n    \n    \n      1\n      0.554435\n      0.629745\n      0.699594\n      00:07\n    \n    \n      2\n      0.540063\n      0.562307\n      0.723951\n      00:07\n    \n    \n      3\n      0.500982\n      0.548926\n      0.704330\n      00:07\n    \n    \n      4\n      0.459079\n      0.565009\n      0.675913\n      00:07\n    \n  \n\n\n\nWe now have our accuracy \\(a_A\\) of \\(79\\%\\) and our training time \\(t_A\\) of \\(5\\) epochs\nTo find the lottery ticket, we will perform iterative pruning but, at each pruning step we will re-initialize the remaining weights to their original values (i.e. before training).\nWe will restart from the same initialization to be sure to not get lucky.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n<All keys matched successfully>\n\n\nWe can pass the parameters lth=True to make the weights of the network reset to their original value after each pruning step, i.e. step 4) of the LTH. To empirically validate the LTH, we need to retrain the found “lottery ticket” after the pruning phase. Lottery tickets are usually found following an iterative pruning schedule. We set the start_epoch parameter to \\(5\\) to begin the pruning process after \\(5\\) epochs.\n\nschedule = Schedule(sched_iterative, start_pct=0.25)\n\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True)\n\nAs our iterative schedule makes \\(3\\) pruning steps by default, it means that we have to train our network for start_epoch + \\(3*t_B\\), so \\(20\\) epochs in order to get our LTH. After each step, the remaining weights will be reinitialized to their original value\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.584362\n      0.607169\n      0.635995\n      00:07\n    \n    \n      1\n      0.547975\n      0.663318\n      0.719215\n      00:07\n    \n    \n      2\n      0.518505\n      0.544549\n      0.716509\n      00:07\n    \n    \n      3\n      0.472541\n      0.470786\n      0.775372\n      00:07\n    \n    \n      4\n      0.425963\n      0.673728\n      0.730717\n      00:07\n    \n    \n      5\n      0.573806\n      0.529624\n      0.723275\n      00:07\n    \n    \n      6\n      0.507284\n      0.458147\n      0.777402\n      00:07\n    \n    \n      7\n      0.447610\n      0.629908\n      0.652233\n      00:07\n    \n    \n      8\n      0.412829\n      0.394333\n      0.823410\n      00:07\n    \n    \n      9\n      0.381195\n      0.413377\n      0.815291\n      00:07\n    \n    \n      10\n      0.480223\n      0.518234\n      0.761840\n      00:07\n    \n    \n      11\n      0.415105\n      0.399813\n      0.823410\n      00:07\n    \n    \n      12\n      0.376185\n      0.387731\n      0.822733\n      00:07\n    \n    \n      13\n      0.353330\n      0.462406\n      0.771989\n      00:07\n    \n    \n      14\n      0.319414\n      0.355309\n      0.837618\n      00:07\n    \n    \n      15\n      0.392155\n      0.446360\n      0.792287\n      00:07\n    \n    \n      16\n      0.360181\n      0.461158\n      0.786874\n      00:07\n    \n    \n      17\n      0.325754\n      0.406724\n      0.824087\n      00:07\n    \n    \n      18\n      0.270658\n      0.589742\n      0.767930\n      00:07\n    \n    \n      19\n      0.263195\n      0.392320\n      0.832882\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\nWe indeed have a network B, whose accuracy \\(a_B > a_A\\) in the same training time."
  },
  {
    "objectID": "tutorial.lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "href": "tutorial.lottery_ticket.html#lottery-ticket-hypothesis-with-rewinding",
    "title": "Lottery Ticket Hypothesis",
    "section": "Lottery Ticket Hypothesis with Rewinding",
    "text": "Lottery Ticket Hypothesis with Rewinding\nIn some case, LTH fails for deeper networks, author then propose a solution, which is to rewind the weights to a more advanced iteration instead of the initialization value.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n<All keys matched successfully>\n\n\nThis can be done in fasterai by passing the rewind_epoch parameter, that will save the weights at that epoch, then resetting the weights accordingly.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, rewind_epoch=1)\n\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.587423\n      0.669452\n      0.696211\n      00:07\n    \n    \n      1\n      0.561204\n      0.543574\n      0.705683\n      00:07\n    \n    \n      2\n      0.541044\n      0.515325\n      0.746279\n      00:07\n    \n    \n      3\n      0.502017\n      0.483110\n      0.756428\n      00:07\n    \n    \n      4\n      0.459558\n      0.570272\n      0.736807\n      00:07\n    \n    \n      5\n      0.549915\n      0.579451\n      0.720568\n      00:07\n    \n    \n      6\n      0.470164\n      0.461384\n      0.799729\n      00:07\n    \n    \n      7\n      0.438228\n      0.657783\n      0.612314\n      00:07\n    \n    \n      8\n      0.388232\n      0.643327\n      0.659675\n      00:07\n    \n    \n      9\n      0.349985\n      0.447483\n      0.783491\n      00:07\n    \n    \n      10\n      0.435783\n      0.565559\n      0.669824\n      00:07\n    \n    \n      11\n      0.403817\n      0.452713\n      0.804465\n      00:07\n    \n    \n      12\n      0.358676\n      0.372348\n      0.836942\n      00:07\n    \n    \n      13\n      0.325332\n      0.596648\n      0.713802\n      00:07\n    \n    \n      14\n      0.296661\n      0.380269\n      0.845061\n      00:07\n    \n    \n      15\n      0.347766\n      0.464424\n      0.781461\n      00:07\n    \n    \n      16\n      0.298053\n      0.430848\n      0.818674\n      00:07\n    \n    \n      17\n      0.277868\n      0.632241\n      0.732070\n      00:07\n    \n    \n      18\n      0.240276\n      0.379765\n      0.850474\n      00:07\n    \n    \n      19\n      0.218686\n      0.785115\n      0.784168\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSaving Weights at epoch 1\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 1 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%"
  },
  {
    "objectID": "tutorial.lottery_ticket.html#super-masks",
    "href": "tutorial.lottery_ticket.html#super-masks",
    "title": "Lottery Ticket Hypothesis",
    "section": "Super-Masks",
    "text": "Super-Masks\nResearchers from Uber AI investigated the LTH and found the existence of what they call “Super-Masks”, i.e. masks that, applied on a untrained neural network, allows to reach better-than-random results.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.model.load_state_dict(initial_weights)\n\n<All keys matched successfully>\n\n\nTo find supermasks, authors perform the LTH method then apply the mask on the original, untrained network. In fasterai, you can pass the parameter reset_end=True, which will reset the weights to their original value at the end of the training, but keeping the pruned weights (i.e. the mask) unchanged.\n\nsp_cb = SparsifyCallback(50, 'weight', 'local', large_final, schedule, lth=True, reset_end=True)\n\n\nlearn.fit(20, 1e-3, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.585188\n      0.581024\n      0.709743\n      00:07\n    \n    \n      1\n      0.553269\n      0.772498\n      0.702300\n      00:07\n    \n    \n      2\n      0.513219\n      0.505785\n      0.736130\n      00:07\n    \n    \n      3\n      0.483751\n      0.452862\n      0.782815\n      00:07\n    \n    \n      4\n      0.437997\n      0.509538\n      0.740866\n      00:07\n    \n    \n      5\n      0.547269\n      0.488490\n      0.763870\n      00:07\n    \n    \n      6\n      0.505039\n      0.517426\n      0.734100\n      00:07\n    \n    \n      7\n      0.452737\n      0.520701\n      0.743572\n      00:07\n    \n    \n      8\n      0.422814\n      0.447591\n      0.795670\n      00:07\n    \n    \n      9\n      0.388168\n      0.444148\n      0.784168\n      00:07\n    \n    \n      10\n      0.496667\n      0.520030\n      0.770636\n      00:07\n    \n    \n      11\n      0.438477\n      0.561501\n      0.684709\n      00:07\n    \n    \n      12\n      0.390725\n      0.371210\n      0.831529\n      00:07\n    \n    \n      13\n      0.361358\n      0.522248\n      0.750338\n      00:07\n    \n    \n      14\n      0.325510\n      0.408143\n      0.825440\n      00:07\n    \n    \n      15\n      0.412453\n      0.391598\n      0.819350\n      00:07\n    \n    \n      16\n      0.359522\n      0.389951\n      0.824763\n      00:07\n    \n    \n      17\n      0.323788\n      0.549834\n      0.766576\n      00:07\n    \n    \n      18\n      0.289855\n      0.481156\n      0.795670\n      00:07\n    \n    \n      19\n      0.257520\n      0.354713\n      0.841678\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0]%\nSparsity at the end of epoch 1: [0.0]%\nSparsity at the end of epoch 2: [0.0]%\nSparsity at the end of epoch 3: [0.0]%\nSparsity at the end of epoch 4: [0.0]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 5: [16.67]%\nSparsity at the end of epoch 6: [16.67]%\nSparsity at the end of epoch 7: [16.67]%\nSparsity at the end of epoch 8: [16.67]%\nSparsity at the end of epoch 9: [16.67]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 10: [33.33]%\nSparsity at the end of epoch 11: [33.33]%\nSparsity at the end of epoch 12: [33.33]%\nSparsity at the end of epoch 13: [33.33]%\nSparsity at the end of epoch 14: [33.33]%\nResetting Weights to their epoch 0 values\nSparsity at the end of epoch 15: [50.0]%\nSparsity at the end of epoch 16: [50.0]%\nSparsity at the end of epoch 17: [50.0]%\nSparsity at the end of epoch 18: [50.0]%\nSparsity at the end of epoch 19: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\n\nlearn.validate()\n\n\n\n\n\n\n\n\n(#2) [0.6950991749763489,0.46008118987083435]"
  },
  {
    "objectID": "criteria.html",
    "href": "criteria.html",
    "title": "Criteria",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=3}\n:::\nThe criteria implemented come from this paper.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=56}\n:::"
  },
  {
    "objectID": "criteria.html#weight-based-criteria",
    "href": "criteria.html#weight-based-criteria",
    "title": "Criteria",
    "section": "Weight Based Criteria",
    "text": "Weight Based Criteria\n\nRandom\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=57}\nrandom = Criteria(torch.randn_like)\n:::\n\ndemo_model(random)\n\n\n\n\n\n\nLarge Final Value\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=9}\nlarge_final = Criteria(torch.abs)\n:::\n\ndemo_model(large_final)\n\n\n\n\n\n\nSquared Final Value\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=11}\nsquared_final = Criteria(torch.square)\n:::\n\ndemo_model(squared_final)\n\n\n\n\n\n\nSmall Final Value\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=13}\nsmall_final = Criteria(compose(torch.abs, torch.neg))\n:::\n\ndemo_model(small_final)\n\n\n\n\n\n\nLarge Init Value\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=15}\nlarge_init = Criteria(torch.abs, needs_init=True, return_init=True)\n:::\n\ndemo_model(large_init)\n\n\n\n\n\n\nSmall Init Value\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=17}\nsmall_init = Criteria(compose(torch.abs, torch.neg), needs_init=True, return_init=True)\n:::\n\ndemo_model(small_init)\n\n\n\n\n\n\nLarge Init Large Final Value\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=19}\nlarge_init_large_final = Criteria(torch.abs, needs_init=True, output_f=torch.min)\n:::\n\ndemo_model(large_init_large_final, 80)\n\n\n\n\n\n\nSmall Init Small Final Value\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=21}\nsmall_init_small_final = Criteria(torch.abs, needs_init=True, output_f=lambda x,y: torch.neg(torch.max(x,y)))\n:::\n\ndemo_model(small_init_small_final)\n\n\n\n\n\n\nIncreasing Magnitude\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=23}\nmagnitude_increase = Criteria(torch.abs, needs_init=True, output_f= torch.sub)\n:::\n\ndemo_model(magnitude_increase, 60)\n\n\n\n\n\n\nMovement Pruning\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=25}\nmovement = Criteria(noop, needs_init=True, output_f= lambda x,y: torch.abs(torch.sub(x,y)))\n:::\n\ndemo_model(movement)"
  },
  {
    "objectID": "criteria.html#updating-versions",
    "href": "criteria.html#updating-versions",
    "title": "Criteria",
    "section": "Updating Versions",
    "text": "Updating Versions\nThe following criteria use an updating value of the weights, i.e. the value from the previous iteration of training, instead of the initialization value to better capture the training dynamics.\n\nUpdating Magnitude Increase\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=27}\nupdating_magnitude_increase = Criteria(torch.abs, needs_update=True, output_f= lambda x,y: torch.sub(x,y))\n:::\n\ndemo_model(updating_magnitude_increase)\n\n\n\n\n\n\nUpdating Movement\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=29}\nupdating_movement = Criteria(noop, needs_update=True, output_f= lambda x,y: torch.abs(torch.sub(x,y)))\n:::\n\ndemo_model(updating_movement, 50)\n\n\n\n\n\n\nmov-magnitude\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=31}\nmovmag = Criteria(noop, needs_init=True, output_f=lambda x,y: torch.abs(torch.mul(x, torch.sub(x,y))))\n:::\n\ndemo_model(movmag)\n\n\n\n\n\n\nUpdating mov-magnitude\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=33}\nupdating_movmag = Criteria(noop, needs_update=True, output_f=lambda x,y: torch.abs(torch.mul(x, torch.sub(x,y))))\n:::\n\ndemo_model(updating_movmag)\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=35}\ncriterias = ('random', 'large_final', 'small_final', 'squared_final', 'small_init', 'small_final', 'large_init_large_final', 'small_init_small_final', 'magnitude_increase', 'movement', 'updating_magnitude_increase', 'updating_movement', 'updating_movmag')\ndef available_criterias():\n    print(criterias)\n:::"
  },
  {
    "objectID": "criteria.html#gradient-based-criteria",
    "href": "criteria.html#gradient-based-criteria",
    "title": "Criteria",
    "section": "Gradient Based Criteria",
    "text": "Gradient Based Criteria"
  },
  {
    "objectID": "fc_decomposer.html",
    "href": "fc_decomposer.html",
    "title": "Fully-Connected Layers Decomposer",
    "section": "",
    "text": "We can factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\) With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=4}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\n\n/Users/nathan/opt/miniconda3/envs/nbdev/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=5}\nclass FC_Decomposer:\n\n    def __init__(self):\n        super().__init__()\n        \n    def decompose(self, model, percent_removed=0.5):\n\n        new_model = copy.deepcopy(model)\n\n        module_names = list(new_model._modules)\n\n        for k, name in enumerate(module_names):\n\n            if len(list(new_model._modules[name]._modules)) > 0:\n                new_model._modules[name] = self.decompose(new_model._modules[name], percent_removed)\n\n            else:\n                if isinstance(new_model._modules[name], nn.Linear):\n                    # Folded BN\n                    layer = self.SVD(new_model._modules[name], percent_removed)\n\n                    # Replace old weight values\n                    new_model._modules[name] = layer # Replace the FC Layer by the decomposed version\n        return new_model\n\n\n    def SVD(self, layer, percent_removed):\n\n        W = layer.weight.data\n        U, S, V = torch.svd(W)\n        L = int((1.-percent_removed)*U.shape[0])\n        W1 = U[:,:L]\n        W2 = torch.diag(S[:L]) @ V[:,:L].t()\n        layer_1 = nn.Linear(in_features=layer.in_features, \n                    out_features=L, bias=False)\n        layer_1.weight.data = W2\n\n        layer_2 = nn.Linear(in_features=L, \n                    out_features=layer.out_features, bias=True)\n        layer_2.weight.data = W1\n\n        if layer.bias.data is None: \n            layer_2.bias.data = torch.zeros(*layer.out_features.shape)\n        else:\n            layer_2.bias.data = layer.bias.data\n\n        return nn.Sequential(layer_1, layer_2)\n:::\n\nshow_doc(FC_Decomposer.decompose)\n\n\n\nFC_Decomposer.decompose\n\n FC_Decomposer.decompose (model, percent_removed=0.5)\n\n\n\n\nA tutorial about how to use the FC_Decomposer functionalities can be found here"
  },
  {
    "objectID": "pruner.html",
    "href": "pruner.html",
    "title": "Pruner",
    "section": "",
    "text": "Important\n\n\n\nThe Pruner method currently works on fully-feedforward ConvNets, e.g. VGG16. Support for residual connections, e.g. ResNets is under development.\n\n\nWhen our network has filters containing zero values, there is an additional step that we may take. Indeed, those zero-filters can be physically removed from our network, allowing us to get a new, dense, architecture.\nThis can be done by reexpressing each layer, reducing the number of filter, to match the number of non-zero filters. However, when we remove a filter in a layer, this means that there will be a missing activation map, which should be used by all the filters in the next layer. So, not only should we physically remove the filter, but also its corresponding kernel in each of the filters in the next layer (see Fig. below)\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=5}\nimport torch\nimport torch.nn as nn\nimport copy\nimport numpy as np\n\n/Users/nathan/opt/miniconda3/envs/nbdev/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=6}\nclass Pruner():\n    \"Remove zero filters from a model\"\n    def __init__(self):\n        super().__init__()\n        \n    def filters_to_keep(self, layer, nxt_layer):\n        \n        ixs = self._get_nz_ixs(layer)\n    \n        filters_keep = layer.weight.index_select(0, ixs[0]).data # keep only the non_zero filters\n        biases_keep = layer.bias.index_select(0, ixs[0]).data\n        \n        nxt_filters_keep = nxt_layer.weight.index_select(1, ixs[0]).data if nxt_layer is not None else None\n            \n        return filters_keep, biases_keep, nxt_filters_keep\n    \n    def prune_conv(self, layer, nxt_layer):\n        assert layer.__class__.__name__ == 'Conv2d'\n    \n        new_weights, new_biases, new_next_weights = self.filters_to_keep(layer, nxt_layer)\n    \n        layer.out_channels = new_weights.shape[0]\n        layer.in_channels = new_weights.shape[1]\n    \n        layer.weight = nn.Parameter(new_weights)\n        layer.bias = nn.Parameter(new_biases)\n\n        if new_next_weights is not None:\n            new_next_in_channels = new_next_weights.shape[1]\n            nxt_layer.weight = nn.Parameter(new_next_weights)\n            nxt_layer.in_channels = new_next_in_channels\n    \n        return layer, nxt_layer\n    \n    def prune_bn(self, layer, prev_conv):\n        \n        ixs = self._get_nz_ixs(prev_conv)\n        \n        weights_keep = layer.weight.data.index_select(0, ixs[0]).data\n    \n        layer.num_features = weights_keep.shape[0]\n        layer.weight = nn.Parameter(weights_keep)\n        layer.bias = nn.Parameter(layer.bias.data.index_select(0, ixs[0]).data)\n        layer.running_mean = layer.running_mean.data.index_select(0, ixs[0]).data\n        layer.running_var = layer.running_var.data.index_select(0, ixs[0]).data\n        \n        return layer\n\n    def delete_fc_weights(self, layer, last_conv, pool_shape):\n        \n        ixs = self._get_nz_ixs(last_conv)\n        \n        new_ixs = torch.cat([torch.arange(i*pool_shape**2,((i+1)*pool_shape**2)) for i in ixs[0]]) if pool_shape else ixs[0]\n        new_ixs = torch.LongTensor(new_ixs).cuda()\n\n        weights_keep = layer.weight.data.index_select(1, new_ixs).data\n        \n        layer.in_features = weights_keep.shape[1]\n        layer.weight = nn.Parameter(weights_keep)\n    \n        return layer\n    \n    def _get_nz_ixs(self, layer):\n        filters = layer.weight\n        nz_filters = filters.data.sum(dim=(1,2,3)) # Flatten the filters to compare them\n        ixs = torch.nonzero(nz_filters).T\n        return ixs.cuda()\n    \n    def _find_next_conv(self, model, conv_ix):\n        for k,m in enumerate(model.modules()):\n            if k > conv_ix and isinstance(m, nn.Conv2d):\n                next_conv_ix = k\n                break\n            else:\n                next_conv_ix = None\n        return next_conv_ix\n    \n    def _find_previous_conv(self, model, layer_ix):\n        for k,m in reversed(list(enumerate(model.modules()))):\n            if k < layer_ix and isinstance(m, nn.Conv2d):\n                prev_conv_ix = k\n                break\n            else:\n                prev_conv_ix = None\n        return prev_conv_ix    \n    \n    def _get_last_conv_ix(self, model):\n        for k,m in enumerate(list(model.modules())):\n            if isinstance(m, nn.Conv2d):\n                last_conv_ix = k\n        return last_conv_ix\n    \n    def _get_first_fc_ix(self, model):\n        for k,m in enumerate(list(model.modules())):\n            if isinstance(m, nn.Linear):\n                first_fc_ix = k\n                break       \n        return first_fc_ix\n    \n    def _find_pool_shape(self, model):\n        for k,m in enumerate(model.modules()):\n            if isinstance(m, nn.AdaptiveAvgPool2d):\n                output_shape = m.output_size\n                break\n            else: output_shape=None\n        return output_shape    \n    \n    def prune_model(self, model):\n        pruned_model = copy.deepcopy(model)\n        \n        layer_names = list(dict(pruned_model.named_modules()).keys())\n        layers = dict(pruned_model.named_modules())\n        old_layers = dict(model.named_modules())\n        \n        last_conv_ix = self._get_last_conv_ix(pruned_model)\n        first_fc_ix = self._get_first_fc_ix(pruned_model)\n        \n        for k,m in enumerate(list(pruned_model.modules())):\n            \n            if isinstance(m, nn.Conv2d):\n                next_conv_ix = self._find_next_conv(model, k)\n                if next_conv_ix is not None: # The conv layer is not the last one\n                    new_m, new_next_m = self.prune_conv(m, layers[layer_names[next_conv_ix]]) # Prune the current conv layer\n                else:\n                    new_m, _ = self.prune_conv(m, None) # Prune the current conv layer without changing the next one\n                    \n            if isinstance(m, nn.BatchNorm2d):\n                new_m = self.prune_bn(m, old_layers[layer_names[self._find_previous_conv(model, k)]])             \n                    \n            if isinstance(m, nn.Linear) and k==first_fc_ix:\n                pool_shape = self._find_pool_shape(model)\n                new_m = self.delete_fc_weights(m, old_layers[layer_names[last_conv_ix]], pool_shape[0])\n\n        return pruned_model\n:::\n\nshow_doc(Pruner.prune_model)\n\n\n\nPruner.prune_model\n\n Pruner.prune_model (model)"
  },
  {
    "objectID": "regularizer.html",
    "href": "regularizer.html",
    "title": "RegularizationCallback",
    "section": "",
    "text": "from fasterai.core.criteria import *\nfrom fasterai.regularize.all import *\n\nGet your data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nTrain a model without Regularization as a baseline\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.681536\n      0.466989\n      0.835589\n      00:11\n    \n    \n      1\n      0.358927\n      0.318825\n      0.865359\n      00:10\n    \n    \n      2\n      0.201207\n      0.220008\n      0.923545\n      00:10\n    \n  \n\n\n\nCreate the RegularizationCallback\n\nreg_cb = RegularizationCallback('filter')\n\nTrain a model with Regularization\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(3, cbs=reg_cb)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.633497\n      1.468702\n      0.812585\n      00:10\n    \n    \n      1\n      1.334702\n      1.173871\n      0.907307\n      00:10\n    \n    \n      2\n      1.152696\n      1.136654\n      0.933694\n      00:11"
  },
  {
    "objectID": "sparsifier.html",
    "href": "sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=6}\n:::\nA sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network’s weight are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\nLet’s start by creating a model\nAs you probably know, weights in a convolutional neural network have 4 dimensions ($ c_{out} c_{in} k_h k_w$)\nIn the case of ResNet18, the dimension of the first layer weights is \\(64 \\times 3 \\times 7 \\times 7\\). We thus can plot each of the \\(64\\) filter as a \\(7 \\times 7\\) color image (because they contains \\(3\\) channels).\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=14}\n:::\nThe Sparsifier class allows us to remove some (part of) the filters, that are considered to be less useful than others. This can be done by first creating an instance of the class, specifying:\nUser can pass a single layer to prune by using the Sparsifier.prune_layer method.\nMost of the time, we may want to prune the whole model at once, using the Sparsifier.prune_model method, indicating the percentage of sparsity to you want to apply.\nThere are several ways in which we can make that first layer sparse. You will find the most important below:\nYou now have a model that is \\(70\\%\\) sparse !"
  },
  {
    "objectID": "sparsifier.html#granularity",
    "href": "sparsifier.html#granularity",
    "title": "Sparsifier",
    "section": "Granularity",
    "text": "Granularity\nAs we said earlier, the granularity defines the structure of parameter that you will remove.\nIn the example below, we removed weight from each convolutional filter, meaning that we now have sparse filters, as can be seen in the image below:\n\nplot_kernels(model.conv1)\n\n\n\n\nAnother granularity is, for example, removing column vectors from the filters. To do so, just change the granularity parameter accordingly.\n\nmodel = resnet18()\npruner = Sparsifier(model, 'column', 'local', large_final)\npruner.prune_layer(model.conv1, 70)\n\n\nplot_kernels(model.conv1)\n\n\n\n\nFor more information and examples about the pruning granularities, I suggest you to take a look at the corresponding section."
  },
  {
    "objectID": "sparsifier.html#context",
    "href": "sparsifier.html#context",
    "title": "Sparsifier",
    "section": "Context",
    "text": "Context\nThe context defines where to look in the model, i.e. from where do we compare weight. The two basic contexts are: - local, i.e. we compare weight from each layer individually. This will lead to layers with similar levels of sparsity. - global, i.e. we compare weight from the whole model. This will lead to layers with different levels of sparsity\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'local', large_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 69.99%\nSparsity in Conv2d 7: 70.00%\nSparsity in Conv2d 10: 70.00%\nSparsity in Conv2d 13: 70.00%\nSparsity in Conv2d 16: 70.00%\nSparsity in Conv2d 20: 70.00%\nSparsity in Conv2d 23: 70.00%\nSparsity in Conv2d 26: 70.00%\nSparsity in Conv2d 29: 70.00%\nSparsity in Conv2d 32: 70.00%\nSparsity in Conv2d 36: 70.00%\nSparsity in Conv2d 39: 70.00%\nSparsity in Conv2d 42: 70.00%\nSparsity in Conv2d 45: 70.00%\nSparsity in Conv2d 48: 70.00%\nSparsity in Conv2d 52: 70.00%\nSparsity in Conv2d 55: 70.00%\nSparsity in Conv2d 58: 70.00%\nSparsity in Conv2d 61: 70.00%\nSparsity in Conv2d 64: 70.00%\n\n\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'global', large_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 66.20%\nSparsity in Conv2d 7: 32.18%\nSparsity in Conv2d 10: 32.09%\nSparsity in Conv2d 13: 31.96%\nSparsity in Conv2d 16: 32.15%\nSparsity in Conv2d 20: 44.07%\nSparsity in Conv2d 23: 44.04%\nSparsity in Conv2d 26: 16.02%\nSparsity in Conv2d 29: 44.15%\nSparsity in Conv2d 32: 44.09%\nSparsity in Conv2d 36: 59.25%\nSparsity in Conv2d 39: 59.30%\nSparsity in Conv2d 42: 22.06%\nSparsity in Conv2d 45: 59.29%\nSparsity in Conv2d 48: 59.30%\nSparsity in Conv2d 52: 75.90%\nSparsity in Conv2d 55: 75.85%\nSparsity in Conv2d 58: 30.31%\nSparsity in Conv2d 61: 75.82%\nSparsity in Conv2d 64: 75.86%"
  },
  {
    "objectID": "sparsifier.html#criteria",
    "href": "sparsifier.html#criteria",
    "title": "Sparsifier",
    "section": "Criteria",
    "text": "Criteria\nThe criteria defines how we select the parameters to remove. It is usually given by a scoring method. The most common one is the large_final, i.e. select parameters with the highest absolute value as they are supposed to contribute the most to the final results of the model.\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'global', large_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 67.08%\nSparsity in Conv2d 7: 32.25%\nSparsity in Conv2d 10: 31.98%\nSparsity in Conv2d 13: 32.28%\nSparsity in Conv2d 16: 31.94%\nSparsity in Conv2d 20: 44.52%\nSparsity in Conv2d 23: 44.23%\nSparsity in Conv2d 26: 15.36%\nSparsity in Conv2d 29: 44.36%\nSparsity in Conv2d 32: 44.24%\nSparsity in Conv2d 36: 59.27%\nSparsity in Conv2d 39: 59.10%\nSparsity in Conv2d 42: 21.94%\nSparsity in Conv2d 45: 59.21%\nSparsity in Conv2d 48: 59.23%\nSparsity in Conv2d 52: 75.89%\nSparsity in Conv2d 55: 75.82%\nSparsity in Conv2d 58: 30.38%\nSparsity in Conv2d 61: 75.90%\nSparsity in Conv2d 64: 75.85%\n\n\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'global', small_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 72.49%\nSparsity in Conv2d 7: 87.78%\nSparsity in Conv2d 10: 87.88%\nSparsity in Conv2d 13: 87.83%\nSparsity in Conv2d 16: 87.99%\nSparsity in Conv2d 20: 83.05%\nSparsity in Conv2d 23: 83.02%\nSparsity in Conv2d 26: 94.21%\nSparsity in Conv2d 29: 83.10%\nSparsity in Conv2d 32: 83.07%\nSparsity in Conv2d 36: 76.27%\nSparsity in Conv2d 39: 76.20%\nSparsity in Conv2d 42: 92.01%\nSparsity in Conv2d 45: 76.22%\nSparsity in Conv2d 48: 76.26%\nSparsity in Conv2d 52: 66.89%\nSparsity in Conv2d 55: 66.87%\nSparsity in Conv2d 58: 88.72%\nSparsity in Conv2d 61: 66.95%\nSparsity in Conv2d 64: 66.87%\n\n\nFor more information and examples about the pruning criteria, I suggest you to take a look at the corresponding section."
  },
  {
    "objectID": "sparsifier.html#remark",
    "href": "sparsifier.html#remark",
    "title": "Sparsifier",
    "section": "Remark",
    "text": "Remark\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of 8, this can be done by passing the round_to parameter.\n\nmodel = resnet18()\npruner = Sparsifier(model, 'filter', 'local', large_final)\npruner.prune_model(70, round_to=8)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 62.50%\nSparsity in Conv2d 7: 62.50%\nSparsity in Conv2d 10: 62.50%\nSparsity in Conv2d 13: 62.50%\nSparsity in Conv2d 16: 62.50%\nSparsity in Conv2d 20: 68.75%\nSparsity in Conv2d 23: 68.75%\nSparsity in Conv2d 26: 68.75%\nSparsity in Conv2d 29: 68.75%\nSparsity in Conv2d 32: 68.75%\nSparsity in Conv2d 36: 68.75%\nSparsity in Conv2d 39: 68.75%\nSparsity in Conv2d 42: 68.75%\nSparsity in Conv2d 45: 68.75%\nSparsity in Conv2d 48: 68.75%\nSparsity in Conv2d 52: 68.75%\nSparsity in Conv2d 55: 68.75%\nSparsity in Conv2d 58: 68.75%\nSparsity in Conv2d 61: 68.75%\nSparsity in Conv2d 64: 68.75%\n\n\n\nmodel = resnet18()\npruner = Sparsifier(model, 'filter', 'global', large_final)\npruner.prune_model(70, round_to=8)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 87.50%\nSparsity in Conv2d 7: 0.00%\nSparsity in Conv2d 10: 0.00%\nSparsity in Conv2d 13: 0.00%\nSparsity in Conv2d 16: 0.00%\nSparsity in Conv2d 20: 93.75%\nSparsity in Conv2d 23: 93.75%\nSparsity in Conv2d 26: 0.00%\nSparsity in Conv2d 29: 93.75%\nSparsity in Conv2d 32: 93.75%\nSparsity in Conv2d 36: 96.88%\nSparsity in Conv2d 39: 96.88%\nSparsity in Conv2d 42: 0.00%\nSparsity in Conv2d 45: 96.88%\nSparsity in Conv2d 48: 90.62%\nSparsity in Conv2d 52: 98.44%\nSparsity in Conv2d 55: 98.44%\nSparsity in Conv2d 58: 0.00%\nSparsity in Conv2d 61: 98.44%\nSparsity in Conv2d 64: 98.44%\n\n\nFor more information about granularities at which you can operate, please check the related page."
  },
  {
    "objectID": "tutorial.transformers.html",
    "href": "tutorial.transformers.html",
    "title": "Prune Transformers",
    "section": "",
    "text": "Note\n\n\n\nThis example code is taken from the fastai docs\nLet’s create our fastai Learner.\nAnd let’s try to extend a given prompt with the pretrained model."
  },
  {
    "objectID": "tutorial.transformers.html#make-it-sparse",
    "href": "tutorial.transformers.html#make-it-sparse",
    "title": "Prune Transformers",
    "section": "Make it sparse !",
    "text": "Make it sparse !\nLet’s see now if we retrain our model, this time introducing sparsity\n\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity())\n\nAlso, when working with text, fastai defines the number of processed batches differently, so we have to adjust our SparsifyCallback accordingly (luckily, fastai makes it available as the n_batches attribute.\n@patch_to(SparsifyCallback)\ndef before_fit(self): print(f’Pruning of {self.granularity} until a sparsity of {self.end_sparsity}%‘) self.end_epoch = self.n_epoch if self.end_epoch is None else self.end_epoch assert self.end_epoch <= self.n_epoch, ’Your end_epoch must be smaller than total number of epoch’\nmodel = self.learn.model if self.model is None else self.model # Pass a model if you don't want the whole model to be pruned\nself.sparsifier = Sparsifier(model, self.granularity, self.method, self.criteria, self.layer_type)\nself.total_iters = self.end_epoch * self.dls.n_batches\nself.start_iter = self.start_epoch * self.dls.n_batches\nLet’s define our SparsifyCallback. Let’s say we want to make our model 30% sparse, by removing the highest-norm weight in each attention head.\n\nsp_cb = SparsifyCallback(sparsity=30, granularity='weight', context='local', criteria=large_final, schedule=one_cycle, layer_type=Conv1D)\n\nWe now only have to pass our callback to fastai\n\nlearn.fit_one_cycle(1, 1e-4, cbs=sp_cb)\n\nPruning of weight until a sparsity of [30]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      3.049835\n      2.859973\n      17.461063\n      09:28\n    \n  \n\n\n\nSparsity at the end of epoch 0: [30.0]%\nFinal Sparsity: [30.0]%\nSparsity in Conv1D 9: 30.00%\nSparsity in Conv1D 10: 30.00%\nSparsity in Conv1D 15: 30.00%\nSparsity in Conv1D 16: 30.00%\nSparsity in Conv1D 22: 30.00%\nSparsity in Conv1D 23: 30.00%\nSparsity in Conv1D 28: 30.00%\nSparsity in Conv1D 29: 30.00%\nSparsity in Conv1D 34: 30.00%\nSparsity in Conv1D 35: 30.00%\nSparsity in Conv1D 40: 30.00%\nSparsity in Conv1D 41: 30.00%\nSparsity in Conv1D 46: 30.00%\nSparsity in Conv1D 47: 30.00%\nSparsity in Conv1D 52: 30.00%\nSparsity in Conv1D 53: 30.00%\nSparsity in Conv1D 58: 30.00%\nSparsity in Conv1D 59: 30.00%\nSparsity in Conv1D 64: 30.00%\nSparsity in Conv1D 65: 30.00%\nSparsity in Conv1D 70: 30.00%\nSparsity in Conv1D 71: 30.00%\nSparsity in Conv1D 76: 30.00%\nSparsity in Conv1D 77: 30.00%\nSparsity in Conv1D 82: 30.00%\nSparsity in Conv1D 83: 30.00%\nSparsity in Conv1D 88: 30.00%\nSparsity in Conv1D 89: 30.00%\nSparsity in Conv1D 94: 30.00%\nSparsity in Conv1D 95: 30.00%\nSparsity in Conv1D 100: 30.00%\nSparsity in Conv1D 101: 30.00%\nSparsity in Conv1D 106: 30.00%\nSparsity in Conv1D 107: 30.00%\nSparsity in Conv1D 112: 30.00%\nSparsity in Conv1D 113: 30.00%\nSparsity in Conv1D 118: 30.00%\nSparsity in Conv1D 119: 30.00%\nSparsity in Conv1D 124: 30.00%\nSparsity in Conv1D 125: 30.00%\nSparsity in Conv1D 130: 30.00%\nSparsity in Conv1D 131: 30.00%\nSparsity in Conv1D 136: 30.00%\nSparsity in Conv1D 137: 30.00%\nSparsity in Conv1D 142: 30.00%\nSparsity in Conv1D 143: 30.00%\nSparsity in Conv1D 148: 30.00%\nSparsity in Conv1D 149: 30.00%\n\n\nAnd we can check the predicion to the same prompt as before\n\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None]\n\npreds = learn.model.generate(inp.cuda(), max_length=40, num_beams=5, temperature=1.5)\n\ntokenizer.decode(preds[0].cpu().numpy())\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n'\\n = Unicorn = \\n \\n A unicorn is a magical creature with a rainbow tail and a horn @-@ like head. It is a member of the <unk> family of unicorns'\n\n\nThat’s it ! You now have a sparse Transformer as performant as the whole model. However, this model is currently not more efficient speed and storage wise. To have such a speed-up, I suggest you to look at the granularity section."
  },
  {
    "objectID": "schedules.html",
    "href": "schedules.html",
    "title": "Schedules",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=42}\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=44}\n:::\nThe Schedule class allows you to create any schedule function according to 4 parameters:  - the sched_func: the function according to which the sparsity will evolve (e.g. linear, cos, …)  - the start_pct: the percentage of training steps at which the sparsification process will start  - the end_pct: the percentage of training steps at which the sparsification process will end  - the start_sparsity: the percentage of sparsity at which the model starts"
  },
  {
    "objectID": "schedules.html#one-shot",
    "href": "schedules.html#one-shot",
    "title": "Schedules",
    "section": "One-Shot",
    "text": "One-Shot\nThe easiest schedule is the one-shot pruning, i.e. prune the network once. This can be done by simply returning the desired sparsity value. The moment when you want to prune will be controlled by the start_epoch argument in the SparsifyCallback.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=39}\ndef sched_oneshot(start, end, pos): return end\n\none_shot = Schedule(sched_oneshot, start_pct=0.5)\n:::\n\nshow_doc(sched_oneshot)\n\n\n\nsched_oneshot\n\n sched_oneshot (start, end, pos)\n\n\n\n\n\none_shot.plot(50)"
  },
  {
    "objectID": "schedules.html#iterative",
    "href": "schedules.html#iterative",
    "title": "Schedules",
    "section": "Iterative",
    "text": "Iterative\nInstead of pruning the network to desired sparsity in one step, you can do it iteratively. In fasterai, you can change the amount of iterations\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\ndef sched_iterative(start, end, pos, n_steps=3):\n    \"Perform iterative pruning, and pruning in `n_steps` steps\"\n    return start + ((end-start)/n_steps)*(np.ceil((pos)*n_steps))\n\niterative = Schedule(sched_iterative, start_pct=0.2)\n:::\n\nshow_doc(sched_iterative)\n\n\n\nsched_iterative\n\n sched_iterative (start, end, pos, n_steps=3)\n\nPerform iterative pruning, and pruning in n_steps steps\n\n\n\n\niterative.plot(50)\n\n\n\n\n\n\nTo modify the default n_steps, you can use the partial function.\n\niterative = Schedule(partial(sched_iterative, n_steps=5), start_pct=0.2)\n\n\niterative.plot(50)"
  },
  {
    "objectID": "schedules.html#automated-gradual-pruning",
    "href": "schedules.html#automated-gradual-pruning",
    "title": "Schedules",
    "section": "Automated Gradual Pruning",
    "text": "Automated Gradual Pruning\nSome researchers have come up with more sophisticated schedules, such as the Automated Gradual Pruning.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=33}\ndef sched_agp(start, end, pos): return end + (start - end) * (1 - pos)**3\n\nagp = Schedule(sched_agp, start_pct=0.2)\n:::\n\nshow_doc(sched_agp)\n\n\n\nsched_agp\n\n sched_agp (start, end, pos)\n\n\n\n\n\nagp.plot(50)"
  },
  {
    "objectID": "schedules.html#one-cycle-pruning",
    "href": "schedules.html#one-cycle-pruning",
    "title": "Schedules",
    "section": "One-Cycle Pruning",
    "text": "One-Cycle Pruning\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=35}\ndef sched_onecycle(start, end, pos, α=14, β=6):\n    out = (1+np.exp(-α+β)) / (1 + (np.exp((-α*pos)+β)))\n    return start + (end-start)*out\n\none_cycle = Schedule(sched_onecycle)\n:::\n\nshow_doc(sched_onecycle)\n\n\n\nsched_onecycle\n\n sched_onecycle (start, end, pos, α=14, β=6)\n\n\n\n\n\none_cycle.plot(50)\n\n\n\n\n\n\nOn top of that, all of the schedules available in fastai by default are also available: - sched_cos - sched_linear\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=15}\ncos = Schedule(sched_cos)\nlin = Schedule(sched_lin)\n:::"
  },
  {
    "objectID": "schedules.html#dense-sparse-dense",
    "href": "schedules.html#dense-sparse-dense",
    "title": "Schedules",
    "section": "Dense-Sparse-Dense",
    "text": "Dense-Sparse-Dense\nYou can also create even more interesting behaviours such as the DSD method, where you prune the model in the first place, then re-grow it to its initial amount of parameter.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=25}\ndef sched_dsd(start, end, pos):\n    if pos<0.5:\n        return start + (1 + math.cos(math.pi*(1-pos*2))) * (end-start) / 2\n    else:\n        return end + (1 - math.cos(math.pi*(1-pos*2))) * (start-end) / 2\n    \ndsd = Schedule(sched_dsd)\n:::\n\nshow_doc(sched_dsd)\n\n\n\nsched_dsd\n\n sched_dsd (start, end, pos)\n\n\n\n\n\ndsd.plot(50)\n\n\n\n\n\n\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=53}\nschedules = ('one_shot', 'iterative', 'agp', 'one_cycle', 'cos', 'lin', 'dsd')\ndef available_schedules():\n    print(schedules)\n:::"
  },
  {
    "objectID": "bn_folding.html",
    "href": "bn_folding.html",
    "title": "Batch Norm Folding",
    "section": "",
    "text": "::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=9}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\n\n/Users/nathan/opt/miniconda3/envs/nbdev/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n:::\nBatch Normalization is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity.\nIt consists of 2 steps:\n\nNormalize the batch by first subtracting its mean \\(\\mu\\), then dividing it by its standard deviation \\(\\sigma\\).\nFurther scale by a factor \\(\\gamma\\) and shift by a factor \\(\\beta\\). Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\\[\n\\begin{aligned}\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\ \\sigma_{\\mathcal{B}}^{2} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\ \\widehat{x}_{i} & \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\ y_{i} & \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) \\end{aligned}\\]\nDue to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?\nOnce the training has ended, each batch normalization layer possesses a specific set of \\(\\gamma\\) and \\(\\beta\\), but also \\(\\mu\\) and \\(\\sigma\\), the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\nAs a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!\nThis would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.\nWith a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.\nAs a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input \\(x\\), as:\n\\[\\begin{aligned} z &=W * x+b \\\\ \\mathrm{out} &=\\gamma \\cdot \\frac{z-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nSo, if we re-arrange the \\(W\\) and \\(b\\) of the convolution to take the parameters of the batch normalization into account, as such:\n\\[\\begin{aligned} w_{\\text {fold }} &=\\gamma \\cdot \\frac{W}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\ b_{\\text {fold }} &=\\gamma \\cdot \\frac{b-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nIn practice, this can be achieved in FasterAI with the BN_folder class\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=21}\nclass BN_Folder():\n    def __init__(self):\n        super().__init__()\n        \n    def fold(self, model):\n\n        new_model = copy.deepcopy(model)\n\n        module_names = list(new_model._modules)\n\n        for k, name in enumerate(module_names):\n\n            if len(list(new_model._modules[name]._modules)) > 0:\n                new_model._modules[name] = self.fold(new_model._modules[name])\n\n            else:\n                if isinstance(new_model._modules[name], nn.BatchNorm2d):\n                    if isinstance(new_model._modules[module_names[k-1]], nn.Conv2d):\n\n                        # Folded BN\n                        folded_conv = self._fold_conv_bn_eval(new_model._modules[module_names[k-1]], new_model._modules[name])\n\n                        # Replace old weight values\n                        #new_model._modules.pop(name) # Remove the BN layer\n                        new_model._modules[module_names[k]] = nn.Identity()\n                        new_model._modules[module_names[k-1]] = folded_conv # Replace the Convolutional Layer by the folded version\n\n        return new_model\n\n\n    def _bn_folding(self, conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n        if conv_b is None:\n            conv_b = bn_rm.new_zeros(bn_rm.shape)\n        bn_var_rsqrt = torch.rsqrt(bn_rv + bn_eps)\n\n        w_fold = conv_w * (bn_w * bn_var_rsqrt).view(-1, 1, 1, 1)\n        b_fold = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n\n        return torch.nn.Parameter(w_fold), torch.nn.Parameter(b_fold)\n\n\n    def _fold_conv_bn_eval(self, conv, bn):\n        assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n        fused_conv = copy.deepcopy(conv)\n\n        fused_conv.weight, fused_conv.bias = self._bn_folding(fused_conv.weight, fused_conv.bias,\n                                 bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n\n        return fused_conv\n:::\n\nshow_doc(BN_Folder.fold)\n\n\n\nBN_Folder.fold\n\n BN_Folder.fold (model)\n\n\n\n\nA tutorial about how to use the BN_Folder functionalities can be found here"
  },
  {
    "objectID": "tutorial.using_fasterai.html",
    "href": "tutorial.using_fasterai.html",
    "title": "How to use FasterAI",
    "section": "",
    "text": "from fastai.vision.all import *\nLet’s start with a bit of context for the purpose of the demonstration. Imagine that we want to deploy a VGG16 model on a mobile device that has limited storage capacity and that our task requires our model to run sufficiently fast. It is known that parameters and speed efficiency are not the strong points of VGG16 but let’s see what we can do with it.\nLet’s first check the number of parameters and the inference time of VGG16.\nSo, VGG16 has 134 millions of parameters\nAnd takes 5.54ms to perform inference on a single image.\nSnap ! This is more than we can afford for deployment, ideally we would like our model to take only half of that…but should we give up ? Nope, there are actually a lot of techniques that we can use to help reducing the size and improve the speed of our models! Let’s see how to apply them with FasterAI.\nWe will first train our VGG16 model to have a baseline of what performance we should expect from it.\nSo we would like our network to have comparable accuracy but fewer parameters and running faster… And the first technique that we will show how to use is called Knowledge Distillation"
  },
  {
    "objectID": "tutorial.using_fasterai.html#knowledge-distillation",
    "href": "tutorial.using_fasterai.html#knowledge-distillation",
    "title": "How to use FasterAI",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation\nKnowledge distillation is a simple yet very efficient way to train a model. It was introduced in 2006 by Caruana et al.. The main idea behind is to use a small model (called the student) to approximate the function learned by a larger and high-performing model (called the teacher). This can be done by using the large model to pseudo-label the data. This idea has been used very recently to break the state-of-the-art accuracy on ImageNet.\nWhen we train our model for classification, we usually use a softmax as last layer. This softmax has the particularity to squish low value logits towards 0, and the highest logit towards 1. This has for effect to completely lose all the inter-class information, or what is sometimes called the dark knowledge. This is the information that is valuable and that we want to transfer from the teacher to the student.\nTo do so, we still use a regular classification loss but at the same time, we’ll use another loss, computed between the softened logits of the teacher (our soft labels) and the softened logits of the student (our soft predictions). Those soft values are obtained when you use a soft-softmax, that avoids squishing the values at its output. Our implementation follows this paper and the basic principle of training is represented in the figure below:\n\n\n\n\n\n‘Distill’\n\n\n\nTo use Knowledge Distillation with FasterAI, you only need to use this callback when training your student model:\n\n\n KnowledgeDistillation(teacher.model, loss) \n\n You only need to give to the callback function your teacher learner. Behind the scenes, FasterAI will take care of making your model train using knowledge distillation. \n\n\n\n\nfrom fasterai.distill.all import *\n\nThe first thing to do is to find a teacher, which can be any model, that preferrably performs well. We will chose VGG19 for our demonstration. To make sure it performs better than our VGG16 model, let’s start from a pretrained version.\n\nteacher = cnn_learner(dls, models.vgg19_bn, metrics=[accuracy])\nteacher.fit_one_cycle(3, 1e-4)\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.674759\n      0.289090\n      0.912357\n      00:36\n    \n    \n      1\n      0.403283\n      0.180294\n      0.946752\n      00:36\n    \n    \n      2\n      0.396881\n      0.176295\n      0.945223\n      00:36\n    \n  \n\n\n\nOur teacher has 97.4% of accuracy which is pretty good, it is ready to take a student under its wing. So let’s create our student model and train it with the Knowledge Distillation callback:\n\nstudent = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nkd_cb = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-4, cbs=kd_cb)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      7.487010\n      6.192549\n      0.446369\n      01:37\n    \n    \n      1\n      5.470930\n      6.126627\n      0.469809\n      01:37\n    \n    \n      2\n      4.444192\n      5.306108\n      0.592102\n      01:37\n    \n    \n      3\n      4.153715\n      3.482993\n      0.669554\n      01:37\n    \n    \n      4\n      3.522233\n      2.846554\n      0.742166\n      01:37\n    \n    \n      5\n      3.066324\n      2.738527\n      0.739363\n      01:37\n    \n    \n      6\n      2.732183\n      2.394343\n      0.768917\n      01:37\n    \n    \n      7\n      2.329082\n      2.012312\n      0.811720\n      01:37\n    \n    \n      8\n      2.095640\n      1.827197\n      0.822166\n      01:37\n    \n    \n      9\n      2.006221\n      1.837958\n      0.817580\n      01:37\n    \n  \n\n\n\nAnd we can see that indeed, the knowledge of the teacher was useful for the student, as it is clearly overperforming the vanilla VGG16.\nOk, so now we are able to get more from a given model which is kind of cool ! With some experimentations we could come up with a model smaller than VGG16 but able to reach the same performance as our baseline! You can try to find it by yourself later, but for now let’s continue with the next technique !"
  },
  {
    "objectID": "tutorial.using_fasterai.html#sparsifying",
    "href": "tutorial.using_fasterai.html#sparsifying",
    "title": "How to use FasterAI",
    "section": "Sparsifying",
    "text": "Sparsifying\nNow that we have a student model that is performing better than our baseline, we have some room to compress it. And we’ll start by making the network sparse. As explained in a previous article, there are many ways leading to a sparse network.\n\n\n\n\n\n\n\nNote\n\n\n\nUsually, the process of making a network sparse is called Pruning. I prefer using the term Pruning when parameters are actually removed from the network, which we will do in the next section.\n\n\n\n\n\n\n‘Pruning’\n\n\n\nBy default, FasterAI uses the Automated Gradual Pruning paradigm as it removes parameters as the model trains and doesn’t require to pretrain the model, so it is usually much faster. In FasterAI, this is also managed by using a callback, that will replace the least important parameters of your model by zeroes during the training. The callback has a wide variety of parameters to tune your Sparsifying operation, let’s take a look at them:\n\n\nSparsifyCallback(learn, sparsity, granularity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ngranularity: on what granularity you want the sparsification to be operated (currently supported: weight, filter)\n\n\nmethod: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e annealing_linear, annealing_cos, … and annealing_gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\n\nBut let’s come back to our example!\nHere, we will make our network 40% sparse, and remove entire filters, selected locally and based on L1 norm. We will train with a learning rate a bit smaller to be gentle with our network because it has already been trained. The scheduling selected is cosinusoidal, so the pruning starts and ends quite slowly.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='filter', context='global', criteria=large_final, schedule=cos)\nstudent.fit(5, 1e-5, cbs=sp_cb)\n\nPruning of filter until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.604952\n      0.570446\n      0.817834\n      01:05\n    \n    \n      1\n      0.608648\n      0.560816\n      0.823694\n      01:05\n    \n    \n      2\n      0.594311\n      0.562816\n      0.826497\n      01:05\n    \n    \n      3\n      0.634639\n      0.575046\n      0.822675\n      01:05\n    \n    \n      4\n      0.605929\n      0.571302\n      0.816561\n      01:05\n    \n  \n\n\n\nSparsity at the end of epoch 0: [4.77]%\nSparsity at the end of epoch 1: [17.27]%\nSparsity at the end of epoch 2: [32.73]%\nSparsity at the end of epoch 3: [45.23]%\nSparsity at the end of epoch 4: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 0.00%\nSparsity in Conv2d 5: 0.00%\nSparsity in Conv2d 9: 0.00%\nSparsity in Conv2d 12: 0.00%\nSparsity in Conv2d 16: 0.00%\nSparsity in Conv2d 19: 0.00%\nSparsity in Conv2d 22: 0.00%\nSparsity in Conv2d 26: 64.26%\nSparsity in Conv2d 29: 70.70%\nSparsity in Conv2d 32: 71.88%\nSparsity in Conv2d 36: 71.29%\nSparsity in Conv2d 39: 70.51%\nSparsity in Conv2d 42: 63.87%\n\n\nOur network now has 50% of its filters composed entirely of zeroes, without even losing accuracy. Obviously, choosing a higher sparsity, makes it more difficult for the network to keep a similar accuracy. Other parameters can also widely change the behaviour of our sparsification process. For example choosing a more fine-grained sparsity usually leads to better results but is then more difficult to take advantage of in terms of speed.\n\nLet’s now see how much we gained in terms of speed. Because we removed 50% of convolution filters, we should expect crazy speed-up right ?\n\nmodel = student.model.eval()\n\n\n%%timeit\nmodel(x[0][None].cuda())\n\n4.26 ms ± 20.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nWell actually, no. We didn’t remove any parameters, we just replaced some by zeroes, remember? The amount of parameters is still the same:\n\ncount_parameters(model)\n\nTotal parameters : 134,309,962\n\n\nWhich leads us to the next section."
  },
  {
    "objectID": "tutorial.using_fasterai.html#pruning",
    "href": "tutorial.using_fasterai.html#pruning",
    "title": "How to use FasterAI",
    "section": "Pruning",
    "text": "Pruning\n\n\n\n\n\n\nImportant\n\n\n\nThis is currently only supported for fully-feedforward models such as VGG-like models as more complex architectures require increasingly difficult and usually model-dependant implementations.\n\n\n\nWhy don’t we see any acceleration even though we removed half of the parameters? That’s because natively, our GPU does not know that our matrices are sparse and thus isn’t able to accelerate the computation. The easiest work around, is to physically remove the parameters we zeroed-out. But this operation requires to change the architecture of the network.\nThis pruning only works if we have zeroed-out entire filters beforehand as it is the only case where you can change the architecture accordingly. Hopefully, sparse computations will soon be available on common deep learning librairies so this section will become useless in the future, but for the moment, it is the best solution I could come up with 🤷\n\nHere is what it looks like with fasterai: \n\n\n\n‘SVD’\n\n\n\n\npruner = Pruner()\npruned_model = pruner.prune_model(learn.model)\n\n You just need to pass the model whose filters has previously been sparsified and FasterAI will take care of removing them. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis operation should be lossless as it only removes filters that already do not participate in the network anymore.\n\n\n\nSo in the case of our example, it gives:\n\nfrom fasterai.sparse.pruner import *\n\n\npruner = Pruner()\npruned_model = pruner.prune_model(student.model)\n\nLet’s now see what our model is capable of now:\n\nmodel = pruned_model.eval()\n\n\ncount_parameters(model)\n\nTotal parameters : 57,202,072\n\n\nAnd in terms of speed:\n\n%%timeit\nmodel(x[0][None].cuda())\n\n4.22 ms ± 12.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nYay ! Now we can talk ! Let’s just double check that our accuracy is unchanged and that we didn’t mess up somewhere:\n\npruned_learner = Learner(dls, pruned_model, metrics=[accuracy])\npruned_learner.validate()\n\n\n\n\n\n\n\n\n(#2) [0.571302056312561,0.8165605068206787]\n\n\n\nAnd there is actually more that we can do ! Let’s keep going !"
  },
  {
    "objectID": "tutorial.using_fasterai.html#batch-normalization-folding",
    "href": "tutorial.using_fasterai.html#batch-normalization-folding",
    "title": "How to use FasterAI",
    "section": "Batch Normalization Folding",
    "text": "Batch Normalization Folding\nBatch Normalization Folding is a really easy to implement and straightforward idea. The gist is that batch normalization is nothing more than a normalization of the input data at each layer. Moreover, at inference time, the batch statistics used for this normalization are fixed. We can thus incorporate the normalization process directly in the convolution by changing its weights and completely remove the batch normalization layers, which is a gain both in terms of parameters and in terms of computations. For a more in-depth explaination, see this blog post.\nThis is how to use it with FasterAI:\n\n\nbn_folder = BN_Folder()\nbn_folder.fold(learn.model))\n\n Again, you only need to pass your model and FasterAI takes care of the rest. For models built using the nn.Sequential, you don’t need to change anything. For others, if you want to see speedup and compression, you actually need to subclass your model to remove the batch norm from the parameters and from the forward method of your network. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis operation should also be lossless as it redefines the convolution to take batch norm into account and is thus equivalent.\n\n\n\n\nfrom fasterai.misc.bn_folding import *\n\nLet’s do this with our model !\n\nbn_f = BN_Folder()\nfolded_model = bn_f.fold(pruned_learner.model)\n\nThe parameters drop is generally not that significant, especially in a network such as VGG where almost all parameters are contained in the FC layers but, hey, any gain is good to take.\n\ncount_parameters(folded_model)\n\nTotal parameters : 57,197,848\n\n\n\nNow that we removed the batch normalization layers, we should again see a speedup.\n\nfolded_model = folded_model.eval()\n\n\n%%timeit\nfolded_model(x[0][None].cuda())\n\n3.02 ms ± 16.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nAgain, let’s double check that we didn’t mess up somewhere:\n\nfolded_learner = Learner(dls, folded_model, metrics=[accuracy])\nfolded_learner.validate()\n\n\n\n\n\n\n\n\n(#2) [0.5713019371032715,0.8165605068206787]\n\n\n\nAnd we’re still not done yet ! As we know for VGG16, most of the parameters are comprised in the fully-connected layers so there should be something that we can do about it, right ?"
  },
  {
    "objectID": "tutorial.using_fasterai.html#fc-layers-factorization",
    "href": "tutorial.using_fasterai.html#fc-layers-factorization",
    "title": "How to use FasterAI",
    "section": "FC Layers Factorization",
    "text": "FC Layers Factorization\nWe can indeed, factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\). With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\n\n\n‘SVD’\n\n\nIn FasterAI, to decompose the fully-connected layers of your model, here is what you need to do: \n\n\nFCD = FCDecomposer()\ndecomposed_model = FCD.decompose(model, percent_removed)\n\n The percent_removed corresponds to the percentage of singular values removed (k value above). \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis time, the decomposition is not exact, so we expect a drop in performance afterwards and further retraining will be needed.\n\n\n\nWhich gives with our example, if we only want to keep half of them:\n\nfrom fasterai.misc.fc_decomposer import *\n\n\nfc_decomposer = FC_Decomposer()\ndecomposed_model = fc_decomposer.decompose(folded_model, percent_removed=0.5)\n\nHow many parameters do we have now ?\n\ncount_parameters(decomposed_model)\n\nTotal parameters : 47,000,906\n\n\nAnd how much time did we gain ?\n\ndecomposed_model = decomposed_model.eval()\n\n\n%%timeit\ndecomposed_model(x[0][None].cuda())\n\n3.16 ms ± 13.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nHowever, this technique is an approximation so it is not lossless, so we should retrain our network a bit to recover its performance.\n\nfinal_learner = Learner(dls, decomposed_model, metrics=[accuracy])\nfinal_learner.fit_one_cycle(5, 1e-5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.847210\n      0.851746\n      0.757707\n      00:37\n    \n    \n      1\n      0.768943\n      0.765675\n      0.778344\n      00:37\n    \n    \n      2\n      0.692119\n      0.794932\n      0.763822\n      00:37\n    \n    \n      3\n      0.666022\n      0.679881\n      0.794650\n      00:37\n    \n    \n      4\n      0.556258\n      0.661012\n      0.801783\n      00:37\n    \n  \n\n\n\nThis operation is usually less useful for more recent architectures as they usually do not have that many parameters in their fully-connected layers.\n\n\n\nSo to recap, we saw in this article how to use fasterai to: 1. Make a student model learn from a teacher model (Knowledge Distillation) 2. Make our network sparse (Sparsifying) 3. Optionnaly physically remove the zero-filters (Pruning) 4. Remove the batch norm layers (Batch Normalization Folding) 5. Approximate our big fully-connected layers by smaller ones (Fully-Connected Layers Factorization)\n\nAnd we saw that by applying those, we could reduce our VGG16 model from 134 million of parameters down to 47 million, and also speed-up the inference from 5.54ms to 3.16ms without any drop in accuracy compared to the baseline.\nOf course, those techniques can be used in conjunction with quantization or mixed-precision training, which are already available in Pytorch for even more compression and speedup.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease keep in mind that the techniques presented above are not magic 🧙‍♂️, so do not expect to see a 200% speedup and compression everytime. What you can achieve highly depend on the architecture that you are using (some are already speed/parameter efficient by design) or the task it is doing (some datasets are so easy that you can remove almost all your network without seeing a drop in performance)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fasterai",
    "section": "",
    "text": "Methods • Features • Installation • Tutorials • Citing • License\nfasterai is a library created to make neural network smaller and faster. It essentially relies on common compression techniques for networks such as pruning, knowledge distillation, Lottery Ticket Hypothesis, …\nThe core feature of fasterai is its Sparsifying capabilities, constructed on 4 main modules: granularity, context, criteria, schedule. Each of these modules is highly customizable, allowing you to change them according to your needs or even to come up with your own !"
  },
  {
    "objectID": "index.html#project-documentation",
    "href": "index.html#project-documentation",
    "title": "fasterai",
    "section": "Project Documentation",
    "text": "Project Documentation\nVisit Read The Docs Project Page or read following README to know more about using fasterai."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "fasterai",
    "section": "Features",
    "text": "Features\n\n1. Sparsifying\n\nMake your model sparse (i.e. prune it) according to a:  - Sparsity:  the percentage of weights that will be replaced by 0  - Granularity:  the granularity at which you operate the pruning (removing weights, vectors, kernels, filters)  - Context:  prune either each layer independantly (local pruning) or the whole model (global pruning)  - Criteria:  the criteria used to select the weights to remove (magnitude, movement, …)  - Schedule:  which schedule you want to use for pruning (one shot, iterative, gradual, …) \nThis can be achieved by using the SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n2. Pruning\n\nOnce your model has useless nodes due to zero-weights, they can be removed to not be a part of the network anymore.\nThis can be achieved by using the Pruner() method\n\n\n3. Regularization\n\n\n\n\n\nalt text\n\n\nInstead of explicitely make your network sparse, let it train towards sparse connections by pushing the weights to be as small as possible.\nRegularization can be applied to groups of weights, following the same granularities as for sparsifying, i.e.: - Granularity:  the granularity at which you operate the regularization (weights, vectors, kernels, filters, …)\nThis can be achieved by using the RegularizationCallback(granularity)\n\n\n4. Knowledge Distillation\n\n\n\n\n\nalt text\n\n\nDistill the knowledge acquired by a big model into a smaller one, by using the KnowledgeDistillation callback.\n\n\n5. Lottery Ticket Hypothesis\n\n\n\n\n\nalt text\n\n\nFind the winning ticket in you network, i.e. the initial subnetwork able to attain at least similar performances than the network as a whole."
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "fasterai",
    "section": "Quick Start",
    "text": "Quick Start\n\n0. Import fasterai\nfrom fasterai.sparse.all import *\n\n\n1. Create your model with fastai\nlearn = cnn_learner(dls, model)\n\n\n2. Get you Fasterai Callback\nsp_cb=SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n3. Train you model to make it sparse !\nlearn.fit_one_cycle(n_epochs, cbs=sp_cb)"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "fasterai",
    "section": "Installation",
    "text": "Installation\npip install git+https://github.com/nathanhubens/fasterai.git\nor\npip install fasterai"
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "fasterai",
    "section": "Tutorials",
    "text": "Tutorials\n\nGet Started with FasterAI\nCreate your own pruning schedule\nFind winning tickets using the Lottery Ticket Hypothesis\nUse Knowledge Distillation to help a student model to reach higher performance\nSparsify Transformers\nMore to come…"
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "fasterai",
    "section": "Citing",
    "text": "Citing\n@software{Hubens,\n  author       = {Nathan Hubens},\n  title        = {fasterai},\n  year         = 2022,\n  publisher    = {Zenodo},\n  version      = {v0.1.6},\n  doi          = {10.5281/zenodo.6469868},\n  url          = {https://doi.org/10.5281/zenodo.6469868}\n}"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "fasterai",
    "section": "License",
    "text": "License\nApache-2.0 License."
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quick Start",
    "section": "",
    "text": "from fastai.vision.all import *\nLet’s start with a bit of context for the purpose of the demonstration. Imagine that we want to deploy a VGG16 model on a mobile device that has limited storage capacity and that our task requires our model to run sufficiently fast. It is known that parameters and speed efficiency are not the strong points of VGG16 but let’s see what we can do with it.\nLet’s first check the number of parameters and the inference time of VGG16.\nSo, VGG16 has 134 millions of parameters\nAnd takes 5.54ms to perform inference on a single image.\nSnap ! This is more than we can afford for deployment, ideally we would like our model to take only half of that…but should we give up ? Nope, there are actually a lot of techniques that we can use to help reducing the size and improve the speed of our models! Let’s see how to apply them with FasterAI.\nWe will first train our VGG16 model to have a baseline of what performance we should expect from it.\nSo we would like our network to have comparable accuracy but fewer parameters and running faster… And the first technique that we will show how to use is called Knowledge Distillation"
  },
  {
    "objectID": "quickstart.html#knowledge-distillation",
    "href": "quickstart.html#knowledge-distillation",
    "title": "Quick Start",
    "section": "Knowledge Distillation",
    "text": "Knowledge Distillation\nKnowledge distillation is a simple yet very efficient way to train a model. It was introduced in 2006 by Caruana et al.. The main idea behind is to use a small model (called the student) to approximate the function learned by a larger and high-performing model (called the teacher). This can be done by using the large model to pseudo-label the data. This idea has been used very recently to break the state-of-the-art accuracy on ImageNet.\nWhen we train our model for classification, we usually use a softmax as last layer. This softmax has the particularity to squish low value logits towards 0, and the highest logit towards 1. This has for effect to completely lose all the inter-class information, or what is sometimes called the dark knowledge. This is the information that is valuable and that we want to transfer from the teacher to the student.\nTo do so, we still use a regular classification loss but at the same time, we’ll use another loss, computed between the softened logits of the teacher (our soft labels) and the softened logits of the student (our soft predictions). Those soft values are obtained when you use a soft-softmax, that avoids squishing the values at its output. Our implementation follows this paper and the basic principle of training is represented in the figure below:\n\n\n\nTo use Knowledge Distillation with FasterAI, you only need to use this callback when training your student model:\n\n\n KnowledgeDistillation(teacher.model, loss) \n\n You only need to give to the callback function your teacher learner. Behind the scenes, FasterAI will take care of making your model train using knowledge distillation. \n\n\n\n\nfrom fasterai.distill.all import *\n\nThe first thing to do is to find a teacher, which can be any model, that preferrably performs well. We will chose VGG19 for our demonstration. To make sure it performs better than our VGG16 model, let’s start from a pretrained version.\n\nteacher = cnn_learner(dls, models.vgg19_bn, metrics=[accuracy])\nteacher.fit_one_cycle(3, 1e-4)\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.674759\n      0.289090\n      0.912357\n      00:36\n    \n    \n      1\n      0.403283\n      0.180294\n      0.946752\n      00:36\n    \n    \n      2\n      0.396881\n      0.176295\n      0.945223\n      00:36\n    \n  \n\n\n\nOur teacher has 97.4% of accuracy which is pretty good, it is ready to take a student under its wing. So let’s create our student model and train it with the Knowledge Distillation callback:\n\nstudent = Learner(dls, models.vgg16_bn(num_classes=10), metrics=[accuracy])\nkd_cb = KnowledgeDistillationCallback(teacher.model, SoftTarget)\nstudent.fit_one_cycle(10, 1e-4, cbs=kd_cb)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      7.487010\n      6.192549\n      0.446369\n      01:37\n    \n    \n      1\n      5.470930\n      6.126627\n      0.469809\n      01:37\n    \n    \n      2\n      4.444192\n      5.306108\n      0.592102\n      01:37\n    \n    \n      3\n      4.153715\n      3.482993\n      0.669554\n      01:37\n    \n    \n      4\n      3.522233\n      2.846554\n      0.742166\n      01:37\n    \n    \n      5\n      3.066324\n      2.738527\n      0.739363\n      01:37\n    \n    \n      6\n      2.732183\n      2.394343\n      0.768917\n      01:37\n    \n    \n      7\n      2.329082\n      2.012312\n      0.811720\n      01:37\n    \n    \n      8\n      2.095640\n      1.827197\n      0.822166\n      01:37\n    \n    \n      9\n      2.006221\n      1.837958\n      0.817580\n      01:37\n    \n  \n\n\n\nAnd we can see that indeed, the knowledge of the teacher was useful for the student, as it is clearly overperforming the vanilla VGG16.\nOk, so now we are able to get more from a given model which is kind of cool ! With some experimentations we could come up with a model smaller than VGG16 but able to reach the same performance as our baseline! You can try to find it by yourself later, but for now let’s continue with the next technique !"
  },
  {
    "objectID": "quickstart.html#sparsifying",
    "href": "quickstart.html#sparsifying",
    "title": "Quick Start",
    "section": "Sparsifying",
    "text": "Sparsifying\nNow that we have a student model that is performing better than our baseline, we have some room to compress it. And we’ll start by making the network sparse. As explained in a previous article, there are many ways leading to a sparse network.\n\n\n\n\n\n\n\nNote\n\n\n\nUsually, the process of making a network sparse is called Pruning. I prefer using the term Pruning when parameters are actually removed from the network, which we will do in the next section.\n\n\n\n\n\nBy default, FasterAI uses the Automated Gradual Pruning paradigm as it removes parameters as the model trains and doesn’t require to pretrain the model, so it is usually much faster. In FasterAI, this is also managed by using a callback, that will replace the least important parameters of your model by zeroes during the training. The callback has a wide variety of parameters to tune your Sparsifying operation, let’s take a look at them:\n\n\nSparsifyCallback(learn, sparsity, granularity, context, criteria, schedule)\n\n\n\nsparsity: the percentage of sparsity that you want in your network\n\n\ngranularity: on what granularity you want the sparsification to be operated (currently supported: weight, filter)\n\n\ncontext: either local or global, will affect the selection of parameters to be choosen in each layer independently (local) or on the whole network (global).\n\n\ncriteria: the criteria used to select which parameters to remove (currently supported: l1, taylor)\n\n\nschedule: which schedule you want to follow for the sparsification (currently supported: any scheduling function of fastai, i.e annealing_linear, annealing_cos, … and annealing_gradual, common schedules such as One-Shot, Iterative or Automated Gradual)\n\n\n\n\n\nBut let’s come back to our example!\nHere, we will make our network 40% sparse, and remove entire filters, selected locally and based on L1 norm. We will train with a learning rate a bit smaller to be gentle with our network because it has already been trained. The scheduling selected is cosinusoidal, so the pruning starts and ends quite slowly.\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='filter', context='global', criteria=large_final, schedule=cos)\nstudent.fit(5, 1e-5, cbs=sp_cb)\n\nPruning of filter until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.604952\n      0.570446\n      0.817834\n      01:05\n    \n    \n      1\n      0.608648\n      0.560816\n      0.823694\n      01:05\n    \n    \n      2\n      0.594311\n      0.562816\n      0.826497\n      01:05\n    \n    \n      3\n      0.634639\n      0.575046\n      0.822675\n      01:05\n    \n    \n      4\n      0.605929\n      0.571302\n      0.816561\n      01:05\n    \n  \n\n\n\nSparsity at the end of epoch 0: [4.77]%\nSparsity at the end of epoch 1: [17.27]%\nSparsity at the end of epoch 2: [32.73]%\nSparsity at the end of epoch 3: [45.23]%\nSparsity at the end of epoch 4: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 2: 0.00%\nSparsity in Conv2d 5: 0.00%\nSparsity in Conv2d 9: 0.00%\nSparsity in Conv2d 12: 0.00%\nSparsity in Conv2d 16: 0.00%\nSparsity in Conv2d 19: 0.00%\nSparsity in Conv2d 22: 0.00%\nSparsity in Conv2d 26: 64.26%\nSparsity in Conv2d 29: 70.70%\nSparsity in Conv2d 32: 71.88%\nSparsity in Conv2d 36: 71.29%\nSparsity in Conv2d 39: 70.51%\nSparsity in Conv2d 42: 63.87%\n\n\nOur network now has 50% of its filters composed entirely of zeroes, without even losing accuracy. Obviously, choosing a higher sparsity makes it more difficult for the network to keep a similar accuracy. Other parameters can also widely change the behaviour of our sparsification process. For example choosing a more fine-grained sparsity usually leads to better results but is then more difficult to take advantage of in terms of speed.\n\nLet’s now see how much we gained in terms of speed. Because we removed 50% of convolution filters, we should expect crazy speed-up right ?\n\nmodel = student.model.eval()\n\n\nmodel(x[0][None].cuda())\n\n4.26 ms ± 20.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nWell actually, no. We didn’t remove any parameters, we just replaced some by zeroes, remember? The amount of parameters is still the same:\n\ncount_parameters(model)\n\nTotal parameters : 134,309,962\n\n\nWhich leads us to the next section."
  },
  {
    "objectID": "quickstart.html#pruning",
    "href": "quickstart.html#pruning",
    "title": "Quick Start",
    "section": "Pruning",
    "text": "Pruning\n\n\n\n\n\n\nImportant\n\n\n\nThis is currently only supported for fully-feedforward models such as VGG-like models as more complex architectures require increasingly difficult and usually model-dependant implementations.\n\n\nWhy don’t we see any acceleration even though we removed half of the parameters? That’s because natively, our GPU does not know that our matrices are sparse and thus isn’t able to accelerate the computation. The easiest work around, is to physically remove the parameters we zeroed-out. But this operation requires to change the architecture of the network.\nThis pruning only works if we have zeroed-out entire filters beforehand as it is the only case where you can change the architecture accordingly. Hopefully, sparse computations will soon be available on common deep learning librairies so this section will become useless in the future, but for the moment, it is the best solution I could come up with 🤷\n\nHere is what it looks like with fasterai: \n\n\n\npruner = Pruner()\npruned_model = pruner.prune_model(learn.model)\n\n You just need to pass the model whose filters has previously been sparsified and FasterAI will take care of removing them. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis operation should be lossless as it only removes filters that already do not participate in the network anymore.\n\n\n\nSo in the case of our example, it gives:\n\nfrom fasterai.sparse.pruner import *\n\n\npruner = Pruner()\npruned_model = pruner.prune_model(student.model)\n\nLet’s now see what our model is capable of now:\n\nmodel = pruned_model.eval()\n\n\ncount_parameters(model)\n\nTotal parameters : 57,202,072\n\n\nAnd in terms of speed:\n\nmodel(x[0][None].cuda())\n\n4.22 ms ± 12.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nYay ! Now we can talk ! Let’s just double check that our accuracy is unchanged and that we didn’t mess up somewhere:\n\npruned_learner = Learner(dls, pruned_model, metrics=[accuracy])\npruned_learner.validate()\n\n\n\n\n\n\n\n\n(#2) [0.571302056312561,0.8165605068206787]\n\n\n\nAnd there is actually more that we can do ! Let’s keep going !"
  },
  {
    "objectID": "quickstart.html#batch-normalization-folding",
    "href": "quickstart.html#batch-normalization-folding",
    "title": "Quick Start",
    "section": "Batch Normalization Folding",
    "text": "Batch Normalization Folding\nBatch Normalization Folding is a really easy to implement and straightforward idea. The gist is that batch normalization is nothing more than a normalization of the input data at each layer. Moreover, at inference time, the batch statistics used for this normalization are fixed. We can thus incorporate the normalization process directly in the convolution by changing its weights and completely remove the batch normalization layers, which is a gain both in terms of parameters and in terms of computations. For a more in-depth explaination, see this blog post.\nThis is how to use it with FasterAI:\n\nbn_folder = BN_Folder()\nbn_folder.fold(learn.model))\n\n Again, you only need to pass your model and FasterAI takes care of the rest. For models built using the nn.Sequential, you don’t need to change anything. For others, if you want to see speedup and compression, you actually need to subclass your model to remove the batch norm from the parameters and from the forward method of your network. \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis operation should also be lossless as it redefines the convolution to take batch norm into account and is thus equivalent.\n\n\n\n\nfrom fasterai.misc.bn_folding import *\n\nLet’s do this with our model !\n\nbn_f = BN_Folder()\nfolded_model = bn_f.fold(pruned_learner.model)\n\nThe parameters drop is generally not that significant, especially in a network such as VGG where almost all parameters are contained in the FC layers but, hey, any gain is good to take.\n\ncount_parameters(folded_model)\n\nTotal parameters : 57,197,848\n\n\n\nNow that we removed the batch normalization layers, we should again see a speedup.\n\nfolded_model = folded_model.eval()\n\n\nfolded_model(x[0][None].cuda())\n\n3.02 ms ± 16.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nAgain, let’s double check that we didn’t mess up somewhere:\n\nfolded_learner = Learner(dls, folded_model, metrics=[accuracy])\nfolded_learner.validate()\n\n\n\n\n\n\n\n\n(#2) [0.5713019371032715,0.8165605068206787]\n\n\n\nAnd we’re still not done yet ! As we know for VGG16, most of the parameters are comprised in the fully-connected layers so there should be something that we can do about it, right ?"
  },
  {
    "objectID": "quickstart.html#fc-layers-factorization",
    "href": "quickstart.html#fc-layers-factorization",
    "title": "Quick Start",
    "section": "FC Layers Factorization",
    "text": "FC Layers Factorization\nWe can indeed, factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\). With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\nIn FasterAI, to decompose the fully-connected layers of your model, here is what you need to do: \n\nFCD = FCDecomposer()\ndecomposed_model = FCD.decompose(model, percent_removed)\n\n The percent_removed corresponds to the percentage of singular values removed (k value above). \n\n\n\n\n\n\n\n\nNote\n\n\n\nThis time, the decomposition is not exact, so we expect a drop in performance afterwards and further retraining will be needed.\n\n\n\nWhich gives with our example, if we only want to keep half of them:\n\nfrom fasterai.misc.fc_decomposer import *\n\n\nfc_decomposer = FC_Decomposer()\ndecomposed_model = fc_decomposer.decompose(folded_model, percent_removed=0.5)\n\nHow many parameters do we have now ?\n\ncount_parameters(decomposed_model)\n\nTotal parameters : 47,000,906\n\n\nAnd how much time did we gain ?\n\ndecomposed_model = decomposed_model.eval()\n\n\ndecomposed_model(x[0][None].cuda())\n\n3.16 ms ± 13.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nHowever, this technique is an approximation so it is not lossless, so we should retrain our network a bit to recover its performance.\n\nfinal_learner = Learner(dls, decomposed_model, metrics=[accuracy])\nfinal_learner.fit_one_cycle(5, 1e-5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.847210\n      0.851746\n      0.757707\n      00:37\n    \n    \n      1\n      0.768943\n      0.765675\n      0.778344\n      00:37\n    \n    \n      2\n      0.692119\n      0.794932\n      0.763822\n      00:37\n    \n    \n      3\n      0.666022\n      0.679881\n      0.794650\n      00:37\n    \n    \n      4\n      0.556258\n      0.661012\n      0.801783\n      00:37\n    \n  \n\n\n\nThis operation is usually less useful for more recent architectures as they usually do not have that many parameters in their fully-connected layers.\n\n\n\nSo to recap, we saw in this article how to use fasterai to:  1. Make a student model learn from a teacher model (Knowledge Distillation)  2. Make our network sparse (Sparsifying)  3. Optionnaly physically remove the zero-filters (Pruning)  4. Remove the batch norm layers (Batch Normalization Folding)  5. Approximate our big fully-connected layers by smaller ones (Fully-Connected Layers Factorization) \n\nAnd we saw that by applying those, we could reduce our VGG16 model from 134 million of parameters down to 47 million, and also speed-up the inference from 5.54ms to 3.16ms without any drop in accuracy compared to the baseline.\nOf course, those techniques can be used in conjunction with quantization or mixed-precision training, which are already available in Pytorch for even more compression and speedup.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease keep in mind that the techniques presented above are not magic 🧙‍♂️, so do not expect to see a 200% speedup and compression everytime. What you can achieve highly depend on the architecture that you are using (some are already speed/parameter efficient by design) or the task it is doing (some datasets are so easy that you can remove almost all your network without seeing a drop in performance)"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Methods • Features • Installation • Tutorials • Citing • License\nfasterai is a library created to make neural network smaller and faster. It essentially relies on common compression techniques for networks such as pruning, knowledge distillation, Lottery Ticket Hypothesis, …\nThe core feature of fasterai is its Sparsifying capabilities, constructed around 4 main modules: granularity, context, criteria, schedule. Each of these modules is highly customizable, allowing you to change them according to your needs or even to come up with your own !"
  },
  {
    "objectID": "overview.html#project-documentation",
    "href": "overview.html#project-documentation",
    "title": "Overview",
    "section": "Project Documentation",
    "text": "Project Documentation\nVisit Read The Docs Project Page or read following README to know more about using fasterai."
  },
  {
    "objectID": "overview.html#features",
    "href": "overview.html#features",
    "title": "Overview",
    "section": "Features",
    "text": "Features\n\n1. Sparsifying\n\nMake your model sparse according to a:  - Sparsity:  the percentage of weights that will be replaced by 0  - Granularity:  the granularity at which you operate the pruning (removing weights, vectors, kernels, filters)  - Context:  prune either each layer independantly (local pruning) or the whole model (global pruning)  - Criteria:  the criteria used to select the weights to remove (magnitude, movement, …)  - Schedule:  which schedule you want to use for pruning (one shot, iterative, gradual, …) \nThis can be achieved by using the SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n2. Pruning\n\nOnce your model has useless nodes due to zero-weights, they can be removed to not be a part of the network anymore.\nThis can be achieved by using the Pruner() method\n\n\n3. Regularization\n\nInstead of explicitely make your network sparse, let it train towards sparse connections by pushing the weights to be as small as possible.\nRegularization can be applied to groups of weights, following the same granularities as for sparsifying, i.e.: - Granularity:  the granularity at which you operate the regularization (weights, vectors, kernels, filters, …)\nThis can be achieved by using the RegularizationCallback(granularity)\n\n\n4. Knowledge Distillation\n\n\n\n\n\nalt text\n\n\nDistill the knowledge acquired by a big model into a smaller one, by using the KnowledgeDistillation callback.\n\n\n5. Lottery Ticket Hypothesis\n\nFind the winning ticket in you network, i.e. the initial subnetwork able to attain at least similar performances than the network as a whole."
  },
  {
    "objectID": "overview.html#quick-start",
    "href": "overview.html#quick-start",
    "title": "Overview",
    "section": "Quick Start",
    "text": "Quick Start\n\n0. Import fasterai\nfrom fasterai.sparse.all import *\n\n\n1. Create your model with fastai\nlearn = cnn_learner(dls, model)\n\n\n2. Get you Fasterai Callback\nsp_cb=SparsifyCallback(sparsity, granularity, context, criteria, schedule)\n\n\n3. Train you model to make it sparse !\nlearn.fit_one_cycle(n_epochs, cbs=sp_cb)"
  },
  {
    "objectID": "overview.html#installation",
    "href": "overview.html#installation",
    "title": "Overview",
    "section": "Installation",
    "text": "Installation\npip install git+https://github.com/nathanhubens/fasterai.git\nor\npip install fasterai"
  },
  {
    "objectID": "overview.html#tutorials",
    "href": "overview.html#tutorials",
    "title": "Overview",
    "section": "Tutorials",
    "text": "Tutorials\n\nGet Started with FasterAI\nCreate your own pruning schedule\nFind winning tickets using the Lottery Ticket Hypothesis\nUse Knowledge Distillation to help a student model to reach higher performance\nSparsify Transformers\nMore to come…"
  },
  {
    "objectID": "overview.html#citing",
    "href": "overview.html#citing",
    "title": "Overview",
    "section": "Citing",
    "text": "Citing\n@software{Hubens,\n  author       = {Nathan Hubens},\n  title        = {fasterai},\n  year         = 2022,\n  publisher    = {Zenodo},\n  version      = {v0.1.6},\n  doi          = {10.5281/zenodo.6469868},\n  url          = {https://doi.org/10.5281/zenodo.6469868}\n}"
  },
  {
    "objectID": "overview.html#license",
    "href": "overview.html#license",
    "title": "Overview",
    "section": "License",
    "text": "License\nApache-2.0 License."
  },
  {
    "objectID": "tutorial.bn_folding.html",
    "href": "tutorial.bn_folding.html",
    "title": "BatchNorm Folding",
    "section": "",
    "text": "This is how to do it with fasterai !\n\nGet the data\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\nTrain the model\n\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(5)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.615641\n      0.596630\n      0.688092\n      00:10\n    \n    \n      1\n      0.582679\n      0.558671\n      0.689445\n      00:10\n    \n    \n      2\n      0.529308\n      0.517995\n      0.744926\n      00:10\n    \n    \n      3\n      0.481804\n      0.449941\n      0.784168\n      00:10\n    \n    \n      4\n      0.400030\n      0.414093\n      0.800406\n      00:10\n    \n  \n\n\n\n\nFold !\n\n\nbn = BN_Folder()\nnew_model = bn.fold(learn.model)\n\nThe batch norm layers have been replaced by an Identity layer, and the weights of the convolutions have been modified accordingly.\n\nnew_model\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n  (bn1): Identity()\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n        (1): Identity()\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn1): Identity()\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (bn2): Identity()\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\nWe can see that the new model possess fewer parameters\n\ncount_parameters(learn.model)\n\n11177538\n\n\n\ncount_parameters(new_model)\n\n11172738\n\n\nBut is also faster to run !\n\nx,y = dls.one_batch()\n\n\nlearn.model(x[0][None].cuda())\n\n5.59 ms ± 547 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nnew_model(x[0][None].cuda())\n\n4.14 ms ± 446 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nBut most importantly, has the exact same perfomance as before:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\n\n\nnew_learn.validate()\n\n\n\n\n(#2) [0.4140927791595459,0.8004059791564941]"
  },
  {
    "objectID": "tutorial.fc_decomposer.html",
    "href": "tutorial.fc_decomposer.html",
    "title": "Fully-Connected layers decomposition",
    "section": "",
    "text": "1. Get the data2. Train the model\n\n\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\n\n\nlearn = Learner(dls, vgg16_bn(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.886644\n      0.652151\n      0.685386\n      00:22\n    \n    \n      1\n      0.692583\n      0.627857\n      0.685386\n      00:21\n    \n    \n      2\n      0.646516\n      0.622866\n      0.685386\n      00:22\n    \n  \n\n\n\n\n\n\n\nDecompose !\n\n\nfc = FC_Decomposer()\nnew_model = fc.decompose(learn.model)\n\nThe fc layers have been factorized and replace by smaller ones.\n\nnew_model\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=25088, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Sequential(\n      (0): Linear(in_features=4096, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Sequential(\n      (0): Linear(in_features=4096, out_features=1, bias=False)\n      (1): Linear(in_features=1, out_features=2, bias=True)\n    )\n  )\n)\n\n\nWe can see compare the amount of parameters before/after:\n\ncount_parameters(learn.model)\n\n134277186\n\n\n\ncount_parameters(new_model)\n\n91281476\n\n\nThis represents a decrease of ~40M parameters !\nNow this is an approximation, so it isn’t really lossless and we should expect to see a performance drop, which will be bigger as we keep fewer singular values. Here we have:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\nnew_learn.validate()\n\n\n\n\n(#2) [0.6868855357170105,0.6853856444358826]"
  },
  {
    "objectID": "misc.bn_folding.html",
    "href": "misc.bn_folding.html",
    "title": "Batch Norm Folding",
    "section": "",
    "text": "Batch Normalization is a technique which takes care of normalizing the input of each layer to make the training process faster and more stable. In practice, it is an extra layer that we generally add after the computation layer and before the non-linearity.\nIt consists of 2 steps:\n\nNormalize the batch by first subtracting its mean \\(\\mu\\), then dividing it by its standard deviation \\(\\sigma\\).\nFurther scale by a factor \\(\\gamma\\) and shift by a factor \\(\\beta\\). Those are the parameters of the batch normalization layer, required in case of the network not needing the data to have a mean of \\(0\\) and a standard deviation of \\(1\\).\n\n\\[\n\\begin{aligned}\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\ \\sigma_{\\mathcal{B}}^{2} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\ \\widehat{x}_{i} & \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\ y_{i} & \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{BN}_{\\gamma, \\beta}\\left(x_{i}\\right) \\end{aligned}\\]\nDue to its efficiency for training neural networks, batch normalization is now widely used. But how useful is it at inference time?\nOnce the training has ended, each batch normalization layer possesses a specific set of \\(\\gamma\\) and \\(\\beta\\), but also \\(\\mu\\) and \\(\\sigma\\), the latter being computed using an exponentially weighted average during training. It means that during inference, the batch normalization acts as a simple linear transformation of what comes out of the previous layer, often a convolution.\nAs a convolution is also a linear transformation, it also means that both operations can be merged into a single linear transformation!\nThis would remove some unnecessary parameters but also reduce the number of operations to be performed at inference time.\nWith a little bit of math, we can easily rearrange the terms of the convolution to take the batch normalization into account.\nAs a little reminder, the convolution operation followed by the batch normalization operation can be expressed, for an input \\(x\\), as:\n\\[\\begin{aligned} z &=W * x+b \\\\ \\mathrm{out} &=\\gamma \\cdot \\frac{z-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nSo, if we re-arrange the \\(W\\) and \\(b\\) of the convolution to take the parameters of the batch normalization into account, as such:\n\\[\\begin{aligned} w_{\\text {fold }} &=\\gamma \\cdot \\frac{W}{\\sqrt{\\sigma^{2}+\\epsilon}} \\\\ b_{\\text {fold }} &=\\gamma \\cdot \\frac{b-\\mu}{\\sqrt{\\sigma^{2}+\\epsilon}}+\\beta \\end{aligned}\\]\nIn practice, this can be achieved in FasterAI with the BN_folder class\n\n\nBN_Folder.fold\n\n BN_Folder.fold (model)\n\nA tutorial about how to use the BN_Folder functionalities can be found here"
  },
  {
    "objectID": "tutorial.regularizer.html",
    "href": "tutorial.regularizer.html",
    "title": "RegularizationCallback",
    "section": "",
    "text": "learn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.681536\n      0.466989\n      0.835589\n      00:11\n    \n    \n      1\n      0.358927\n      0.318825\n      0.865359\n      00:10\n    \n    \n      2\n      0.201207\n      0.220008\n      0.923545\n      00:10\n    \n  \n\n\n\nThe RegularizationCallbackcan be used to perform \\(l_1\\) regularization on any granularity available in the criteria class.\n\nreg_cb = RegularizationCallback('filter')\n\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\nlearn.fit_one_cycle(3, cbs=reg_cb)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.633497\n      1.468702\n      0.812585\n      00:10\n    \n    \n      1\n      1.334702\n      1.173871\n      0.907307\n      00:10\n    \n    \n      2\n      1.152696\n      1.136654\n      0.933694\n      00:11"
  },
  {
    "objectID": "tutorial.sparsifier.html",
    "href": "tutorial.sparsifier.html",
    "title": "Sparsifier",
    "section": "",
    "text": "A sparse vector, as opposed to a dense one, is a vector which contains a lot of zeroes. When we speak about making a neural network sparse, we thus mean that the network’s weight are mostly zeroes.\nWith fasterai, you can do that thanks to the Sparsifier class.\nLet’s start by creating a model\nAs you probably know, weights in a convolutional neural network have 4 dimensions ($ c_{out} c_{in} k_h k_w$)\nIn the case of ResNet18, the dimension of the first layer weights is \\(64 \\times 3 \\times 7 \\times 7\\). We thus can plot each of the \\(64\\) filter as a \\(7 \\times 7\\) color image (because they contains \\(3\\) channels)."
  },
  {
    "objectID": "tutorial.sparsifier.html#granularity",
    "href": "tutorial.sparsifier.html#granularity",
    "title": "Sparsifier",
    "section": "Granularity",
    "text": "Granularity\nAs we said earlier, the granularity defines the structure of parameter that you will remove.\nIn the example below, we removed weight from each convolutional filter, meaning that we now have sparse filters, as can be seen in the image below:\n\nplot_kernels(model.conv1)\n\n\n\n\nAnother granularity is, for example, removing column vectors from the filters. To do so, just change the granularity parameter accordingly.\n\nmodel = resnet18()\npruner = Sparsifier(model, 'column', 'local', large_final)\npruner.prune_layer(model.conv1, 70)\n\n\nplot_kernels(model.conv1)\n\n\n\n\nFor more information and examples about the pruning granularities, I suggest you to take a look at the corresponding section."
  },
  {
    "objectID": "tutorial.sparsifier.html#context",
    "href": "tutorial.sparsifier.html#context",
    "title": "Sparsifier",
    "section": "Context",
    "text": "Context\nThe context defines where to look in the model, i.e. from where do we compare weight. The two basic contexts are: - local, i.e. we compare weight from each layer individually. This will lead to layers with similar levels of sparsity. - global, i.e. we compare weight from the whole model. This will lead to layers with different levels of sparsity\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'local', large_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 69.99%\nSparsity in Conv2d 7: 70.00%\nSparsity in Conv2d 10: 70.00%\nSparsity in Conv2d 13: 70.00%\nSparsity in Conv2d 16: 70.00%\nSparsity in Conv2d 20: 70.00%\nSparsity in Conv2d 23: 70.00%\nSparsity in Conv2d 26: 70.00%\nSparsity in Conv2d 29: 70.00%\nSparsity in Conv2d 32: 70.00%\nSparsity in Conv2d 36: 70.00%\nSparsity in Conv2d 39: 70.00%\nSparsity in Conv2d 42: 70.00%\nSparsity in Conv2d 45: 70.00%\nSparsity in Conv2d 48: 70.00%\nSparsity in Conv2d 52: 70.00%\nSparsity in Conv2d 55: 70.00%\nSparsity in Conv2d 58: 70.00%\nSparsity in Conv2d 61: 70.00%\nSparsity in Conv2d 64: 70.00%\n\n\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'global', large_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 66.20%\nSparsity in Conv2d 7: 32.18%\nSparsity in Conv2d 10: 32.09%\nSparsity in Conv2d 13: 31.96%\nSparsity in Conv2d 16: 32.15%\nSparsity in Conv2d 20: 44.07%\nSparsity in Conv2d 23: 44.04%\nSparsity in Conv2d 26: 16.02%\nSparsity in Conv2d 29: 44.15%\nSparsity in Conv2d 32: 44.09%\nSparsity in Conv2d 36: 59.25%\nSparsity in Conv2d 39: 59.30%\nSparsity in Conv2d 42: 22.06%\nSparsity in Conv2d 45: 59.29%\nSparsity in Conv2d 48: 59.30%\nSparsity in Conv2d 52: 75.90%\nSparsity in Conv2d 55: 75.85%\nSparsity in Conv2d 58: 30.31%\nSparsity in Conv2d 61: 75.82%\nSparsity in Conv2d 64: 75.86%"
  },
  {
    "objectID": "tutorial.sparsifier.html#criteria",
    "href": "tutorial.sparsifier.html#criteria",
    "title": "Sparsifier",
    "section": "Criteria",
    "text": "Criteria\nThe criteria defines how we select the parameters to remove. It is usually given by a scoring method. The most common one is the large_final, i.e. select parameters with the highest absolute value as they are supposed to contribute the most to the final results of the model.\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'global', large_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 67.08%\nSparsity in Conv2d 7: 32.25%\nSparsity in Conv2d 10: 31.98%\nSparsity in Conv2d 13: 32.28%\nSparsity in Conv2d 16: 31.94%\nSparsity in Conv2d 20: 44.52%\nSparsity in Conv2d 23: 44.23%\nSparsity in Conv2d 26: 15.36%\nSparsity in Conv2d 29: 44.36%\nSparsity in Conv2d 32: 44.24%\nSparsity in Conv2d 36: 59.27%\nSparsity in Conv2d 39: 59.10%\nSparsity in Conv2d 42: 21.94%\nSparsity in Conv2d 45: 59.21%\nSparsity in Conv2d 48: 59.23%\nSparsity in Conv2d 52: 75.89%\nSparsity in Conv2d 55: 75.82%\nSparsity in Conv2d 58: 30.38%\nSparsity in Conv2d 61: 75.90%\nSparsity in Conv2d 64: 75.85%\n\n\n\nmodel = resnet18()\npruner = Sparsifier(model, 'weight', 'global', small_final)\npruner.prune_model(70)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 72.49%\nSparsity in Conv2d 7: 87.78%\nSparsity in Conv2d 10: 87.88%\nSparsity in Conv2d 13: 87.83%\nSparsity in Conv2d 16: 87.99%\nSparsity in Conv2d 20: 83.05%\nSparsity in Conv2d 23: 83.02%\nSparsity in Conv2d 26: 94.21%\nSparsity in Conv2d 29: 83.10%\nSparsity in Conv2d 32: 83.07%\nSparsity in Conv2d 36: 76.27%\nSparsity in Conv2d 39: 76.20%\nSparsity in Conv2d 42: 92.01%\nSparsity in Conv2d 45: 76.22%\nSparsity in Conv2d 48: 76.26%\nSparsity in Conv2d 52: 66.89%\nSparsity in Conv2d 55: 66.87%\nSparsity in Conv2d 58: 88.72%\nSparsity in Conv2d 61: 66.95%\nSparsity in Conv2d 64: 66.87%\n\n\nFor more information and examples about the pruning criteria, I suggest you to take a look at the corresponding section."
  },
  {
    "objectID": "tutorial.sparsifier.html#remark",
    "href": "tutorial.sparsifier.html#remark",
    "title": "Sparsifier",
    "section": "Remark",
    "text": "Remark\nIn some case, you may want to impose the remaining amount of parameters to be a multiple of 8, this can be done by passing the round_to parameter.\n\nmodel = resnet18()\npruner = Sparsifier(model, 'filter', 'local', large_final)\npruner.prune_model(70, round_to=8)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 62.50%\nSparsity in Conv2d 7: 62.50%\nSparsity in Conv2d 10: 62.50%\nSparsity in Conv2d 13: 62.50%\nSparsity in Conv2d 16: 62.50%\nSparsity in Conv2d 20: 68.75%\nSparsity in Conv2d 23: 68.75%\nSparsity in Conv2d 26: 68.75%\nSparsity in Conv2d 29: 68.75%\nSparsity in Conv2d 32: 68.75%\nSparsity in Conv2d 36: 68.75%\nSparsity in Conv2d 39: 68.75%\nSparsity in Conv2d 42: 68.75%\nSparsity in Conv2d 45: 68.75%\nSparsity in Conv2d 48: 68.75%\nSparsity in Conv2d 52: 68.75%\nSparsity in Conv2d 55: 68.75%\nSparsity in Conv2d 58: 68.75%\nSparsity in Conv2d 61: 68.75%\nSparsity in Conv2d 64: 68.75%\n\n\n\nmodel = resnet18()\npruner = Sparsifier(model, 'filter', 'global', large_final)\npruner.prune_model(70, round_to=8)\n\n\npruner.print_sparsity()\n\nSparsity in Conv2d 1: 87.50%\nSparsity in Conv2d 7: 0.00%\nSparsity in Conv2d 10: 0.00%\nSparsity in Conv2d 13: 0.00%\nSparsity in Conv2d 16: 0.00%\nSparsity in Conv2d 20: 93.75%\nSparsity in Conv2d 23: 93.75%\nSparsity in Conv2d 26: 0.00%\nSparsity in Conv2d 29: 93.75%\nSparsity in Conv2d 32: 93.75%\nSparsity in Conv2d 36: 96.88%\nSparsity in Conv2d 39: 96.88%\nSparsity in Conv2d 42: 0.00%\nSparsity in Conv2d 45: 96.88%\nSparsity in Conv2d 48: 90.62%\nSparsity in Conv2d 52: 98.44%\nSparsity in Conv2d 55: 98.44%\nSparsity in Conv2d 58: 0.00%\nSparsity in Conv2d 61: 98.44%\nSparsity in Conv2d 64: 98.44%\n\n\nFor more information about granularities at which you can operate, please check the related page."
  },
  {
    "objectID": "regularize.regularizer.html",
    "href": "regularize.regularizer.html",
    "title": "RegularizationCallback",
    "section": "",
    "text": "RegularizationCallback\n\n RegularizationCallback (granularity, wd=0.01)\n\nCallback to apply grouped weight decay\nThe RegularizationCallbackcan be used to perform \\(l_1\\) regularization on any granularity available in the criteria class."
  },
  {
    "objectID": "tutorial.pruner.html",
    "href": "tutorial.pruner.html",
    "title": "Pruner",
    "section": "",
    "text": "Important\n\n\n\nThe Pruner method currently works on fully-feedforward ConvNets, e.g. VGG16. Support for residual connections, e.g. ResNets is under development.\n\n\nWhen our network has filters containing zero values, there is an additional step that we may take. Indeed, those zero-filters can be physically removed from our network, allowing us to get a new, dense, architecture.\nThis can be done by reexpressing each layer, reducing the number of filter, to match the number of non-zero filters. However, when we remove a filter in a layer, this means that there will be a missing activation map, which should be used by all the filters in the next layer. So, not only should we physically remove the filter, but also its corresponding kernel in each of the filters in the next layer (see Fig. below)\n\nLet’s illustrate this with an example:\n\npath = untar_data(URLs.PETS)\n\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\n\nlearn = Learner(dls, vgg16_bn(num_classes=2), metrics=accuracy)\n\n\ncount_parameters(learn.model)\n\n134277186\n\n\nOur initial model, a VGG16, possess more than 134 million parameters. Let’s see what happens when we make it sparse, on a filter level\n\nsp_cb=SparsifyCallback(end_sparsity=50, granularity='filter', method='local', criteria=large_final, sched_func=sched_onecycle)\n\n\nlearn.fit_one_cycle(3, 3e-4, cbs=sp_cb)\n\nPruning of filter until a sparsity of 50%\nSaving Weights at epoch 0\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.897482\n      0.611214\n      0.698241\n      00:14\n    \n    \n      1\n      0.658607\n      0.561114\n      0.706360\n      00:13\n    \n    \n      2\n      0.555238\n      0.527486\n      0.718539\n      00:13\n    \n  \n\n\n\nSparsity at the end of epoch 0: 10.43%\nSparsity at the end of epoch 1: 48.29%\nSparsity at the end of epoch 2: 50.00%\nFinal Sparsity: 50.00\n\n\n\ncount_parameters(learn.model)\n\n134277186\n\n\nThe total amount of parameters hasn’t changed! This is because we only replaced the values by zeroes, leading to a sparse model, but they are still there.\nThe Pruner will take care of removing those useless filters.\n\npruner = Pruner()\npruned_model = pruner.prune_model(learn.model)\n\nDone! Let’s see if the performance is still the same\n\npruned_learn = Learner(dls, pruned_model.cuda(), metrics=accuracy)\n\n\npruned_learn.validate()\n\n\n\n\n(#2) [0.5265399813652039,0.7212449312210083]\n\n\n\ncount_parameters(pruned_learn.model)\n\n71858210\n\n\nNow we have 71 million of parameters, approximately 50% of the initial parameters as we asked!"
  },
  {
    "objectID": "tutorial.sparsify_callback.html",
    "href": "tutorial.sparsify_callback.html",
    "title": "SparsifyCallback",
    "section": "",
    "text": "path = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))\n\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\n\nlearn.fit_one_cycle(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.714170\n      0.534177\n      0.802436\n      00:08\n    \n    \n      1\n      0.405863\n      0.466950\n      0.861976\n      00:07\n    \n    \n      2\n      0.229647\n      0.234999\n      0.902571\n      00:07\n    \n    \n      3\n      0.141966\n      0.198904\n      0.924222\n      00:07\n    \n    \n      4\n      0.073327\n      0.191152\n      0.930988\n      00:07\n    \n  \n\n\n\nLet’s now try adding some sparsity in our model\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n/home/HubensN/miniconda3/envs/deep/lib/python3.8/site-packages/fastai/vision/learner.py:265: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n\n\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\n\nlearn = Learner(dls, resnet18(num_classes=2), metrics=accuracy)\n\n\nsp_cb = SparsifyCallback(sparsity=50, granularity='weight', context='local', criteria=large_final, schedule=cos)\n\n\nlearn.fit(10, cbs=sp_cb)\n\nPruning of weight until a sparsity of [50]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.597135\n      0.568558\n      0.694181\n      00:09\n    \n    \n      1\n      0.543739\n      0.527585\n      0.730717\n      00:07\n    \n    \n      2\n      0.508932\n      0.507831\n      0.748309\n      00:07\n    \n    \n      3\n      0.451922\n      0.454692\n      0.799053\n      00:07\n    \n    \n      4\n      0.427453\n      0.434664\n      0.801759\n      00:07\n    \n    \n      5\n      0.377218\n      0.402817\n      0.823410\n      00:07\n    \n    \n      6\n      0.340924\n      0.410856\n      0.820027\n      00:07\n    \n    \n      7\n      0.319503\n      0.363846\n      0.837618\n      00:07\n    \n    \n      8\n      0.271233\n      0.377996\n      0.853180\n      00:07\n    \n    \n      9\n      0.228336\n      0.334722\n      0.865359\n      00:07\n    \n  \n\n\n\nSparsity at the end of epoch 0: [1.22]%\nSparsity at the end of epoch 1: [4.77]%\nSparsity at the end of epoch 2: [10.31]%\nSparsity at the end of epoch 3: [17.27]%\nSparsity at the end of epoch 4: [25.0]%\nSparsity at the end of epoch 5: [32.73]%\nSparsity at the end of epoch 6: [39.69]%\nSparsity at the end of epoch 7: [45.23]%\nSparsity at the end of epoch 8: [48.78]%\nSparsity at the end of epoch 9: [50.0]%\nFinal Sparsity: [50.0]%\nSparsity in Conv2d 1: 50.00%\nSparsity in Conv2d 7: 50.00%\nSparsity in Conv2d 10: 50.00%\nSparsity in Conv2d 13: 50.00%\nSparsity in Conv2d 16: 50.00%\nSparsity in Conv2d 20: 50.00%\nSparsity in Conv2d 23: 50.00%\nSparsity in Conv2d 26: 50.00%\nSparsity in Conv2d 29: 50.00%\nSparsity in Conv2d 32: 50.00%\nSparsity in Conv2d 36: 50.00%\nSparsity in Conv2d 39: 50.00%\nSparsity in Conv2d 42: 50.00%\nSparsity in Conv2d 45: 50.00%\nSparsity in Conv2d 48: 50.00%\nSparsity in Conv2d 52: 50.00%\nSparsity in Conv2d 55: 50.00%\nSparsity in Conv2d 58: 50.00%\nSparsity in Conv2d 61: 50.00%\nSparsity in Conv2d 64: 50.00%\n\n\nSurprisingly, our network that is composed of \\(50 \\%\\) of zeroes performs reasonnably well when compared to our plain and dense network.\nThe SparsifyCallback also accepts a list of sparsities, corresponding to each layer of layer_type to be pruned. Below, we show how to prune only the intermediate layers of ResNet-18.\n\nlearn = cnn_learner(dls, resnet18, metrics=accuracy)\nlearn.unfreeze()\n\n\nsparsities = [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]\n\n\nsp_cb = SparsifyCallback(sparsity=sparsities, granularity='weight', context='local', criteria=large_final, schedule=cos)\n\n\nlearn.fit_one_cycle(5, cbs=sp_cb)\n\nPruning of weight until a sparsity of [0, 0, 0, 0, 0, 0, 50, 50, 50, 50, 50, 50, 50, 50, 0, 0, 0, 0, 0, 0]%\nSaving Weights at epoch 0\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.748184\n      0.876642\n      0.826116\n      00:08\n    \n    \n      1\n      0.422033\n      0.255813\n      0.889039\n      00:08\n    \n    \n      2\n      0.262884\n      0.234100\n      0.904601\n      00:08\n    \n    \n      3\n      0.132767\n      0.228366\n      0.921516\n      00:08\n    \n    \n      4\n      0.075110\n      0.210104\n      0.930311\n      00:08\n    \n  \n\n\n\nSparsity at the end of epoch 0: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 4.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 17.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 32.73, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 45.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity at the end of epoch 4: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nFinal Sparsity: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]%\nSparsity in Conv2d 2: 0.00%\nSparsity in Conv2d 8: 0.00%\nSparsity in Conv2d 11: 0.00%\nSparsity in Conv2d 14: 0.00%\nSparsity in Conv2d 17: 0.00%\nSparsity in Conv2d 21: 0.00%\nSparsity in Conv2d 24: 50.00%\nSparsity in Conv2d 27: 50.00%\nSparsity in Conv2d 30: 50.00%\nSparsity in Conv2d 33: 50.00%\nSparsity in Conv2d 37: 50.00%\nSparsity in Conv2d 40: 50.00%\nSparsity in Conv2d 43: 50.00%\nSparsity in Conv2d 46: 50.00%\nSparsity in Conv2d 49: 0.00%\nSparsity in Conv2d 53: 0.00%\nSparsity in Conv2d 56: 0.00%\nSparsity in Conv2d 59: 0.00%\nSparsity in Conv2d 62: 0.00%\nSparsity in Conv2d 65: 0.00%\n\n\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nstart_sparsity: the sparsity that the schedule will use as a starting point (default to 0)\nstart_epoch: the epoch at which the schedule will start pruning (default to 0)\nend_epoch: the epoch at which the schedule will stop pruning (default to the training epochs passed in fit)\nlth: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you don’t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`\n\nFor example, we correctly pruned the convolution layers of our model, but we could imagine pruning the Linear Layers of even only the BatchNorm ones !"
  },
  {
    "objectID": "misc.fc_decomposer.html",
    "href": "misc.fc_decomposer.html",
    "title": "Fully-Connected Layers Decomposer",
    "section": "",
    "text": "We can factorize our big fully-connected layers and replace them by an approximation of two smaller layers. The idea is to make an SVD decomposition of the weight matrix, which will express the original matrix in a product of 3 matrices: \\(U \\Sigma V^T\\) With \\(\\Sigma\\) being a diagonal matrix with non-negative values along its diagonal (the singular values). We then define a value \\(k\\) of singular values to keep and modify matrices \\(U\\) and \\(V^T\\) accordingly. The resulting will be an approximation of the initial matrix.\n\n\n\nFC_Decomposer.decompose\n\n FC_Decomposer.decompose (model, percent_removed=0.5)\n\nA tutorial about how to use the FC_Decomposer functionalities can be found here"
  },
  {
    "objectID": "tutorial.fc_decomposer.html#get-the-data",
    "href": "tutorial.fc_decomposer.html#get-the-data",
    "title": "Fully-Connected layers decomposition",
    "section": "1. Get the data",
    "text": "1. Get the data\n\npath = untar_data(URLs.PETS)\nfiles = get_image_files(path/\"images\")\n\ndef label_func(f): return f[0].isupper()\n\ndls = ImageDataLoaders.from_name_func(path, files, label_func, item_tfms=Resize(64))"
  },
  {
    "objectID": "tutorial.fc_decomposer.html#train-the-model",
    "href": "tutorial.fc_decomposer.html#train-the-model",
    "title": "Fully-Connected layers decomposition",
    "section": "2. Train the model",
    "text": "2. Train the model\n\nlearn = Learner(dls, vgg16_bn(num_classes=2), metrics=accuracy)\nlearn.fit_one_cycle(3)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.886644\n      0.652151\n      0.685386\n      00:22\n    \n    \n      1\n      0.692583\n      0.627857\n      0.685386\n      00:21\n    \n    \n      2\n      0.646516\n      0.622866\n      0.685386\n      00:22\n    \n  \n\n\n\n\nDecompose !\n\n\nfc = FC_Decomposer()\nnew_model = fc.decompose(learn.model)\n\nThe fc layers have been factorized and replace by smaller ones.\n\nnew_model\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): ReLU(inplace=True)\n    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): ReLU(inplace=True)\n    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (16): ReLU(inplace=True)\n    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (19): ReLU(inplace=True)\n    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (26): ReLU(inplace=True)\n    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (32): ReLU(inplace=True)\n    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (36): ReLU(inplace=True)\n    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (39): ReLU(inplace=True)\n    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (42): ReLU(inplace=True)\n    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=25088, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Sequential(\n      (0): Linear(in_features=4096, out_features=2048, bias=False)\n      (1): Linear(in_features=2048, out_features=4096, bias=True)\n    )\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Sequential(\n      (0): Linear(in_features=4096, out_features=1, bias=False)\n      (1): Linear(in_features=1, out_features=2, bias=True)\n    )\n  )\n)\n\n\nWe can see compare the amount of parameters before/after:\n\ncount_parameters(learn.model)\n\n134277186\n\n\n\ncount_parameters(new_model)\n\n91281476\n\n\nThis represents a decrease of ~40M parameters !\nNow this is an approximation, so it isn’t really lossless and we should expect to see a performance drop, which will be bigger as we keep fewer singular values. Here we have:\n\nnew_learn = Learner(dls, new_model, metrics=accuracy)\nnew_learn.validate()\n\n\n\n\n(#2) [0.6868855357170105,0.6853856444358826]"
  },
  {
    "objectID": "distill.knowledge_distillation.html",
    "href": "distill.knowledge_distillation.html",
    "title": "Knowledge Distillation",
    "section": "",
    "text": "Knowledge Distillation, sometimes called teacher-student training, is a compression method in which a small (the student) model is trained to mimic the behaviour of a larger (the teacher) model.\nThe main goal is to reveal what is called the Dark Knowledge hidden in the teacher model.\nIf we take the same example provided by Geoffrey Hinton et al., we have\nThe main problem of classification is that the output activation function (softmax) will, by design, make a single value really high and squash others.\n\\[\np_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j} \\exp \\left(z_{j}\\right)}\n\\]\nWith \\(p_i\\) the probability of class \\(i\\), computed from the logits \\(z\\)\nHere is an example to illustrate this phenomenon:\nLet’s say that we have trained a model to discriminate between the following 5 classes: [cow, dog, plane, cat, car]\nAnd here is the output of the final layer (the logits) when the model is fed a new input image:\n\nlogits = torch.tensor([1.3, 3.1, 0.2, 1.9, -0.3])\n\nBy judging on the predictions, the model seems confident that the input data is a dog and quite confident that it is definitely not a plane nor a car, with predictions for cow and cat being moderately high.\nSo the model not only has learned to recognize a dog in the image, but also that a dog is very different from a car and a plane and share similarities with cats and cows. This information is what is called dark knowledge !\nWhen passing those predictions through a softmax, we have:\n\npredictions = F.softmax(logits, dim=-1); predictions\n\ntensor([0.1063, 0.6431, 0.0354, 0.1937, 0.0215])\n\n\nThis is accuenting the differences that we had earlier, discarding some of the dark knowledge acquired earlier. The way to keep this knowledge is to “soften” our softmax outputs, by adding a temperature parameter. The higher the temperature, the softer the predictions.\n\nsoft_predictions = F.softmax(logits/3, dim=-1); soft_predictions\n\ntensor([0.1879, 0.3423, 0.1302, 0.2294, 0.1102])\n\n\n\n\n\n\n\n\nNote\n\n\n\nif the Temperature is equal to 1, then we have regular softmax\n\n\nWhen applying Knowledge Distillation, we want to keep the Dark Knowledge that the teacher model has acquired during its training but not rely entirely on it. So we combine two losses:\n\nThe Teacher loss between the softened predictions of the teacher and the softened predictions of the student\nThe Classification loss, which is the regular loss between hard labels and hard predictions\n\nThe combination between those losses are weighted by an additional parameter α, as:\n\\[\nL_{K D}=\\alpha  * \\text { CrossEntropy }\\left(p_{S}^{\\tau}, p_{T}^{\\tau}\\right)+(1-\\alpha) * \\text { CrossEntropy }\\left(p_{S}, y_{\\text {true }}\\right)\n\\]\nWith \\(p^{\\tau}\\) being the softened predictions of the student and teacher\n\n\n\n\n\n\nNote\n\n\n\nIn practice, the distillation loss will be a bit different in the implementation\n\n\n\nThis can be done with fastai, using the Callback system !\nThe loss function that is used may depend on the use case. For classification, we usually use the one presented above, named SoftTarget in fasterai. But for regression cases, we may want to perform regression on the logits directly."
  },
  {
    "objectID": "sparse.sparsify_callback.html",
    "href": "sparse.sparsify_callback.html",
    "title": "SparsifyCallback",
    "section": "",
    "text": "SparsifyCallback\n\n SparsifyCallback (sparsity, granularity, context, criteria, schedule,\n                   lth=False, rewind_epoch=0, reset_end=False,\n                   save_tickets=False, model=None, round_to=None,\n                   layer_type=<class 'torch.nn.modules.conv.Conv2d'>)\n\nSparsify model during training\nThe most important part of our Callback happens in before_batch. There, we first compute the sparsity of our network according to our schedule and then we remove the parameters accordingly.\nThe SparsifyCallback requires a new argument compared to the Sparsifier. Indeed, we need to know the pruning schedule that we should follow during training in order to prune the parameters accordingly.\nYou can use any scheduling function already available in fastai or come up with your own ! For more information about the pruning schedules, take a look at the Schedules section.\nOn top of that, the SparsifyCallbackcan also take many optionnal arguments:\n\nlth: whether training using the Lottery Ticket Hypothesis, i.e. reset the weights to their original value at each pruning step (more information in the Lottery Ticket Hypothesis section)\nrewind_epoch: the epoch used as a reference for the Lottery Ticket Hypothesis with Rewinding (default to 0)\nreset_end: whether you want to reset the weights to their original values after training (pruning masks are still applied)\nsave_tickets: whether to save intermediate winning tickets.\nmodel: pass a model or a part of the model if you don’t want to apply pruning on the whole model trained.\nround_to: if specified, the weights will be pruned to the closest multiple value of round_to.\nlayer_type: specify the type of layer that you want to apply pruning to (default to nn.Conv2d)`"
  },
  {
    "objectID": "core.schedules.html#one-shot",
    "href": "core.schedules.html#one-shot",
    "title": "Schedules",
    "section": "One-Shot",
    "text": "One-Shot\nThe easiest schedule is the one-shot pruning, i.e. prune the network once. This can be done by simply returning the desired sparsity value. The moment when you want to prune will be controlled by the start_epoch argument in the SparsifyCallback.\n\n\nsched_oneshot\n\n sched_oneshot (start, end, pos)\n\n\none_shot.plot(50)"
  },
  {
    "objectID": "core.schedules.html#iterative",
    "href": "core.schedules.html#iterative",
    "title": "Schedules",
    "section": "Iterative",
    "text": "Iterative\nInstead of pruning the network to desired sparsity in one step, you can do it iteratively. In fasterai, you can change the amount of iterations\n\n\nsched_iterative\n\n sched_iterative (start, end, pos, n_steps=3)\n\nPerform iterative pruning, and pruning in n_steps steps\n\niterative.plot(50)\n\n\n\n\n\n\nTo modify the default n_steps, you can use the partial function.\n\niterative = Schedule(partial(sched_iterative, n_steps=5), start_pct=0.2)\n\n\niterative.plot(50)"
  },
  {
    "objectID": "core.schedules.html#automated-gradual-pruning",
    "href": "core.schedules.html#automated-gradual-pruning",
    "title": "Schedules",
    "section": "Automated Gradual Pruning",
    "text": "Automated Gradual Pruning\nSome researchers have come up with more sophisticated schedules, such as the Automated Gradual Pruning.\n\n\nsched_agp\n\n sched_agp (start, end, pos)\n\n\nagp.plot(50)"
  },
  {
    "objectID": "core.schedules.html#one-cycle-pruning",
    "href": "core.schedules.html#one-cycle-pruning",
    "title": "Schedules",
    "section": "One-Cycle Pruning",
    "text": "One-Cycle Pruning\n\n\nsched_onecycle\n\n sched_onecycle (start, end, pos, α=14, β=6)\n\n\none_cycle.plot(50)\n\n\n\n\n\n\nOn top of that, all of the schedules available in fastai by default are also available: - sched_cos - sched_linear"
  },
  {
    "objectID": "core.schedules.html#dense-sparse-dense",
    "href": "core.schedules.html#dense-sparse-dense",
    "title": "Schedules",
    "section": "Dense-Sparse-Dense",
    "text": "Dense-Sparse-Dense\nYou can also create even more interesting behaviours such as the DSD method, where you prune the model in the first place, then re-grow it to its initial amount of parameter.\n\n\nsched_dsd\n\n sched_dsd (start, end, pos)\n\n\ndsd.plot(50)"
  }
]